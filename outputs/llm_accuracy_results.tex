\begin{table*}[t]
\centering
\footnotesize
\setlength{\tabcolsep}{6pt}
\begin{tabularx}{\linewidth}{l r r r r r r r r }
\toprule
\textbf{Model} & \multicolumn{6}{c}{\textbf{Test Set}} & \textbf{Avg. Validation} & \textbf{Avg. Test} \\
\cmidrule(lr){2-7}
 & atomic & electro & mechanics & optics & quantum & statistics & Avg. Validation & Avg. Test \\
\midrule
o3-mini-together-output & 52.4 & 64.9 & 59.8 & 51.5 & 66.0 & 60.0 & 55.0 & 59.9 \\
deepseek-r1-together-output & 37.0 & 48.6 & 38.3 & 43.1 & 44.5 & 51.5 & 44.2 & 44.3 \\
gemini-1.5-pro-output & 35.5 & 40.2 & 31.5 & 32.2 & 44.5 & 43.7 & 35.3 & 38.4 \\
gemini-1.5-pro-self-reflect & 35.3 & 41.4 & 34.2 & 33.9 & 37.3 & 41.0 & 38.5 & 37.5 \\
gpt-4o-output & 35.3 & 44.1 & 33.4 & 23.4 & 33.8 & 45.0 & 34.7 & 36.7 \\
claude-3-5-sonnet-20241022-output & 37.2 & 34.8 & 27.6 & 35.5 & 35.1 & 38.4 & 31.7 & 34.7 \\
gpt-4o-self-reflect & 36.5 & 31.2 & 34.6 & 26.0 & 37.3 & 39.5 & 33.2 & 34.6 \\
Qwen2.5-Math-72B-Instruct-output & 27.0 & 34.8 & 27.3 & 27.4 & 36.2 & 37.0 & 38.5 & 32.2 \\
Llama-3.3-70B-Instruct-RAG & 33.0 & 37.5 & 30.6 & 24.9 & 24.4 & 36.3 & 30.2 & 31.6 \\
Llama-3.3-70B-Instruct-AWQ-output & 28.2 & 35.8 & 27.9 & 17.2 & 31.4 & 41.3 & 34.3 & 31.5 \\
Qwen2.5-72B-Instruct-RAG & 35.9 & 28.9 & 22.8 & 22.8 & 32.6 & 32.5 & 31.9 & 30.5 \\
Llama-3.3-70B-Instruct-self-reflect & 36.0 & 31.2 & 20.7 & 20.1 & 31.7 & 33.6 & 35.6 & 30.3 \\
phi-4-outputs & 32.8 & 33.0 & 19.8 & 27.2 & 23.4 & 35.2 & 28.7 & 29.1 \\
Qwen2.5-72B-Instruct-outputs & 28.8 & 30.9 & 23.0 & 25.4 & 27.4 & 33.2 & 31.5 & 28.7 \\
Qwen2.5-Math-72B-self-reflect & 26.6 & 27.8 & 31.2 & - & - & - & 30.8 & 28.2 \\
Qwen2.5-32B-Instruct-output & 25.5 & 27.5 & 19.4 & 20.8 & 24.7 & 41.1 & 23.3 & 27.6 \\
Llama-3.3-70B-Instruct-outputs & 25.9 & 30.0 & 23.5 & 19.9 & 26.8 & 32.7 & 29.1 & 27.4 \\
Mistral-Small-24B-Instruct-2501-output & 19.1 & 29.5 & 19.6 & 17.6 & 15.2 & 28.4 & 25.1 & 21.8 \\
Qwen2.5-7B-Instruct-output & 21.8 & 28.1 & 11.2 & 18.7 & 17.4 & 22.1 & 20.9 & 20.4 \\
Qwen2.5-14B-Instruct-output & 23.8 & 19.7 & 14.1 & 12.3 & 13.5 & 28.2 & 25.3 & 19.6 \\
gemma-2-27b-it-output & 14.3 & 19.0 & 16.2 & 13.4 & 18.4 & 25.9 & 21.7 & 18.3 \\
Yi-1.5-34B-Chat-output & 11.0 & 15.4 & 18.0 & 13.2 & 19.6 & 25.2 & 25.3 & 17.4 \\
Qwen2.5-Math-1.5B-Instruct-output & 13.3 & 14.8 & 16.5 & 16.2 & 17.2 & 19.5 & 15.1 & 16.4 \\
InternVL2-5-38B-AWQ-outputs & 15.3 & 12.5 & 12.5 & 7.7 & 18.0 & 23.1 & 16.7 & 15.3 \\
Aria-outputs & 13.0 & 14.0 & 14.2 & 11.7 & 9.7 & 14.4 & 12.7 & 12.9 \\
QwQ-32B-Preview-outputs & 16.7 & 7.5 & 10.1 & 11.2 & 10.6 & 14.8 & 12.4 & 12.1 \\
gemma-2-9b-it-output & 9.4 & 8.2 & 9.1 & 16.5 & 12.1 & 16.9 & 15.2 & 11.9 \\
Mistral-7B-Instruct-v0.3-output & 10.1 & 10.4 & 5.1 & 13.7 & 11.6 & 17.6 & 12.6 & 11.7 \\
Llama-3.1-8B-Instruct-output & 8.4 & 17.4 & 6.8 & 14.7 & 7.4 & 16.1 & 9.1 & 11.7 \\
Mathstral-7B-v0.1-output & 7.3 & 10.0 & 12.0 & 9.6 & 8.2 & 17.6 & 12.0 & 10.8 \\
CohereForAI-c4ai-command-r-v01 & 2.0 & 7.8 & 7.5 & 3.8 & 7.5 & 11.4 & 6.8 & 7.0 \\
DeepSeek-R1-Distill-Qwen-32B-outputs & 9.1 & 5.4 & 4.8 & 9.8 & 2.3 & 10.2 & 7.1 & 6.8 \\
c4ai-command-r-08-2024-outputs & 5.8 & 4.6 & 1.0 & 5.9 & 8.0 & 9.1 & 5.7 & 6.1 \\
gemma-2-2b-it-output & 6.6 & 6.2 & 3.9 & 10.3 & 3.9 & 7.3 & 6.1 & 6.1 \\
Qwen2-VL-72B-Instruct-AWQ-outputs & 11.8 & 3.5 & 4.6 & 4.0 & 2.9 & 4.2 & 4.5 & 5.0 \\
internlm3-8b-instruct-awq-output & 1.8 & 4.6 & 4.7 & 3.2 & 4.0 & 9.2 & 4.1 & 4.8 \\
Pixtral-12B-2409-outputs & 13.5 & 0.0 & 0.0 & 10.9 & 0.0 & 1.4 & 6.0 & 3.7 \\
deepseek-vl2-small-outputs & 3.1 & 1.8 & 1.8 & 4.5 & 0.0 & 0.3 & 4.8 & 1.7 \\
THUDM-chatglm3-6b & 0.9 & 2.3 & 0.0 & 0.7 & 0.9 & 2.0 & 0.9 & 1.2 \\
Qwen2.5-Math-7B-output & 1.4 & 1.7 & 0.0 & 2.1 & 0.0 & 1.5 & 1.9 & 1.0 \\
deepseek-math-7b-rl-output & 0.7 & 0.0 & 0.0 & 1.5 & 0.0 & 0.6 & 0.9 & 0.4 \\
o1-output & - & - & - & - & - & - & 0.0 & 0.0 \\
gemini-2.0-flash-thinking-exp-01-21-output & - & - & - & - & - & - & 0.0 & 0.0 \\
glm-4-9b-chat-hf-output & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
datasets & - & - & - & - & - & - & 0.0 & 0.0 \\
gemini-2.0-flash-exp-output & - & - & - & - & - & - & 0.0 & 0.0 \\
text-only-dataset & - & - & - & - & - & - & 0.0 & 0.0 \\
gemini-1.5-flash-output & - & - & - & - & - & - & 0.0 & 0.0 \\
Velvet-14B-outputs & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
\bottomrule
\end{tabularx}
\caption{Performance comparison across tasks.}
\label{tab:llm-accuracy}
\end{table*}
