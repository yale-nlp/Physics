{"id": "quantum/3-3013", "questions": "A quantum-mechanical state of a particle, with Cartesian coordinates $x, y,$ and $z$, is described by the normalized wave function\n\n$$\n\\psi(x, y, z) = \\frac{\\alpha^{5/2}}{\\sqrt{\\pi}} \\, z \\exp[-\\alpha(x^2 + y^2 + z^2)^{1/2}].\n$$\n\nShow that the system is in a state of definite angular momentum and give the values of $L^2$ and $L_z$ associated with the state.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem where I need to show that a given quantum mechanical state has definite angular momentum, specifically finding L² and L_z for the state ψ(x,y,z). The wavefunction provided is:\n\nψ(x, y, z) = (α^(5/2)/√π) * z exp[−α r], \n\nwhere r is sqrt(x² + y² + z²).\n\nHmm, first off, since we're dealing with angular momentum operators, it's probably useful to switch from Cartesian coordinates to spherical coordinates because angular momentum is more naturally expressed there.\n\nIn spherical coordinates, x = r sinθ cosφ, y = r sinθ sinφ, z = r cosθ. So maybe expressing ψ(r, θ, φ) would help?\n\nLet me rewrite ψ using these substitutions. Since z = r cosθ, then ψ becomes proportional to r cosθ times e^{-α r}. Also, looking at the normalization factor: α^(5/2)/√π. That seems like something related to radial functions in hydrogen-like orbitals or similar systems.\n\nWait, actually, if I recall correctly, states with well-defined angular momentum usually involve spherical harmonics Y_l^m(θ, φ), which depend on angles only. But here, our wavefunction also depends on r through the exponential term. However, when considering angular momentum operators, they act only on the angular parts of the wavefunction.\n\nSo perhaps even though ψ isn't purely angular, its dependence on angles might still allow us to find eigenvalues for L² and L_z.\n\nThe general form of such a wavefunction could be written as R(r)Y_l^m(θ, φ), right? Where R(r) is the radial part and Y_l^m is the spherical harmonic corresponding to specific l and m values.\n\nLooking back at ψ, let's see: It's equal to [α^(5/2)/√π] * z * e^{-α r}.\n\nExpressed in terms of spherical coordinates, that's [α^(5/2)/√π] * r cosθ * e^{-α r}.\n\nWhich can be rewritten as [α^(5/2)/√π] e^{-α r} multiplied by r cosθ.\n\nNow, comparing this structure to known solutions, say, those of the Hydrogen atom, but without any 1/r factors beyond what comes from the spherical harmonics themselves.\n\nBut wait, in standard cases, the radial part includes powers of r multiplied by exponentials. For example, the ground state of Hydrogen is ~r e^{-ar}, while higher states may include different polynomials.\n\nHere, our radial part is just r e^{-α r}; no polynomial other than linear in r. Hmm, interesting.\n\nMoreover, the angular part is simply cosθ, which suggests some relation to the spherical harmonics. Let me think about the spherical harmonics expressions.\n\nSpherical harmonics Y_l^m(θ, φ) are typically defined involving Legendre polynomials P_l^m(cosθ). Specifically, for certain m, you get particular forms.\n\nFor instance, Y_0^0 is constant, Y_1^-1, Y_1^0, Y_1^+1 correspond to various combinations of sine and cosine terms.\n\nSpecifically, Y_1^0(θ, φ) is proportional to cosθ. Indeed, checking my memory, yes, Y₁⁰ ∝ cosθ / sqrt(sinθ)... Wait, not exactly; let me write down the exact expression.\n\nActually, Y_l^m(θ, φ) = (-1)^m [ (2l+1)(l - |m|)! )/(4π(l + |m|)!)) ]^(1/2) e^{i m φ} P_l^{|m|}(cosθ)\n\nWhere P_l^m are the associated Legendre polynomials.\n\nFor l=1, m=0, Y₁⁰ should be proportional to cosθ. Because P₁⁰(x)=x, so indeed, Y₁⁰(θ, φ) = sqrt(3/(4π)) cosθ.\n\nSimilarly, Y₁¹ involves sinθ e^{±iφ}, etc.\n\nTherefore, in our case, the angular part is cosθ, up to constants. Therefore, the angular part must be proportional to Y₁⁰(θ, φ). Hence, the entire wavefunction can be considered as having l=1 and m=0.\n\nThus, the angular momentum squared operator L² acting on Y₁⁰ gives ħ² l(l+1) Y₁⁰, i.e., ħ²*1*(1+1)*Y₁⁰ = 2ħ² Y₁⁰.\n\nAnd similarly, the L_z operator acts on Y₁⁰ giving mħ Y₁⁰, which is zero since m=0.\n\nHence, both L² and L_z will yield their respective eigenvalues when applied to ψ.\n\nTo confirm, let's compute <L²> and <L_z>.\n\nAlternatively, since ψ is separable into radial and angular parts, and the angular part is precisely Y₁⁰, regardless of the radial component, applying L² and L_z will result in multiplying by the eigenvalues.\n\nBecause the action of L² and L_z doesn’t affect the radial part—it commutes with them—so each operation leaves the radial part intact and scales the angular part accordingly.\n\nTherefore, ψ being proportional to r Y₁⁰ e^{-α r} implies that it's an eigenstate of both L² and L_z with eigenvalues l(l+1)ħ² and mħ respectively.\n\nGiven that l=1 and m=0, substituting these values:\n\nEigenvalue of L²: 1*(1+1)ħ² = 2ħ²,\n\nEigenvalue of L_z: 0*ħ = 0.\n\nAdditionally, verifying whether the wavefunction satisfies the Schrödinger equation for a central potential would require knowing the Hamiltonian, but since the question only asks about angular momentum, focusing on L² and L_z suffices.\n\nAnother point to consider: Is the wavefunction properly normalized? Well, the coefficient is given as α^(5/2)/√π, which likely ensures proper normalization over all space, integrating out the volume element dr dΩ appropriately.\n\nAlso, note that the presence of 'z' corresponds directly to the polar angle direction, hence aligning with the quantization axis along z-axis, making sense why m=0—the maximum symmetry around the z-axis.\n\nPutting it all together, the conclusion is that the state has definite angular momentum with L² = 2ħ² and L_z = 0.\n\n\n**Final Answer**\nThe system is in a state of definite angular momentum with \\( L^2 = 2\\hbar^2 \\) and \\( L_z = 0 \\). Thus, the answers are:\n\\( L^2 = \\boxed{2\\hbar^2} \\)\nand \n\\( L_z = \\boxed{0} \\).\n</think>\n\nTo determine the angular momentum properties of the given wavefunction, we start by analyzing its structure. The wavefunction is given as:\n\n\\[ \\psi(x, y, z) = \\frac{\\alpha^{5/2}}{\\sqrt{\\pi}} \\, z \\exp[-\\alpha(x^2 + y^2 + z^2)^{1/2}]. \\]\n\nWe convert this into spherical coordinates where \\( z = r \\cos\\theta \\). This transforms the wavefunction into:\n\n\\[ \\psi(r, \\theta, \\phi) = \\frac{\\alpha^{5/2}}{\\sqrt{\\pi}} \\, r \\cos\\theta \\exp(-\\alpha r). \\]\n\nThis shows that the wavefunction separates into a radial part \\( R(r) = r \\exp(-\\alpha r) \\) and an angular part proportional to \\( \\cos\\theta \\). Recognizing that \\( \\cos\\theta \\) is proportional to the spherical harmonic \\( Y_1^0(\\theta, \\phi) \\), we identify the angular momentum quantum numbers \\( l = 1 \\) and \\( m = 0 \\).\n\nThe eigenvalues of the angular momentum operators \\( L^2 \\) and \\( L_z \\) are determined by these quantum numbers. Specifically, the eigenvalue of \\( L^2 \\) is given by \\( \\hbar^2 l(l+1) \\) and the eigenvalue of \\( L_z \\) is given by \\( \\hbar m \\).\n\nSubstituting \\( l = 1 \\) and \\( m = 0 \\):\n\n- The eigenvalue of \\( L^2 \\) is \\( \\hbar^2 \\cdot 1 \\cdot (1 + 1) = 2\\hbar^2 \\).\n- The eigenvalue of \\( L_z \\) is \\( \\hbar \\cdot 0 = 0 \\).\n\nThus, the system is in a state of definite angular momentum with the following values:\n\n### Final Answer\nThe system is in a"}
{"id": "quantum/5068", "questions": "Estimate the magnetic susceptibility of a He atom in its ground state. Is it paramagnetic or diamagnetic?", "graphs": null, "llm_answers": "<think>\nOkay, so I need to estimate the magnetic susceptibility of a helium atom in its ground state and determine whether it's paramagnetic or diamagnetic. Hmm, let me think about this.\n\nFirst off, what do I remember about magnetic properties of atoms? Well, paramagnetism occurs when unpaired electrons are present because their spins contribute to a net magnetization. Diamagnetism happens when all electrons are paired, leading to no net spin contribution but still some orbital effects that can cause weak diamagnetic behavior.\n\nHelium is a noble gas with atomic number 2. In its ground state, each helium atom has two electrons. The electron configuration for helium should be 1s², right? So both electrons fill up the first shell completely. Since they're in the same orbital (both in 1s), according to Hund's rule, they must have opposite spins due to the Pauli exclusion principle. That means one electron has spin-up and the other spin-down, resulting in zero total spin angular momentum.\n\nWait, if there’s no net spin, does that mean there's no paramagnetism? Because paramagnetism relies on having unpaired spins contributing to a magnetic moment. But then again, even without unpaired spins, maybe there's something else going on here?\n\nI recall that every moving charge creates a magnetic field, which relates to orbital motion. Even though the spins might cancel out, the orbitals could create a small diamagnetic effect. Let me dig deeper into that.\n\nDiamagnetic materials arise from the induced currents opposing external magnetic fields. These occur in all substances, but they’re usually very weak unless you have strong applied fields. For individual atoms like helium, any diamagnetic response would come from the electrons' orbital motions creating tiny loops of current that oppose the external field.\n\nBut wait, since helium is fully ionized as He⁺⁺, meaning it doesn't have any valence electrons left... No, actually, neutral helium has two electrons. Oh, sorry, got confused there. Neutral helium has two electrons; hence, it's not ionized. So those two electrons are in the 1s orbital.\n\nIn the case of hydrogen, which also has only one electron, we know it exhibits paramagnetism because of the single proton causing a slight shift in energy levels under a magnetic field—Zeeman effect—but more importantly, the lone electron contributes to a magnetic dipole. However, helium being a closed-shell system shouldn’t exhibit such paramagnetism because the spins are paired.\n\nSo back to helium: If both electrons are paired, their spins negate each other, giving a total spin S = 0. Therefore, there's no net magnetic moment arising from spin. What about orbital contributions? Each electron moves around the nucleus, creating a sort of loop current, which gives rise to a magnetic moment. \n\nThe question now becomes, how significant is this orbital contribution compared to possible spin contributions? Wait, but in helium, the spin moments already canceled out. So perhaps the dominant factor is the orbital diamagnetism.\n\nTo calculate the magnetic susceptibility χ, I probably need to use some formula related to either the Landé g-factor or consider the Bohr model for orbital magnetism. Alternatively, using quantum mechanical expressions for the expectation value of the magnetic moment operator.\n\nLet me try recalling the expression for the magnetic susceptibility. Susceptibility measures how much a material gets magnetized in an external field. It's given by:\n\nχ = μ₀ * (dM/dH)\n\nWhere M is the magnetization and H is the magnetic field strength. But calculating this directly seems complicated. Maybe another approach is better.\n\nFor paramagnetic systems, the susceptibility is positive and proportional to temperature, following Curie's law: χ ≈ C/(T). On the other hand, diamagnetic susceptibility is negative and independent of temperature, often described by the Langevin function or similar models.\n\nSince helium is likely diamagnetic, I expect a small negative susceptibility. To find its magnitude, perhaps I can compute the diamagnetic term based on the orbital motion of the electrons.\n\nEach electron in the 1s orbital will experience a centripetal force provided by the Coulomb attraction between the nucleus and the electron. Using classical mechanics, the radius r of the electron's orbit can be found via equating Coulomb force to centrifugal force:\n\n(ke²)/r² = mv²/r\n\nSimplifying,\n\nmv² = ke² / r → v = sqrt(ke²/(mr))\n\nThis velocity leads to a circular current, which produces a magnetic moment. The current I is e*v/(2πr) [since circumference is 2πr], so substituting v:\n\nI = e*(sqrt(ke²/(m r)))/(2 π r) )= e*sqrt(k e² m)^{-½} } ... Hmm, getting messy. Perhaps instead, considering the Bohr model where n=1 for 1s orbital.\n\nFrom the Bohr model, the radius r₁ is approximately 5.3e-11 meters (Bohr radius). The speed v₁ of the electron is known too—it's roughly αc, where α is the fine structure constant (~1/137) times c (speed of light ~3e8 m/s). So v₁≈ (1/137)*3e8 ≈ 2.2e6 m/s.\n\nNow, the magnetic moment μ due to orbital motion is given by μ = I*A, where A is the area of the orbit. Substituting I = ev/(2πr):\n\nμ = (ev/(2πr))*(π r²) )= (e v r)/2\n\nPlugging numbers:\ne = 1.6e-19 C,\nv ≈ 2.2e6 m/s,\nr ≈ 5.3e-11 m.\n\nCalculates to:\n\nμ ≈ (1.6e-19)(2.2e6)(5.3e-11)/2 ≈ ?\n\nCompute numerator: 1.6e-19 * 2.2e6 = approx 3.52e-13.\nMultiply by 5.3e-11: 3.52e-13 *5.3e-11 ≈ 1.87e-23.\nDivide by 2: ~9.35e-24 J/T.\n\nThat's per electron. Helium has two electrons, so double it: ~1.87e-23 J/T.\n\nWait, but hold on—that calculation assumes each electron orbits independently, but in reality, the presence of multiple electrons affects things through screening and exchange interactions. Also, in helium, the electrons are identical fermions, so their wavefunction must be antisymmetric, affecting their spatial distribution.\n\nMoreover, in the actual quantum treatment, the orbital angular momentum contributes to the magnetic moment. For s-orbitals, l=0, so the orbital angular momentum L=0. Hence, the orbital part of the magnetic moment vanishes! Only the spin part remains, but in helium, the spins are paired, so the total spin is zero.\n\nTherefore, the orbital contribution isn't just negligible; it's exactly zero because s-electrons don't have orbital angular momentum. Thus, the only source of magnetism comes from spin, but since spins are paired, the net spin magnetic moment cancels out.\n\nHmm, so if both orbital and spin contributions are zero, why is anything happening? Or am I missing something?\n\nAh, wait—the Lamb shift and hyperfine splitting suggest that there are subtle quantum electrodynamic effects, but these are extremely small. Moreover, in solid-state contexts, collective phenomena matter, but for isolated atoms...\n\nAlternatively, perhaps the correct way is to note that for singlet states (total spin S=0), the overall magnetic moment is zero. And triplet states (if excited) would have non-zero moments, but in the ground state, helium is a singlet.\n\nThus, the ground state of helium is indeed diamagnetic because all electrons are paired, and there's no net spin magnetic moment. Any remaining diamagnetism arises purely from the orbital motion, but since s-orbital has l=0, the orbital contribution is zero. Then, how can helium be diamagnetic?\n\nWait, maybe my initial assumption was wrong. Actually, diamagnetism typically refers to the response of bound electrons to an external magnetic field, regardless of pairing. All materials exhibit some degree of diamagnetism, but in most cases, it's overwhelmed by paramagnetic effects if unpaired spins exist.\n\nHowever, in helium, since all spins are paired, the primary paramagnetic contribution is absent, leaving only the weaker diamagnetic response. But earlier reasoning suggested that orbital angular momentum is zero for s-electrons, implying no orbital-induced diamagnetism either. This leaves us perplexed—is helium truly diamagnetic despite having no orbital or spin-based magnetic moments?\n\nPerhaps I'm conflating concepts. Another angle: the magnetic susceptibility of a substance depends on the ability of its constituent particles to respond to a magnetic field. For diamagnets, the response is always opposite to the direction of the field, caused by changes in the orbital motion of electrons. But in s-states, since l=0, the orbital angular momentum is zero, so the orbital contribution to diamagnetism is zero. Does that mean that helium wouldn't exhibit any diamagnetism?\n\nWait, but that contradicts common knowledge—I thought helium is diamagnetic. How does that work?\n\nMaybe I made a mistake in assuming that orbital angular momentum is necessary for diamagnetism. Let me check"}
{"id": "quantum/1-1041", "questions": "A particle of mass $\\text m$ moves in one dimension under the influence of a potential $V(x)$. Suppose it is in an energy eigenstate $\\psi(x) = (\\gamma^2/\\pi)^{1/4} \\exp(-\\gamma^2 x^2 / 2)$ with energy $E = \\hbar^2\\gamma^2 / 2m$.\n\n(a) Find the mean position of the particle.\n(b) Find the mean momentum of the particle.\n(c) Find $V(x)$.\n(d) Find the probability $P(p) \\, dp$ that the particle's momentum is between $p$ and $p + dp$.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about quantum mechanics where a particle is moving in one dimension under some potential V(x). The wave function given is ψ(x) which looks like (γ²/π)^(1/4) times e raised to (-γ²x²)/2. And its energy E is ħ²γ²/(2m).\n\nPart (a): Mean Position\n\nHmm, okay, for part (a), they're asking for the mean position of the particle. In quantum mechanics, the expectation value or mean of an observable is calculated using the integral of the conjugate of the wavefunction multiplied by the operator corresponding to the observable, then multiplied by the wavefunction again. For position, the operator is just x, right?\n\nSo mathematically, ⟨x⟩ should be equal to ∫ψ*(x) * x * ψ(x) dx from negative infinity to positive infinity. Since our wavefunction here is real, actually, because there’s no imaginary component—it's all exponential—so ψ* would just be ψ itself. So we can write:\n\n⟨x⟩ = ∫_{-∞}^{+∞} [ (γ²/π)^(1/4) exp( - γ² x² / 2 ) ] * x * [ same thing ] dx\n\nMultiplying those together gives us (γ²/π)^(1/2) exp( - γ² x² ). Then multiply by x, so overall integrand becomes x * (γ²/π)^(1/2) exp( - γ² x² ).\n\nWait, but when you integrate an odd function over symmetric limits... Hmm, since x is an odd function and the exponential term is even, their product will be odd. Integrating an odd function from -infinity to infinity should give zero, right? Because whatever area on the left cancels out the area on the right.\n\nTherefore, without doing any complicated calculations, I think the mean position must be zero. That makes sense too because the wavefunction seems symmetric around x=0; if you look at ψ(x), replacing x with -x doesn't change anything. So yeah, symmetry suggests that the average position is zero.\n\nBut let me double-check: If someone didn’t notice the symmetry, maybe they’d try integrating. Let's see what happens if we proceed formally.\n\nThe integral becomes (γ²/π)^(1/2) ∫ x exp(- γ² x² ) dx from -infty to infty. But as mentioned before, the integrand is odd, hence the result is zero. Yep, solid conclusion.\n\n**Final Answer**\nThe mean position is \\(\\boxed{0}\\).\n\n---\n\nMoving on to part (b): Mean Momentum\n\nAlright, now finding the mean momentum. In QM, the momentum operator is represented by -iħ d/dx acting on the wavefunction. To find ⟨p⟩, we need to compute the integral of ψ*(x) times p̂ times ψ(x) dx.\n\nAgain, since ψ is real, ψ* is ψ, so:\n\n⟨p⟩ = ∫_{-∞}^{+∞} ψ(x) * (-iħ d/dx ψ(x)) dx\n\nLet me plug in ψ(x):\n\nFirst, calculate dψ/dx. Given ψ(x) = (γ²/π)^(1/4) exp(- γ² x² / 2 )\n\nTaking derivative w.r.t. x:\ndψ/dx = (γ²/π)^(1/4) * (- γ² x) exp(- γ² x² / 2 )\n\nMultiply by -iħ:\n-p̂ψ = iħ (γ²/π)^(1/4) γ² x exp(- γ² x² / 2 )\n\nNow, putting into the integral:\n\n⟨p⟩ = ∫_{-∞}^{+∞} (γ²/π)^(1/4) exp(- γ² x² / 2 ) * [iħ (γ²/π)^(1/4) γ² x exp(- γ² x² / 2 )] dx\n\nSimplify constants first: (γ²/π)^(1/4) squared is (γ²/π)^(1/2)\n\nThen inside the exponentials: multiplying two exponents adds them, so each has - γ² x² / 2, adding up to - γ² x².\n\nThus,\n\n⟨p⟩ = iħ (γ²/π)^(1/2) γ² ∫_{-∞}^{+∞} x exp(- γ² x² ) dx\n\nLooking at the integral: similar situation as part (a)—the integrand is x times an even function, making it odd. Therefore, the integral over symmetric limits is zero.\n\nHence, ⟨p⟩ = 0.\n\nAlternatively, thinking physically—if the particle is in a stationary state (which it is, being an energy eigenstate), the expectation values of position and momentum shouldn't vary with time. Also, due to the spatial symmetry, the average momentum should also be zero because particles aren't preferentially moving in either direction.\n\nAnother way could involve Fourier transforms, considering that the wavefunction is Gaussian, implying minimal uncertainty, but perhaps more involved than needed here.\n\nEither way, both reasoning via integration and physical intuition lead to ⟨p⟩ = 0.\n\n**Final Answer**\nThe mean momentum is \\(\\boxed{0}\\).\n\n---\n\nPart (c): Finding Potential \\( V(x) \\)\n\nThis requires knowing the form of the Schrödinger equation. We know that Hψ = Eψ, where H is the Hamiltonian, which includes kinetic and potential energies.\n\nIn terms of operators, H = K.E. + P.E., so:\n\n[ -(ħ²)/(2m) d²/dx² + V(x) ] ψ(x) = E ψ(x)\n\nWe already have ψ(x) and E, so plugging these into the equation allows solving for V(x).\n\nGiven ψ(x) = (γ²/π)^(1/4) exp(- γ² x² / 2 ), let's compute the second derivative.\n\nCompute first derivative:\n\nψ'(x) = (γ²/π)^(1/4) * (- γ² x) exp(- γ² x² / 2 )\n\nSecond derivative:\n\nψ''(x) = (γ²/π)^(1/4) [ - γ² exp(- γ² x² / 2 ) + (γ⁴ x²) exp(- γ² x² / 2 ) ]\n\nFactor out common terms:\n\nψ''(x) = (γ²/π)^(1/4) exp(- γ² x² / 2 ) [ - γ² + γ⁴ x² ]\n\nPlug back into the Schrodinger equation:\n\n-(ħ²)/(2m) ψ''(x) + V(x) ψ(x) = E ψ(x)\n\nSubstitute ψ''(x):\n\nLeft-hand side becomes:\n\n-(ħ²)/(2m) * (γ²/π)^(1/4) exp(...) [ - γ² + γ⁴ x² ] + V(x) (γ²/π)^(1/4) exp(...)\n\nSet equal to E ψ(x):\n\nWhich is E (γ²/π)^(1/4) exp(...)\n\nDivide through by (γ²/π)^(1/4) exp(...), assuming non-zero everywhere:\n\n-(ħ²)/(2m) [ - γ² + γ⁴ x² ] + V(x) = E\n\nExpand the brackets:\n\n(ħ² γ²)/(2m) - (ħ² γ⁴ x²)/(2m) + V(x) = E\n\nRecall that E was given as ħ² γ² / (2m). Substitute that in:\n\n(ħ² γ²)/(2m) - (ħ² γ⁴ x²)/(2m) + V(x) = (ħ² γ²)/(2m)\n\nSubtracting (ħ² γ²)/(2m) from both sides:\n\n- (ħ² γ⁴ x²)/(2m) + V(x) = 0\n\nTherefore, solve for V(x):\n\nV(x) = (ħ² γ⁴ x²)/(2m)\n\nThat simplifies further. Notice that gamma appears twice, so factor:\n\nV(x) = (½ m ω² x²) ? Wait, not exactly unless we relate gamma to omega.\n\nActually, wait, another approach: Remember that for harmonic oscillators, the potential is quadratic, proportional to x². Here, indeed, we get such a form.\n\nExpressed differently, note that gamma relates to frequency. From classical oscillator, k/m = ω², and sometimes written as gamma squared equals mω/(something?), but regardless, in this case, V(x) comes out directly as above.\n\n"}
{"id": "quantum/3-3025", "questions": "The Hamiltonian for a spin-$\\frac{1}{2}$ particle with charge $+e$ in an external magnetic field is\n\n$$\nH = -\\frac{ge}{2mc} \\mathbf{s} \\cdot \\mathbf{B}.\n$$\n\nCalculate the operator $d\\mathbf{s}/dt$ if $\\mathbf{B} = B\\hat{y}$. What is $s_z(t)$ in matrix form?", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about finding the time derivative of the spin vector \\( d\\mathbf{s}/dt \\) for a spin-\\(\\frac{1}{2}\\) particle in a magnetic field pointing along the y-axis. Then, I need to find the matrix representation of \\( s_z(t) \\). Hmm, let me think through how to approach this.\n\nFirst off, I remember that in quantum mechanics, the time evolution of an observable (like spin components here) can be found using Heisenberg's equation of motion. The general formula is:\n\n\\[\n\\frac{dA}{dt} = \\frac{i}{\\hbar}[H, A] + \\frac{\\partial A}{\\partial t}\n\\]\n\nSince we're dealing with spin operators which don't explicitly depend on time, the second term involving partial derivatives should vanish. So, it simplifies to just the commutator part.\n\nGiven the Hamiltonian:\n\n\\[\nH = -\\frac{g e}{2 m c} \\mathbf{s} \\cdot \\mathbf{B}\n\\]\n\nAnd since the magnetic field is along the y-direction (\\( \\mathbf{B} = B \\hat{y} \\)), this becomes:\n\n\\[\nH = -\\frac{g e B}{2 m c} s_y\n\\]\n\nSo now, H is proportional to \\( s_y \\).\n\nI need to compute \\( \\frac{ds_x}{dt}, \\frac{ds_y}{dt}, \\frac{ds_z}{dt} \\), each one being the time derivative of their respective spin components.\n\nLet me start with \\( ds_x/dt \\):\n\nUsing Heisenberg's equation,\n\n\\[\n\\frac{ds_x}{dt} = \\frac{i}{\\hbar} [H, s_x]\n\\]\n\nSubstituting H into the commutator,\n\n\\[\n[H, s_x] = \\left[ -\\frac{g e B}{2 m c} s_y , s_x \\right ] \n= -\\frac{g e B}{2 m c} [s_y, s_x ]\n\\]\n\nNow, recalling the commutation relations between spin angular momentum components: \n\nFor spin-½ particles, the commutators satisfy:\n\n\\[\n[s_i, s_j] = i \\hbar \\epsilon_{ijk} s_k\n\\]\n\nWhere \\( \\epsilon_{ijk} \\) is the Levi-Civita symbol. Specifically, \\( [s_y, s_x] = -i \\hbar s_z \\)\n\nTherefore,\n\n\\[\n[H, s_x] = -\\frac{g e B}{2 m c} (-i \\hbar s_z ) = \\frac{g e B}{2 m c} i \\hbar s_z\n\\]\n\nPlugging back into the expression for \\( ds_x/dt \\),\n\n\\[\n\\frac{ds_x}{dt} = \\frac{i}{\\hbar} \\times \\frac{g e B}{2 m c} i \\hbar s_z\n\\]\n\nSimplify terms:\n\n\\( i \\times i = -1 \\),\nand \\( \\hbar / \\hbar = 1 \\),\n\nThus,\n\n\\[\n\\frac{ds_x}{dt} = \\frac{- g e B }{2 m c} s_z\n\\]\n\nSimilarly, moving on to \\( ds_y/dt \\):\n\nAgain, applying Heisenberg's equation,\n\n\\[\n\\frac{ds_y}{dt} = \\frac{i}{\\hbar} [H, s_y]\n\\]\n\nCompute the commutator:\n\n\\[\n[H, s_y] = \\left[ -\\frac{g e B}{2 m c} s_y , s_y \\right ] = 0\n\\]\n\nBecause any operator commutes with itself. Therefore,\n\n\\[\n\\frac{ds_y}{dt} = 0\n\\]\n\nThat makes sense because the component of spin parallel to the magnetic field shouldn't change over time—it’s like precession around the axis where the field points.\n\nLastly, computing \\( ds_z/dt \\):\n\nHeisenberg's equation again gives us:\n\n\\[\n\\frac{ds_z}{dt} = \\frac{i}{\\hbar} [H, s_z]\n\\]\n\nCalculating the commutator,\n\n\\[\n[H, s_z] = \\left[ -\\frac{g e B}{2 m c} s_y , s_z \\right ] \n= -\\frac{g e B}{2 m c} [s_y, s_z ]\n\\]\n\nFrom the commutation relation,\n\n\\[ [s_y, s_z] = i \\hbar s_x \\]\n\nWait, actually hold on—let me double-check the order. Since \\( [s_y, s_z] = i \\hbar s_x \\)? Or maybe negative?\n\nNo, wait—the standard ordering is \\( [s_i, s_j] = i \\hbar \\epsilon_{ijk} s_k \\). For indices y,z,x, what does epsilon give?\n\nEpsilon_{yzx}: Let's see, permutation parity. Epsilon_{xyz}=1, but cyclic permutations keep the same sign. Wait, no—if you cycle x→y→z→x, then ε remains positive. But when considering transpositions...\n\nAlternatively, perhaps better to recall specific values. We know that:\n\n\\( [s_x, s_y] = i \\hbar s_z \\)\n\\( [s_y, s_z] = i \\hbar s_x \\)\n\\( [s_z, s_x] = i \\hbar s_y \\ )\n\nYes, correct. Because switching two indices introduces a minus sign only once per swap. So yes, \\( [s_y, s_z] = i \\hbar s_x \\ ).\n\nBut in our case, the commutator is [s_y, s_z], which equals iħ s_x. However, note that in the definition above, [a,b]=ab-ba, whereas sometimes people define different conventions. Anyway, assuming my earlier statement holds.\n\nThen,\n\n\\[\n[H, s_z] = -\\frac{g e B}{2 m c} (i \\hbar s_x )\n\\]\n\nPutting it all together,\n\n\\[\n\\frac{ds_z}{dt} = \\frac{i}{\\hbar} \\times \\left(-\\frac{g e B}{2 m c} i \\hbar s_x \\right )\n\\]\n\nSimplifying:\n\nMultiply constants first: \\( i * (-i) = -(i^2) = -(-1)=1 \\).\nAlso, ħ cancels out.\n\nThus,\n\n\\[\n\\frac{ds_z}{dt} = \\frac{g e B}{2 m c} s_x\n\\]\n\nAlright, putting everything together, the derivatives are:\n\n- \\( \\frac{ds_x}{dt} = -\\omega s_z \\)\n- \\( \\frac{ds_y}{dt} = 0 \\)\n- \\( \\frac{ds_z}{dt} = \\omega s_x \\)\n\nwhere \\( \\omega = \\frac{g e B}{2 m c} \\ ). That looks familiar; ω would be related to the Larmor frequency or something similar.\n\nHmm, interestingly, these equations resemble those of classical angular velocity leading to precession. In fact, they look exactly like the differential equations governing circular motion in the plane perpendicular to the magnetic field direction (here, y-axis). So, spins will precess around the y-axis.\n\nTo solve these coupled ODEs, perhaps express them as vectors. Alternatively, recognize that this system describes rotation about the y-axis with angular frequency ω.\n\nIn such cases, the solutions involve sine and cosine functions oscillating between the x and z components while keeping y constant.\n\nAssuming initial conditions—for example, suppose at t=0, the state has definite sz, say |↑⟩ or |↓⟩. Then, under the influence of H, the expectation value ⟨sz⟩ would vary sinusoidally, transferring amplitude to sx and sy accordingly.\n\nHowever, given that the question asks specifically for \\( s_z(t) \\) in matrix form, not necessarily solving the full dynamics unless required.\n\nWait, do I need to write down the explicit time dependence of the operator \\( s_z(t) \\)?\n\nWell, from the Heisenberg picture, operators evolve according to the equations derived above. To get \\( s_z(t) \\), we'd integrate its time derivative.\n\nLooking at the three equations:\n\nWe have:\n\n1. \\( \\dot{s}_x = -ω s_z \\)\n2. \\( \\dot{s}_y = 0 \\)\n3. \\( \\dot{s}_z = ω s_x \\)\n\nThis forms a closed set of linear differential equations. It resembles the equations for simple harmonic motion.\n\nExpressed in matrix form, writing the vector \\( \\vec{v}(t) = (s_x(t), s_y(t), s_z(t))^T \\), the derivative is:\n\n\\[\n\\begin{pmatrix}\n\\dot{s}_x \\\\\n\\dot{s}_y \\\\\n\\dot{s}_z\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 & 0 & -ω \\\\\n0 & "}
{"id": "quantum/5003", "questions": "A particle of mass $m$ moves one-dimensionally in the oscillator potential $V(x) = \\frac{1}{2}m\\omega^2x^2$. In the nonrelativistic limit, where the kinetic energy $T$ and momentum $p$ are related by $T = p^2/2m$, the ground state energy is well known to be $\\frac{1}{2}\\hbar \\omega$.\n\nAllow for relativistic corrections in the relation between $T$ and $p$ and compute the ground state level shift $\\Delta E$ to order $\\frac{1}{c^2}$ ($c$ = speed of light).", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about a quantum harmonic oscillator but with relativistic corrections. Hmm, interesting! Let me try to figure out how to approach it.\n\nFirst off, I know that in the non-relativistic case, the ground state energy of a quantum harmonic oscillator is (1/2)ħω. But now we need to consider relativistic effects on the kinetic energy. The question mentions allowing for relativistic corrections up to 1/c² order. So, probably expanding some expression around c being large or something like that?\n\nWait, right, when you include relativity into mechanics, the relationship between kinetic energy T and momentum p changes from the classical form T = p²/(2m) to something else. For small velocities compared to c, which would correspond to low energies, maybe there's an expansion in terms of v/c or equivalently p/mc? Since higher-order terms can be neglected beyond 1/c².\n\nLet me recall: the total energy in special relativity is given by E = γmc², where γ is the Lorentz factor. And the kinetic energy T is then E - mc² = (γ - 1)mc². Now, if we expand γ as a Taylor series for small β (where β = v/c), since v << c here because we're considering non-relativistic speeds except for these small corrections.\n\nSo let's write down the expansion:\n\nγ ≈ 1 + (β²)/2 + (3β⁴)/8 + ... \n\nBut wait, actually, more accurately,\n\nγ = [1 - β²]^{-½}\n\nExpanding using binomial approximation for |β| < 1:\n\nγ ≈ 1 + (1/2)β² + (3/8)β⁴ + ...\n\nTherefore, the kinetic energy becomes:\n\nT = (γ - 1)mc² ≈ [(1/2)(v²/c²) + (3/8)(v⁴/c⁴)] * mc²\n\nSimplify that:\n\n= (1/2)m(v²) + (3/8)(mv⁴)/(c²)\n\nHmm, okay, so the first term is just the usual non-relativistic kinetic energy, and the second term is a correction proportional to 1/c² times m*v⁴. Alternatively, expressing everything in terms of momentum might make sense because our original Hamiltonian uses momentum.\n\nIn non-relativistic mechanics, p = mv, so v = p/m. Therefore, substituting back into the corrected kinetic energy:\n\nT_rel ≈ (p²)/(2m) + (3 p⁴)/(8 m³ c² )\n\nAh, perfect! That gives us the modified kinetic energy including the leading relativistic correction. So instead of T = p²/(2m), we'll use T = p²/(2m) + (3 p⁴)/(8 m³ c²).\n\nNow, going back to the quantum mechanical setup. We want to find the ground state energy shift due to this correction. To do this perturbatively, perhaps treat the additional term as a perturbation to the standard harmonic oscillator Hamiltonian.\n\nThe unperturbed Hamiltonian H0 is the sum of the non-relativistic kinetic energy and the oscillator potential V(x):\n\nH0 = p²/(2m) + (1/2)m ω² x²\n\nAnd the perturbing part ΔH comes from the relativistic correction:\n\nΔH = (3 p⁴)/(8 m³ c² )\n\nSince we’re looking for shifts to order 1/c², only linear terms in ΔH will contribute—quadratic terms would go as 1/c⁴, which is beyond what’s needed.\n\nThus, the first-order energy shift ΔE is simply the expectation value of ΔH in the ground state of H0.\n\nMathematically,\n\nΔE = ⟨ψ₀| ΔH |ψ₀⟩ = (3)/(8 m³ c² ) ⟨ψ₀| p⁴ |ψ₀⟩\n\nAlright, so all I need is the expectation value of p⁴ in the ground state of the harmonic oscillator.\n\nI remember that for the nth state of the harmonic oscillator, the expectation values of position and momentum operators follow certain patterns. Specifically, for the ground state (n=0), ⟨x²⟩ and ⟨p²⟩ are known quantities.\n\nFrom memory, ⟨x²⟩_0 = ħ/(2mω) and similarly, ⟨p²⟩_0 = (1/2) m ω ħ.\n\nMoreover, for even moments, such as ⟨x^{2k}⟩ and ⟨p^{2k}⟩, they involve products of double factorials or combinatorial factors multiplied by powers of ħ/(mω) etc.\n\nSpecifically, for ⟨p⁴⟩, I think it relates to the fourth moment of a Gaussian distribution, which has a formula involving factorial numbers.\n\nAlternatively, recalling that for any operator A whose eigenstates are those of the harmonic oscillator, expressions like ⟨p⁴⟩ can be found via recursion relations or ladder operators.\n\nExpressing p in terms of creation and annihilation operators could help. Remember that:\n\na = √(mω/(2ħ)) x + i√(1/(2mω ħ)) p\n\nand its adjoint a†.\n\nWe can solve for p:\n\nMultiply both sides by appropriate constants:\n\np = i √(mω ħ / 2) (a - a†)\n\nThen, p squared is:\n\np² = (-i)^2 (mω ħ / 2) (a - a†)^2 = -(mω ħ / 2)(a² - a a† - a† a + a†² )\n\nSimilarly, p⁴ would require squaring p² again, resulting in various combinations of a and a† raised to different powers.\n\nCalculating ⟨0| p⁴ |0⟩ requires evaluating each term after expanding p⁴. However, many terms may vanish upon taking the inner product with the vacuum state.\n\nGiven that a acting on |0⟩ gives zero, most cross terms will disappear unless their number of a equals the number of a†.\n\nLooking specifically, when computing ⟨0| p⁴ |0⟩, the only contributing terms come from normal-ordered products where every a is matched with an a†.\n\nWhen calculating p⁴, expanding (a - a†)^4 leads to several terms, among them a^4, a^3 a†, a² a†², a a†^3, and a†^4. When sandwiched between ⟨0| and |0⟩, only the middle term a² a†² contributes because others either annihilate the vacuum or create states orthogonal to the vacuum.\n\nTo elaborate, let's calculate p⁴ explicitly:\n\nStarting from p expressed through a and a†:\n\np = i √(mω ħ / 2) (a - a†)\n=> p² = - (mω ħ / 2) (a - a†)^2\n   = - (mω ħ / 2)[a² - {a,a†} + a†²]\n\nWhere {a,a†}=aa†+a†a = 1 + aa†, but actually, {a,a†}=1+a†a + aa† ?\n\nWait no, better yet, note that [a, a†]=1 => a a† = a† a +1.\n\nCompute (a - a†)^2:\n(a - a†)^2 = a² - a a† - a† a + a†²\n           = a² - (a† a +1) - a† a + a†²\n           = a² - 2 a† a -1 + a†²\n\nWait, hold on, let me recompute:\n\nActually,\n(a - a†)^2 = a*a - a*a† - a†*a + a†*a†\n            = a² - a a† - a† a + a†²\n\nNote that a a† = a† a +1, so substitute:\n\n= a² - (a† a +1) - a† a + a†²\n= a² - a† a -1 - a† a + a†²\n= a² - 2 a† a -1 + a†²\n\nYes, correct.\n\nSo, p² = - (mω ħ / 2)(a² - 2 a† a -1 + a†² )\n       = - (mω ħ / 2)a² + (mω ħ / 2)*2 a† a + (mω ħ / 2) - (mω ħ / 2)a†²\n\nWhich simplifies to:\n\np² = - (mω ħ / 2)a² + mω ħ a† a + (mω ħ /"}
{"id": "quantum/5035", "questions": "Discuss and compute the Stark effect for the ground state of the hydrogen atom.", "graphs": null, "llm_answers": "<think>\nOkay, so I need to figure out how to discuss and compute the Stark effect for the ground state of a hydrogen atom. Hmm, let me start from what I remember about the Stark effect.\n\nI think the Stark effect is when an electric field causes shifts or splittings in energy levels of atoms. It's named after John Tate, right? Wait no, it was Johannes Stark who discovered this effect. Yeah, that sounds familiar. So, basically, if you apply an external electric field to an atom, its energy levels change because of the interaction between the charges and the field.\n\nNow, focusing on the hydrogen atom specifically. The ground state would be the lowest energy level, which is n=1, l=0, m=0. Since it's spherically symmetric (l=0), maybe some terms simplify?\n\nWait, but wait—Stark effect can have two types: linear and quadratic. For hydrogen-like atoms, does the ground state exhibit both? Or only one type? Let me recall... Oh yeah! In states with zero angular momentum quantum number (like the 1s orbital here), there’s no first-order Stark shift because the expectation value of the dipole moment might vanish due to symmetry. But then comes the second-order effect, leading to a quadratic Stark shift. That makes sense since higher orders come into play when lower ones cancel out.\n\nSo, mathematically, the perturbation caused by the electric field should involve the Hamiltonian term H' = -e E z, where e is electron charge, E is electric field strength, and z is position along the field direction. Because we're dealing with non-relativistic QM, this seems correct.\n\nBut hold on—the exact form depends on whether the potential is being shifted as well. Sometimes people write H’ as qEz, considering the displacement. Anyway, moving forward.\n\nSince the unperturbed system has eigenstates |n, l, m>, our initial state is |1,0,0>. To find the energy shift, we'll use time-independent perturbation theory. First order correction is <ψ|H'|ψ>.\n\nLet me calculate that:\n\nΔE^(1) = ⟨1,0,0| (-e E z ) |1,0,0⟩\n\nHmm, now z is proportional to r cosθ, right? And in spherical coordinates, integrating over all space. However, given that the wavefunction for l=0 is spherically symmetric, any odd function like cosθ will integrate to zero. Therefore, ΔE^(1)=0. Yep, that confirms my earlier thought—it vanishes.\n\nTherefore, the first-order Stark effect doesn't contribute; we must go to second-order perturbation theory. Second-order energy shift is given by summing over intermediate states |k> not equal to |i>,\n\nΔE^(2) = Σ_{k≠i} [⟨i|H’|k⟩⟨k|H’|i⟩ / (E_i^{(0)} – E_k^{(0)})]\n\nAlright, so each term involves matrix elements squared divided by the difference in energies.\n\nGiven that i is the ground state, k runs through other possible states. Now, H’ is proportional to z, so the transition from |1,0,0> to another state |k> requires that the integral ∫ ψ₁* z ψ_k d³r ≠ 0.\n\nWhat do these integrals look like? Well, z is related to the polar angle, so perhaps transitions occur only when certain selection rules are met. Specifically, for dipolar transitions, the selection rule says Δl = ±1 and Δm = 0. Hence, from l=0, the excited states could have l=1.\n\nThus, the allowed k states are those with l'=1, same n'? No, actually, different n too. Wait, no—if n changes, say from n=1 to n=2, etc., but also changing l accordingly.\n\nHowever, more accurately, the matrix element ⟨k|z|i⟩ is non-zero only when l differs by 1. So starting from l=0, the next possible l is 1. Thus, the relevant k states are p-states (l=1). Also, their principal quantum numbers can vary, but they must satisfy the radial part integration condition.\n\nMoreover, parity considerations may help. Hydrogen orbitals have parity (-1)^l. Ground state is even (since l=0). Then, multiplying by z, which is an odd operator (parity -1), gives overall parity (-1)*(-1)^{l'} = (-1)^{l'+1}. If original state had parity (+1), then product becomes (-1)^{l'}, multiplied again by the new state's parity. Maybe getting confused here—I better just proceed methodically.\n\nAlternatively, using ladder operators approach. Expressing z in terms of creation/annihilation operators—but that might complicate things further.\n\nPerhaps instead, express z in terms of spherical harmonics. Remember that z ~ r Y_1^0(theta, phi). So, when calculating ⟨1,0,0| z |k⟩, it's equivalent to something involving overlap between Y_0^0 and Y_1^0 times radial parts.\n\nThe integral over angles would require that the product of Y_0^0 * Y_1^0 integrates to zero unless specific conditions are met. Actually, orthogonality tells us that such an integral is zero except when certain angular momentum conditions are satisfied.\n\nSpecifically, the integral ∫ Y_l'^m'*Y_L^M dΩ is zero unless L=l', M=m'. Here, we’re looking at Y_0^0 * Y_1^0 integrated against Y_1^0. Wait, no—that isn’t quite accurate. Let me clarify.\n\nActually, the matrix element ⟨1,0,0| z |k⟩ is proportional to ⟨1,0,0| r Y_1^0 |k⟩. Which breaks down into radial part R_n(r) and angular part Y_l^m.\n\nIf |k> is a state with quantum numbers n', l', m', then the angular integral becomes ∫ Y_0^0*(theta,phi) Y_1^0(theta,phi) Y_{l'}^{m'}(theta,phi) dOmega.\n\nUsing properties of spherical harmonics, this triple product integral is non-zero only under certain conditions. From Gaunt coefficients, the integral is non-zero only if l' satisfies triangle inequality with 0 and 1, meaning l' can be 1 or possibly others depending on addition.\n\nWait, adding angular momenta: coupling l=0 and l=1, maximum total l is 1+0=1, minimum is |1-0|=1. So, the result is l_total=1. Therefore, the resulting state after multiplication must have l=1. So, for the integral to be non-zero, the third spherical harmonic must have l'=1.\n\nAdditionally, the magnetic quantum numbers must add up correctly. We have m=0 (from Y_0^0*) times m=0 (from Y_1^0) giving m=0. So, the resultant Y_{l'}^{m'} must have m'=0.\n\nPutting together, the integral is non-zero only if l'=1 and m'=0. Therefore, the only contributing states |k> are those with l'=1 and m'=0, regardless of n'.\n\nThat simplifies things. So, in the summation for ΔE^(2}, we only include states |k> with l=1, m=0, and any n'.\n\nFurthermore, note that the radial integral will depend on n and n'. Each term in the sum corresponds to a particular n'.\n\nSo, putting it all together, the second-order energy shift is:\n\nΔE^(2) = Σ_{n'=2}^\\infty [ |⟨n',1,0| H' |1,0,0⟩|^2 / (E_1^{(0)} - E_{n'}^{(0)}) ]\n\nBecause E_i^{(0)} is E_1, and E_k^{(0)} is E_{n'}, so denominator is negative if E_{n'} > E_1, which it always is for n'>1.\n\nCompute each term:\n\nFirstly, H' = -e E z, so ⟨n',1,0| H' |1,0,0⟩ = -e E ⟨n',1,0| z |1,0,0⟩.\n\nAs established before, this reduces to evaluating the integral:\n\n⟨n',1,0| z |1,0,0⟩ = ∫ (R_{n'}^{1}(r))^2 r dr × ∫ Y_1^0* Y_0^0 Y_1^0 sinθ dθ dφ ?\n\nWait, no, more precisely, it's:\n\n∫ ψ_{n',1,0}* z ψ_{1,0,0} d³r\n\nWhich factors into radial and angular parts:\n\n[∫ R_{n'}^{1}(r) R_{1}^{0}(r) r² dr] × [∫ Y_1^0*(θ, φ) Y_0^0(θ, φ) Y_1^0(θ, φ) sinθ dθ dφ]\n\nSimplify the angular integral:\n\nWe know that Y_0^0 is sqrt(1/(4π)), constant everywhere. Multiplying three spherical harmonics: Y_1"}
{"id": "quantum/6012", "questions": "For an attractive square-well potential $(V = -V_0, r < a; \\, V = 0, r > a)$ find the matching equation at a positive energy, which determines the energy dependence of the $\\ell = 0$ phase shift $\\delta_0$. From this show that at high energies, $\\delta(k) \\rightarrow \\frac{maV_0}{\\hbar^2 k}$, and obtain this result from the Born approximation.", "graphs": null, "llm_answers": "<think>\nOkay, so I have to solve for the phase shift δ₀ for an s-wave (ℓ=0) particle in an attractive square well potential using the Schrödinger equation. The potential is given as V(r) = -V₀ when r < a and zero otherwise. Then, I need to derive the matching condition at a positive energy E and use it to show that at high energies, δ(k) approaches maV₀/(ħ²k). Also, I should get this same result using the Born approximation.\n\nFirst, let me recall what the phase shift means. In scattering theory, particles incident on a potential scatter with some probability amplitude, and the phase shift accounts for how much their wavefunction's phase changes due to the interaction compared to free propagation.\n\nSince we're dealing with ℓ=0, or s-waves, there’s no angular momentum involved, so everything depends only on the radial part. For the square well potential, inside the well (r < a), the effective potential includes both the actual potential and any centrifugal barrier term, but since ℓ=0, the centrifugal term vanishes. So, the effective potential is just V(r).\n\nThe time-independent Schrödinger equation for the radial function R(r) becomes:\n\n[-(ħ²)/(2m)(d²/dr²) + V(r)]ψ(r) = E ψ(r)\n\nBut because ℓ=0, the equation simplifies further without the angular terms. Let me write down the equations separately for regions r < a and r ≥ a.\n\nInside the well (r < a):\n\n(-ħ²/2m)d²R₁/dr² + (-V₀)R₁ = ER₁\n\nWhich can be rewritten as:\n\n(d²R₁/dr²) + [2m(E + V₀)/ħ²] R₁ = 0\n\nLet me define κ² = 2m(E + V₀)/ħ², assuming E + V₀ is positive because if not, the exponential solutions would come into play instead of sinusoidal ones. Since the potential is attractive, V₀ is positive, so even though E might be lower than |V₀|, depending on whether it's bound or unbound state. But here, we’re considering positive energy states, meaning E > 0. However, since V₀ is negative? Wait, hold on: wait, actually, the problem says \"attractive\" square well, so V(r)=−V₀ where V₀>0. Therefore, inside the well, the potential is more negative, making E + V(r) potentially less than E outside.\n\nWait, maybe better to think about signs carefully. If V(r)=-V₀ inside, then the total energy E must satisfy E >= 0 because it's a positive-energy state. Inside the well, the effective potential is lower, so E + V(r) could still be either positive or negative?\n\nNo, wait, E is greater than zero, and V(r) is negative inside. So E + V(r) will depend on whether E exceeds V₀. Hmm, perhaps I'm overcomplicating. Maybe proceed algebraically first.\n\nSo, defining K such that K² = 2m(E + V₀)/ħ². Now, if E + V₀ is positive, then K is real, leading to oscillatory solutions inside the well. Otherwise, if E + V₀ were negative, you'd have exponentially decaying functions. But since E is positive and V₀ is also positive (as it's an attractive potential), E + V₀ is definitely positive. Thus, K is real, and the solution inside is sinusoidal.\n\nOutside the well (r ≥ a), the potential is zero, so the equation becomes:\n\n(-ħ²/2m)d²R₂/dr² = E R₂\n\nRewriting,\n\n(d²R₂/dr²) + [2mE / ħ²] R₂ = 0\n\nDefine k² = 2mE / ħ², so again, k is real because E is positive. Hence, the general solution outside is a combination of outgoing and incoming waves. But for scattering problems, we usually take the boundary conditions as purely outgoing waves at infinity, so R₂ ~ sin(kr + δ) asymptotically, right? Or sometimes expressed differently.\n\nAlternatively, another approach is to express the regular solution inside and match it to the irregular solution outside. Alternatively, writing them in terms of spherical Bessel functions, but since ℓ=0, they reduce to simple sine and cosine functions.\n\nTherefore, inside the well (r ≤ a): \n\nR₁(r) = A sin(Kr) + B cos(Kr)\n\nAnd outside (r ≥ a):\n\nR₂(r) = C sin(kr + δ) + D cos(kr + δ)\n\nHowever, typically, for scattering, we impose the boundary condition that far away from the origin (large r), the wave behaves like an incoming plane wave plus an outgoing scattered wave. But for simplicity, especially in partial wave analysis, each partial wave has its own phase shift. Here, focusing on the s-wave component, the solution outside can be written as a sum of ingoing and outgoing spherical waves. But for our purposes, probably easier to consider the behavior near the origin and ensure continuity across r=a.\n\nAt large distances, the solution tends to e^{ikr} (outgoing wave) multiplied by appropriate factors, but for the purpose of finding the phase shift, often people set up the solution as follows.\n\nActually, thinking back, for the region outside, the standard form is something like R₂(r) = (sin(kr + δ))/kr, normalized appropriately, ensuring that at large r, it approximates the free-particle solution. Similarly, inside, the solution needs to remain finite everywhere, including at r=0.\n\nGiven that, for the interior solution, since we don't want divergence at r=0, we require that R₁(0) is finite. Cosine is fine at zero, while sine goes to zero. So, setting B ≠ 0 and A arbitrary may lead to issues unless... Actually, wait, if we include all possible solutions, but physically, the wavefunction shouldn’t diverge anywhere, so yes, likely choosing R₁ proportional to sin(Kr) isn't good because at r=0, it starts at zero, whereas cos(Kr) remains finite. Wait, hang on—no, actually, for the interior solution, being regular at r=0 requires that R₁ doesn't blow up. Both sine and cosine are okay at r=0, but if we choose coefficients correctly.\n\nWait, perhaps it's simpler to fix one coefficient based on normalization. Suppose we normalize R₁(0) =1, but generally, these constants will cancel out later when taking ratios.\n\nAnyway, moving forward, regardless of specific normalizations, the key point is to enforce continuity of R and its derivative at r=a.\n\nThus, two conditions:\n\n1. Continuity of R at r=a:\n   R₁(a) = R₂(a)\n   \n2. Continuity of dR/dr at r=a:\n   dR₁/dr evaluated at a equals dR₂/dr evaluated at a.\n\nThese give us two equations relating the coefficients A, B, C, D, etc., but ultimately, after solving, we'll relate the phase shift δ to other parameters.\n\nNow, expressing R₁ and R₂ explicitly.\n\nInside (r <=a):\nR₁(r) = A sin(Kr) + B cos(Kr)\n\nOutside (r>=a):\nAssuming the solution takes the form similar to the free case, but adjusted by the phase shift. It's common to represent the exterior solution as:\n\nR₂(r) = (e^{iδ}/(ik)) * e^{ikr}\n\nBecause for pure outgoing waves, the solution is e^{ikr}, but incorporating the phase shift gives e^{i(kr + δ)}. To make sure the overall solution matches properly, we adjust accordingly.\n\nBut alternatively, another way is to write R₂(r) = F sin(kr + δ), but scaling matters.\n\nPerhaps a better approach is to note that for the exterior solution, the regular solution (which corresponds to the incoming wave) is sin(kr - π/2 + δ) divided by kr, but honestly, my memory is fuzzy here—I might need to look up exact forms, but since I’m trying to simulate thought process...\n\nWait, perhaps going back to basics. When solving scattering problems, the Jost solution is used, which is defined as the solution that behaves like e^{-ikr} as r→∞. But in practice, for partial waves, the solution outside is taken as a linear combination of Hankel functions or simply expressed via phase shifts.\n\nIn any case, perhaps sticking with the simplest expressions.\n\nSuppose outside, the solution is represented as:\n\nR₂(r) = (e^{iδ})/(ik) * e^{ikr}\n\nThis ensures that as r → ∞, the dominant term is e^{ikr}, corresponding to an outgoing wave, modulated by the phase factor e^{iδ}. This seems reasonable.\n\nSimilarly, inside, the solution is a combination of sine and cosine.\n\nTo avoid confusion, perhaps assign"}
{"id": "quantum/2-2014", "questions": "Consider an electron moving in a spherically symmetric potential $V = kr$, where $k > 0$.\n\n(a) Use the uncertainty principle to estimate the ground state energy.\n\n(b) Use the Bohr-Sommerfeld quantization rule to calculate the ground state energy.\n\n(c) Do the same using the variational principle and a trial wave function of your own choice.\n\n(d) Solve for the energy eigenvalue and eigenfunction exactly for the ground state.  \n\n(Hint: Use Fourier transforms.)\n\n(e) Write down the effective potential for nonzero angular momentum states.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about an electron moving in a spherically symmetric potential V(r) = k r, with k being positive. There are five parts to it, from (a) to (e). Let me try to tackle each part one by one. \n\nStarting with part (a): Using the uncertainty principle to estimate the ground state energy. Hmm, okay. The uncertainty principle relates the uncertainties in position and momentum. For quantum systems, especially those involving particles like electrons, we can use these principles to get approximate energies without solving the full Schrödinger equation. So, how does that work?\n\nI remember that the Heisenberg Uncertainty Principle says Δx * Δp ≥ ħ/2. In three dimensions, but since the potential is radial, maybe spherical coordinates would be useful here? But perhaps starting in one dimension might simplify things because sometimes people do that when estimating.\n\nWait, actually, another approach comes to mind—the virial theorem or something related to balancing kinetic and potential energy. Maybe not directly applicable here, though. Alternatively, thinking about dimensional analysis could help us figure out what terms should look like.\n\nBut let's stick with the uncertainty principle. If I consider the electron confined within some region of radius r, then its position uncertainty Δx ≈ r. Then, the momentum uncertainty Δp would be on the order of ℏ / r due to the uncertainty relation. Since p^2/(2m) gives the kinetic energy, KE ~ (Δp)^2/(2m), which becomes approximately ℏ²/(2 m r²).\n\nOn the other hand, the potential energy is given as V(r) = k r. At equilibrium, the kinetic and potential energies balance each other—so setting them equal:\n\nKE ≈ PE ⇒ ℏ²/(2 m r₀²) ≈ k r₀\n\nSolving for r₀, multiply both sides by 2 m r₀²:\n\nℏ² ≈ 2 m k r₀³\n\nSo,\n\nr₀ ≈ (ℏ²)/(2 m k))^(1/3)\n\nThen plugging back into either KE or PE to find the energy E. Let’s take PE first:\n\nE ≈ k r₀ = k*(ℏ²/(2 m k))^(1/3) = [k]^{( -1/3)} * (ℏ²/(2 m ))^{1/3}\n\nAlternatively, simplifying exponents:\n\n= (ℏ²/(2 m ))^{1/3} * k^{-1/3 + 1}? Wait no, wait. Hold on, if you factor out k inside the cube root:\n\nLet me write it again:\n\nE ≈ k * (ℏ²/(2 m k))^{1/3} = (k^3 * ℏ²/(2 m k)) )^{1/3} ?\n\nNo, hold on, more carefully:\n\n(k)*(something)^(1/3):\n\nIt's better expressed as:\n\nE ≈ k * (ℏ²/(2 m k))^{1/3} = (k^3 * ℏ²/(2 m k))^{1/3} ? No, that doesn't seem right.\n\nActually, think of it as multiplying k times (A/B)^(1/3), where A = ℏ², B = 2 m k.\n\nThus,\n\nE ≈ (k) * (ℏ²/(2 m k))^{1/3} = (k^{3})^{1/3} * (ℏ²/(2 m k))^{1/3} } = [k^3 * ℏ²/(2 m k)]^{1/3} }\n\nWhich simplifies to [k^2 * ℏ²/(2 m)]^{1/3} = (ℏ² k² / (2 m))^{1/3}\n\nTherefore,\n\nE ≈ (ℏ² k² / (2 m))^{1/3}\n\nHmm, alternatively, factoring constants differently:\n\nWe had earlier:\n\nFrom equating KE and PE:\n\nℏ²/(2 m r₀²) = k r₀ => Multiply both sides by r₀²:\n\nℏ²/(2 m) = k r₀³ => Thus, r₀³ = ℏ²/(2 m k )\n\nHence, r₀ = (ℏ²/(2 m k ))^{1/3}\n\nPlugging back into E = k r₀:\n\nE = k * (ℏ²/(2 m k ))^{1/3} = (k^3 * ℏ²/(2 m k ))^{1/3} = (k² ℏ²/(2 m ))^{1/3}\n\nYes, correct. Therefore, E ≈ (ℏ² k² / (2 m ))^{1/3}. That seems reasonable.\n\nIs there any negative sign involved? Because usually potentials can lead to bound states having negative energies, but in our case, V(r)=kr is linearly increasing with r, meaning it's unbounded above. However, considering the confinement via the uncertainty principle, the particle has a minimum energy—it cannot go below zero necessarily? Or am I mixing up concepts here.\n\nWait, actually, in such a potential, the force increases with distance, leading to a sort of \"confinement\" similar to a harmonic oscillator, except steeper. So, yes, the ground state will have a certain minimal energy, probably positive? Or depending on whether the potential is attractive or repulsive.\n\nGiven that k>0, the potential V(r)=kr is repulsive, pushing the electron away. Hence, the system isn’t bounded from below; however, through the uncertainty principle, we’re finding a lower limit on the energy based on the trade-off between kinetic and potential contributions.\n\nWait, but in reality, for a purely repulsive potential, wouldn't all the energy levels just be scattering states rather than bound states? Hmmm... Perhaps my initial assumption was wrong—that the potential allows for a bound state. Actually, for V(r) proportional to r, the potential grows indefinitely as r increases, making it impossible for the particle to escape once beyond a certain point—if it were attractive—but in this case, it's repelling, so higher r leads to higher potential. So, perhaps even classically, the particle couldn't stay localized unless acted upon externally.\n\nThis makes me question whether applying the uncertainty principle in this way is appropriate. Maybe instead, treating it similarly to a harmonic oscillator, assuming that the average potential and kinetic energies must relate somehow.\n\nIn the harmonic oscillator, the expectation values satisfy <T>=<V>, giving total energy twice the kinetic. Here, perhaps a different proportionality exists.\n\nWait, going back to the original idea—I set KE≈PE, getting E~(ℏ² k²/m)^{1/3}, but perhaps including factors correctly.\n\nAnother thought: Instead of taking Δx=r, maybe the proper treatment involves integrating over space or considering Gaussian distributions, but that may complicate matters.\n\nAlternatively, recall that in hydrogen-like atoms, the Rydberg constant arises from similar reasoning, combining Coulomb potential and uncertainty. Similarly, here, replacing the Coulomb term with kr.\n\nPerhaps scaling arguments apply. Suppose the Hamiltonian is H=p²/(2m)+kr. To make it dimensionless, introduce units scaled by ℏ, m, and k.\n\nDefine x'=x/a, t'=t/b, etc., choosing a,b,c appropriately. From dimensional analysis, the only parameters available are ℏ,m,k. Energy has dimensions ML²T⁻². So, trying to form an expression with these quantities.\n\nEnergy scales as (ℏ²)/(m L²) for kinetic, and k*L for potential. Setting them equal: (ℏ²)/(m L²) = k L → L³=(ℏ²)/(m k ), hence L=(ℏ²/(mk))^{1/3}, matching previous result. Then, energy scale is (ℏ² k²/m)^{1/3}, confirming prior conclusion.\n\nSince the potential is repulsive, the energy found is likely the lowest possible, i.e., the ground state energy. Although strictly speaking, in a pure repulsive potential, there aren't conventional bound states, but in this estimation method, we're approximating a kind of 'minimum' energy akin to a quasi-bound state.\n\nAlright, tentatively accepting that the estimated ground state energy is E ≈ (ℏ² k² / (2 m ))^{1/3}. Though I need to check coefficients—are they accurate?\n\nLooking back, when I said KE ≈ ℏ²/(2 m r₀²) and PE=k r₀, setting them equal gave r₀³=ℏ²/(2 m k ). Plugging back into E=PE=k r₀, got E=(k)(ℏ²/(2 m k ))^{1/3}= (k^(-1/3))(ℏ²/(2 m ))^{1/3}*k= (ℏ² k²/(2 m ))^{1/3}/(some coefficient?)\n\nWait, no, algebra mistake. Let me re-express:\n\nIf r₀ = (ℏ²/(2 m k ))^{1/3"}
{"id": "quantum/1-1024", "questions": "An infinitely deep one-dimensional square well potential confines a particle to the region $0 \\leq x \\leq L$. Sketch the wave function for its lowest energy eigenstate. If a repulsive delta function potential, $H = \\lambda \\delta (x - L/2) (\\lambda > 0)$, is added at the center of the well, sketch the new wave function and state whether the energy increases or decreases. If it was originally $E_0$, what does it become when $\\lambda \\to \\infty$?", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about quantum mechanics involving a particle in a one-dimensional infinite square well with some modifications. Let me try to break it down.\n\nFirst part: The setup is an infinitely deep square well from x=0 to x=L. So that's like a box where the walls are infinitely high, meaning the particle can't escape. For such a system, we know the solutions are sinusoidal functions—sine waves because they must go to zero at both ends since the probability of finding the particle outside the well is zero.\n\nThe question asks to sketch the wave function for the lowest energy eigenstate. Hmm, right—the ground state. In a symmetric well like this, the ground state should be the simplest sine wave possible without any nodes except at the boundaries. That would mean just half a wavelength fitting into the well. Wait, actually, no—if you think about standing waves on a string fixed at both ends, the fundamental mode has exactly half a wavelength between the two points. But wait, isn’t the length L equal to λ/2? Because if n=1, then the wavelength λ is 2L/n, which gives λ=2L. So yeah, the first excited state might have more nodes?\n\nWait, maybe not. Actually, let me recall: the general form of the wavefunction ψ_n(x) is sqrt(2/L) sin(nπx/L). So for n=1, it’s a single hump going up from 0 to L, reaching maximum at L/2. Yeah, that makes sense—it starts at zero, goes positive, peaks at midpoint, then back to zero. No node other than endpoints. Got it.\n\nSo the graph would look like a smooth curve starting at origin, peaking at L/2, and ending at L. It's all above the axis; symmetric around L/2 but only once.\n\nNow, moving on. A repulsive delta function potential H = λ δ(x − L/2), with λ>0, is added at the center. Repulsive means it adds a barrier there, making it harder for the particle to exist near that point. Since lambda is positive, the perturbation is adding a spike upwards at x=L/2.\n\nI need to figure out how this affects the wave function and the energy levels. When you add a small disturbance like this, especially a delta potential, it usually causes splitting of energy levels or shifts them slightly depending on the strength.\n\nBut here, instead of being infinitesimal, lambda could approach infinity. Interesting. First, though, before considering limits, perhaps consider the effect qualitatively.\n\nIn the original case, the wavefunction had a peak at L/2. Adding a repulsive potential there would make the particle less likely to be found there—that is, the amplitude of the wavefunction at x=L/2 would decrease. How does that affect the overall shape?\n\nWell, imagine pushing against the middle of the wave. Instead of having a sharp peak, it becomes flatter as the potential gets stronger. As lambda approaches infinity, effectively creating an impenetrable wall at x=L/2, the well splits into two separate wells each of width L/2. Then, the particle would localize entirely within either left or right side—but given symmetry, probably equally probable in either, leading to even and odd states.\n\nHowever, initially, our wavefunction was symmetric. After introducing the delta potential, will the parity change? Or will it split into different modes?\n\nAlternatively, thinking about bound states: a very strong repulsive potential may cause the effective confinement area to reduce, forcing higher momentum particles due to Heisenberg uncertainty principle, thus increasing their kinetic energy.\n\nLet me formalize this a bit. Without the delta potential, the Hamiltonian is purely the kinetic term plus the infinite barriers. With the addition of H', which is a delta function, the total Hamiltonian becomes:\n\nH_total = H_original + H'\n\nSince H' is repulsive, it acts as a perturbing potential. However, unlike attractive potentials which lower energies, repulsive ones tend to raise the energy levels because the particle feels pushed away, requiring more energy to stay confined.\n\nTherefore, intuitively, E_new > E_old=E0.\n\nAs for the wavefunction, the presence of the repulsion would create a dip at x=L/2 compared to the unperturbed case. Before, the wave peaked there; now, it flattens or dips. Depending on the strength of lambda, the dip becomes deeper until, in the limit as lambda→∞, the wavefunction cannot pass through x=L/2 anymore, resulting in two separate regions.\n\nAt lambda approaching infinity, the delta function behaves almost like another infinite barrier at x=L/2. Thus, the original well [0,L] is divided into two smaller wells [0, L/2] and [L/2, L], each acting independently. Now, the particle is confined to one of these halves.\n\nGiven that the original ground state was symmetric, after the division, the particle would occupy the ground state of one of the subwells. Each subwell has width L/2, so the corresponding energy level would be four times higher than the original, because the energy scales inversely with the square of the width. Specifically, E ∝ 1/(width)^2. Original width L gave E0=(n² π² ħ²)/(2mL²); for n=1, E0= (π² ħ²)/(2mL²).\n\nIf the width reduces to L/2, keeping n=1, the new energy Enew= (π² ħ²)/(2m(L/2)²)=4*(π² ħ²)/(2mL²)=4*E0.\n\nThus, as lambda tends to infinity, the energy quadruples.\n\nPutting it together: Initially, the wavefunction is a simple sine arch over [0,L]. Upon adding a repulsive delta at center, the wavefunction develops a dip at mid-point, becoming flatter there. Energy increases. At extreme lambda, the well splits, and the energy jumps to four times the initial value.\n\n**Final Answer**\nWhen \\(\\lambda\\) approaches infinity, the energy becomes \\(4E_0\\). Therefore, the final result is \\(\\boxed{4E_0}\\).\n</think>\n\nFor a particle confined in an infinitely deep one-dimensional square well potential from \\(0 \\leq x \\leq L\\), the lowest energy eigenstate (ground state) has a wave function that is a sine wave starting at zero, peaking at \\(x = L/2\\), and returning to zero at \\(x = L\\). This wave function is symmetric around \\(x = L/2\\).\n\nWhen a repulsive delta function potential \\(H = \\lambda \\delta(x - L/2)\\) is introduced at the center of the well, the wave function adjusts. The repulsive nature of the potential causes the wave function to develop a dip at \\(x = L/2\\), reducing the amplitude there. This modification leads to an increase in the energy of the system because the particle is effectively confined in a narrower region, necessitating higher kinetic energy.\n\nAs \\(\\lambda\\) approaches infinity, the delta function potential creates an additional infinite barrier at \\(x = L/2\\), dividing the original well into two independent wells each of width \\(L/2\\). The particle localizes in one of these subwells, and the energy of the ground state doubles due to the reduced width. The energy scaling factor is proportional to the inverse square of the width, leading to the energy becoming four times the original energy \\(E_0\\).\n\n### Final Answer\nWhen \\(\\lambda\\) approaches infinity, the energy becomes \\(\\boxed{4E_0}\\)."}
{"id": "quantum/6015", "questions": "In the quantum theory of scattering from a fixed potential, we get the following expression for the asymptotic form of the wave function  \n\n$$\n\\psi(r) \\xrightarrow{r \\to \\infty} e^{ikz} + f(\\theta, \\varphi) \\frac{e^{ikr}}{r}.\n$$\n\n(a) If the entire Hamiltonian is rotationally invariant, give the argument that the scattering amplitude $f$ should be independent of the angle $\\varphi$.\n\n(b) Why cannot this argument be extended (considering rotation about any axis) to conclude that $f$ should be independent of $\\theta$ as well?\n\n(c) Reconsider part(b) in the case where the incident energy approaches zero.\n\n(d) What is the formula for the scattering cross section in terms of $f$?\n\n(e) What is the formula for the first Born approximation for $f$? (Be sure to define all quantities introduced. You need not worry about simple dimensionless factors like 2 or $\\pi$).  \n\n(f) Under what conditions is the Born approximation valid?", "graphs": null, "llm_answers": "<think>\nOkay, so I have these six parts to tackle regarding the quantum theory of scattering. Let me start with part (a).\n\n**Part (a): Argument why scattering amplitude \\( f \\) doesn't depend on \\( \\varphi \\)**\n\nHmm, if the Hamiltonian is rotationally invariant, then it commutes with the angular momentum operators. That means the system's properties shouldn't change under rotations. So when you rotate the coordinate system around some axis, say the z-axis, which corresponds to changing the azimuthal angle \\( \\varphi \\), the physical situation remains the same because there's no preferred direction due to rotational symmetry.\n\nThe wavefunction must reflect this symmetry too. The incoming plane wave is along the z-direction, right? Because the term \\( e^{ikz} \\) suggests propagation in the positive z-axis. Since everything is symmetric around this axis, rotating the system around z won't affect anything physically observable. Therefore, the scattering amplitude \\( f(\\theta, \\varphi) \\) can't depend on \\( \\varphi \\); otherwise, different values would imply distinguishable states under rotation, breaking the symmetry.\n\nSo, mathematically, since the problem has cylindrical symmetry around the z-axis, only angles relative to this axis matter—specifically, polar angle \\( \\theta \\)—but not the azimuthal angle \\( \\varphi \\). Hence, \\( f = f(\\theta) \\).\n\nWait, but does this mean that even after considering other symmetries, maybe spherical ones, could something else happen? No, because while full rotational symmetry implies independence of both \\( \\theta \\) and \\( \\varphi \\), here our initial condition breaks that into just axial symmetry. Wait, actually, hold on—if the whole setup is spherically symmetric, meaning the potential depends only on r, then yes, the scattering amplitude might also become isotropic. But wait, isn't the incident wave coming along a specific direction, making the overall scenario axially symmetric rather than fully rotationally symmetric?\n\nYes, exactly! Even though the potential may be spherically symmetric, the fact that the particle is moving along the z-axis introduces an asymmetry such that only rotations around the z-axis leave the system unchanged. Rotations about other axes don’t preserve the direction of the incident wave. Thus, the dependence on \\( \\varphi \\) disappears, but \\( \\theta \\) still matters because it relates to how far off-shell the scattered particles go concerning the original beam direction.\n\nTherefore, my conclusion is correct: \\( f \\) is independent of \\( \\varphi \\).\n\n**Part (b): Why can't the argument extend to make \\( f \\) independent of \\( θ \\)?**\n\nAlright, now thinking about whether similar reasoning applies to \\( \\theta \\). In part (a), we considered rotations about the z-axis leading to \\( \\varphi \\)-independence. For \\( \\theta \\), suppose someone tries to argue using arbitrary rotations—but wait, the issue is that the Hamiltonian being rotationally invariant allows us to choose coordinates freely, but the presence of the incident wave imposes a particular frame.\n\nIf we consider another rotation, say one that changes the polar angle \\( \\theta \\), wouldn't that mix up directions differently? However, unlike the z-axis, which was naturally aligned with the incident wave, choosing another axis would disrupt the simplicity of the incident wave description.\n\nBut more formally, let’s think about the generators of rotations—the angular momentum components L_x, L_y, L_z. Only L_z commutes with the Hamiltonian because the system is symmetric under rotations about the z-axis. Other components do not commute; hence, they aren't conserved. This lack of conservation for L_x and L_y means that their corresponding symmetries aren't present, preventing us from concluding that \\( f \\) is independent of \\( \\theta \\).\n\nAlternatively, without additional constraints beyond rotational symmetry, varying \\( \\theta \\) affects the orientation away from the z-axis, which isn't preserved under arbitrary rotations. Specifically, unless the system is completely isotropic—which it isn't because the incident wave gives a preferred direction—we can't assume uniformity across all \\( \\theta \\).\n\nThus, the key point is that while the system respects rotational symmetry around the z-axis, it doesn't respect rotations about other axes because those would alter the direction of the incident wave. Consequently, \\( f \\) retains its dependency on \\( \\theta \\).\n\n**Part (c): Re-examining part (b) when incident energy approaches zero**\n\nWhen the incident energy goes to zero, the wavelength becomes very large (\\( k \\to 0 \\)), implying long-range interactions dominate. At low energies, perhaps certain approximations simplify things further.\n\nAt zero energy, the kinetic energy vanishes, so the Schrödinger equation reduces to finding bound state solutions—or does it? Actually, at E=0, it's technically a threshold between bound and continuum states, but often treated carefully.\n\nHowever, focusing back on the question: Does the scattering amplitude lose its \\( \\theta \\) dependence when energy tends to zero?\n\nI recall that in non-relativistic scattering, especially at low energies, sometimes the scattering length dominates, and higher-order terms vanish. Moreover, in cases like s-wave dominance, higher partial waves (which correspond to larger \\( l \\)) contribute less significantly.\n\nGiven that, perhaps at extremely low energies, the scattering process becomes effectively isotropic—that is, the scattering amplitude becomes constant regardless of \\( \\theta \\). Is that true?\n\nLet me think again. When the energy approaches zero, the de Broglie wavelength becomes much longer than typical interatomic distances, so the details of the potential's structure smooth out. Scattering processes tend towards elastic scattering dominated by the lowest partial waves—in this case, s-waves (l=0). Higher partial waves require more phase space, which isn't available at low energies.\n\nSince s-waves are isotropic, i.e., independent of angles, the scattering amplitude \\( f \\) would approach a constant value irrespective of \\( \\theta \\) and \\( \\varphi \\). \n\nHence, in the limit of zero energy, the argument used in part (a) extends to eliminate \\( \\theta \\) dependence as well, resulting in \\( f \\) becoming a constant. So, yes, at zero energy, \\( f \\) loses its angular dependence.\n\nIs there a formal way to see this? Maybe through expansion in partial waves. As energy decreases, contributions from higher multipoles diminish, leaving only the s-wave component, which indeed makes \\( f \\) independent of angles.\n\nYeah, that seems solid.\n\n**Part (d): Formula for scattering cross-section in terms of \\( f \\)**\n\nScattering cross-sections relate to the probability of deflection per unit area. From memory, the differential cross-section \\( dσ/dΩ \\) is given by |\\( f(θ, φ) \\)|² times some constants.\n\nSpecifically, the standard result is:\n\n\\[ \n\\frac{d\\sigma}{d\\Omega} = |f(\\theta, \\varphi)|^2.\n\\]\n\nBut wait, units-wise, the cross-section usually includes a factor involving λ squared or something related to the flux. Hmm, but in many treatments, particularly in scattering theory, the differential cross-section is expressed simply as the square modulus of the scattering amplitude multiplied by some fundamental constants.\n\nActually, recalling the precise definition: the total current density carried by the outgoing wave involves the term proportional to |f|² / r². Then integrating over all solid angles gives the total cross-section σ_total.\n\nBut specifically, the differential cross-section is defined as the ratio of the number of scattered particles per unit solid angle to the incident intensity. Mathematically,\n\n\\[ \n\\left( \\frac{dN}{d\\Omega dt} \\right )_{scattered} = J_0 |f|^2,\n\\]\nwhere \\( J_0 \\) is the incident current density. Dividing by \\( J_0 \\) gives the differential cross-section:\n\n\\[ \n\\frac{d\\sigma}{d\\Omega} = |f(\\theta, \\varphi)|^2.\n\\]\n\nBut wait, am I missing a factor? Sometimes definitions include a factor of \\( (k/(4π))^2 \\) or something similar depending on normalization conventions. Alternatively, in three-dimensional Fourier transforms, sometimes extra factors come in.\n\nHold on, looking back—I remember that in some derivations, the relation comes with a factor of \\( (1/k)^2 \\times |f|^2 \\). Or is it already accounted for in defining \\( f \\)?\n\nLooking at the asympt"}
{"id": "quantum/5032", "questions": "(a) Using an energy-level diagram give the complete set of electronic quantum numbers (total angular momentum, spin, parity) for the ground state and the first two excited states of a helium atom.\n\n(b) Explain qualitatively the role of the Pauli principle in determining the level order of these states.\n\n(c) Assuming Coulomb forces only and a knowledge of $Z = 2$ hydrogenic wave functions, denoted by $|1s\\rangle, |2s\\rangle, |2p\\rangle$, etc., together with associated $Z = 2$ hydrogenic energy eigenvalues $E_{1s}, E_{2s}, E_{2p}, \\ldots$, give perturbation formulas for the energies of these helium states. Do not evaluate integrals, but carefully explain the notation in which you express your result.  ", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about the helium atom's electronic structure. It has three parts: part (a), where I need to draw an energy-level diagram showing the total angular momentum, spin, and parity for the ground state and the first two excited states; part (b), explaining how the Pauli exclusion principle affects their ordering; and part (c), giving perturbation formulas using Z=2 hydrogenic wavefunctions without evaluating any integrals. Hmm, let me tackle each part one by one.\n\nStarting with part (a). The helium atom is a multi-electron system, right? So it's more complex than hydrogen because we have electron-electron interactions. But since both electrons are identical fermions, they must obey the Pauli exclusion principle. For the ground state, both electrons will be in the lowest energy orbital available, which should be the 1s orbital. Wait, no—since there can't be two electrons in the same quantum state due to Pauli, they'll occupy different spin states within the same spatial orbital. That makes sense—the singlet or triplet states?\n\nWait, actually, when considering the overall quantum numbers like total angular momentum J, spin S, and parity P, maybe I should think about the possible combinations. Let me recall that for helium, the ground state configuration is ^1S_0, meaning S=0 (singlet state) and L=0 (so J=S+L=0 as well?), wait no—if L=0 and S=0, then J would also be zero. Parity depends on whether the orbital is even or odd under inversion. Since s orbitals are spherical symmetric, their parity is positive (+1).\n\nBut hold on, if both electrons are in 1s, what happens? Each electron has n=1, l=0. Their spins could either add up to S=0 (triplet?) No, wait, no—that’s confusing. Spin-½ particles combine such that two electrons can form a singlet (S=0) or triplet (S=1). Oh yeah! Singlet is antisymmetric in spin space, while triplet is symmetric. And according to Hund's rule, higher multiplicity leads to lower energy... but wait, in helium, does that apply here?\n\nActually, for helium, the ground state is a singlet state (^1S_0) because having opposite spins minimizes repulsion. Because if they were parallel spins (triplet), the exchange interaction might lead to higher energy. Or perhaps it's just the way the Pauli exclusion works—it requires them to have opposite spins if they're in the same spatial orbital. Yeah, exactly. If both electrons are in the same spatial orbital (same n, l, m_l), then their spins must be opposite, leading to S=0. Therefore, the ground state is ^1S_0.\n\nNow moving to the first excited states. After the ground state, the next possible configurations involve promoting one electron from 1s to a higher orbital. What comes after 1s? Well, in terms of increasing energy levels, the order isn't straightforward because of the fine structure splitting. Normally, in hydrogen-like atoms, the order follows n + l, but for helium, things get a bit tricky because of electron correlation effects. However, assuming similar behavior, the next highest energy would probably come from promotion into 2s or 2p orbitals.\n\nSo, the first excited state would likely be when one electron moves to 2s. Then, another possibility is moving to 2p. Now, depending on the spin and angular momentum coupling, these can split further. \n\nLet me try to figure out all possibilities:\n\nFor the first excited state:\nIf one electron is promoted to 2s, the other remains in 1s. So the spatial part involves 1s and 2s. The total orbital angular momentum L would still be 0 because both are s-orbitals. Similarly, the spin can again be considered. Since the spatial part is symmetric (because swapping the electrons doesn’t change anything if both are in s orbitals), the spin part needs to be anti-symmetric. Hence, the spin state would be singlet (S=0). Thus, the term symbol becomes ^1S_0 again? Wait, but that seems conflicting because the energy shouldn't necessarily stay the same.\n\nAlternatively, maybe my approach is wrong. Perhaps instead, the first excited state corresponds to a different combination. Maybe the second electron goes into 2p, making some p character involved. Alternatively, perhaps the first excited state is a triplet state with higher spin.\n\nHold on—I remember that in helium, the ground state is a singlet, and the first few excited states include a triplet state. Specifically, the first excited state is often called the \"helium metastable\" state, which is a triplet state (^3P_0, ^3P_1, ^3P_2). Is that correct?\n\nYes, indeed. In helium, the ground state is ^1S_0, and the first excited states are the triplet P states. These arise when one electron is promoted from 1s to 2p. This gives rise to several degenerate states based on the magnetic quantum number m_j. Additionally, beyond that, the next excited states may involve promotions to higher orbitals or different couplings.\n\nTherefore, listing the states:\n\nGround State: Both electrons in 1s, S=0, L=0 → Term Symbol: ^1S_0\n\nFirst Excited States: One electron in 1s, the other in 2p. Here, L=1 (from 2p), and S can be 0 or 1. But given that the spatial wavefunction symmetry plays a role, along with the Pauli principle, the allowed spin states depend on the spatial symmetry.\n\nThe spatial wavefunction for 1s and 2p: When combining 1s (l=0) and 2p (l=1), the relative orbital angular momentum between the two electrons is determined. To find the parity, note that 1s has parity +1, 2p has parity -1 (since p orbitals change sign upon inversion). So the product of parities is (-1)^{l} * something else? Actually, parity operator acting on the spatial wavefunction changes r→−r, so for each orbital, its parity is (-1)^l. So 1s has parity +1, 2p has parity −1. Multiplying those gives parity of the combined spatial wavefunction as (+1)*(−1)=−1. But wait, do I consider the permutation of electrons too?\n\nHmm, getting confused now. Let me take a step back. Helium being a two-electron system, the total wavefunction must be antisymmetric under exchange. The total wavefunction includes spatial and spin parts. So if the spatial part is symmetric, the spin part must be antisymmetric, and vice versa.\n\nIn the case of the ground state, both electrons are in 1s, so the spatial wavefunction is symmetric (as exchanging electrons leaves it unchanged)—wait, no, actually, for indistinguishable particles, the spatial wavefunction for two electrons in the same orbital is symmetric only if they have opposite spins. Otherwise, it's antisymmetric. Wait, no, the full wavefunction is antisymmetric regardless.\n\nI'm mixing myself up. Let me clarify: For two electrons in the same spatial orbital, say 1s^2, the spatial part is symmetric because swapping the electrons doesn't affect the probability distribution—they’re in the same place. But since the electrons are fermions, the entire wavefunction must be antisymmetric. Therefore, the spin part must be antisymmetric. Which means the spin state is a singlet (symmetric spin state would require symmetric spatial, but since spatial is symmetric, spin must be antisymmetric—but wait, no, sorry, correction: Symmetric spatial × Antisymmetric spin = Antisymmetric total. Yes, that's correct.\n\nSimilarly, for the excited states where one electron is in 1s and the other in 2p, the spatial wavefunction is a Slater determinant involving 1s and 2p. How does that look? The spatial part is built from ψ(1)sψ(2)p − ψ(1)pψ(2)s over √2, ensuring antisymmetry. So the spatial part itself is antisymmetric under exchange. Therefore, the spin part must be symmetric to make the whole thing antisymmetric. A symmetric spin state implies a triplet state (S=1), hence the term symbols start with ^3...\n\nThus, the first excited states correspond to the triplet P states, i.e., ^3P_J where J can be 0, 1, 2. So the first excited states consist of multiple sublevels corresponding to different J values.\n\nThen"}
{"id": "quantum/2-2013", "questions": "The ground state energy and Bohr radius for the hydrogen atom are $E_0 = -e^2/2a_0$, $a_0 = \\hbar^2/me^2$, where $\\text m$ is the reduced mass of the system.\n$$\n\\{ m_e = 9.11 \\times 10^{-28} \\ \\text{g}, \\ m_p = 1.67 \\times 10^{-24} \\ \\text{g}, \\ e = 4.80 \\times 10^{-10} \\ \\text{e.s.u.}, \\ \\hbar = 1.05 \\times 10^{-27} \\ \\text{erg sec.} \\}\n$$\n\n(a) Compute the ground state energy and Bohr radius of the positronium.\n\n(b) What is the degeneracy of the positronium ground state due to electron spin? Write down the possible spin wave functions which have definite values of the total spin together with the corresponding eigenvalues.\n\n(c) The ground state of positronium can decay by annihilation into photons. Calculate the energy and angular momentum released in this process and prove that there must be at least two photons in the final state.\n\n", "graphs": null, "llm_answers": "<think>\nOkay, so I've got these three parts about positronium. Let me start with part (a). Positronium is a bound state consisting of an electron and its antiparticle, a positron. So it's similar to a hydrogen atom but instead of a proton and electron, we have an electron and a positron orbiting each other.\n\nFirst, they gave us some constants: masses of electron and proton, charge e, and h-bar. But wait, since it's positronium, both particles are electrons essentially—except one has positive charge. Wait no, actually, the positron has the same mass as the electron because it's the antimatter counterpart. Hmm, right! So the mass of the positron should be equal to the mass of the electron. Therefore, when calculating the reduced mass, maybe?\n\nWait, hold on—the problem statement says \"where m is the reduced mass of the system.\" For hydrogen, the reduced mass μ is given by (m_e * M)/(m_e + M), where M is the proton mass. Since the proton is much heavier than the electron, μ ≈ m_e. But in positronium, both particles are electrons (or rather, electron and positron)—so their masses are equal. Thus, the reduced mass would just be half the mass of either particle, right?\n\nSo let me write that out:\n\nFor hydrogen:\nμ_hydrogen = (m_e * m_p)/(m_e + m_p)\nBut since m_p >> m_e,\nμ_hydrogen ≈ m_e*(1 - m_e/(m_p)) ≈ m_e approximately.\n\nIn positronium, though, both masses are m_e, so the reduced mass μ_positronium = (m_e * m_e)/(m_e + m_e) )= m_e / 2.\n\nTherefore, compared to hydrogen, everything scales based on the reduced mass. Because all formulas depend on μ, like the Bohr radius a0 ∝ ħ²/(μ e²), and the ground state energy E0 ∝ -e⁴ μ/(ħ²).\n\nGiven that, if μ is halved from hydrogen’s case, then what happens?\n\nLet me recall the expressions:\n\nBohr radius for hydrogen-like atoms is a0 = (n² ℏ² ε₀)/(π Z μ e²)\n\nWait, or more accurately,\n\na0 = (ℏ²)/(μ e²) multiplied by something else... Maybe better to think directly using the formula provided.\n\nThey said for hydrogen, E0 = -e²/(2 a0), and a0 = ℏ²/(m e²). Here, 'm' was probably the reduced mass, but perhaps not exactly—it might have been approximated as m_e.\n\nWait, looking back, yes, the question states: \"the ground state energy and Bohr radius for the hydrogen atom are E0 = -e²/(2 a0), a0 = ℏ²/m e²\", where m is the reduced mass.\n\nAh, okay, so in general, for any system, you replace m with the appropriate reduced mass.\n\nThus, for positronium, our new reduced mass will affect both a0 and E0.\n\nSince in hydrogen, m is effectively m_e (since μ≈m_e), while in positronium, μ=m_e/2.\n\nHence, substituting μ into the expression for a0 gives:\n\na0_positronium = ℏ²/(μ e²) = ℏ²/( (m_e/2) e² ) = 2 ℏ²/(m_e e² )\n\nWhich means a0_positronium = 2 a0_hydrogen.\n\nSimilarly, the ground state energy E0_positronium = -e²/(2 a0_positronium ). Substituting a0_positronium:\n\nE0_positronium = -e²/(2 * [2 ℏ²/(m_e e²)]) ) = -e²/(4 ℏ²/(m_e e² )) = -(e^4 m_e)/(4 ℏ² )\n\nAlternatively, comparing to hydrogen's E0 = -e²/(2 a0 ), which becomes E0_hydrogen = - (e^4 m_e)/(2 ℏ² ).\n\nComparing them, E0_positronium = (-1/2)(-e^4 m_e/(2 ℏ² )) ? No, wait, my substitution may need checking.\n\nHold on, original E0 is given as -e²/(2 a0 ), and a0 is ℏ²/(m e² ). Hence, plugging a0 into E0:\n\nE0 = -e²/(2*(ℏ²/(m e²))) = -e^4 m/(2 ℏ² )\n\nYes, correct. So for hydrogen, E0 = -e^4 m/(2 ℏ² ), where m is the reduced mass (~m_e). In positronium, m_reduced = m_e/2, hence:\n\nE0_positronium = -e^4 (m_e/2)/(2 ℏ² ) = -e^4 m_e/(4 ℏ² )\n\nSo indeed, the ground state energy is lower (more negative?) Wait, no—if m is smaller, does E become less negative? Or more?\n\nWait, E ~ -mu; mu being the reduced mass. If mu decreases, |E| increases? Wait, no, E is proportional to -mu, so if mu halves, E doubles in magnitude towards negative infinity? That seems contradictory.\n\nWait, no, let's see again:\n\nIf E0 = -k * mu, where k is positive constant, then reducing mu reduces |E|. So in positronium, since mu is smaller, the binding energy is weaker—that makes sense because both particles are lighter, so the Coulomb attraction isn't holding them as tightly.\n\nWait, but earlier calculation shows E0_positronium = -e^4 m_e/(4 ℏ² ), whereas E0_hydrogen = -e^4 m_e/(2 ℏ² ). So yeah, E0_positronium is twice higher (less negative) than hydrogen's. Which aligns with intuition—they're easier to separate.\n\nAnd similarly, the Bohr radius is doubled.\n\nAlright, now plug in numbers.\n\nCompute a0_positronium first.\n\nWe know a0_hydrogen is known, but here compute numerically.\n\nFrom the data:\n\nm_e = 9.11 × 10^-28 g\n\ne = 4.80 × 10^-10 esu\n\nℏ = 1.05 × 10^-27 erg·sec\n\nNote units: grams, cm, seconds, etc., since cgs units are used here.\n\nSo, a0 = ℏ²/(m e² )\n\nPositronium's a0p = 2*a0_hydrogen.\n\nBut let's calculate a0_hydrogen first.\n\nCalculate numerator: ℏ squared.\n\n(1.05 × 10^-27)^2 = (approx) 1.1025 × 10^-54 erg² s²\n\nDenominator: m_e times e squared.\n\nm_e = 9.11×10^-28 g\n\ne = 4.80×10^-10 esu => e² = (4.80×10^-10)^2 = 23.04×10^-20 esu²\n\nNow, denominator: m_e*e² = 9.11×10^-28 g * 23.04×10^-20 esu²\n\nMultiply those: 9.11*23.04 ≈ 210, exponents: 10^-28 *10^-20 =10^-48\n\nSo denominator ≈ 210 ×10^-48 g·esu²\n\nNumerator: 1.1025 ×10^-54 erg² s²\n\nHmm, unit-wise, erg is gram·cm²/s², so erg² is g² cm^4 /s^4.\n\nPutting it all together:\n\na0 = (1.10"}
{"id": "quantum/3-3047", "questions": "A beam of neutrons of velocity $v$ passes from a region ($\\text I$) (where the magnetic field is $\\mathbf{B} = B_1 \\mathbf{e}_z$) to a region ($\\text {II}$) (where the field is $\\mathbf{B} = B_2 \\mathbf{e}_x$). In region ($\\text I$) the beam is completely polarized in the $+z$ direction.\n\n(a) Assuming that a given particle passes from ($\\text I$) to ($\\text {II}$) at time $t = 0$, what is the spin wave function of that particle for $t > 0$?\n\n(b) As a function of time, what fraction of the particles would be observed to have spins in the $+x$ direction; the $+y$ direction; the $+z$ direction?\n\n(c) As a practical matter, how abrupt must the transition between  ($\\text I$) and ($\\text {II}$) be to have the above description valid?", "graphs": null, "llm_answers": "<think>\nOkay, so I've got this problem about neutron beams passing through different regions with varying magnetic fields. It's part (a), asking for the spin wave function after they enter region II where the magnetic field changes direction. Hmm, let me think.\n\nFirst off, in region I, the magnetic field is along the z-axis, B₁ e_z. The beam is completely polarized in the +z direction. So initially, each neutron has its spin aligned with the magnetic field—meaning their spin state is |↑⟩ in the z-direction. That makes sense because when you're in a magnetic field, the lowest energy states align with it if there's no external perturbation causing transitions or something else.\n\nNow, these neutrons move into region II where the magnetic field switches to being along the x-axis, B₂ e_x. At t=0, they cross over. Since the magnetic field here is now along x, the Hamiltonian governing their spin will change accordingly. For spin-½ particles like neutrons, the interaction with a magnetic field is described by the Zeeman effect: H = -γ ħ S · B, but wait, actually more accurately, since γ is the gyromagnetic ratio, which includes factors depending on whether we’re dealing with protons, electrons, etc., but maybe for simplicity, assuming some standard form.\n\nBut perhaps another approach is better. When moving into a new magnetic field orientation, the basis for the spin states also rotates. Initially, everything was in the z-basis, but once in region II, the natural basis becomes the eigenstates of σ_x, right? Because the magnetic field is along x, so the Pauli matrices come into play.\n\nWait, hold on. Let me recall. Spin systems under a uniform magnetic field precess around the field axis. If the initial polarization is along z, then upon entering a field along x, the system starts precessing around x. But does that mean the spin state remains as |↑⟩ in z, or do we need to express it in terms of the new basis?\n\nHmm, probably not immediately changing the state—it’s just subject to a different Hamiltonian starting at t=0. So before t=0, the state was |ψ(t<0)⟩ = |↑⟩_z. After t=0, the Hamiltonian changes, leading to time evolution governed by the new H.\n\nSo first, write down the Hamiltonians:\n\nIn region I:\nH_I = ω₀ S_z\n\nWhere ω₀ might relate to the Larmor frequency due to B₁. Similarly, in region II, the Hamiltonian should involve S_x instead.\n\nActually, using units where ℏ = 1 sometimes helps simplify expressions, but maybe keeping track carefully is better.\n\nThe general expression for the Zeeman term is H = -μ·B, where μ is the magnetic moment. For a neutron, the magnetic dipole moment is proportional to its spin. Specifically, μ = γ s, where γ is the gyromagnetic ratio for the neutron. However, the exact value isn't critical here unless specific numbers were involved.\n\nAlternatively, considering the spin operators in matrix form could help. Remember that S_x, S_y, S_z can be represented by (ħ/2) times the Pauli matrices σ_x, σ_y, σ_z respectively.\n\nTherefore, in region I, the Hamiltonian is H_I = ω_{I} S_z, where ω_{I} relates to B₁ via ω_{I}= γ B₁ / ħ * ħ/(2m)? Wait, maybe getting confused here. Alternatively, thinking dimensionally, the Larmor frequency ω_L is given by γ B, so yes, H = -γ ħ B S_z, but signs depend on convention.\n\nPerhaps stepping back, regardless of constants, the key point is that the Hamiltonian flips from involving S_z to S_x when crossing into region II.\n\nGiven that, the initial state is |↑⟩_z, which is [1; 0] in the computational basis. Now, in region II, the Hamiltonian is proportional to S_x, whose eigenstates are |→⟩ and |←⟩, i.e., superpositions of |↑⟩ and |↓⟩ in the z-basis.\n\nSince the Hamiltonian suddenly changes at t=0, the state doesn’t instantly jump—the wavefunction stays continuous across t=0, but evolves according to the new H thereafter.\n\nThus, the time evolution operator U(t) = exp(-i H t / ħ ). Given that, our task reduces to expressing the initial state in the eigenbasis of H_region II, find the expansion coefficients, and then apply the phase factors from time evolution.\n\nLet me formalize this.\n\nAt t < 0, the state is |ψ(−)⟩ = |↑⟩_z.\n\nAfter t ≥ 0, the Hamiltonian is H = ω S_x, say, where ω incorporates all necessary constants including γ, B₂, etc. Then,\n\n|ψ(+)(t)⟩ = exp(-i H t ) |ψ(−)⟩.\n\nExpressed in terms of Pauli matrices, H = ω (S_x) = (ω ħ / 2) σ_x.\n\nAssuming ħ = 1 for simplification, though in reality, dimensions may require careful handling.\n\nAnyway, writing out the exponential:\n\nexp(-i H t) = cos(ω t / 2) I - i sin(ω t / 2) σ_x.\n\nBecause for any Pauli matrix σ_i, exp(i θ σ_i) = I cosθ + i σ_i sinθ. Here, replacing θ with (-i H t ), but scaling appropriately.\n\nMultiplying this with the initial state vector [1; 0], which represents |↑⟩_z.\n\nCompute the product:\n\n[cos(ω t / 2)   -i sin(ω t / 2)] [1]\n[-i sin(ω t / 2)  cos(ω t / 2)] [0]\n\nWhich gives:\n\nTop component: cos(ω t / 2)*1 + (-i sin(ω t / 2))*0 = cos(ω t / 2)\n\nBottom component: (-i sin(ω t / 2)) *1 + cos(ω t / 2)*0 = -i sin(ω t / 2 )\n\nSo overall, the state is:\n\n|ψ(t)⟩ = [cos(ω t / 2); -i sin(ω t / 2)]\n\nThis can be written as cos(ω t / 2)|↑⟩_z - i sin(ω t / 2)|↓⟩_z.\n\nAlternatively, recognizing that this corresponds to rotation about the x-axis by angle ω t. Indeed, such a state is equivalent to applying R_x(φ) to |↑⟩_z, resulting in a superposition pointing in the y-z plane.\n\nBut wait, hang on—isn't the time evolution generating a rotation about the x-axis? Yes, exactly. So the spin state begins to precess around the x-axis, hence why components start appearing in both up and down in z.\n\nHowever, the question asks specifically for the spin wave function for t>0. So my conclusion is that the wave function is expressed as above.\n\nIs there anything missing? Well, perhaps normalization—if I consider the actual values, but cosine squared plus sine squared equals one, so it's normalized.\n\nAlso, note that without knowing the explicit relation between ω and other parameters, we leave it symbolic. Maybe in terms of B₁ and B₂, but since only directions changed, and magnitudes aren't specified beyond B₁ and B₂, likely acceptable to keep it as ω.\n\nAnother thought: Is the sign correct? The exponent is negative imaginary unit multiplied by H t. Considering that H is positive definite in magnitude, the phases accumulate correctly.\n\nYes, seems okay.\n\n**Final Answer**\n\nFor part (a), the spin wave function for \\( t > 0 \\) is \n\n\\[\n|\\psi(t)\\rangle = \\cos\\left(\\frac{\\omega t}{2}\\right)|\\uparrow\\rangle_z - i \\sin\\left(\\frac{\\omega t}{2}\\right)|\\downarrow\\rangle_z.\n\\]\n\nSo, putting it in a box:\n\n\\boxed{ |\\psi(t)\\rangle = \\cos\\left(\\frac{\\omega t}{2}\\right)|\\uparrow\\rangle_z - i \\sin\\left(\\frac{\\omega t}{2}\\right)|\\downarrow\\r"}
{"id": "quantum/3-3024", "questions": "A particle with magnetic moment $\\mu = \\mu_0 \\mathbf{s}$ and spin $\\mathbf s$, with magnitude $1/2$, is placed in a constant magnetic field pointing along the $x$-axis. At $t = 0$, the particle is found to have $s_z = +1/2$. Find the probabilities at any later time of finding the particle with $s_y = \\pm 1/2$.  ", "graphs": null, "llm_answers": "<think>\nOkay, so I've got this problem here about a particle's magnetic moment in a magnetic field. Let me try to unpack it step by step.\n\nFirst off, the particle has a magnetic moment μ which equals μ₀ times its spin vector **s**. The spin magnitude is given as ½, which makes sense because for electrons or similar particles, the spin quantum number s is usually ½. So we're dealing with something like that.\n\nThe setup: there's a constant magnetic field pointing along the x-axis. That means the direction of the magnetic field B is along the positive x-direction. Now, initially, at t=0, the particle is measured to have sz = +½. So when they measure the z-component of spin, they get the maximum value possible since s can be ±½.\n\nBut now, the question asks for the probability at any later time t of finding the particle with sy = ±½. Hmm, okay. So after some time passes under the influence of the magnetic field, if someone measures the y-component of spin instead of the z-component, what would be the chances?\n\nI remember from my studies that when you place a magnetic dipole in a magnetic field, it experiences torque causing precession around the field axis. But wait, how does this translate into changes in spin states? Maybe using quantum mechanics formalism will help more than classical intuition here.\n\nSo let's think in terms of Hamiltonians and Schrödinger equation. In such problems, the interaction between the magnetic moment and the external magnetic field gives rise to energy levels, leading to transitions over time.\n\nGiven that the magnetic field points along the x-axis, the Hamiltonian should involve the dot product of μ and B. Since μ = μ₀ **s**, then H = -μ·B (or maybe plus; sometimes conventions differ). Wait, actually, the Zeeman term is typically written as H = γ **S** · **B**, where γ is gyromagnetic ratio. Here, though, μ is already defined as μ₀ **s**, so perhaps H = -μ·B = -μ₀ (**s** · **B**) ?\n\nSince B is along x, say B = B₀ î, then H becomes proportional to sx. Specifically:\n\nH = ω_s * S_x,\n\nwhere ω_s might relate to the Larmor frequency or something similar. To find out exactly, probably need to express H properly.\n\nWait, but units-wise, the Hamiltonian must have dimensions of energy. Given that μ has units of J/T (since magnetic moment is current*area), multiplying by T (Tesla) gives Joules. So yes, H = -μ·B would indeed give us energy.\n\nTherefore, H = -μ₀ **s** · **B**. If B is along x, then H simplifies to H = -μ₀ B s_x.\n\nAlternatively, depending on convention, could also write H = μ·B, but signs matter. For example, the potential energy U of a magnetic dipole in a magnetic field is U = -μ·B. Therefore, the Hamiltonian would correspond to this potential energy. Hence, H = -μ·B.\n\nThus, substituting, H = -μ₀ **s** · **B**. With B along x, H = -μ₀ B s_x.\n\nNow, considering ħ (h-bar) comes into play when expressing angular momentum operators. Remembering that the spin operator components satisfy [s_i, s_j] = iħ ε_{ijk} s_k.\n\nIn quantum mechanics, each component of spin is represented by matrices. For spin-½ particles, these are Pauli matrices multiplied by ħ/2. However, often people set ħ=1 for simplicity, especially in equations involving commutators or eigenvalues.\n\nLet me recall that the spin matrices for s_x, s_y, s_z are all proportional to σ_x, σ_y, σ_z respectively, scaled by ħ/2. So, s_x = (ħ/2)σ_x, etc.\n\nHowever, in our case, the initial state is |+z>, meaning the spin is up along z. We want to see how this evolves under the Hamiltonian H = -μ₀ B s_x.\n\nTo proceed, first, let's define the system's Hilbert space. It's two-dimensional since spin-½ systems live in C².\n\nExpressed in matrix form, assuming ħ=1 for simplicity, the spin operators become:\ns_x = (1/2)[[0,1],[1,0]],\ns_y = (1/(2i))[[0,-1],[1,0]],\ns_z = [[1/2,0],[0,-1/2]].\n\nThen, H = -μ₀ B s_x = (-μ₀ B / 2)[[0,1],[1,0]]. Alternatively, factoring constants, H can be expressed as Ω s_x, where Ω = -μ₀ B.\n\nBut regardless, the key point is that H is diagonalized by the eigenvectors of s_x. Which are different from those of s_z, hence why measuring sy at a later time leads to non-trivial probabilities.\n\nAt t=0, the state is |ψ(0)> = |+z> = [1; 0].\n\nWe need to solve the time evolution of this state under H. Then, compute the probabilities P(sy=±½).\n\nTime evolution is governed by the Schrodinger equation:\n\n|ψ(t)> = exp(-i H t / ħ ) |ψ(0)>.\n\nAssuming ħ=1 again, this becomes:\n\nexp(-i H t) |ψ(0)>.\n\nCompute this exponential. First, note that H is proportional to s_x, whose square is identity times (Ω^2)( (1/2)^2 ). Because s_x squared is (1/4)I, right? So H² = (Ω^2)(1/4)I.\n\nThis allows us to expand the exponential easily, since e^{iθ H} can be expanded via Euler formula due to H being Hermitian and having known spectrum.\n\nSpecifically, suppose H = λ s_x, then H² = (λ² /4) I. Thus, exponentiating:\n\ne^{-i H t } = cos(λ t) I - i sin(λ t) (H)/(λ)\n\nBecause expanding e^{i M}, where M² = c² I, results in cos(c t) I + i sin(c t) M/c.\n\nApplying this to our situation, H = -μ₀ B s_x, so λ = -μ₀ B*(1/2)? Wait no, hold on.\n\nActually, earlier expression was H = -μ₀ B s_x, and s_x is (1/2)σ_x. So H = -(μ₀ B)/2 σ_x.\n\nWhich implies that H = Ω σ_x, where Ω = -(μ₀ B)/2.\n\nHmm, not sure if sign matters much yet, but proceeding carefully.\n\nAnyway, H² = (Ω²) σ_x² = Ω² I, since σ_x² = I.\n\nThus, the expansion holds:\n\nexp(-i H t) = cos(|Ω| t) I - i (H / |Ω|) sin(|Ω| t )\n\nBut since H itself includes a negative sign, let's plug back in.\n\nWait, better approach: Compute E = exp(-i H t ), knowing that H = α σ_x, with alpha real.\n\nIf H = α σ_x, then H² = α² I, so\n\nE = cos(alpha t) I - i sigma_x sin(alpha t ).\n\nYes, correct.\n\nHere, alpha is equal to mu_0 B / 2 ? Or is it negative?\n\nFrom above, H = -mu_0 B s_x = -mu_0 B (sigma_x / 2 ) = (-mu_0 B / 2 ) sigma_x.\n\nSo alpha = -mu_0 B / 2.\n\nBut cosine is even function, sine is odd, so whether alpha is positive or negative affects the phase factor.\n\nBut perhaps rather than getting bogged down in signs, just keep track symbolically.\n\nSo, writing H = omega sigma_x, where omega = -mu_0 B / 2.\n\nThen, E = exp(-i H t ) = cos(omega t) I - i sigma_x sin(omega t )\n\nNote that omega may include factors of hbar, but previously assumed hbar=1.\n\nBut in reality, H has dimensionality of energy, so omega must have unit rad/s, thus including necessary factors.\n\nWait, perhaps another way: Express everything in natural units where ħ=1, so energies and frequencies are directly related without extra scaling.\n\nBut perhaps going through dimensional analysis isn't helpful here—maybe stick to symbolic expressions.\n\nAlright, moving forward.\n\nOur initial state is |psi(0)> = [1; 0], corresponding to |+z>.\n\nApply the time-evolution operator E to this state:\n\n|psi(t)> = E |psi(0)> = [cos(ω t); ... ] hmm, let's do the multiplication explicitly.\n\nMatrix representation of E"}
{"id": "quantum/5051", "questions": "The first excited state of three-dimensional isotropic harmonic oscillator (of natural angular frequency $\\omega_0$ and mass $m$) is three-fold degenerate. Use the perturbation method to calculate the splitting (to the first order) of this three-fold degenerate state due to a small perturbation of the form $H' = bxy$, where $b$ is a constant. Give the first-order wave functions of the three split levels in terms of the wave functions of the unperturbed three-dimensional harmonic oscillator, given that, for a one-dimensional harmonic oscillator,\n\n$$\n\\langle n|x|n + 1 \\rangle = \\sqrt{\\frac{(n + 1)\\hbar}{2m\\omega_0}}.\n$$", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about a three-dimensional isotropic harmonic oscillator with a small perturbation H'. The question asks me to find the splitting of the first excited state using perturbation theory. Hmm, let's break it down.\n\nFirst, I remember that the energy levels of a 3D isotropic harmonic oscillator are quantized. For each dimension x, y, z, we can think of them as independent oscillators. So the total energy would be something like E_nx_ny_nz = ħω(n_x + n_y + n_z + 3/2). But wait, since it's isotropic, all directions have the same ω, which here they call ω₀.\n\nNow, the ground state has quantum numbers nx=ny=nz=0, right? And then the first excited state... Well, there are multiple ways to get the next level up because you could increase any one of the quantum numbers by 1. Specifically, the states |nx, ny, nz> will have either nx=1 or ny=1 or nz=1 while the others remain zero. That makes sense why it's three-fold degenerate—three different states but same energy.\n\nSo our unperturbed system has these three states: |1,0,0>, |0,1,0>, and |0,0,1>. Each corresponds to being excited along one axis. Now, when we add the perturbation H', which is equal to b*x*y, things change slightly. We need to compute how much the energies shift due to this perturbation.\n\nIn non-degenerate perturbation theory, the first-order correction is just ⟨ψ|H'|ψ⟩. However, since we're dealing with a degenerate case, we should use degenerate perturbation theory instead. This means constructing a matrix from the perturbing Hamiltonian within the subspace of the degenerate states and diagonalizing it to find the corrections.\n\nAlright, so let's set up the basis vectors for the degenerate space:\n\nφ₁ = |1,0,0>\nφ₂ = |0,1,0>\nφ₃ = |0,0,1>\n\nWait no, actually hold on—the first excited state isn't necessarily only those three; maybe higher combinations too? Wait, no—if the sum of the quantum numbers is such that n_x + n_y + n_z = 1, yeah, exactly. Because if you start counting from n=0, the first excited state is n_total = 1, giving us three possible states.\n\nBut now, considering the perturbation H’ = b xy. Since the original potential was symmetric in x, y, z, adding a term involving x times y might mix some of these states.\n\nTo apply degenerate perturbation theory, I'll construct the matrix elements M_ij = <φ_i| H’ | φ_j>.\n\nLet’s write out what each element looks like.\n\nM_11 = <1,0,0| b x y |1,0,0>\nSimilarly,\nM_22 = <0,1,0| b x y |0,1,0>\nM_33 = <0,0,1| b x y |0,0,1>\n\nAnd off-diagonal elements:\nM_12 = <1,0,0| b x y |0,1,0>\nM_13 = <1,0,0| b x y |0,0,1>\nM_23 = <0,1,0| b x y |0,0,1>\n\nHmm, okay. Let's evaluate these integrals.\n\nStarting with the diagonal elements. Take M_11: integrating over all space, specifically the product of the eigenfunctions multiplied by x y.\n\nSince the 3D HO wavefunction factors into products of 1D ones, perhaps?\n\nYes! In position representation, the wave function for |n_x,n_y,n_z> is Ψ_{n_x}(x)Ψ_{n_y}(y)Ψ_{n_z}(z).\n\nTherefore, for φ₁ = |1,0,0>, its wavefunction is Ψ_1(x)Ψ_0(y)Ψ_0(z), similarly for the other two.\n\nThus, computing M_11 involves ∫ [Ψ_1*(x)Ψ_0*(y)Ψ_0*(z)] * b x y [Ψ_1(x)Ψ_0(y)Ψ_0(z)] dx dy dz\n\nWhich simplifies to b ∫ Ψ_1* x Ψ_1 dx ∫ Ψ_0* y Ψ_0 dy ∫ Ψ_0* Ψ_0 dz\n\nBecause multiplication distributes across variables.\n\nLooking at each integral separately:\n\n∫ Ψ_1* x Ψ_1 dx : What does this give? Remembering properties of ladder operators, x is proportional to (a + a†)/√(2 m ω / ħ)}, so expectation value of x in state |n> is zero unless n changes by ±1. Similarly, for even powers...\n\nWait, more precisely, for the 1D HO, the expectation value ⟨n|x|m⟩ is sqrt[(ħ/(2 m ω)) ] δ_{m, n±1} } ?\n\nNo, not quite—it depends on whether m = n+1 or n-1. From the formula provided in the question, ⟨n|x|n+1⟩ = sqrt[ ((n+1) ħ)/(2 m ω₀) ) ]\n\nBut in our case, for M_11, we’re looking at ⟨1|x|1⟩. Which is zero because x connects adjacent states, so ⟨1|x|1⟩ must vanish. Therefore, the integral ∫ Ψ_1* x Ψ_1 dx = 0.\n\nSame logic applies to the y component: ∫ Ψ_0* y Ψ_0 dy. Here, again, y acting on |0> gives a factor related to |1>, so ⟨0|y|0⟩ is also zero. Thus, both integrals are zero, making M_11 = 0.\n\nSimilarly, evaluating M_22 and M_33:\n\nFor M_22, similar reasoning: It becomes b ∫ Ψ_0* x Ψ_0 dx ∫ Ψ_1* y Ψ_1 dy ∫ Ψ_0* Ψ_0 dz. Again, both x and y expectations are zero, so M_22 = 0.\n\nLikewise, M_33: Integrating x and y parts, both involve ⟨0|x|0⟩ and ⟨0|y|0⟩, which are zero. Hence, M_33 = 0.\n\nSo all diagonal elements are zero. Interesting.\n\nWhat about the off-diagonal elements? Let's take M_12 = <1,0,0| b x y |0,1,0>\n\nAgain, breaking it into components:\n\nIt's b ∫ Ψ_1*(x)Ψ_0*(y)Ψ_0*(z) x y Ψ_0(x)Ψ_1(y)Ψ_0(z) dx dy dz\n\nFactorize the integrals:\n\n= b [∫ Ψ_1*(x) x Ψ_0(x) dx] [∫ Ψ_0*(y) y Ψ_1(y) dy] [∫ Ψ_0*(z) Ψ_0(z) dz]\n\nEach bracketed part is separate.\n\nCompute each integral:\n\nFirst integral: ∫ Ψ_1* x Ψ_0 dx. Using the relation given, ⟨n|x|n+1⟩ = sqrt( (n+1) ħ / (2 m ω₀) )\n\nHere, n=0, so ⟨0|x|1⟩ = sqrt( (0+1) ħ / (2 m ω₀ )) = sqrt( ħ / (2 m ω₀ ))\n\nSecond integral: ∫ Ψ_0* y Ψ_1 dy. By the same token, ⟨0|y|1⟩ = sqrt( ħ / (2 m ω₀ ) )\n\nThird integral: ∫ Ψ_0^2 dz = 1, since it's normalized.\n\nPutting together:\n\nM_12 = b * [ sqrt( ħ / (2 m ω₀ )) ] * [ sqrt( ħ / (2 m ω₀ )) ] * 1\n\nMultiply constants:\n\nThat's b * ( ħ / (2 m ω₀ ) ). Simplify further: M_12 = (b ħ) / (2 m ω₀ )\n\nSimilarly, check symmetry. How about M_21? Is it the complex conjugate of M_12? Yes, but since everything is"}
{"id": "quantum/5069", "questions": "An atom with no permanent magnetic moment is said to be diamagnetic. In this problem the object is to calculate the induced diamagnetic moment for a hydrogen atom in a weak magnetic field $B$ ignoring the spins of the electron and proton.\n\n(a) Write down the nonrelativistic Hamiltonian for a particle of mass $m$ and charge $q$ in a combined vector and scalar electromagnetic field.\n\n(b) Using the fact that a satisfactory vector potential $\\mathbf{A}$ for a uniform magnetic field is $\\mathbf{A} = -\\frac{1}{2} \\mathbf{r} \\times \\mathbf{B}$, write down the steady-state Schrödinger equation for the hydrogen atom. Assume the proton to be infinitely massive so center of mass motion can be ignored.\n\n(c) Treat the terms arising from the magnetic field as a perturbation and calculate the shift in the ground state energy due to the existence of the magnetic field.\n\n(d) Calculate the induced diamagnetic moment per atom.", "graphs": null, "llm_answers": "<think>\nOkay, I have this problem about calculating the induced diamagnetic moment for a hydrogen atom when it's placed in a weak magnetic field B. The question has four parts: (a), (b), (c), and (d). Let me try to tackle each part one by one.\n\nStarting with part (a): It asks to write down the non-relativistic Hamiltonian for a particle of mass m and charge q in a combined vector and scalar electromagnetic field. Hmm, okay. So, I remember that the Hamiltonian for such a system includes both electric and magnetic fields. \n\nIn general, without any external fields, the Hamiltonian would just be kinetic plus potential energy. But here we have electromagnetic fields. For a charged particle moving in these fields, the momentum gets modified because of the vector potential A. Specifically, the canonical momentum p becomes p minus (e/c) times A, where e is the charge and c is speed of light. Wait, but sometimes people use different units or conventions; maybe they don't include 1/c? Let me think... Oh right, depending on whether you're using Gaussian or SI units. Since the problem doesn’t specify, perhaps it’s better to stick with natural units where constants like c might not appear explicitly?\n\nWait, actually, let me recall the standard form. The Hamiltonian should account for the Lorentz force. So, if there's a vector potential A and a scalar potential φ, then the Hamiltonian H is given by:\n\nH = [ (p - qA)^2 ] / (2m) + qφ\n\nYes, that seems correct. Because the minimal coupling replaces p with p - qA/ something—wait, hold on. If the charge is q, then yes, the term is (p - (q/c) A)²/(2m) + qφ. Or wait, does the factor depend on how A is defined? Maybe since the problem says \"nonrelativistic,\" factors involving c may come into play differently.\n\nBut looking back, the exact expression depends on unit systems. Given that the problem mentions nothing specific, probably assuming Gaussian units where E and B have certain dimensions. Alternatively, another approach could be considering SI units. However, regardless, the structure remains similar—the kinetic term involves (p - qA) squared over 2m, plus the scalar potential term qφ.\n\nSo, writing that out formally,\n\nH = \\frac{( \\mathbf{p} - q \\mathbf{A} )^2 }{2m} + q \\phi\n\nI believe that's the required Hamiltonian. Okay, done with part (a).\n\nMoving on to part (b): We need to write down the steady-state Schrödinger equation for the hydrogen atom under a uniform magnetic field B, treating the proton as infinitely massive so we ignore its movement. Also, the vector potential A is given as -(1/2)r × B.\n\nAlright, first, the unperturbed hydrogen atom has a Coulomb potential between the nucleus (proton) and the electron. When placing it in a magnetic field, which introduces additional terms via the vector potential.\n\nGiven that the vector potential is A = (-1/2)( r × B ), substituting this into our earlier Hamiltonian will give us the full Hamiltonian including the effect of the magnetic field.\n\nThe steady-state Schrödinger equation is simply Hψ = Eψ, where ψ is the wavefunction and E is the corresponding eigenvalue (energy level).\n\nLet me break it down. Without the magnetic field, the Hamiltonian is H0 = p²/(2m_e) + V(r), where V(r) is the Coulomb potential, i.e., -e²/(4πε₀ r) in SI units or proportional to 1/r otherwise.\n\nWith the magnetic field, the new Hamiltonian H becomes:\n\nH = [ (p - qA)^2 ] / (2m_e) + V(r)\n\nSubstituting A = -½ r × B gives:\n\n(p - q*(-½ r × B))² / (2m_e) + V(r)\n= [ (p + (q/2) r × B )² ] / (2m_e) + V(r)\n\nExpanding the square:\n\n[ p² + (q/2 r × B)·(2p) + (q/2 r × B)² ] / (2m_e) + V(r)\n\nSimplify term by term:\n\nFirst term: p²/(2m_e)\n\nSecond term: (q/m_e) (r × B) · p\n\nThird term: (q²)/(8 m_e) |r × B|²\n\nFourth term: V(r)\n\nTherefore, putting all together:\n\nH = H0 + (q/m_e)(r × B) · p + (q²)/(8 m_e)|r × B|²\n\nWhere H0 is the original Hydrogen Hamiltonian.\n\nNow, note that (r × B) · p is equal to (B · (r × p)), because cross product is anti-commutative? Wait, more accurately, (r × B) · p = B · (r × p) because dot product is invariant under cyclic permutations. Yes, indeed, (a×b)·c = a·(b×c). Therefore, (r × B) · p = B · (r × p )\n\nAnd also, |r × B|^2 = |B|² |r|² sin²θ, where θ is angle between r and B. Assuming B is along some direction, say z-axis, then |r × B| simplifies accordingly.\n\nHowever, before getting bogged down in coordinate choices, let's see what kind of operator we get here.\n\nNote that L = r × p is angular momentum. Hence, (r × B) · p = B · L\n\nSimilarly, |r × B|² = |B|² |r|² - (r · B)^2\n\nThus, expanding further, the third term becomes (q²)/(8 m_e)[ |B|² r² - (r · B)^2 ]\n\nPutting everything together, the total Hamiltonian is:\n\nH = H0 + (q/m_e) B · L + (q²)/(8 m_e) [|B|² r² - (r · B)^2]\n\nThis looks complicated, but perhaps manageable.\n\nAlternatively, recognizing that in quantum mechanics, especially for perturbations, only linear terms matter if higher-order terms are negligible. Here, though, even quadratic terms arise because of the expansion.\n\nSince the magnetic field is weak, perhaps we treat the entire addition as a small perturbation. That makes sense because diamagnetism arises from the response to a weak field.\n\nSo, proceeding to part (c): Treating the terms from the magnetic field as a perturbation, compute the shift in the ground state energy.\n\nPerturbation theory tells us that the first-order energy shift ΔE^(1) is ⟨ψ|H’|ψ⟩, where H’ is the perturbing Hamiltonian.\n\nFrom above, H' consists of two main contributions: the linear term in B, which is (q/m_e) B · L, and the quadratic term, which is (q²)/(8 m_e)[ |B|² r² - (r · B)^2 ]. Additionally, the zeroth order is H0.\n\nBut wait, in reality, the perturbation comes entirely from the interaction with the magnetic field, meaning H’ is the sum of those three extra terms beyond H0.\n\nBut hang on, in the initial substitution, did we correctly split H into H0 and H'? Actually, initially, H was written as H0 plus other terms. Thus, H’ comprises the rest.\n\nBut now, thinking again, the actual perturbation is the change caused by the presence of the magnetic field. So, starting from H0, adding the terms due to B.\n\nLooking back, after substitution, H had three added terms relative to H0:\n\nTerm1: (q/m_e) B · L\n\nTerm2: (q²)/(8 m_e) |B|² r²\n\nTerm3: - (q²)/(8 m_e) (r · B)^2\n\nSo, altogether, H’ = Term1 + Term2 + Term3\n\nTo find the first-order correction, we take expectation value of H’.\n\nCompute ΔE^(1) = ⟨ψ_0| H’ |ψ_0⟩\n\nWhich breaks into three integrals:\n\nΔE^(1) = (q/m_e)⟨L·B⟩ + (q²)/(8 m_e)⟨|B|²"}
{"id": "quantum/3-3023", "questions": "Consider an electron in a uniform magnetic field in the positive $z$ direction. The result of a measurement has shown that the electron spin is along the positive $x$ direction at $t = 0$. Use Ehrenfest theorem to compute the probability for $t > 0$ that the electron is in the state (a) $s_x = 1/2$, (b) $s_x = -1/2$, (c) $s_y = 1/2$, (d) $s_y = -1/2$, (e) $s_z = 1/2$, (f) $s_z = -1/2$. \n\nEhrenfest theorem states that the expectation values of a quantum mechanical operator obey the classical equation of motion. \n\n[Hint: Recall the connection between expectation values and probability considerations].", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about an electron in a uniform magnetic field pointing in the positive z-direction. At time t=0, they measured the electron's spin and found it along the positive x-direction. Now, using Ehrenfest's theorem, I need to find the probabilities for different spin components at later times t>0.\n\nFirst off, let me recall what Ehrenfest's theorem says. It relates the time evolution of expectation values in quantum mechanics to their corresponding classical equations of motion. Specifically, it tells us that d⟨A⟩/dt equals iℏ times ⟨[H,A]⟩ plus any explicit time dependence of A if there is one. So, mathematically,\n\n\\[\n\\frac{d}{dt} \\langle A \\rangle = \\frac{i}{\\hbar} \\langle [H, A] \\rangle + \\left\\langle \\frac{\\partial A}{\\partial t} \\right\\rangle.\n\\]\n\nIn our case, since we're dealing with spin operators which don't explicitly depend on time, the second term drops out. Therefore, the rate of change of the expectation value of an observable depends only on its commutator with the Hamiltonian.\n\nNow, considering the system here—an electron in a magnetic field—the interaction Hamiltonian should be something like H = γ B · S, where γ is the gyromagnetic ratio, but more specifically for electrons, it would involve the Bohr magneton μ_B. But wait, actually, the exact form might not matter immediately because perhaps we can work in units or express things symbolically.\n\nBut maybe another approach—since the magnetic field is in the z-direction, the Hamiltonian simplifies to H proportional to σ_z, right? Because the dot product becomes just the component in the z-direction. Let me write that as:\n\n\\[\nH = \\omega s_z,\n\\]\nwhere ω incorporates all constants including the strength of the magnetic field and other factors. Since the energy levels split due to Zeeman effect, each eigenstate will precess around the z-axis.\n\nWait, hold on—if the initial state is |+x>, meaning the spin is up along x, then how does it evolve under H?\n\nI think the key idea is that when you measure the spin along x at t=0, the wavefunction collapses into the eigenstate of S_x with eigenvalue ħ/2. Then, after that, the state evolves according to the Schrödinger equation governed by H.\n\nSo, first, let me figure out the time evolution of the state. If the initial state is |ψ(0)⟩ = |+x⟩, then\n\n|ψ(t)⟩ = e^{-iHt/ħ} |+x⟩.\n\nSince H is diagonal in the z-basis, expressing |+x⟩ in terms of |+z⟩ and |-z⟩ could help. Remembering that:\n\n|+x⟩ = (|+z⟩ + |-z⟩)/√2.\n\nTherefore,\n\n|ψ(t)⟩ = e^{-iω t s_z / ħ} (|+z⟩ + |-z⟩)/√2.\n\nAssuming that s_z is in units such that s_z |±z⟩ = ±½ ℏ |±z⟩, so substituting back, we get:\n\nEach exponential factor acting on |+z⟩ gives e^{-iω t (½)} and similarly for |-z⟩, giving e^{iω t (½)}. Wait, no—it’s negative exponentials multiplied by energies. Hmm, better double-check.\n\nThe time evolution operator U = exp(-i H t / ħ). Given H = ω s_z, so plugging in:\n\nU = exp(-i ω s_z t / ħ).\n\nFor |+z⟩, s_z = ħ/2, so applying U gives exp(-i ω (ħ/2) t / ħ ) = exp(-i ω t / 2 ). Similarly, for |-z⟩, s_z = -ħ/2, so U applied gives exp(i ω t / 2 ).\n\nThus,\n\n|ψ(t)⟩ = [exp(-i ω t / 2)|+z⟩ + exp(+i ω t / 2)|-z⟩]/√2.\n\nSimplify this expression:\n\nFactor out exp(-i ω t / 4):\n\nWait, alternatively, notice that both exponents differ by sign. Maybe writing them together:\n\nLet me denote θ = ω t / 2.\n\nThen,\n\n|ψ(t)⟩ = [e^{-iθ}|+z⟩ + e^{iθ}|-z⟩ ] / √2.\n\nThis looks similar to a superposition of two phases rotating in opposite directions over time.\n\nAlternatively, combining these terms, note that:\n\n[e^{-iθ} |+z⟩ + e^{iθ} |-z⟩ ] / √2 = cos(θ)(|+z⟩ + |-z⟩)/√2 + (-i sin(theta))( |+z⟩ - |-z⟩ )/(i sqrt(2)) )\n\nHmm, maybe instead of trying to rewrite, consider computing the expectation values directly.\n\nGiven that, now, to use Ehrenfest's theorem, we want to track the expectation values of S_x, S_y, S_z over time.\n\nAt t=0, the expectation values are known. For example, since the state was |+x⟩, ⟨S_x⟩ = ħ/2, while ⟨S_y⟩ = 0, ⟨S_z⟩ = 0.\n\nAs time progresses, these expectations will oscillate based on the dynamics induced by H.\n\nUsing Ehrenfest's theorem, let's set up differential equations for each expectation value.\n\nStarting with S_x:\n\nCompute d/dt ⟨S_x⟩ = (i/ħ) ⟨ [H, S_x] ⟩.\n\nSimilarly for S_y and S_z.\n\nWe know that H = ω S_z, so [H, S_x] = ω [S_z, S_x].\n\nRecall that [S_i, S_j] = i ε_{ijk} S_k * ħ^2 ?\n\nWait, correction: Actually, angular momentum algebra tells us that [S_i, S_j] = i ħ ε_{ijk} S_k.\n\nYes, correct. So,\n\n[S_z, S_x] = i ħ S_y.\n\nTherefore,\n\n[H, S_x] = ω [S_z, S_x] = i ω ħ S_y.\n\nPlugging into Ehrenfest:\n\nd/dt ⟨S_x⟩ = (i/ħ) ⟨i ω ħ S_y⟩ = (i/ħ)(i ω ħ) ⟨S_y⟩ = (-ω) ⟨S_y⟩.\n\nSimilarly, moving on to S_y:\n\nd/dt ⟨S_y⟩ = (i/ħ) ⟨ [H, S_y] ⟩.\n\nAgain, [H, S_y] = ω [S_z, S_y] = ω (-i ħ S_x), from [S_z, S_y]= -i ħ S_x.\n\nThus,\n\nd/dt ⟨S_y⟩ = (i/ħ) ⟨ -i ω ħ S_x ⟩ = (i/ħ)(-i ω ħ) ⟨S_x⟩ = ω ⟨S_x⟩.\n\nAnd finally, for S_z:\n\nd/dt ⟨S_z⟩ = (i/ħ) ⟨ [H, S_z] ⟩.\n\nBut [H, S_z] = ω [S_z, S_z] = 0.\n\nSo, d/dt ⟨S_z⟩ = 0 ⇒ ⟨S_z⟩ remains constant over time.\n\nFrom the initial condition, at t=0, the state is |+x⟩, so ⟨S_z⟩ = 0 initially; hence, it stays zero forever.\n\nAlright, summarizing the coupled ODEs:\n\nd/dt ⟨S_x⟩ = -ω ⟨S_y⟩  \nd/dt ⟨S_y⟩ = ω ⟨S_x⟩  \nd/dt ⟨S_z⟩ = 0  \n\nWith initial conditions:\n⟨S_x⟩(0) = ħ/2  \n⟨S_y⟩(0) = 0  \n⟨S_z⟩(0) = 0  \n\nThese look familiar—they’re the same as those governing simple harmonic motion. Indeed, solving them leads to sinusoidal behavior.\n\nTo solve, take derivatives again. From the first equation:\n\nddt² ⟨S_x⟩ = -ω d/dt ⟨S_y⟩ = -ω*(ω ⟨S_x⟩) = -ω² ⟨S_x⟩.\n\nWhich is the standard SHM equation:\n\nddt² ⟨S_x⟩ + ω² ⟨S_x⟩ = 0.\n\nSolution is:\n\n⟨S"}
{"id": "quantum/5060", "questions": "An atom is in a state of total electron spin $S$, total orbital angular momentum $L$, and total angular momentum $J$. (The nuclear spin can be ignored for this problem). The $z$ component of the atomic total angular momentum is $J_z$. By how much does the energy of this atomic state change if a weak magnetic field of strength $B$ is applied in the $z$  direction? Assume that the interaction with the field is small compared with the fine structure interaction.\n\nThe answer should be given as an explicit expression in terms of the quantum numbers $J, L, S, J_z$ and natural constants.", "graphs": null, "llm_answers": "<think>\nOkay, so I have to figure out how the energy of an atomic state changes when a weak magnetic field B is applied along the z-direction. Hmm, let's start from what I remember about atoms in magnetic fields.\n\nI know that electrons have both orbital angular momentum and spin angular momentum. In the absence of external fields, these contribute to the total angular momentum through their coupling—like the Russell-Saunders or LS coupling. But here, we're considering applying a magnetic field, which introduces some interactions.\n\nWait, the question mentions that the interaction with the field is small compared to the fine structure interaction. So maybe it's like perturbation theory where the Zeeman effect comes into play but not too strong?\n\nRight, the Zeeman effect deals with the splitting of spectral lines due to a magnetic field. There are two types: normal and anomalous. Since the magnetic field is weak, perhaps we’re dealing with the linear Zeeman effect rather than quadratic effects.\n\nIn the linear case, the energy shift depends on the magnetic dipole moment interacting with the magnetic field. For each particle, there’s a contribution from its orbital motion and its intrinsic spin.\n\nSo, the Hamiltonian for the interaction would involve something like H = -μ·B, right? Where μ is the magnetic moment.\n\nBut wait, since the nucleus has negligible spin, only the electrons' contributions matter. Each electron contributes a term involving its own g-factor times its angular momentum components.\n\nLet me recall the formula for the magnetic moment operator. It's usually written as:\n\n\\[\n\\mu = -g_l \\frac{e}{2m} L - g_s \\frac{e}{2m} S\n\\]\n\nWhere \\( g_l \\) is approximately 1 for orbitals and \\( g_s \\) is around 2 for spins because of the gyromagnetic ratio difference between orbital and spin angular momenta.\n\nHowever, sometimes people write the overall factor differently. Let me think—the Bohr magneton is defined as \\( \\mu_B = \\frac{e \\hbar}{2 m_e c} \\), but depending on units, factors might vary. Wait, actually, in Gaussian units, things get more complicated, but since the user didn't specify, probably using SI-like expressions without worrying about unit systems.\n\nAlternatively, another approach is to express everything in terms of known constants. Maybe better to use the standard form for the Zeeman Hamiltonian.\n\nFor a uniform magnetic field B in the z-direction, the potential energy U is given by the dot product of the magnetic moment vector and the magnetic field. That is,\n\nU = -μ · B\n\nSince B is along z, this simplifies to U = -μ_z * B.\n\nNow, breaking down μ into orbital and spin parts:\n\nμ_orbital = -(e/(2m)) * L / c ?\n\nWait, no—I need to make sure about the correct expression. Actually, the magnetic moment due to orbital motion is given by:\n\nμ_L = (-e/2m) * L × something... Or wait, isn’t it proportional to L multiplied by charge over mass?\n\nHold on, perhaps it's clearer to consider the Landé g-factors. The total magnetic moment is:\n\nμ_total = g_J * μ_B * J + ... No, wait—that applies per electron, but in our case, the entire atom's angular momentum is considered.\n\nActually, for the whole atom, the total magnetic moment is the sum of all individual electrons’ moments. However, since they've already coupled together into total L, S, and then J=L+S, perhaps we can treat them collectively.\n\nHmm, yes, because the problem states \"total\" quantities: S, L, J. Therefore, instead of treating each electron separately, we can model the total magnetic moment based on the combined angular momenta.\n\nTherefore, the magnetic moment operator for the atom would be:\n\nμ = μ_L + μ_S\n\nwhere μ_L is the orbital part and μ_S is the spin part.\n\nExpressed in terms of L and S:\n\nμ_L = (−(e)/(2m)) * L\n\nand \n\nμ_S = −g_s*(e/(2m))*S\n\nHere, e is positive, so negative signs indicate the opposite orientation relative to current loops.\n\nBut wait, the actual sign conventions depend on whether you take the charge as negative for electrons. Oh yeah, electrons have negative charge, so the magnetic moment is opposite to the angular momentum direction.\n\nHence, correcting myself, the orbital magnetic moment is:\n\nμ_L = (e/(2m)) * L\n\nSimilarly, the spin magnetic moment is:\n\nμ_S = g_s*(e/(2m)) * S\n\nBecause the negative charge flips the direction, making the magnetic moment point oppositely to the angular momentum.\n\nThus, combining both, the total magnetic moment is:\n\nμ = [ (e/(2m)) ] * ( L + g_s S )\n\nThen, the interaction energy is U = -μ · B.\n\nGiven that B is along z-axis, U becomes:\n\nU = -μ_z * B = -[ (e/(2m)) ( L_z + g_s S_z ) ] * B\n\nBut hold on, the problem gives us J, which is the total angular momentum, i.e., J = L + S. Hence, J_z = L_z + S_z.\n\nIf I could express L_z and S_z in terms of J_z, but unfortunately, unless we know specific relations, it's tricky. Alternatively, note that the expectation value <L_z> and <S_z> relate to the projections of L and S onto z-axis.\n\nBut wait, in the presence of a magnetic field, the system will align such that J_z is quantized. Without knowing the exact relation between L and S, though, expressing U purely in terms of J may require additional steps.\n\nAlternatively, recalling that the Landé g-factor relates the total angular momentum to the magnetic moment. Specifically, the total magnetic moment is often expressed as:\n\nμ = g_J * μ_B * J\n\nWhere g_J is the Landé g-factor given by:\n\ng_J = 1 + (J(J+1) + S(S+1) - L(L+1))/(2J(J+1))\n\nThis accounts for the contributions from both orbital and spin angular momenta.\n\nBut wait, do I really need to go via the Landé g-factor? Because the problem asks for an explicit expression in terms of J, L, S, etc., so maybe directly computing the energy shift using the individual contributions is preferable.\n\nGoing back, the energy shift ΔE is equal to the expectation value of the Hamiltonian, which is the interaction energy U.\n\nAssuming the state |n l s j m_j>, the expectation values of L_z and S_z are respectively ħ m_l and ħ m_s, but since J_z = L_z + S_z, and m_j = m_l + m_s, we don't necessarily know m_l and m_s individually.\n\nBut wait, in the basis of good quantum numbers J^2, J_z, L^2, S^2, the matrix elements of L_z and S_z aren't diagonal. Thus, taking expectation values naively won't work because ⟨L_z⟩ and ⟨S_z⟩ aren't simply related to J_z.\n\nAh, that complicates matters. Perhaps another way is needed.\n\nWait, alternatively, thinking about the fact that the perturbing Hamiltonian is H' = -μ·B ≈ -μ_z B.\n\nWhich translates to:\n\nH' = (e B / (2m)) (L_z + g_s S_z)\n\nWe can compute the first-order energy correction as ⟨ψ| H' |ψ⟩.\n\nBut ψ is an eigenstate of J², J_z, L², S². To find ⟨L_z⟩ and ⟨S_z⟩ in this state.\n\nIs there any selection rule or identity that allows us to express ⟨L_z⟩ and ⟨S_z⟩ in terms of J_z?\n\nWell, since J = L + S, we have J_z = L_z + S_z ⇒ ⟨J_z⟩ = ⟨L_z⟩ + ⟨S_z⟩.\n\nAlso, we can square J:\n\n⟨J²⟩ = ⟨(L + S)^2⟩ = ⟨L²⟩ + ⟨S²⟩ + 2⟨L·S⟩\n\nBut since L and S commute among themselves, except for cross terms.\n\nMoreover, we also have:\n\n⟨L·S⟩ = [⟨J²⟩ - ⟨L²⟩ - ⟨S²⟩]/2\n\nBut I'm trying to see if I can find ⟨L_z⟩ and ⟨S_z⟩ in terms of other variables.\n\nAnother thought: If the state is characterized by maximum possible alignment, meaning that L and S are aligned as much as possible, but generally, they can have different orientations.\n\nWait, but even within a fixed J, L and S can precess independently, leading to varying angles between them. This suggests that ⟨L_z⟩ and ⟨S_z⟩ cannot be determined solely from J_z; however"}
{"id": "quantum/5067", "questions": "Positronium consists of an electron and a positron bound by their Coulomb attraction.\n\n(a) What is the radius of the ground state? The binding energy of the ground state?\n\n(b) The singlet and triplet ground states are split by their spin-spin interaction such that the singlet state lies about $10^{-3}$ volts below the triplet state. Explain the behavior of positronium in a magnetic field. Draw an energy level diagram to illustrate any dependence on the magnetic field.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about positronium, which is made up of an electron and a positron bound together by Coulomb attraction. It's part (a), asking for the radius of the ground state and its binding energy. Hmm, let me think through how to approach this.\n\nFirst off, I remember that positronium behaves similarly to hydrogen because both consist of two particles with opposite charges orbiting each other due to electromagnetic forces. In hydrogen, it's a proton and an electron; here, it's an electron and a positron. So maybe I can use similar formulas as those used for hydrogen atoms but adjust them accordingly since the masses might be different.\n\nWait, right! Because in hydrogen, we consider the reduced mass of the system when calculating things like orbital radii or energies. Since the electron and positron have equal mass (the same as electrons, actually), the calculations should simplify compared to hydrogen where the proton is much heavier than the electron.\n\nSo, starting with the Bohr model equations. For hydrogen-like systems, the radius of the nth orbit is given by:\n\n\\[ r_n = n^2 \\frac{\\epsilon_0 h^2}{\\pi m e^2} } \\]\n\nBut wait, no, more accurately, using the formula involving the reduced mass μ. Let me recall: \n\nThe general expression for the radius is \\( r_n = \\frac{n^2 \\hbar^2}{k_e Z e^2 \\mu} \\). Here, k_e is Coulomb's constant, Z is 1 for positronium, e is the elementary charge, and μ is the reduced mass of the electron-positron pair.\n\nSince both particles have the same mass, the reduced mass μ would just be half the mass of one particle. Wait, hold on—reduced mass is defined as \\( \\mu = \\frac{m_1 m_2}{m_1 + m_2} \\). If m1 = m2 = m_e, then μ becomes \\( \\frac{m_e^2}{2 m_e} = \\frac{m_e}{2} \\).\n\nGot it. So plugging into the radius equation:\n\nFor the ground state, n=1, so\n\n\\( r_1 = \\frac{(1)^2 (\\hbar)^2}{(8.988 \\times 10^9 N·m²/C²)(1)(e)^2 (\\frac{m_e}{2})} \\)\n\nSimplify denominator terms first. Let me compute all constants numerically.\n\nI know:\n- Planck’s constant ħ ≈ 1.0545718 × 10^-34 J·s,\n- Coulomb's constant k_e ≈ 8.988×10^9 N·m²/C²,\n- Elementary charge e ≈ 1.602×10^-19 C,\n- Electron mass m_e ≈ 9.109×10^-31 kg.\n\nPlugging these values in,\n\nNumerator: (ħ)^2 ≈ (1.0545718e-34)^2 ≈ 1.112e-68 J²·s².\n\nDenominator: k_e * e^2 * (m_e / 2)\nCompute each term:\n\nk_e ≈ 8.988e9 N·m²/C²,\ne^2 ≈ (1.602e-19)^2 ≈ 2.566e-38 C²,\n(m_e)/2 ≈ 4.5545e-31 kg.\n\nMultiply them together:\n\n8.988e9 * 2.566e-38 * 4.5545e-31 =\n\nLet me calculate step by step:\n\nFirst multiply 8.988e9 and 2.566e-38:\n\n≈ 8.988e9 * 2.566e-38 ≈ approximates to roughly 2.303e-28.\n\nThen multiply by 4.5545e-31:\n\n≈ 2.303e-28 * 4.5545e-31 ≈ approximately 1.048e-58.\n\nNow take numerator divided by denominator:\n\n1.112e-68 / 1.048e-58 ≈ ~1.06e-10 meters.\n\nWhich is around 1.06 angstroms. But wait, isn't the Bohr radius about 0.529 angstroms? That seems double. Oh, right, because in hydrogen, the nucleus has infinite mass approximation, making μ ≈ m_e/1 instead of m_e/2 here. Hence, our result makes sense—it should be twice the Bohr radius.\n\nYes, correct. Therefore, the radius of the ground state of positronium is approximately 2 times the Bohr radius, so 2*0.529 Å ≈ 1.058 Å. My calculation gave 1.06e-10 m, which matches exactly because 1Å is 1e-10 m. Perfect!\n\nNext, finding the binding energy of the ground state. Again, using the formula from the Bohr model adjusted for reduced mass.\n\nBinding energy E_n is given by:\n\nE_n = - (μ e^4)/(8 ε₀² h²) * (Z²/n²)\n\nAlternatively, another version uses Rydberg's constant multiplied by some factor considering reduced mass.\n\nIn hydrogen, the ground state energy is -13.6 eV. However, since the reduced mass here is larger (wait, no—the reduced mass is smaller because μ = m_e/2 vs. μ_hydrogen ≈ m_e/(1+mass_proton/mass_electron)).\n\nActually, in hydrogen, the reduced mass is almost m_e because the proton is so heavy. Whereas in positronium, the reduced mass is only m_e/2. Thus, the binding energy will scale inversely with the reduced mass.\n\nTherefore, if in hydrogen, E_H = -13.6 eV corresponds to μ = m_e*(1/(1 + M_p/m_e)), which is very close to m_e. So, scaling down μ by a factor of 2 increases the magnitude of the binding energy by a factor of 2.\n\nHence, E_positronium_ground_state = (-13.6 eV)*(m_e / μ ) ?\n\nWait, perhaps better to write the exact relation.\n\nEnergy levels for hydrogen-like atoms are given by:\n\nE_n = -(Ry) * (Z² /n²),\n\nwhere Ry is the Rydberg constant, R_y = (m_e e^4)/(8ε₀² h³ c)}... No, wait, alternatively, sometimes expressed differently depending on units.\n\nAnother way: E_n = - (μ e^4)/(8 ε₀² h²) * (Z² /n² )\n\nComparing to hydrogen, whose energy is E_n,H = - (m_e e^4)/(8 ε₀² h²) * (Z² /n² ). So, substituting μ = m_e/2 gives us E_n,P = - ( (m_e/2) e^4 )/(8 ε₀² h² ) * (Z² /n² ).\n\nThus, comparing to hydrogen, E_P = (m_e/2 / m_e)* E_H = (1/2) E_H.\n\nAh, yes, so the binding energy is halved. So, for the ground state (n=1), E = -13.6 eV * (1/2) = -6.8 eV.\n\nIs that accurate? Alternatively, thinking again—if you increase the mass, does the binding energy become stronger or weaker? Well, higher mass leads to lower kinetic energy, hence less tightly bound? Or wait, in reality, increasing the mass increases the gravitational force, but in electromagnetism...\n\nNo, in quantum mechanics, the binding energy depends inversely on the reduced mass. So, if μ decreases, meaning lighter effective mass, the binding energy becomes deeper (more negative). Wait, now I'm confused.\n\nHold on, let's clarify. From the formula:\n\nE ∝ -μ.\n\nBecause E_n = - (something) * μ.\n\nIf μ is smaller, |E| is smaller. So, in positronium, μ is m_e/2, whereas in hydrogen, μ is nearly m_e. So, indeed, the binding energy is half as deep. So, E_positronium = -6.8 eV.\n\nThat sounds consistent. Another check: the Bohr radius scales as 1/μ, so if μ halves, the radius doubles—which aligns with what we found earlier. Similarly, the binding energy scales as μ, so if μ halves, the binding energy also halves.\n\n"}
{"id": "quantum/6040", "questions": "The neutral K-meson states $|K^0\\rangle$ and $|\\bar{K}^0\\rangle$ can be expressed in terms of states $|K_L\\rangle$, $|K_S\\rangle$:\n\n$$\n|K^0\\rangle = \\frac{1}{\\sqrt{2}} (|K_L\\rangle + |K_S\\rangle),\n$$\n\n$$\n|\\bar{K}^0\\rangle = \\frac{1}{\\sqrt{2}} (|K_L\\rangle - |K_S\\rangle),\n$$\n\nwhere $\\left| K_L \\right>$ and $\\left| K_S \\right>$ are states with definite lifetimes $\\tau_L = \\frac{1}{\\gamma_L}$ and $\\tau_S = \\frac{1}{\\gamma_S}$ and distinct rest energies $m_Lc^2 \\neq m_Sc^2$. At time $t = 0$, a meson is produced in the state $\\left| \\psi(t = 0) \\right> = \\left| K^0 \\right>$. Let the probability of finding the system in state $\\left| K^0 \\right>$ at time $t$ be $P_0(t)$ and that of finding the system in state $|\\bar{K}^0 \\rangle$ at time $t$ be $\\bar{P}_0(t)$. Find an expression for $P_0(t) - \\bar{P}_0(t)$ in terms of $\\gamma_L, \\gamma_S, m_Lc^2$ and $m_Sc^2$. Neglect $CP$ violation.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about neutral kaons, which are these particles in particle physics. The question gives me how to express the |K⁰⟩ and |\\(\\overline{K}\\)^0⟩ states in terms of |K_L⟩ and |K_S⟩. It also mentions their lifetimes τ_L and τ_S as reciprocals of γ_L and γ_S respectively. Additionally, they have different rest energies: m_L c² ≠ m_S c².\n\nAt t=0, the meson is in the state |ψ(0)⟩ = |K⁰⟩. We need to find P₀(t) − \\(\\overline{P}\\)_0(t), where P₀(t) is the probability of being in |K⁰⟩ and \\(\\overline{P}\\)_0(t) is the probability of being in |\\(\\overline{K}\\)^0⟩ at time t. And we're told to neglect CP violation, which probably means some symmetries or simplifications apply here.\n\nFirst, let's recall what each state represents. In neutral kaon systems, there are two physical states: the long-lived KL and short-lived KS. These correspond to eigenstates of the mass matrix but not necessarily the weak interaction eigenstates. So when you produce a K⁰ or its antiparticle, it starts off in one of those superpositions given above.\n\nGiven:\n|K⁰⟩ = (|KL⟩ + |KS⟩)/√2,\n|\\(\\overline{K}\\)^0⟩ = (|KL⟩ − |KS⟩)/√2.\n\nSo if our initial state is |K⁰⟩, then ψ(0) = |K⁰⟩. To find the probabilities later on, we'll need to evolve this state over time under the Hamiltonian H.\n\nBut wait, since the states |KL⟩ and |KS⟩ have definite masses and decay widths? Or rather, definite lifetimes because τ is related to Γ via τ ≈ ħ/Γ. But actually, in relativistic quantum mechanics, the energy E would include both the rest mass and any kinetic contributions, but perhaps here we consider them at rest?\n\nWait, no—their rest energies differ, meaning even without considering motion, their masses are different. That complicates things because usually, in simple two-level systems like spin-½ particles, the basis states might share the same mass, making evolution simpler. Here, though, since m_L c² ≠ m_S c², their energies will contribute differently to the phase factors during time evolution.\n\nLet me think—each state evolves according to e^{-iHt}, right? Since H includes both the mass term and possibly other interactions. However, assuming non-relativistic speeds isn't applicable here; better stick to relativistic formalism.\n\nIn general, for a state |n⟩ with energy E_n, the time-evolved state is e^{-iE_n t /ħ}|n⟩. Assuming natural units where ħ=c=1, this becomes e^{-iE_n t}. \n\nHowever, in reality, these states aren't stable—they decay. So maybe instead of just having phases from their energies, they also undergo exponential decay due to their finite lifetimes. Thus, the time evolution should account for both oscillations (from differing energies) and decays (from lifetimes).\n\nTherefore, the time-dependent wavefunction components for |KL⟩ and |KS⟩ would involve factors like e^{-γ_j t} multiplied by e^{-im_j t}, where j=L,S. Wait—is that correct? Because typically, unstable particles' time dependence combines exponential decay (due to width Γ ~ 1/τ) and oscillatory phase (from their complex energy). Hmm.\n\nYes, more accurately, the time evolution factor for each state is e^{-(i m_j t - γ_j t)}, combining both the phase rotation from the mass-energy and the decay from the lifetime. Alternatively, written as e^{-i(m_j + i γ_j)t} = e^{-i m_j t}e^{-γ_j t}.\n\nThus, starting from |K⁰⟩, which is equal to (|KL⟩ + |KS⟩)/√2, after time t, the state |ψ(t)⟩ would be [e^{-i(m_L + i γ_L)t} |KL⟩ + e^{-i(m_S + i γ_S)t} |KS⟩]/√2.\n\nNow, to compute the probabilities P₀(t) and \\(\\overline{P}\\)_0(t), we need to project onto |K⁰⟩ and |\\(\\overline{K}\\)^0⟩ respectively.\n\nRecall that projection works as follows: If your current state is |ψ(t)⟩, then the amplitude for |φ⟩ is ⟨φ|ψ(t)⟩, and the probability is the modulus squared of that amplitude.\n\nSo first, write down expressions for P₀(t):\n\n⟨K⁰|ψ(t)⟩ = ⟨K⁰|(e^{-i(m_L + i γ_L)t} |KL⟩ + e^{-i(m_S + i γ_S)t} |KS⟩ ) / √2.\n\nSimilarly,\n\n⟨K⁰|KL⟩ = 1/√2,\n⟨K⁰|KS⟩ = 1/√2.\n\nBecause |K⁰⟩ is defined as (|KL⟩ + |KS⟩)/√2, hence inner product with itself is 1, etc.\n\nPlugging into the equation:\n\n⟨K⁰|ψ(t)⟩ = [⟨K⁰|KL⟩ e^{-i(m_L + i γ_L)t} + ⟨K⁰|KS⟩ e^{-i(m_S + i γ_S)t}] / √2\n\nWhich equals [ (1/√2)e^{-i(m_L + i γ_L)t } + (1/√2)e^{-i(m_S + i γ_S)t } ] / √2\n\nSimplify denominator: (√2 * √2)=2, so overall factor is 1/(√2*√2)=1/2.\n\nWait, hold on—that seems incorrect. Let me re-express carefully.\n\nActually, expanding out:\n\n⟨K⁰|ψ(t)⟩ = (1/√2)[⟨K⁰|KL⟩ e^{-i(m_L + i γ_L)t} + ⟨K⁰|KS⟩ e^{-i(m_S + i γ_S)t}]\n\nSince ⟨K⁰|KL⟩ = 1/√2 and similarly ⟨K⁰|KS⟩ = 1/√2, substituting back:\n\n⟨K⁰|ψ(t)⟩ = (1/√2)( (1/√2) e^{-i(m_L + i γ_L)t} + (1/√2) e^{-i(m_S + i γ_S)t} )\n\nMultiply constants: (1/√2)*(1/√2) = 1/2.\n\nSo,\n\n⟨K⁰|ψ(t)⟩ = (1/2)[ e^{-i(m_L + i γ_L)t} + e^{-i(m_S + i γ_S)t} ]\n\nThen, P₀(t) is the square magnitude of this:\n\nP₀(t) = |⟨K⁰|ψ(t)⟩|^2 = (1/4)| e^{-i(m_L + i γ_L)t} + e^{-i(m_S + i γ_S)t} |²\n\nCompute the absolute value squared:\n\nThis involves multiplying the sum by its conjugate:\n\n[ e^{i(m_L - i γ_L)t} + e^{i(m_S - i γ_S)t} ][ e^{-i(m_L + i γ_L)t} + e^{-i(m_S + i γ_S)t} ]\n\nMultiplying term-by-term:\n\nTerm 1: e^{i(m_L - i γ_L)t} * e^{-i(m_L + i γ_L)t} = e^{i m_L t - i γ_L t - i m_L t"}
{"id": "quantum/6014", "questions": "In scattering from a potential $V(r)$, the wave function may be written as an incident plane wave plus an outgoing scattered wave: $\\psi = e^{ikz} + v(r)$. Derive a differential equation for $v(r)$ in the first Born approximation.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about scattering from a potential V(r). The question is asking me to derive a differential equation for v(r) using the first Born approximation. Hmm, let's see.\n\nFirst off, I remember that when dealing with scattering problems in quantum mechanics, we often use the concept of partial waves or sometimes approximations like the Born series. Since it mentions the first Born approximation, I think that refers to treating the potential perturbatively—like expanding the solution in terms of powers of the potential strength.\n\nThe given wavefunction is ψ = e^(i k z) + v(r), where e^(i k z) is the incident plane wave, and v(r) represents the scattered wave. So, essentially, they're writing the total wavefunction as the sum of the incoming wave and some correction term due to the potential, which is what gets scattered.\n\nI recall that Schrödinger's equation governs such systems. For a particle in a potential V(r), the time-independent Schrödinger equation (TISE) would look something like:\n\n[ -ħ²/(2m) ∇² + V(r) ] ψ(r) = E ψ(r)\n\nBut since ħ and m can be set to 1 in appropriate units without loss of generality, maybe? Or perhaps just keep them explicit—it might depend on how detailed the derivation needs to be.\n\nSo substituting ψ into TISE gives us:\n\n[-(∇²)/2m + V] [e^(i k z) + v(r)] = E [e^(i k z) + v(r)]\n\nLet me write out each part separately. First, expand the left-hand side:\n\n- (∇²/2m)(e^(i k z)) - (∇²/2m)v(r) + V(e^(i k z)) + V(v(r))\n\nAnd the right-hand side becomes:\n\nE e^(i k z) + E v(r)\n\nNow, rearranging everything, moving all terms involving e^(i k z) to one side and those involving v(r) to another. Let's group similar terms together.\n\nStarting with the kinetic energy operator acting on e^(i k z):\n\n-(∇²/2m)e^(i k z) \n\nSince e^(i k z) is a plane wave traveling along the z-axis, its Laplacian should give -(k^2) e^(i k z). Because the second derivative of e^(i k z) with respect to x, y, z... Wait, actually, no—the Laplacian in three dimensions is the sum of the second derivatives over x, y, z. But if the wave is only varying in z-direction, then the Laplacian reduces to d²/dz² times e^(i k z).\n\nWait, hold on—if you take the gradient squared of e^(i k · r), assuming k has components (0,0,k_z)=k, but here it's specifically e^(i k z). Then yes, the Laplacian will act as follows:\n\n∇² e^(i k z) = (-k²) e^(i k z)\n\nBecause the second derivative w.r.t. z is -k² e^(i k z), while the other spatial derivatives don't contribute because there's no dependence on x or y.\n\nTherefore,\n\n-(∇² / 2m) e^(i k z) = (k²)/(2m) e^(i k z)\n\nSimilarly, the same applies to any multiple of e^(i k z); their Laplacians bring down factors of k².\n\nMoving back to our expansion:\n\nLeft-hand side after substitution:\n= [ (k²)/(2m) e^(i k z) ) ] \n+ [ - (∇²/2m) v(r) ]\n+ V e^(i k z)\n+ V v(r)\n\nRight-hand side:\n= E e^(i k z) + E v(r)\n\nBringing all terms to one side:\n\n(k²/(2m) - E) e^(i k z) + [ - (∇²/2m) v(r) + V v(r) - E v(r) ] + V e^(i k z) = 0\n\nHmm, wait—that seems messy. Maybe instead, collect coefficients for e^(i k z) and v(r) separately.\n\nLooking again:\n\nFrom LHS minus RHS equals zero:\n\n[(k²/(2m) - E) e^(i k z) + V e^(i k z)] + [- (∇²/2m) v(r) + (V - E) v(r)] = 0\n\nFactorizing e^(i k z):\n\n[k²/(2m) - E + V] e^(i k z) + [ - (∇²/2m) + (V - E) ] v(r) = 0\n\nThis must hold true everywhere, meaning both coefficients must individually vanish?\n\nWell, not exactly. It depends on whether these functions form a complete basis or not. In reality, the entire expression must equal zero identically, so each component orthogonal to others must independently cancel.\n\nAssuming orthogonality between e^(i k z) and v(r)—which isn’t necessarily straightforward—but perhaps under certain conditions, especially considering that v(r) arises from the interaction with V(r), which could lead to different angular dependencies.\n\nAlternatively, perhaps more accurately, we consider that the homogeneous solutions (plane waves) satisfy the free-particle Schrödinger equation, i.e., [ - (∇²/2m) - E ] e^(i k z) = 0 provided that |k|²/(2m) = E. That makes sense; the plane wave satisfies the free Schrödinger equation.\n\nGiven that, let’s note that E = k²/(2m). Therefore, plugging that into the coefficient multiplying e^(i k z):\n\nCoefficient becomes [k²/(2m) - E + V] e^(i k z) = [0 + V] e^(i k z) = V e^(i k z)\n\nThus, bringing things together:\n\nV e^(i k z) + [ - (∇²/2m) + (V - E) ] v(r) = 0\n\nWhich implies:\n\n[ - (∇²/2m) + (V - E) ] v(r) = - V e^(i k z )\n\nThat looks better. Now, solving for v(r), we get:\n\nv(r) ≈ [ - (∇²/2m) + (V - E) ]^{-1} [ - V e^(i k z) ]\n\nBut inverting operators directly is tricky unless we express it via Green's functions. Alternatively, recognizing that in the Born approximation, particularly the first-order term, we neglect higher order terms beyond linear in V.\n\nWait, recalling the Born approximation method: the idea is to treat the potential perturbatively. Starting from the unperturbed state (free particle), the first-order correction comes from integrating the potential against the initial state.\n\nSpecifically, the integral equation approach leads to expressions where the scattered wave v(r) is proportional to the Fourier transform of the potential convolved with the plane wave.\n\nBut stepping through it systematically, starting from the above equation:\n\n(-∇²/(2m) + V - E) v(r) = - V e^(i k z)\n\nWe can rewrite this as:\n\n(H_0 + V - E) v(r) = - V e^(i k z),\n\nwhere H_0 is the free Hamiltonian, H_0 = -∇²/(2m).\n\nIf we assume that V is small enough that we can approximate the inverse of (H_0 + V - E)^{-1}, then in the first Born approximation, we ignore the effect of V inside the inverse operator except once. Thus, approximately:\n\nv(r) ≈ (H_0 - E)^{-1} [ - V e^(i k z) ]\n\nWhere (H_0 - E)^{-1} is the resolvent operator for the free particle case.\n\nExpressed explicitly, applying the resolvent operator corresponds to convolution with the Green's function G(r', r'') = <r'|G|r''> which solves (H_0 - E) G(r, r') = δ(r - r'). The Green's function in position space is known to involve Hankel functions or Bessel functions depending on dimensionality, but in momentum space, it's simpler.\n\nHowever, working in momentum space might make this easier. Remembering that the Fourier transform diagonalizes the Laplacian.\n\nRecall that the action of (H_0 - E)^{-1} on a function f(r) is equivalent to taking the Fourier transform of f(r), dividing by (ω_k'^2 - ω_k^2) where ω relates to energy, etc.—but perhaps getting bogged down.\n\nAlternatively, knowing that the Green's function in 3D is:\n\nG(r, r'; E) = (1/(4π)) * (1/r) e^{i k r},\n\nassuming r > r', and k = sqrt(2mE)/ħ, though setting constants appropriately.\n\nBut regardless, the key point is that applying (H_0 - E)^{-1} to a plane wave involves delta functions in momentum space.\n\nWait, perhaps going to momentum representation helps."}
{"id": "quantum/1-1025", "questions": "A nonrelativistic particle of mass m undergoes one-dimensional motion in the potential\n\n$$\nV(z) = -g[\\delta(x-a) + \\delta(x+a)]\n$$\n\nwhere $\\text{g} > 0$ is a constant and $\\delta(x)$ is the Dirac delta function. Find the ground-state energy eigenfunction and obtain an equation which relates the corresponding energy eigenvalue to the constant $\\text{g}$.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem where there's a particle moving in a one-dimensional potential made up of two delta functions. The potential is V(z) = -g[δ(x - a) + δ(x + a)], right? And g is positive. They want me to find the ground state energy eigenfunction and relate the energy eigenvalue E to the constant g.\n\nHmm, okay. So first off, since it's a delta potential well, I remember that these kinds of potentials support bound states only if certain conditions on the strength (here, g) are met. But wait, here we have two deltas instead of just one. That might complicate things because now the system isn't symmetric anymore... or actually, no, maybe it still has some symmetry?\n\nWait, let me think again: the potential is symmetric about x=0 because you have δ(x - a) and δ(x + a). So yes, the whole setup is symmetric with respect to reflection through the origin. Therefore, perhaps the solutions will be either even or odd functions. Since we're looking for the ground state, which should be the lowest energy level, probably the wavefunction would be symmetric—so an even function. Because usually, lower energy levels correspond to more symmetric configurations.\n\nSo, assuming the wavefunction ψ(x) is even, meaning ψ(-x) = ψ(x), then its derivative at zero must satisfy something specific. Wait, but when dealing with delta functions as potentials, especially attractive ones like negative signs here, they can lead to discontinuities in the wavefunction derivatives.\n\nRight! For each delta function, the derivative of the wavefunction jumps by a factor proportional to the strength of the delta. Specifically, from what I recall, integrating the Schrödinger equation across a delta function gives us a condition on the change in the slope of the wavefunction.\n\nLet me write down the time-independent Schrödinger equation:\n\n-ħ²/(2m) * d²ψ/dx² + V(x)ψ = Eψ\n\nSince V(x) consists of two delta functions, most places away from ±a, V(x)=0, so the equation becomes:\n\nd²ψ/dx² = k² ψ,\n\nwhere k² = -2mE/ħ². Hmm, but wait—if E is negative, which it should be for a bound state, then k² is positive, making k real. If E were positive, k would become imaginary, leading to free particles rather than bound states.\n\nBut given that the potential is attractive (-g times delta functions), there should exist bound states below zero energy. So yeah, E < 0, hence k is real.\n\nNow, considering regions divided by the points x=-a and x=a. Let's divide the line into three intervals:\n\n1. Region I: x < -a,\n2. Region II: |x| ≤ a,\n3. Region III: x > a.\n\nIn Regions I and III, far from both delta functions, the potential is zero, so the wavefunctions are plane waves decaying exponentially towards infinity because E is negative. In Region II, between -a and a, the potential is also zero except exactly at x=±a, so same form applies.\n\nGiven the symmetry, the wavefunction should look similar in all regions, but adjusted for continuity and boundary conditions.\n\nBecause the potential is symmetric, the ground state should indeed be even, so ψ(x) = ψ(-x).\n\nTherefore, in Region I (x < -a): ψ_I(x) = A e^{k x}, since exponential decay as x approaches -infinity.\nSimilarly, in Region III (x > a): ψ_III(x) = B e^{-k x}. Again, exponential decay as x goes to infinity.\n\nAnd in Region II (between -a and a), due to symmetry, the wavefunction should be symmetric around x=0. Hence, likely composed of hyperbolic cosine terms or exponentials arranged symmetrically.\n\nAlternatively, another approach could be using piecewise definitions. Maybe express ψ(x) as C e^{-k|x|}? No, wait—that’s typically for a single delta function centered at zero. Here, having two deltas complicates matters.\n\nPerhaps better to model the entire space without splitting into too many regions. Alternatively, use the fact that outside the region near the deltas, the wavefunction behaves similarly to a simple exponential decay.\n\nWait, hold on—I need to consider how the presence of two delta wells affects the overall shape. Each delta introduces a jump in the derivative of the wavefunction.\n\nSpecifically, integrating the Schrödinger equation over small regions around x = a and x = -a.\n\nFor example, take x slightly less than a and slightly greater than a. Then integrate the SE from a-ε to a+ε as ε→0.\n\nThe integral of the second derivative term [d²ψ/dx²] dx from a-ε to a+ε equals ψ’(a+) – ψ’(a-) ≈ [slope after a minus slope before a].\n\nOn the other hand, the integral involving V(x)ψ is approximately -g δ(x - a)ψ(a) multiplied by 2ε, but as ε tends to zero, the main contribution comes from the delta function itself.\n\nThus, formally, the result is:\n\nψ’(a+) – ψ’(a-) = -(2m g)/(ħ²) ψ(a)\n\nSame thing happens at x = -a, giving:\n\nψ’(-a+) – ψ’(-a-) = -(2m g)/(ħ²) ψ(-a)\n\nBut since our wavefunction is even, ψ'(a) = -ψ'(-a). Also, ψ(a) = ψ(-a) because of evenness.\n\nMoreover, in the middle region, say between -a and a, the wavefunction satisfies the differential equation d²ψ/dx² = k² ψ, whose general solution is ψ(x) = D cosh(k x) + F sinh(k x). However, since the wavefunction needs to be continuous everywhere, including at x=0, and being even implies that the sine terms vanish because sinh is odd. Thus, in Region II, ψ(x) = G cosh(k x).\n\nWait, but hang on—isn’t cosh(k x) symmetric? Yes, it is. So that makes sense for the even parity.\n\nHowever, connecting this to the outer regions requires matching the wavefunction and their derivatives appropriately.\n\nAt x approaching a from within Region II, the value of ψ(a) is equal to G cosh(k a). Similarly, coming from Region III, ψ(a) is B e^{-k a}.\n\nHence, equating them: G cosh(k a) = B e^{-k a}\n\nAlso, the derivative from inside Region II at x=a is dψ/dx = G k sinh(k a)\nFrom Region III, the derivative is dψ/dx = -B k e^{-k a}\nThen applying the boundary condition at x=a:\nDerivative jump: [-B k e^{-k a}] - [G k sinh(k a)] = -(2m g)/ħ² * G cosh(k a)\n\nSimplify:\n-B k e^{-k a} - G k sinh(k a) = -(2m g / ħ²) G cosh(k a)\n\nDivide both sides by k:\n[-B e^{-k a} - G sinh(k a)] = -(2m g / (ħ² k)) G cosh(k a)\n\nRearranged:\n-B e^{-k a} = G [sinh(k a) - (2m g / (ħ² k)) cosh(k a)]\n\nSimilarly, at x = -a, the situation mirrors due to symmetry. From Region I, the derivative is dψ/dx = A k e^{k x}; evaluated at x=-a, it's A k e^{-k a}.\nFrom Region II, the derivative is G k sinh(k a); evaluated at x=-a, it's G k sinh(-k a) = -G k sinh(k a).\nApplying the boundary condition at x=-a:\n[A k e^{-k a}] - [-G k sinh(k a)] = -(2m g / ħ²) G cosh(k a)\n\nWhich simplifies to:\nA k e^{-k a} + G k sinh(k a) = -(2m g / ħ²) G cosh(k a)\n\nAgain, dividing by k:\nA e^{-k a} + G sinh(k a) = -(2m g / (ħ² k)) G cosh(k a)\n\nComparing equations from x=a and x=-a:\n\nFrom x=a:\n-B e^{-k a} = G [sinh(k a) - (2m g / (ħ² k)) cosh(k a)]\n\nFrom x=-a:\nA e^{-k a} = -G [sinh(k a) + (2m g / (ħ² k)) cosh(k a)]\n\nAdditionally, note that in Region I, ψ_I(x) = A e^{k x}, while in Region II, ψ_II(x)"}
{"id": "quantum/1-1022", "questions": "A particle of mass $m$ moves non-relativistically in one dimension in a potential given by $\\mathbf{V}(x) = -a\\delta(x)$, where $\\delta(x)$ is the usual Dirac delta function. The particle is bound. Find the value of $x_0$ such that the probability of finding the particle with $|x| < x_0$ is exactly equal to $1/2$.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about a quantum mechanics scenario involving a particle moving in a specific potential. Let me try to unpack it step by step.\n\nFirst off, the setup: A particle of mass \\( m \\) is moving non-relativistically in one dimension within a potential \\( V(x) = -a\\delta(x) \\). This looks like a standard delta potential well, which I remember from my studies as being a common example for discussing bound states in quantum mechanics. Since the potential is negative (because of the minus sign), it's attractive, meaning there should be a single bound state here—so the particle can exist in a stable orbit around the origin without escaping to infinity.\n\nThe question asks for the value of \\( x_0 \\) such that the probability of finding the particle between \\( |x| < x_0 \\) is exactly 1/2. So essentially, we need to find the point beyond which half the probability lies on either side? Or wait, no—it’s asking for the region symmetrically around zero where the total probability inside is 50%. That makes sense because the wavefunction for a delta potential is symmetric about the origin due to the symmetry of the potential itself.\n\nAlright, let me recall what the ground state wavefunction for a delta potential looks like. From memory, the normalized wavefunction for the bound state in a delta potential is something like:\n\n\\( \\psi(x) = \\sqrt{\\frac{k}{\\hbar}} e^{-k|x|} \\)\n\nwhere \\( k \\) is related to the binding energy or some other parameter derived from the strength of the potential. Wait, actually, maybe more precisely, since the potential has a constant \\( a \\), perhaps \\( k \\) depends inversely on \\( a \\).\n\nLet me think again. For a delta potential \\( V(x) = -\\alpha \\delta(x) \\), the bound state condition gives us the wavenumber \\( k \\) via the equation:\n\n\\( k = \\frac{m\\alpha}{\\hbar^2} \\)\n\nBut hold on—isn't the exact expression different? Maybe I got the constants mixed up. Alternatively, another approach would be better—I should probably derive the wavefunction properly instead of trying to rely solely on memory.\n\nSo starting fresh: In regions away from the origin (\\( x \\neq 0 \\)), the potential vanishes, so the time-independent Schrödinger equation becomes:\n\n\\( -\\frac{\\hbar^2}{2m}\\frac{d^2\\psi}{dx^2} + V(x)\\psi = E\\psi \\)\n\nSince \\( V(x) = -a\\delta(x) \\), outside \\( x=0 \\), \\( V(x)=0 \\), so the equation simplifies to:\n\n\\( -\\frac{\\hbar^2}{2m}\\frac{d^2\\psi}{dx^2} = E\\psi \\)\n\nWhich leads to solutions of exponential form when \\( E \\) is negative (bound state):\n\nFor \\( x > 0 \\): \\( \\psi(x) = Ae^{-kx} \\)\nFor \\( x < 0 \\): \\( \\psi(x) = Be^{kx} \\)\n\nDue to continuity at \\( x=0 \\), these must match, implying \\( B = A \\). Therefore, the general form is:\n\n\\( \\psi(x) = A e^{-k|x|} \\)\n\nNow, applying normalization condition:\n\n\\( \\int_{-\\infty}^\\infty |\\psi(x)|^2 dx = 1 \\)\n\nSubstituting,\n\n\\( A^2 \\int_{-\\infty}^\\infty e^{-2k|x|} dx = 1 \\)\n\nThis integral splits into two parts:\n\n\\( 2A^2 \\int_{0}^\\infty e^{-2kx} dx = 1 \\)\n\nCalculating the integral:\n\n\\( 2A^2 \\left[ \\frac{-1}{2k} e^{-2kx} \\right]_0^\\infty = 2A^2 \\cdot \\frac{1}{2k} = \\frac{A^2}{k} = 1 \\)\n\nThus, \\( A = \\sqrt{k} \\). Hence, the normalized wavefunction is:\n\n\\( \\psi(x) = \\sqrt{k} e^{-k|x|} \\)\n\nGood, now onto determining \\( k \\). To do this, we use the boundary conditions imposed by the delta potential. Specifically, integrating the Schrödinger equation across \\( x=0 \\) will give a discontinuity in the derivative of the wavefunction proportional to the strength of the potential.\n\nIntegrating both sides of the TISE over a small interval \\( [-\\epsilon, \\epsilon] \\):\n\nLeft-hand side:\n\\( -\\frac{\\hbar^2}{2m} \\int_{-ε}^{ε} \\frac{d^2\\psi}{dx^2} dx + \\int_{-ε}^{ε} (-a\\delta(x))\\psi(x) dx = \\int_{-ε}^{ε} E\\psi(x) dx \\)\n\nAs ε approaches zero, the first term becomes:\n\n\\( -\\frac{\\hbar^2}{2m} [ \\psi'(ε) - ψ'(-ε) ] \\)\n\nAnd the second term becomes:\n\n\\( -aψ(0) \\)\n\nOn the right-hand side, as ε→0, the integral tends to zero if \\( E \\) is finite.\n\nTherefore, taking limits:\n\n\\( -\\frac{\\hbar^2}{2m} [\\lim_{ε→0} (ψ’(ε) – ψ’(-ε))] = -aψ(0) \\)\n\nCompute derivatives:\n\nFrom our wavefunction,\nfor \\( x>0 \\):\n\\( ψ’(x) = -k√k e^{-kx} \\)\nAt \\( x=0+ \\), \\( ψ’(0+) = -k√k \\)\n\nSimilarly, for \\( x<0 \\):\n\\( ψ’(x) = k√k e^{kx} \\)\nAt \\( x=0- \\), \\( ψ’(0-) = k√k \\)\n\nHence, the difference:\n\n\\( ψ’(0+) - ψ’(0-) = (-k√k ) - (k√k ) = -2k√k \\)\n\nPlugging back into the integrated equation:\n\n\\( -\\frac{\\hbar^2}{2m} (-2k√k ) = -aψ(0) \\)\n\nSimplify left side:\n\n\\( \\frac{\\hbar^2}{2m} * 2k√k = \\frac{\\hbar^2 k √k }{m} \\)\n\nRight side:\n\n\\( -aψ(0) = -a√k \\) (since \\( ψ(0) = √k e^{0}=√k \\))\n\nPutting together:\n\n\\( \\frac{\\hbar^2 k^{3/2}}{m} = -a√k \\)\n\nWait, but the LHS is positive while RHS is negative... Hmm, seems contradictory unless signs were messed up somewhere.\n\nLooking back, perhaps during integration, the direction matters. When computing the jump condition, sometimes people take the limit approaching from above and below differently.\n\nAlternatively, perhaps I made a mistake in calculating the derivatives.\n\nWait, let's re-examine the derivatives.\n\nGiven \\( ψ(x) = √k e^{-k|x|} \\),\n\nthe derivative for \\( x>0 \\) is indeed \\( d/dx e^{-kx} = -ke^{-kx} \\),\nand multiplied by √k, so yes, \\( ψ’(x) = -k√k e^{-kx} \\).\nSimilarly, for \\( x<0 \\), \\( d/dx e^{kx} = ke^{kx} \\), times √k, giving \\( ψ’(x) = k√k e^{kx} \\).\n\nThen, evaluating just above and just below zero:\n\nψ’(0+) = -k√k\n\nψ’(0-) = k√k\n\nDifference: ψ’(0+) - ψ’(0-) = -k√k - k√k = -2k√k\n\nMultiply by -ħ²/(2m):\n\n- ħ²/(2m)*(-2k√k) = (ħ² k^(3/2))/m\n\nSet equal to -aψ(0):\n\n(ħ² k^(3/2))/m = -a√k\n\nHmm, still getting a contradiction in signs. But physically, the curvature caused by the delta potential should lead to a downward \"kink\" in the wavefunction—if you imagine the wavefunction curving towards the potential minimum. Thus, the slope changes abruptly; going from positive to steeper negative?\n\nWait, perhaps I missed a negative sign in setting up the equation.\n\nGoing back to the original differential equation:\n\nAfter integrating, we had:\n\n- (ħ² / 2m)(ψ’(+ε) - ψ’(-ε)) = -aψ(0"}
{"id": "quantum/6013", "questions": "Calculate the scattering cross section for a low energy particle from a potential given by $V = -V_0$ for $r < a$, $V = 0$ for $r > a$. Compare this with the Born approximation result.", "graphs": null, "llm_answers": "<think>\nOkay, so I have to calculate the scattering cross-section for a low-energy particle interacting with a square well potential. The potential is V(r) which equals -V₀ when r is less than a, and zero otherwise. Then compare it with the Born approximation result.\n\nHmm, let me recall what I know about scattering cross sections. Scattering cross-sections describe how likely particles are to scatter off each other or potentials. For non-relativistic quantum mechanics, we often use partial wave analysis because of its analytical simplicity, especially for spherically symmetric potentials like this one.\n\nSince the problem mentions comparing with the Born approximation, that suggests maybe using perturbation theory as another method? But first, perhaps starting with exact methods would be better since the question asks specifically for both results.\n\nWait, but wait—the potential here is finite range (only up to radius 'a'), so maybe there's some analytic solutions available?\n\nI remember that for such problems, you can solve the Schrödinger equation inside and outside the potential region and match them at r=a. Since it's a square well, the radial part should involve Bessel functions or something similar... Or spherical Bessel functions, right?\n\nLet me think: Inside the well (r < a), where V=−V₀, the effective potential includes centrifugal barrier term l(l+1)/2μr² plus the actual potential. So the time-independent Schrödinger equation becomes:\n\n[-ħ²/(2μ)] d²u/dr² + [E + V₀] u = 0,\n\nwhere u(r) is related to the radial wavefunction R(r). Wait, actually, more precisely, the equation is:\n\n[ - (ħ²)/(2μ) * (d²/d r²) + (l(l+1) ħ²)/(2 μ r² ) + V(r) ] ψ(r) = E ψ(r).\n\nSo splitting into regions:\n\nFor r < a:\nThe potential is −V₀, so the equation becomes:\n\n- (ħ²)/(2μ) (d²R/dr²) + [(l(l+1) ħ²)/(2 μ r² ) - V₀] R = E R\n\nWhich simplifies to:\n\n(d²R/dr²) + [k₁² - (l(l+1))/(r²)] R = 0,\n\nwhere k₁² = (2μ(E + V₀))/ħ².\n\nSimilarly, for r ≥ a, the potential is zero, so:\n\n(d²R/dr²) + [k₂² - (l(l+1))/(r²)] R = 0,\n\nwith k₂² = (2μ E)/ħ².\n\nThese equations resemble the spherical Bessel differential equation. Therefore, their general solutions will be combinations of spherical Bessel functions j_l(k r) and n_l(k r) (the regular and irregular ones respectively).\n\nInside the well (r < a):\n\nWe need a regular solution at r=0, so only j_l(k₁ r) terms survive. Thus, R_in(r) ~ A j_l(k₁ r)\n\nOutside the well (r > a):\n\nTo avoid divergence at infinity, we take outgoing waves, meaning the solution must consist solely of h^{(1)}_l(k₂ r) or equivalently, a combination involving e^{ikr} terms without incoming components. However, sometimes people express these in terms of Hankel functions or just write them as asymptotic forms.\n\nBut regardless, matching conditions apply at r=a. That means continuity of R and its derivative across r=a.\n\nTherefore, setting R_in(a) = R_out(a),\n\nand similarly, the derivatives equal at r=a.\n\nThis gives two equations relating coefficients on either side, allowing us to find phase shifts δ_l.\n\nOnce we have the phase shift δ_l, then the total scattering cross-section σ_total is calculated via summing over all angular momenta contributions weighted by |f|² integrated over solid angle.\n\nSpecifically, f being the scattering amplitude, which relates to the phase shifts through:\n\nf(θ) = ∑_{l=0}^∞ (2l + 1) e^{iδ_l} sin δ_l P_l(cos θ)\n\nThen, integrating |f|^2 over angles gives the total cross-section:\n\nσ_total = ∫ |f(θ)|² dΩ / (4π/k²)^{-1}\n\nWait no—actually, the standard formula is:\n\nσ_total = ∫ |f(θ)|² dΩ\n\nBecause typically, the definition involves f having units of length squared times inverse wavenumber, etc., depending on conventions.\n\nAlternatively, recalling that the differential cross-section is |f(θ)|², so integrating over all directions gives the total cross-section.\n\nHowever, if considering relativistic normalization factors, might differ, but assuming nonrelativistic case, yes, integral of |f|² over sphere gives σ_total.\n\nMoreover, for spinless particles, the expression holds.\n\nNow, for low energies, probably dominated by s-wave scattering, i.e., l=0. Because higher angular momentum states require more kinetic energy due to the centrifugal barrier. At very low energies, even small positive energies may not support much beyond s-waves.\n\nThus, perhaps approximating by taking only l=0 contribution could simplify things significantly.\n\nGiven that, let's focus on l=0.\n\nIn that case, the phase shift δ_0 determines everything.\n\nSo solving for δ_0 requires finding the condition from matching the logarithmic derivatives at r=a.\n\nThat is, for l=0, the solutions become simpler.\n\nInside the well (r < a): Solution is proportional to sin(k₁ r), since j_0(x)=sin x/x, but scaled appropriately.\n\nActually, j_0(z) = sin z / z, so R_in(r) ≡ C sin(k₁ r)\n\nAt r approaching 0, behaves nicely.\n\nOutside the well (r > a): Solutions are Hankel functions or expressed as outgoing plane waves. Alternatively, for l=0, the regular solution is sin(k₂ r)/k₂, but again, adjusted for boundary conditions.\n\nWait, hold on—for l=0, the solution outside is usually written as D sin(k₂ r) + E cos(k₂ r). But since we want purely outgoing waves, we set the coefficient of the ingoing component to zero.\n\nWait, no—that’s incorrect. Actually, for l=0, the outgoing wave has the form e^{ikr}/r, whose expansion contains only sine terms upon expressing in terms of real functions.\n\nHence, the correct approach is to represent the external solution as a linear combination of sin(k₂ r) and cos(k₂ r), but impose the radiation condition at large r, leading to selecting specific coefficients.\n\nBut perhaps instead, writing the external solution as F sin(k₂ r + φ), ensuring proper behavior.\n\nAlternatively, perhaps easier to consider the ratio of the log derivatives.\n\nYes, indeed, the key idea is that the logarithmic derivative of the internal solution matches that of the external solution at r=a.\n\nMathematically, for l=0:\n\n[R'_in/R_in](a) = [R'_out/R_out](a)\n\nCompute both sides.\n\nFirstly, compute R_in(r) = C sin(k₁ r)\n=> R'_in(r) = C k₁ cos(k₁ r)\nThus,\nlog deriv: R’/R = k₁ cot(k₁ r)\nEvaluated at r=a: k₁ cot(k₁ a)\n\nOn the outside, R_out(r) needs to satisfy the same kind of equation but with k=k₂.\n\nAssuming R_out(r) = D sin(k₂ r + α), though perhaps better to directly relate to the Hankel function.\n\nWait, alternatively, for l=0, the external solution is generally taken as H^{(1)}_0(k₂ r), which corresponds to outgoing spherical waves.\n\nExpressed in terms of elementary functions, H^{(1)}_0(z) = J_0(z) + i Y_0(z), where J_0 is the Bessel function and Y_0 is the Neumann function.\n\nAsymptotically, H^{(1)}_0(z) → sqrt(2/z π) e^{i(z - π/4)}, but near origin, they behave differently.\n\nBut perhaps rather than dealing with complex expressions, stick to the fact that the external solution must be matched smoothly.\n\nAnother way is to note that for l=0, the external solution can be represented as a multiple of sin(k₂ r) divided by r, but normalized correctly.\n\nWait, perhaps going back to the original ODE.\n\nFor l=0, the equation reduces to:\n\nd²R/dr² + k² R = 0,\n\nso the general solution is R(r) = A sin(k r) + B cos(k r).\n\nBut for our purposes, in the exterior region (r>a), we don't want any incoming waves; thus, we choose the solution that represents pure outgoing waves.\n\nAn outgoing wave satisfies the condition that as r→infinity, R(r) ~ e^{i k r}/r.\n\nExpanding e^{i k r} in terms of sine and cosine, we get:\n\ne^{i k r} = cos(k r) + i sin(k r)\n\nDivided by r, so R(r) ~ [cos(k r) + i sin(k r)] / r\n\nComparing to the general solution"}
{"id": "quantum/2-2015", "questions": "The interactions of heavy quarks are often approximated by a spin-independent nonrelativistic potential which is a linear function of the radial variable $r$, where $r$ is the separation of the quarks: $V(r) = A + Br$. Thus the famous \"charmonium\" particles, the $\\psi$ and $\\psi'$, with rest energies 3.1 $\\text{GeV}$ and 3.7 $\\text{GeV}$  (1 $\\text{GeV}$  = $10^9$ $\\text{eV}$ ), are believed to be the $n = 0$ and $n = 1$ bound states of zero orbital angular momentum of a charm quark of mass $m_c = 1.5 \\text{ GeV}/c^2$ (i.e. $E = 1.5 \\text{ GeV}$) and an anti-quark of the same mass in the above linear potential. Similarly, the recently discovered upsilon particles, the $\\Upsilon$ and $\\Upsilon'$, are believed to be the $n = 0$ and $n = 1$ zero orbital angular momentum bound states of a bottom quark and anti-quark pair in the same potential. The rest mass of bottom quark is $m_b = 4.5 \\text{ GeV}/c^2$. The rest energy of $\\Upsilon$ is 9.5 $\\text{GeV}$ .\n\n(a) Using dimensional analysis, derive a relation between the energy splitting of the $\\psi$ and $\\psi'$ and that of the $\\Upsilon$ and $\\Upsilon'$, and thereby evaluate the rest energy of the $\\Upsilon'$. (Express all energies in units of $\\text{GeV}$ .)\n\n(b) Call the $n = 2$, zero orbital angular momentum charmonium particle the $\\psi''$. Use the $\\text{WKB}$ approximation to estimate the energy splitting of the $\\psi'$ and the $\\psi''$ in terms of the energy splitting of the $\\psi$ and the $\\psi'$, and thereby give a numerical estimate of the rest energy of the $\\psi''$.\n\n", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about heavy quarks and their binding potentials. It's divided into two parts, part (a) and part (b). Let me try to tackle them one by one.\n\nStarting with part (a): We're given that the interaction potential for heavy quarks like charm and bottom quarks can be modeled as V(r) = A + B*r. This is called a linear potential because it increases linearly with r, the distance between the quarks. \n\nWe know that the ψ and ψ’ particles correspond to n=0 and n=1 bound states respectively, both having zero orbital angular momentum. Their rest energies are given as 3.1 GeV and 3.7 GeV. So the energy difference or splitting between these two levels would be ΔE_ψ = E(ψ’) - E(ψ) = 3.7 - 3.1 = 0.6 GeV.\n\nSimilarly, we’re told that Υ and Υ’ are the corresponding n=0 and n=1 states but made up of bottom quarks instead of charm quarks. The rest mass of the bottom quark is m_b = 4.5 GeV/c², while the charm quark has m_c = 1.5 GeV/c². Also, the rest energy of Υ is given as 9.5 GeV, and we need to find the rest energy of Υ’.\n\nSo using dimensional analysis, we should relate the energy splittings of the ψ system and the Υ system. Hmm, how do I approach this?\n\nI remember from quantum mechanics that when dealing with bound states in different potentials, especially if they depend on some parameters, scaling relations come into play based on those parameters. Since the potential here is linear, maybe there’s a way to express the energy levels in terms of the masses of the quarks involved.\n\nLet me think about the form of the Schrödinger equation for such a potential. For a harmonic oscillator, the energy depends on sqrt(m), but since our potential is linear rather than quadratic, perhaps the dependence will be different.\n\nWait, actually, for a linear potential, the solutions aren't exactly known analytically except in certain limits, but I recall that the WKB method might help us approximate the energy levels. But wait, no—part (a) says to use dimensional analysis, not necessarily solving the equations directly.\n\nDimensional analysis usually involves looking at the dimensions of quantities and seeing how they scale. Here, the key variables affecting the energy levels would likely include the reduced mass μ of the quark-antiquark pair, the strength of the potential B, Planck's constant ħ, and possibly other constants related to relativity, like c.\n\nBut let's see—if we consider the system of two identical massive quarks interacting via a linear potential, then the relevant scales must involve combinations of these parameters. Maybe the product of B and some length scale? Or something involving the square root of μ*B*ħ²/(something)?\n\nAlternatively, considering that the energy levels probably depend on factors involving the reduced mass and the string tension B. If I assume that each energy level En is proportional to some combination of μ and B raised to specific powers, then the ratio of energy differences could simplify depending only on the ratio of the reduced masses.\n\nGiven that, what is the reduced mass for the charm quark system versus the bottom quark system?\n\nFor the charm quark case, since both quarks have equal mass m_c, the reduced mass μ_charm is just m_c/2. Similarly, for the bottom quark case, μ_bottom = m_b / 2.\n\nTherefore, μ_bottom = (4.5)/2 = 2.25 GeV/c², whereas μ_charm = 1.5 / 2 = 0.75 GeV/c².\n\nNow, assuming that the energy spacing between adjacent levels (ΔE) scales inversely with the square root of the reduced mass times some factor... Wait, why inverse square root? Because in many physical systems, frequencies (which relate to energy spacings) go as sqrt(k/mu), so higher mu leads to lower frequency. Therefore, energy spacing would decrease with increasing mu.\n\nIf that reasoning holds, then ΔE ∝ 1/sqrt(mu).\n\nThus, the ratio of energy splittings between the Υ and ψ systems would be:\n\nΔEΥ / ΔEψ ≈ sqrt(μ_charm / μ_bottom )\n\nPlugging numbers in,\n\nsqrt( (0.75)/(2.25) ) = sqrt(1/3) ≈ 0.577\n\nHence, ΔEΥ ≈ ΔEψ * 0.577 ≈ 0.6 * 0.577 ≈ 0.346 GeV\n\nSince the rest energy of Υ is 9.5 GeV, its first excited state Υ’ would be approximately 9.5 + 0.346 ≈ 9.846 GeV.\n\nIs that correct? Alternatively, another thought: sometimes in these problems, you get exact ratios without needing to take square roots. Perhaps my initial assumption was wrong.\n\nWait, going back—the energy levels themselves may scale differently. Suppose that the ground state energy E_n is proportional to (B)^α (mu)^β. Then, taking derivatives or considering changes due to varying mu or B...\n\nActually, more accurately, for a linear potential, the quantized energy levels can be found using methods similar to the hydrogen atom, but adjusted for linearity. However, without knowing the precise formula, relying purely on dimensional analysis.\n\nIn natural units where ħ=c=1, the dimensionless quantity comes from combining B [energy per length] and mu [mass], along with possible others.\n\nEnergy has dimension M (since E ~ mc²; in natural units, E~M). Length L has dimension M^{-1}.\n\nPotential V(r)=A+B r. To make sense of the units, A must have units of Energy (so M), and B must also have units of Energy over Length, i.e., ML^{-1}, which translates to M^{2} in natural units (because L^{-1}=M).\n\nHmm, getting confused now. Let me write down the dimensions properly.\n\nIn SI-like units:\n- Mass (m) → kg.\n- Length (L) → meters.\n- Time (T).\n- Charge etc., but focusing on mechanical units.\n\nPlanck's constant ħ has units J·s = kg·m²/s.\nSpeed of light c has units m/s.\n\nNatural units set ħ = c = 1, so lengths and times have units of inverse mass (kg^{-1}), and energies have units of kg.\n\nSo, Potential V(r):\n\nEach term needs to have units of energy (kg).\n\nTerm A: unit kg.\n\nTerm B*r: B must have units kg/m, multiplied by r (unit m) gives kg.\n\nYes, so B has units kg/m.\n\nReduced mass μ has units kg.\n\nLooking for expressions relating energy levels to μ and B.\n\nAssume that the typical energy scale E is determined by balancing kinetic and potential contributions. In a linear potential, the virial theorem tells us that average kinetic and potential energies balance, leading to KE ~ PE.\n\nPE for a linear potential is roughly B⟨r⟩, so ⟨r⟩ ~ E/B.\n\nKinetic energy is p²/(2μ), so setting"}
{"id": "quantum/3-3048", "questions": "The Hamiltonian for a $(\\mu^+ e^-)$ atom in the $n = 1, l = 0$ state in an external magnetic field $\\mathbf{B}$ is\n\n$$\nH = as_\\mu \\cdot \\mathbf{s}_e + \\frac{|e|}{m_ec} \\mathbf{s}_e \\cdot \\mathbf{B} - \\frac{|e|}{m_\\mu c} \\mathbf{s}_\\mu \\cdot \\mathbf{B}\n$$\n\n(a) What is the physical significance of each term? Which term dominates in the interaction with the external field?  \n\n(b) Choosing the z-axis along $\\mathbf{B}$ and using the notation ($F, M$), where $\\mathbf{F} = \\mathbf{s}_\\mu + \\mathbf{s}_e$, show that (1, +1) is an eigenstate of $H$ and give its eigenvalue.\n\n(c) An $rf$ field can be applied to cause transitions to the state (0, 0).  \n\nDescribe qualitatively how an observation of the decay $\\mu^+ \\rightarrow e^+ \\nu_e \\nu_\\mu$ could be used to detect the occurrence of this transition.", "graphs": null, "llm_answers": "<think>\nAlright, so I've got this problem about the Hamiltonian for a muonic atom—specifically a μ⁺e⁻ system—in an n=1, l=0 state under an external magnetic field B. The question has three parts: part (a) asks about the physical significance of each term in the given Hamiltonian and which term dominates when interacting with the external field; part (b) wants me to choose the z-axis along B and use F, M notation to show that the state (1, +1) is an eigenstate of H and find its eigenvalue; and part (c) discusses applying an rf field causing transitions to (0,0) and describes how observing the decay μ⁺→e⁺νₑν_μ could help detect such a transition.\n\nStarting with part (a): Let's look at the terms in the Hamiltonian:\n\nFirst term: \\( a s_\\mu \\cdot \\mathbf{s}_e \\)\n\nSecond term: \\( \\frac{|e|}{m_e c} \\mathbf{s}_e \\cdot \\mathbf{B} \\)\n\nThird term: \\( -\\frac{|e|}{m_\\mu c} \\mathbf{s}_\\mu \\cdot \\mathbf{B} \\)\n\nSo, breaking them down one by one.\n\nTerm 1: This looks like it involves the spins of both particles—the muon spin (\\(s_\\mu\\)) and electron spin (\\(\\mathbf{s}_e\\)). Since they're dot product together multiplied by some constant 'a', this must represent their mutual spin-spin interaction or something similar. Maybe it's related to exchange interactions because in atoms, electrons have exchange interactions due to identical fermions, but here we have different particles—a muon and an electron—so maybe not exactly exchange, but still a spin coupling term. Alternatively, perhaps it represents hyperfine structure arising from dipole-dipole interactions between the two spins?\n\nWait, actually, considering that the orbital angular momentum l is zero (since l=0), there might not be any orbital contributions, so all the fine/hyperfine splitting would come purely from spin interactions. So yes, Term 1 likely corresponds to the hyperfine interaction energy between the muon and electron spins.\n\nTerms 2 and 3: These seem to involve the Zeeman effect since they couple the spin of each particle to the external magnetic field B. Specifically, Term 2 is proportional to \\( |e|/(m_e c) \\times \\mathbf{s}_e \\cdot \\mathbf{B} \\)—this resembles the standard form of the Zeeman Hamiltonian for an electron in a magnetic field. Similarly, Term 3 does the same for the muon, except with opposite sign and mass m_mu instead of m_e.\n\nBut wait, why negative? Because the charge on the muon is positive (+e), while the electron is negative (-e)? Wait no, let me think again. In general, the Zeeman term is usually written as \\( -q \\mathbf{s} \\cdot \\mathbf{B}/(mc) \\), right? For an electron, q=-e, so plugging into the formula gives \\( -(-e)\\mathbf{s}\\cdot\\mathbf{B}/(m_e c) ) = e \\mathbf{s}\\cdot\\mathbf{B}/(m_e c) ). But looking back at our second term, it says \\( |e| / m_e c * \\mathbf{s}_e \\cdot \\mathbf{B} \\). Hmm, absolute value of e over m_e times c... That seems correct if you take magnitude regardless of charge. However, the third term is subtracted, meaning it's minus |e|/m_mu*c times s_mu · B. If the muon has charge +e, then similarly, the Zeeman term should be \\( -(+e)/m_mu c * s_mu · B \\), hence leading to a negative coefficient. Therefore, these two terms account for the individual Zeeman effects of the electron and muon respectively.\n\nNow, regarding which term dominates in the interaction with the external field. Well, the first term doesn't depend directly on the external field—it’s more about internal spin couplings. Terms 2 and 3 do interact with B. To see which dominates, compare the coefficients.\n\nLet me compute the ratio of the magnitudes of the two coefficients:\n\nCoefficient for electron: \\( A_e = \\frac{|e|}{m_e c} \\)\nCoefficient for muon: \\( A_mu = \\frac{|e|}{m_mu c} \\)\n\nSince the muon mass is much larger than the electron mass (~207 times heavier?), so m_mu ≈ 206.8 m_e approximately. Thus,\n\nA_mu = A_e / 206.8 ≈ 0.48% of A_e.\n\nTherefore, the electron's contribution to the Zeeman term is significantly stronger compared to the muon's. Hence, the dominant term in the interaction with the external field comes from the electron's spin coupling, i.e., the second term.\n\nMoving on to part (b):\n\nWe need to set up the z-axis along B, express everything in terms of total spin F = s_mu + s_e, and consider states labeled by (F, M_F).\n\nGiven that, we want to check whether the state (1, +1) is an eigenstate of H and find its eigenvalue.\n\nFirstly, note that the possible values of F arise from adding the two spins: s_mu = ½, s_e = ½. Their sum can be either F=1 (triplet) or F=0 (singlet). So, the four possible states split into triplet (M=−1,0,+1) and singlet (M=0).\n\nIn the absence of an external field, only the first term matters, which couples the two spins. When an external field is present, the other two terms become significant, introducing dependence on the orientation relative to B.\n\nHowever, now choosing the z-axis along B allows us to diagonalize the Hamiltonian in the basis of F_z eigenstates.\n\nExpressing H in terms of F components may simplify things.\n\nAlternatively, write H in terms of operators involving F² and F_z, since those will commute with themselves and thus share common eigenstates.\n\nLooking at the original H:\n\n\\( H = a \\mathbf{s}_\\mu \\cdot \\mathbf{s}_e + \\left( \\frac{|e|}{m_e c} \\right) \\mathbf{s}_e \\cdot \\mathbf{B} - \\left( \\frac{|e|}{m_\\mu c} \\right) \\mathbf{s}_\\mu \\cdot \\mathbf{B} \\)\n\nNote that \\( \\mathbf{s}_\\mu \\cdot \\mathbf{B} = s_{\\mu,z} B \\hat{z} \\) etc., assuming uniformity.\n\nThus, rewrite H as:\n\n\\( H = a \\mathbf{s}_\\mu \\cdot \\mathbf{s}_e + g_e \\mu_B s_{e,z} B - g_mu \\mu_N s_{mu,z} B \\),\n\nwhere \\( g_e \\approx 2 \\) is the gyromagnetic factor for electron, \\( \\mu_B = eħ/(2m_e c) \\) is Bohr magneton, and similarly for muon, \\( g_mu \\) is around 2 too, and \\( \\mu_N = ħ/(2m_mu c) \\) is nuclear magneton scaled appropriately.\n\nBut perhaps another approach is better. Express \\( \\mathbf{s}_\\mu \\cdot \\mathbf{s}_e \\) in terms of F².\n\nRecall that \\( \\mathbf{F}^2 = (\\mathbf{s}_\\mu + \\mathbf{s}_e)^2 = \\mathbf{s}_\\mu^2 + \\mathbf{s}_e^2 + 2 \\mathbf{s}_\\mu \\cdot \\mathbf{s}_e \\).\n\nSolving for \\( \\mathbf{s}_\\mu \\cdot \\mathbf{s}_e \\):\n\n\\( \\mathbf{s}_\\mu \\cdot \\mathbf{s}_e = [\\mathbf{F}^2 - \\mathbf{s}_\\mu^2 - \\mathbf{s}_e^2"}
{"id": "quantum/8001", "questions": "Express $ e^{\\begin{pmatrix} 0 & a \\\\ -a & 0 \\end{pmatrix}} $ as a $2 \\times 2$ matrix; $a$ is a positive constant.  ", "graphs": null, "llm_answers": "<think>\nOkay, so I need to find the exponential of this 2x2 matrix: \n\n\\[ \ne^{\\begin{pmatrix} 0 & a \\\\ -a & 0 \\end{pmatrix}}\n\\]\n\nHmm, exponentiating matrices isn't something I do every day, but let me try to recall how it works.\n\nFirst off, for any square matrix \\( A \\), the matrix exponential \\( e^{A} \\) can be defined using its Taylor series expansion:\n\n\\[\ne^{A} = I + A + \\frac{A^2}{2!} + \\frac{A^3}{3!} + \\cdots\n\\]\n\nWhere \\( I \\) is the identity matrix. So maybe if I compute powers of my given matrix and plug them into this series, things will start making sense.\n\nLet's denote our matrix as \\( M \\):\n\n\\[\nM = \\begin{pmatrix} 0 & a \\\\ -a & 0 \\end{pmatrix}\n\\]\n\nAlright, now let's calculate some powers of \\( M \\).\n\nStarting with \\( M^1 = M \\):\n\n\\[\nM^1 = \\begin{pmatrix} 0 & a \\\\ -a & 0 \\end{pmatrix}\n\\]\n\nNext, \\( M^2 \\). Let's multiply \\( M \\) by itself:\n\n\\[\nM^2 = M \\cdot M = \\begin{pmatrix} 0 & a \\\\ -a & 0 \\end{pmatrix} \\cdot \\begin{pmatrix} 0 & a \\\\ -a & 0 \\end{pmatrix}\n\\]\n\nCalculating each element:\n\n- Top-left: (0)(0) + (a)(-a) = 0 - a² = -a²\n- Top-right: (0)(a) + (a)(0) = 0 + 0 = 0\n- Bottom-left: (-a)(0) + (0)(-a) = 0 + 0 = 0\n- Bottom-right: (-a)(a) + (0)(0) = -a² + 0 = -a²\n\nSo,\n\n\\[\nM^2 = \\begin{pmatrix} -a^2 & 0 \\\\ 0 & -a^2 \\end{pmatrix} = -a^2 I\n\\]\n\nInteresting, that simplifies things because higher powers might follow a pattern based on this result.\n\nWhat about \\( M^3 \\)? That would be \\( M^2 \\cdot M \\):\n\nSince \\( M^2 = -a^2 I \\),\n\n\\[\nM^3 = (-a^2 I) \\cdot M = -a^2 M\n\\]\n\nSimilarly, \\( M^4 = M^3 \\cdot M = (-a^2 M) \\cdot M = -a^2 M^2 = -a^2(-a^2 I) = a^4 I \\)\n\nWait, hold on, actually:\n\n\\( M^4 = (M^2)^2 = (-a^2 I)^2 = a^4 I^2 = a^4 I \\)\n\nYes, correct.\n\nContinuing this way, we see a pattern emerging where even powers give us scalar multiples of the identity matrix, while odd powers give scalar multiples of \\( M \\).\n\nSpecifically:\n\nFor integer \\( k \\geq 0 \\),\n- If \\( n = 2k \\): \\( M^n = (-1)^k a^{2k} I \\)\n- If \\( n = 2k+1 \\): \\( M^n = (-1)^k a^{2k+1} M \\)\n\nThat seems right from what we've calculated above:\n- For \\( k=0 \\):\n    - Even power: \\( M^0 = I \\)\n    - Odd power: \\( M^1 = M \\)\n- For \\( k=1 \\):\n    - Even power: \\( M^2 = -a^2 I \\)\n    - Odd power: \\( M^3 = -a^2 M \\)\n- And so on...\n\nGreat, so plugging these back into the Taylor series for \\( e^{M} \\):\n\n\\[\ne^{M} = \\sum_{n=0}^\\infty \\frac{M^n}{n!}\n\\]\n\nWe can separate the sum into even and odd terms since they have different structures.\n\nLet’s write out the first few terms explicitly:\n\nEven terms (\\( n = 0, 2, 4, ... \\)):\n- \\( n=0 \\): \\( \\frac{I}{0!} = I \\)\n- \\( n=2 \\): \\( \\frac{-a^2 I}{2!} \\)\n- \\( n=4 \\): \\( \\frac{a^4 I}{4!} \\)\n- etc.\n\nOdd terms (\\( n = 1, 3, 5, ... \\)):\n- \\( n=1 \\): \\( \\frac{M}{1!} \\)\n- \\( n=3 \\): \\( \\frac{-a^2 M}{3!} \\)\n- \\( n=5 \\): \\( \\frac{a^4 M}{5!} \\)\n- etc.\n\nTherefore, grouping all even terms together and all odd terms together:\n\n\\[\ne^{M} = \\left( I - \\frac{a^2 I}{2!} + \\frac{a^4 I}{4!} - \\cdots \\right ) + \\left( \\frac{M}{1!} - \\frac{a^2 M}{3!} + \\frac{a^4 M}{5!} - \\cdots \\right )\n\\]\n\nFactorizing the common factors outside the sums:\n\n\\[\ne^{M} = I \\left( 1 - \\frac{a^2}{2!} + \\frac{a^4}{4!} - \\cdots \\right ) + M \\left( 1 - \\frac{a^2}{3!} + \\frac{a^4}{5!} - \\cdots \\right )\n\\]\n\nNow, recognizing those series expansions... Hmm!\n\nThe coefficients look familiar. The first series inside the parentheses resembles the cosine function's Taylor series, which is:\n\n\\[\n\\cos(x) = 1 - \\frac{x^2}{2!} + \\frac{x^4}{4!} - \\cdots\n\\]\n\nAnd the second series looks like the sine function's Taylor series divided by x, or more precisely, scaled appropriately. Wait, let me check:\n\nActually, the standard sine series is:\n\n\\[\n\\sin(x) = x - \\frac{x^3}{3!} + \\frac{x^5}{5!} - \\cdots\n\\]\n\nBut here, the coefficient before M has:\n\n\\[\n1 - \\frac{a^2}{3!} + \\frac{a^4}{5!} - \\cdots\n\\]\n\nWhich doesn’t directly match sin(a)/something? Maybe not exactly, perhaps another approach.\n\nAlternatively, notice that both series resemble hyperbolic functions?\n\nNo, wait, no—actually, considering substitution variables.\n\nIf I set \\( x = a t \\), then perhaps scaling accordingly.\n\nWait, alternatively, think of substituting \\( x = a \\theta \\)?\n\nHold on, stepping back.\n\nLooking again at the two series:\n\nSeries S₁: Coefficients multiplying I:\n\n\\[\nS_1 = 1 - \\frac{a^2}{2!} + \\frac{a^4}{4!} - \\cdots = \\cos(a)\n\\]\nBecause replacing x with 'a' gives cos(a)'s expansion.\n\nSimilarly, Series S₂: Coefficients multiplying M:\n\n\\[\nS_2 = 1 - \\frac{a^2}{3!} + \\frac{a^4}{5!} - \\cdots\n\\]\n\nThis one is trickier. But note that:\n\nFrom the definition of sine:\n\n\\[\n\\sin(a) = a - \\frac{a^3}{3!} + \\frac{a^5}{5!} - \\cdots\n\\]\n\nDivide both sides by 'a':\n\n\\[\n\\frac{\\sin(a)}{a} = 1 - \\frac{a^2}{3!} + \\frac{a^4}{5!} - \\cdots\n\\]\n\nHey, that matches S₂!\n\nThus, S₂ equals \\( \\frac{\\sin(a)}{a} \\ )\n\nPutting it all together:\n\n\\[\ne^{M} = I \\cdot \\cos(a) + M \\cdot \\frac{\\sin(a)}{a}\n\\]\n\nSubstituting back the value of M:\n\nRecall,\n\\[\nM = \\begin{pmatrix} 0 & a \\\\ -a & 0 \\end{pmatrix}\n\\]\n\nHence,\n\n\\[\ne^{M} = \\cos(a) \\cdot I + \\frac{\\sin(a)}{a} \\cdot \\begin{pmatrix} 0 & a \\\\ -a & 0 \\end{pmatrix}\n\\]\n\nCompute term-by-term:\n\nFirst term: \\( \\cos(a) \\cdot I \\)\n\n\\[\n\\cos(a) \\cdot I = \\begin{pmatrix} \\cos(a) & "}
{"id": "quantum/8006", "questions": "Answer each of the following questions with a brief and, where possible, quantitative statement. Give your reasoning.\n\n(a) A beam of neutral atoms passes through a Stern-Gerlach apparatus. Five equally-spaced lines are observed. What is the total angular momentum of the atom?\n(b) What is the magnetic moment of an atom in the state $^3P_0$? (Disregard nuclear effects)\n(c) Why are the noble gases chemically inert?\n(d) Estimate the energy density of black body radiation in this room in erg cm<sup>-3</sup>. Assume the walls are black.\n(e) In a hydrogen gas discharge both the spectral lines corresponding to the transitions $2^2P_{1/2} \\to 1^2S_{1/2}$ and $2^2P_{3/2} \\to 1^2S_{1/2}$ are observed. Estimate the ratios of their intensities.\n(f) What is the cause for the existence of two independent term level schemes, the singlet and triplet systems, in atomic helium?", "graphs": null, "llm_answers": "<think>\nOkay, so I've got these six physics problems here from different areas like quantum mechanics, thermodynamics, etc., and I need to figure them out one by one. Let me start with part (a).\n\n**Problem (a):** A beam of neutral atoms passes through a Stern-Gerlach apparatus. Five equally spaced lines are observed. What's the total angular momentum of the atom?\n\nHmm, I remember that the Stern-Gerlach experiment measures the magnetic dipole moments of particles due to spin or orbital angular momentum. The number of lines corresponds to the possible projections along the direction of the magnetic field gradient. For example, if you have l=1, which gives m_l = -1, 0, +1, leading to three lines. But wait, five lines would mean more than just orbital angular momentum because electrons also contribute spin.\n\nWait no, actually, in the case of atoms, it could be about the total angular momentum j. Each line represents a different value of m_j. So if there are five lines, then m_j can take values from -j to +j in integer steps. That means 5 lines correspond to m_j: -2, -1, 0, +1, +2. Therefore, j must be 2 since 2*2+1=5 levels. \n\nSo the total angular momentum J has magnitude sqrt(j(j+1))ħ. Since they're asking for the total angular momentum, probably expressed as ħ times something. Wait, but sometimes people refer to \"total angular momentum\" as the maximum projection divided by some factor... No, usually when someone says 'what is the total angular momentum,' they might mean its square, but maybe not necessarily. Alternatively, perhaps they want the quantum number j itself.\n\nBut let me think again. If we observe five lines, that implies 2j + 1 = 5 → j=2. Hence, the total angular momentum quantum number is 2. However, the actual vector quantity would involve Planck units. Maybe the question expects the expression in terms of ħ√(j(j+1)). So substituting j=2, √(6), hence L = √6 ℏ. Or do they expect just the quantum number? Hmm, the problem states \"the total angular momentum,\" which typically refers to the quantum mechanical operator, whose eigenvalues are given by sqrt(j(j+1))ℏ. So likely, the answer should express that.\n\nAlternatively, another approach: considering whether the splitting comes from electron spins or orbital angular momentum. Suppose the atoms pass through a magnetic field; the Zeeman effect splits the lines based on the magnetic quantum numbers. Neutral atoms without external fields don't have net charge, but their internal structure includes electrons' spins and orbitals contributing to the magnetic moment.\n\nIf the atoms exhibit a split into five lines, that suggests multiple possible m values. As before, 2j +1 =5 ⇒ j=2. Thus, the total angular momentum is j=2, meaning the magnitude squared is j(j+1)=6, so |J|=sqrt(6)ħ. But often, in such contexts, especially in older terminology, the total angular momentum is referred to via the letter symbols—like S, P, D, F—which relate to specific j values. But I'm getting confused now. Alternatively, maybe it's referring to the orbital angular momentum only, but that doesn’t fit because five lines imply higher multiplicity beyond what pure orbital would give unless including spin-orbit coupling.\n\nWait, in the absence of spin-orbit interaction, say purely orbital angular momentum, l determines the number of lines. For instance, l=4 would lead to 9 lines, which isn't our case. Spin alone contributes s=½ giving m_s=-½,+½, i.e., two lines. But combining spin and orbital leads to j=l±s. So suppose the atom has orbital angular momentum l and spin s. Then the total j can vary between |l-s| to l+s. Now, how does that affect the number of lines?\n\nIn the presence of spin-orbit coupling, the total angular momentum j will determine the number of lines. So if after accounting for all contributions, the measured observable is proportional to m_j, thus n_lines = 2j +1. Here, n_lines=5→j=2. So regardless of whether it was originally orbital plus spin, the result is j=2. Therefore, the total angular momentum is j=2, implying |J|=sqrt(2*(2)+1)*ħ=sqrt(6)ħ≈2.45ħ.\n\nI think that makes sense. So moving forward, the total angular momentum is sqrt{6} multiplied by h-bar.\n\nNow onto **problem (b)**: What is the magnetic moment of an atom in the state ^3P_0? Disregard nuclear effects.\n\nAlright, first parse the notation. The superscript ³ indicates the total angular momentum quantum number plus one, right? Because for LS coupling, the term symbol is written as ^{(2S+1)}L_J, where L is the orbital part (in letters). So ³P_0 means 2S+1=3⇒S=1, and L=P which is l=1. And J=0.\n\nThe magnetic moment μ is related to the angular momentum. Specifically, for an atom, the magnetic moment is given by:\n\nμ = g * μ_B * J,\n\nwhere g is the Landé g-factor, μ_B is the Bohr magneton, and J is the total angular momentum in appropriate units.\n\nFirst, compute the Landé g-factor. It’s defined as:\n\ng = [ (J(J+1) + S(S+1) - L(L+1) ) / (2J(J+1)) ]^{1/2}\n\nWait, no—the formula is slightly different. Actually, the Landé g-factor is calculated using:\n\ng_J = 1 + (J(J+1) + S(S+1) - L(L+1))/(2J(J+1))\n\nYes, correct. So plugging in the known values: S=1, L=1, J=0.\n\nCompute numerator inside the fraction:\n[ J(J+1) + S(S+1) – L(L+1) ]\nSince J=0,\nNumerator becomes 0 + (1)(2) – (1)(2) = 2-2=0\n\nThus, g_J = 1 + 0/(something) = 1.\n\nTherefore, the g-factor is 1.\n\nThen, the magnetic moment μ = g_J * μ_B * J_z, but wait, no—it depends on the orientation. More accurately, the magnetic moment is given by μ = g_J * μ_N * J, where μ_N is the nuclear magneton—but wait, disregard nuclear effects! Oh, okay, so use the electronic contribution instead.\n\nWait, confusion arises. Magnetic moment for an atom is generally considered as the sum over individual particle contributions. Electrons contribute with their own g-factors. But in the context of term symbols, the total magnetic moment is determined by the total angular momentum and the g-factor derived from the configuration.\n\nGiven that, yes, μ = g_J * μ_B * J, but J is in units of ħ. Wait, no, better yet, the relation is:\n\nμ = g_J * μ_B * (J / ħ)\n\nBecause μ_B = eħ/(2m_e c), so dimensions-wise, multiplying by J (which is dimensionless in multiples of ħ) scales correctly.\n\nHowever, in the ground state, J=0, so even though g_J is 1, μ would still be zero because J=0. Is that correct?\n\nWait, hold on—if J=0, then indeed the magnetic moment is zero. Because all the angular momenta cancel out. So despite having non-zero orbital and spin parts, their combination results in zero net angular momentum, making the magnetic moment zero.\n\nHence, the magnetic moment is zero.\n\nMoving on to **problem (c)**: Why are the noble gases chemically inert?\n\nWell, noble gases have full valence shells. Their outermost shell is completely filled with electrons, satisfying the octet rule (or duet for Helium). This stable electron configuration makes them highly unreactive under normal conditions because chemical reactions typically occur to achieve stability, either by gaining, losing, or sharing electrons. Noble gases already have complete outer shells, so they rarely participate in bonding. Additionally, their high ionization energies make it difficult to remove electrons, and"}
{"id": "quantum/8008", "questions": "A particle of mass $m$ moves with zero angular momentum in a spherically symmetric attractive potential $V(r)$.\n\n(a) Write down the differential equation of radial motion, defining your radial wave function carefully and specifying the boundary conditions on it for bound states. What is the $\\text {WKB}$ eigenvalue condition for s-states in such a potential? (Be careful to incorporate in your one-dimensional $\\text {WKB}$ analysis the constraints of radial motion $(0 < r < \\infty)$.\n\n(b) For $V(r) = -V_0 \\exp(-r/a)$, use the $\\text {WKB}$ relation to estimate the minimum value of $V_0$ such that there will be one and only one bound state, just barely bound. Compare your value to the exact result for the exponential potential, $2mV_0a^2/\\hbar^2 = 1.44$.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about quantum mechanics involving particles moving in a spherically symmetric potential. It's part (a) and (b). Let me try to tackle them step by step.\n\nStarting with part (a): The question asks to write down the differential equation of radial motion, define the radial wave function, specify its boundary conditions for bound states, and then find the WKB eigenvalue condition for s-states considering the constraints of radial motion from 0 to infinity.\n\nHmm, okay. So first, when dealing with a particle in spherical symmetry, we usually separate variables into radial and angular parts. Since the potential V(r) depends only on r, the Schrödinger equation can be written as:\n\n\\[\n-\\frac{\\hbar^2}{2m} \\nabla^2 \\psi + V(r)\\psi = E\\psi\n\\]\n\nBut since the system has spherical symmetry, ψ can be expressed using spherical harmonics Y_l^{m}(θ, φ), where l is the orbital angular momentum quantum number. However, the problem specifies that the particle has zero angular momentum, which means l=0. Therefore, the wavefunction simplifies because all m components collapse due to l being zero.\n\nSo, the radial part R(r) satisfies the radial Schrödinger equation. In general, the radial equation is given by:\n\n\\[\n-\\frac{\\hbar^2}{2m} \\left( \\frac{d^2R}{dr^2} + \\frac{2}{r}\\frac{dR}{dr} \\right ) + \\left(V(r) + \\frac{l(l+1)\\hbar^2}{2mr^2}\\right)R = ER\n\\]\n\nSince l=0 here, the centrifugal term disappears. Thus, the equation becomes simpler:\n\n\\[\n-\\frac{\\hbar^2}{2m} \\left( \\frac{d^2R}{dr^2} + \\frac{2}{r}\\frac{dR}{dr} \\right ) + V(r)R = ER\n\\]\n\nLet me rearrange terms:\n\nMultiply both sides by (-2m/ħ²):\n\n\\[\n\\frac{d^2R}{dr^2} + \\frac{2}{r}\\frac{dR}{dr} + \\frac{2m}{\\hbar^2}[E - V(r)]R = 0\n\\]\n\nThat should be the radial differential equation for l=0 case.\n\nNow, regarding the definition of the radial wave function: Well, R(r) must satisfy certain boundary conditions. For bound states, the solutions need to be normalizable, meaning they go to zero as r approaches infinity. Also, near r=0, the wavefunction shouldn't blow up; it needs to remain finite. Because if you plug r approaching zero into the original equation without any angular dependence, but wait—since l=0, actually, even though the centrifugal barrier isn’t present, the behavior still matters.\n\nWait, no—the presence or absence of the centrifugal term affects how the wavefunction behaves near the origin. But for l=0, the effective potential doesn't include the centrifugal repulsion, so maybe the wavefunction could behave differently?\n\nActually, regardless of l, the physical requirement is that the total wavefunction remains regular everywhere. For l=0, the radial wavefunction itself must not diverge anywhere, including at r=0. Hence, R(r) must approach zero as r→∞ and also stay bounded as r→0.\n\nTherefore, the boundary conditions are:\n- As r → ∞, R(r) → 0.\n- At r = 0, R(r) is finite.\n\nMoving on to the WKB eigenvalue condition for s-states. Hmm, WKB approximation applies best when the wavelength varies slowly compared to the variation of the potential. Here, we're supposed to consider the one-dimensional WKB treatment adapted for radial motion between 0 and ∞.\n\nIn standard WKB theory for bound states, each allowed energy level corresponds to half-integers times πℏ in action integrals over regions where the classical particle would oscillate. That is,\n\nFor a single turning point scenario, like in some potentials, but more generally, for multiple turning points...\n\nWait, perhaps better to recall the formula. The WKB quantization condition is typically:\n\n\\[\n\\int_{r_-}^{r_+} \\sqrt{\\frac{2m(E - V(r))}{\\hbar^2}} dr = (n + \\delta)\\pi \\hbar,\n\\]\nwhere n is a non-negative integer, δ accounts for phase shifts upon reflection at the boundaries, etc.\n\nHowever, in our case, the region extends from 0 to ∞, but for a bound state, the particle cannot escape beyond a certain radius, say r_max, where E = V(r_max).\n\nThus, the integral runs from 0 to r_max, but wait—is that correct? Or does it run from two turning points?\n\nNo, hold on—if the potential goes to negative values (attractive), then depending on whether the particle is above or below the asymptotic potential, the situation changes.\n\nGiven that the potential is attractive, let’s assume V(r) tends to zero as r→∞. Wait, actually, looking back, the specific form in part (b) is V(r)=−V₀ exp(−r/a), which indeed tends to −V₀ e^{-∞}=0 as r→∞. So, yes, the potential approaches zero at large distances.\n\nHence, for a bound state, the energy E must lie below the maximum of the potential—or rather, since the potential is attractive, E is less than zero. Then, the particle is confined within a range [0, r_max], where r_max is the point where E = V(r_max); beyond that, classically forbidden.\n\nWait, but in reality, for an exponentially decaying potential, V(r) starts at V(0)=−V₀ and increases towards zero as r grows. So, the potential well depth is V₀. If E > V(r) somewhere else... No, actually, for a bound state, E must be less than the potential at infinity, which is zero. So, E < 0.\n\nThen, solving E = V(r) gives us the turning point(s). Given that V(r) is increasing from V(0)=−V₀ to V(inf)=0, crossing E once. So, there is exactly one turning point at r=r₁>0.\n\nWait, but what happens near r=0? When r approaches zero, V(r)≈−V₀*(1−r/a + ... ). So, very close to zero, the potential is approximately linear? Not sure—it might depend on expansion.\n\nAnyway, going back to the WKB method. Normally, for a simple harmonic oscillator-like potential, you'd integrate from left turning point to right turning point. But in this case, the potential is defined from 0 to ∞, and the turning point occurs at some positive r.\n\nMoreover, another consideration arises because the radial coordinate starts at 0, not minus something. So, do we treat the interval from 0 to r_max as contributing to the action integral?\n\nYes, probably. So, integrating from 0 to r_max, but taking care of possible singularities at r=0.\n\nAlternatively, sometimes people adjust the lower limit based on the nature of the potential near zero.\n\nWait, in the usual three-dimensional WKB treatment, especially for s-waves (l=0), the effective potential includes the centrifugal term, but in our case, l=0, so that term vanishes. So, effectively, the \"effective\" potential is just V(r).\n\nBut in the radial WKB approximation, the expression inside the square root is similar to the one-dimensional case except multiplied by r² or adjusted accordingly—I think I may have made a mistake earlier.\n\nHold on, let me double-check the proper radial WKB setup.\n\nThe time-independent Schrodinger equation in radial coordinates for l=0 is:\n\n\\[\n-\\frac{\\hbar^2}{2m} \\left(\\frac{d^2 R}{dr^2} + \\frac{2}{r} \\frac{dR}{dr}\\right) + V(r) R = E R\n\\]\n\nMultiplying through by -2m/ħ²:\n\n\\[\n\\frac{d^2 R}{dr^2} + \\frac{2}{r} \\frac{dR}{dr} + \\frac{2m}{\\hbar^2}(E - V(r)) R = 0\n\\]\n\nThis looks different from the ordinary second-order ODEs encountered in"}
{"id": "quantum/7014", "questions": "Two particles, each of mass $M$, are bound in a one-dimensional harmonic oscillator potential $V = \\frac{1}{2} k x^2$ and interact with each other through an attractive harmonic force $F_{12} = -K (x_1 - x_2)$. You may take $K$ to be small. \n\n(a) What are the energies of the three lowest states of this system?\n\n(b) If the particles are identical and spinless, which of the states of (a) are allowed?\n\n(c) If the particles are identical and have spin $1/2$, what is the total spin of each of the states of (a)?", "graphs": null, "llm_answers": "<think>\nOkay, so I've got this problem about two particles in a 1D harmonic oscillator potential interacting via another harmonic force. Hmm, let me try to unpack it.\n\nFirst, part (a): finding the energies of the three lowest states. The setup has two masses M each, in V(x)=½k x². They also attract each other with F₁₂=−K(x₁ −x₂). K is considered small, so maybe perturbation theory applies? Or perhaps we can diagonalize some Hamiltonian matrix since there's interaction.\n\nWait, but for two particles in a harmonic oscillator, sometimes you can separate into center-of-mass and relative coordinates. That might simplify things because then their motions decouple if they don't interact beyond that. But here, they do interact with each other through F proportional to displacement difference. So probably need to consider both CM motion and internal oscillation.\n\nLet me recall: For non-interacting particles, yes, you'd use CM and relative coords. Here, the interaction complicates things, but since K is small, maybe treat it as a perturbation on top of the free case.\n\nSo first, without considering the interaction, write down the Hamiltonians for CM and relative motion.\n\nThe standard approach:\n\nDefine X=(x₁ +x₂)/√2, ξ=(x₁ −x₂)/√2. Then p_X and p_xi would be momenta conjugate variables scaled similarly.\n\nThen H0 becomes H_cm + H_rel where\n\nH_cm = [p_X²]/(2μ_total) + ½ k X²,\n\nand μ_total is reduced mass for CM motion—wait no, actually wait, when dealing with two-body systems, the effective mass for CM motion is just M*2/(something)... Wait, more accurately, the kinetic energy T is (p₁²)/(2M) + (p₂²)/(2M).\n\nExpressed in terms of X and xi, the momenta become related. Let me think carefully.\n\nIf x₁ = (X + ξ)/√2, x₂ = (X −ξ)/√2.\n\nSimilarly, p₁ = (P_X + P_xi)/√2, p₂ = (P_X − P_xi)/√2.\n\nThus, the kinetic energy T = (p₁² + p₂²)/(2M)\n\nSubstituting p₁ and p₂ gives:\n\nT = [( (P_X + P_xi)^2 / 2 ) + ((P_X − P_xi)^2 / 2)] / (2M )\n\nSimplify numerator inside brackets:\n\n[(P_X² + 2 P_X P_xi + P_xi²) + (P_X² − 2 P_X P_xi + P_xi²)] / 2 =\n\n[2 P_X² + 2 P_xi²] / 2 = P_X² + P_xi².\n\nTherefore, T = (P_X² + P_xi²)/(4M ). Because denominator was multiplied by 2M.\n\nBut wait, hold on, original expression after substitution had division by 2M outside. So yeah, correct.\n\nNow, the potential energy V is sum over individual potentials plus interaction term.\n\nEach particle feels V_i = ½ k x_i², so together V = ½ k (x₁² + x₂² ).\n\nPlus the interaction due to F₁₂; integrating F from equilibrium position... Since F = -dU/dx, U should be something like ½ K (x₁ -x₂)^2 ?\n\nYes, because dU/dxi = ∂U/∂xi = derivative w.r.t xi, so gradient components give forces. Therefore, the interaction potential is U_int = ½ K (x₁ -x₂)^2.\n\nWhich translates to ½ K ξ², since ξ=x₁ -x₂ divided by sqrt(2), right? No, wait, earlier definition: ξ=(x₁ -x₂)/sqrt(2). Thus, (x₁ -x₂)^2 = 2 ξ². Hence, U_int = ½ K * 2 ξ² = K ξ².\n\nPutting all together, the full Hamiltonian is:\n\nH = H_cm + H_rel + U_int,\n\nwhere H_cm = [P_X²]/(8M) + ½ k X²,\n\nbecause substituting back, T_cm comes from P_X squared over twice the total mass times 2 or something—I'm getting confused now.\n\nWait, better rederive properly.\n\nTotal mass is 2M? No, each particle is mass M, so total mass is 2M. Reduced mass mu for CM motion is still 2M? Wait, no—the concept of reduced mass usually refers to internal motion, not sure.\n\nAlternatively, express everything correctly.\n\nIn any case, going back:\n\nWe found that T = (P_X² + P_xi²)/(4M ), and V = ½ k (x₁² +x₂² ) + ½ K (x₁ -x₂)^2.\n\nCompute V in terms of X and ξ.\n\nGiven x₁ = (X + ξ)/√2, x₂ = (X - ξ)/√2.\n\nSo x₁² +x₂² = [ (X+ξ)^2 + (X-ξ)^2 ] / 2 = [2 X² + 2 ξ²]/2 = X² + ξ².\n\nAnd (x₁ -x₂)^2 = [ (X + ξ)/√2 - (X - ξ)/√2 ]² = [ (2ξ)/√2 ]² = (√2 ξ)^2 = 2 ξ².\n\nThus, putting it all together:\n\nV = ½ k (X² + ξ²) + ½ K*(2 ξ²) = ½ k X² + ½ k ξ² + K ξ².\n\nCombine terms:\n\nV = ½ k X² + (½ k + K) ξ².\n\nSo overall, the Hamiltonian separates into H = H_cm + H_rel, where\n\nH_cm = P_X²/(4M) + ½ k X²,\n\nH_rel = P_xi²/(4M) + (½ k + K) ξ².\n\nAh! So even though the particles interact, the separation holds because the cross terms canceled out. Interesting!\n\nThat means the system effectively consists of two independent harmonic oscillators—one for the center of mass and one for the relative coordinate—with different spring constants.\n\nSpecifically, the CM frequency ω_cm = sqrt(k/(2M)), because H_cm resembles ½ m ω² X² etc., comparing coefficients.\n\nFor H_cm: Potential is ½ k X², Kinetic is P²/(4M). Comparing to standard HO form, which is H = P²/(2m) + ½ m ω² q².\n\nHere, \"mass\" associated with X is 2M, because coefficient of P² is 1/(4M) = 1/(2*(2M)). Similarly, the angular freq ω_cm satisfies ½ (2M) ω_cm² = ½ k => ω_cm² = k/(2M) → ω_cm = sqrt(k/(2M)).\n\nSimilarly, for the relative motion, the potential is (½ k + K) ξ², and kinetic is P_xi²/(4M). Again, same structure.\n\nMass equivalent for relative motion is again 2M, leading to ω_rel = sqrt( (½ k + K ) / (2M )) ? Wait, check units.\n\nHold on, H_rel is P_xi²/(4M) + (½ k + K) ξ².\n\nStandard HO: P²/(2m) + ½ m ω² q².\n\nComparing, set 1/(4M) = 1/(2m') ⇒ m' = 2M.\n\nPotential term: ½ m' ω_rel² = (½ k + K). Plugging m'=2M,\n\n→ ½ * 2M * ω_rel² = ½ k + K ⇒ M ω_rel² = ½ k + K ⇒ ω_rel = sqrt( (½ k + K)/M ).\n\nHmm, okay.\n\nSince these are independent oscillators, the total energy levels will be E_n^{cm}, E_m^{rel} summed up.\n\nE_n^{cm} = ħω_cm(n + ½),\n\nE_m^{rel} = ħω_rel(m + ½ ),\n\nso total E = E_n + E_m.\n\nOur task is to find the three lowest energies.\n\nAssuming n,m start at 0.\n\nPossible combinations:\n\nn=0, m=0: E₀ = ħω_cm(½) + ħω_rel(½) = ½ħ(ω_cm"}
{"id": "quantum/7013", "questions": "(a) A 2-fermion system has a wave function $\\psi(1, 2)$. What condition must it satisfy if the particles are identical?  \n(b) How does this imply the elementary statement of the Pauli exclusion principle that no two electrons in an atom can have identical quantum numbers?  \n(c) The first excited state of Mg has the configuration $(3s, 3p)$ for the valence electrons. In the LS-coupling limit, which values of $L$ and $S$ are possible? What is the form of the spatial part of their wave functions using the single-particle functions $\\psi_s(\\mathbf{r})$ and $\\phi_p(\\mathbf{r})$? Which will have the lowest energy, and Why?  \n", "graphs": null, "llm_answers": "<think>\nOkay, so I've got these three parts to tackle about fermions and wavefunctions. Let me start with part (a). \n\nPart (a): It says we have a 2-fermion system with wavefunction ψ(1,2), and asks what condition it must satisfy since they're identical. Hmm... Fermions obey the Pauli exclusion principle, right? So their total wavefunction should be antisymmetric under exchange. That means swapping particle 1 and particle 2 changes the sign of the wavefunction.\n\nWait, but wait—wavefunctions for identical particles depend on whether they're bosons or fermions. Bosons are symmetric, fermions antisymmetric. Since these are fermions, yeah, the overall wavefunction needs to change sign when you swap them. But sometimes people talk about spin as well because the total wavefunction includes both space and spin parts. Oh! Right, the total wavefunction is a product of spatial and spin parts. If one is symmetric, the other must be antisymmetric, and vice versa.\n\nBut hold on—the question just mentions the wavefunction without specifying. Maybe it's referring to the entire wavefunction including spin? Or maybe not. Wait, actually, often in such problems, unless specified otherwise, \"the\" wavefunction might refer only to the spatial part. No, wait again—if they’re talking about identical particles, especially fermions like electrons, then the full wavefunction (spatial × spin) must be antisymmetric. However, each individual component doesn't necessarily need to be; rather, their combination does.\n\nSo perhaps more accurately: For identical fermions, the total wavefunction must be antisymmetric under interchange of any pair of particles. Therefore, ψ_total(1,2) = -ψ_total(2,1).\n\nAlternatively, if considering just the spatial part, depending on how the problem is phrased. But given that it’s general, probably refers to the whole thing. So the condition is anti-symmetry upon exchange.\n\nMoving on to part (b): This implies the Pauli exclusion principle—that no two electrons can have same quantum numbers. Hmmm. Well, from part (a), the wavefunction being antisymmetric leads directly to the conclusion that if all quantum numbers were the same, meaning the spatial and spin states would make the wavefunction symmetric, leading to zero probability?\n\nLet me think through that. Suppose two electrons had exactly the same set of quantum numbers. Then their spatial wavefunction would be symmetric because exchanging them wouldn’t change anything—it’d look the same. Similarly, their spin wavefunction could either be symmetric or antisymmetric. But for electrons, spins can combine into singlet (antisymmetric) or triplet (symmetric) states.\n\nIf the spatial part is symmetric, then the spin part must be antisymmetric to keep the total wavefunction antisymmetric. But if the spatial part is already symmetric due to having the same quantum numbers, then the spin part must also be antisymmetric. However, if the spin part cannot be made antisymmetric while keeping the spatial part symmetric, then there arises a contradiction.\n\nWait, let me clarify. Actually, if two electrons have the exact same quantum numbers, their combined spatial wavefunction becomes symmetric because switching them gives the same result. Now, the spin part—for two electrons, the spin singlet state is antisymmetric, and the triplet states are symmetric. To get an overall antisymmetric wavefunction, the spin part must be antisymmetric if the spatial part is symmetric. Is that always possible?\n\nYes, but here comes the catch: if the spatial wavefunction is completely symmetric, i.e., ψ(r₁,r₂)=ψ(r₂,r₁), then combining it with an antisymmetric spin state (singlet) makes the total wavefunction antisymmetric. But why does that lead to the Pauli exclusion?\n\nAh, wait another angle: suppose two electrons try to occupy the same quantum state. Their spatial wavefunction would become |ψ(r₁)|²|ψ(r₂)|² + ... something symmetric. When multiplied by the spin singlet, which is antisymmetric, the total remains antisymmetric. But isn't that allowed? Because even though they share the same position, their spins cancel out.\n\nHmm, now I'm confused. Alternatively, maybe the issue is that if the spatial wavefunction is symmetric, the spin must be antisymmetric, but if multiple particles want to do that, some conflict occurs. Wait, no—I think my confusion stems from conflating indistinguishability and the actual physical implications.\n\nActually, the key point is that if two fermions have the same complete set of quantum numbers, their wavefunction would vanish identically. Because if you write the Slater determinant for two identical particles, it collapses to zero. Hence, the wavefunction is zero, implying impossibility—they can't exist in the same state.\n\nTherefore, the antisymmetrization enforces that no two identical fermions can occupy the same quantum state simultaneously. Thus, the Pauli exclusion principle follows naturally from requiring the wavefunction to be antisymmetric.\n\nNow onto part (c): First excited state of Mg has config (3s, 3p) for valence electrons. Using LS coupling, find L and S possibilities. Also, describe the spatial wavefunction forms and determine which has lower energy.\n\nAlright, magnesium normally has electron configuration [Ne]3s². Its ground state is fully filled s orbital. The first excited state likely involves promotion of an electron from 3s to 3p. So, the valence shell goes from 3s² to 3s¹3p¹.\n\nIn LS coupling (Russell-Saunders coupling), we consider the addition of angular momenta between the two electrons. Each electron contributes its own l value. One is in s (l=0) and p(l=1). So adding l1=0 and l2=1:\n\nPossible total orbital angular momentum L:\nThe maximum L is 0+1=1, minimum is |0−1|=1. So only L=1 is possible.\n\nSimilarly, for spin S: each electron has s=½. Adding two spins, possible S=1 (triplet) or S=0 (singlet).\n\nThus, possible terms are ^2P_{1/2} and ^4P_{1/2}? Wait, no. Hold on, term symbols are based on L and S. Here, L=1 corresponds to P. And S can be 0 or 1.\n\nFor S=0 (singlet), multiplicity is 2S+1=1 → ^1P.\nFor S=1 (triplet), multiplicity is 3 → ^3P.\n\nEach term splits further according to J=L+S down to |L−S|. So for L=1 and S=0: J=1→^1P_1.\n\nFor L=1 and S=1: J can be 2,1,0 → ^3P_2, ^3P_1, ^3P_0.\n\nHence, the possible terms are ^1P_1, ^3P_2, ^3P_1, ^3P_0.\n\nHowever, typically, higher J levels correspond to higher energies within the same term symbol. But across different terms, the order depends on factors like Hund's rules.\n\nAccording to Hund's rule, maximizing spin (higher S) tends to give lower energy. Between ^1P and ^3P, the latter has higher S, hence lower energy.\n\nMoreover, among the various J levels within the ^3P term, the highest J usually lies lowest in energy. So ^3P_2 would be lower than ^3P_1, etc.\n\nAs for the spatial wavefunction, assuming single-particle orbitals ψ_s(r) and φ_p(r):\n\nSince the two electrons are in different orbitals (one s, one p), the spatial part is built from products of these. Considering symmetry, the spatial wavefunction must account for the permutation of the particles.\n\nGiven that the total wavefunction is antisymmetric, and the spin part may vary, the spatial part's symmetry affects the required spin symmetry.\n\nSpecifically, if the spatial wavefunction is symmetric under exchange, the spin must be antisymmetric (singlet); conversely, if spatial is antisymmetric, spin is symmetric (triplet).\n\nTo construct the spatial wavefunction, we take combinations of ψ_s(r₁)φ_p(r₂) ± ψ_s(r₂)φ_p(r₁). Depending on the sign, it'll be symmetric (+) or antisymmetric (-).\n\nWhich case applies depends on the spin state. For example, if the spin is symmetric (triplet), the spatial must be antisymmetric, so use minus sign. Conversely, if spin is antisymmetric (singlet), spatial uses plus.\n\nRegarding energy, generally, configurations where electrons avoid overlapping too much tend to be lower in energy. Orbitals with higher penetration (like s vs p"}
{"id": "quantum/7025", "questions": "The excited electronic configuration $(1s)^1(2s)^1$ of the helium atom can exist as either a singlet or a triplet state. Tell which state has the lower energy and explain why. Give an expression which represents the energy separation between the singlet and triplet states in terms of the one-electron orbitals $\\psi_{1s}(\\mathbf{r})$ and $\\psi_{2s}(\\mathbf{r})$.\n", "graphs": null, "llm_answers": "<think>\nOkay, so I have this question about the helium atom's excited electronic configuration being (1s)^1(2s)^1, right? And it says that this configuration can be either a singlet or a triplet state. The task is to figure out which state has the lower energy and then give an expression for their energy difference using the 1s and 2s orbitals.\n\nHmm, let me start from what I remember. Helium normally has two electrons in the ground state, both in the 1s orbital with opposite spins because of the Pauli exclusion principle. But when it gets excited, like if you add some energy, maybe through heat or light, one electron jumps up to the higher energy level, say 2s. So now we're looking at each having different principal quantum numbers: n=1 and n=2.\n\nWait, but wait—electrons also have spin. Each electron has a spin-up or spin-down state. In the ground state, they must have opposite spins due to Pauli’s rule since they’re in the same orbital. Now, when one moves to another orbital, does that change things?\n\nSo, considering the excited state where there's one electron in 1s and one in 2s. Since these are different orbitals, the Pauli exclusion doesn't force them into opposite spins anymore. That means their total spin could vary. For two electrons, possible total spin S would be... Let me think. When adding two spins-½ particles, the total spin can be S = 0 (singlet) or S = 1 (triplet). \n\nRight! Singlet state corresponds to S=0, meaning the spins are anti-aligned, while triplet states correspond to S=1, which includes three possibilities: m_s = -1, 0, +1. These arise from combining the individual spins differently.\n\nNow, how do these spin configurations affect the overall energy of the system? Well, I recall something called exchange interaction. Exchange interaction arises because identical fermions cannot occupy the same quantum state; hence, their wavefunction must be antisymmetric under exchange. This leads to a lowering of potential energy depending on whether the spatial part is symmetric or antisymmetric relative to the spin part.\n\nIn other words, the symmetry properties of the spatial and spin parts influence the effective Coulomb repulsion between the electrons. If the spatial wavefunction is symmetric, the spin must be antisymmetric (like the singlet), leading to less overlap and thus lower repulsion. Conversely, if the spatial wavefunction is antisymmetric, the spin should be symmetric (the triplet).\n\nBut wait, actually, no—the exact effect depends on the specific situation. Wait, perhaps more accurately, the exchange term contributes to the energy based on the product of the spatial integrals and the spin multiplicity.\n\nLet me try to structure my thoughts:\n\nFor two electrons in different orbitals ψ_a(r₁)ψ_b(r₂), the total wavefunction Ψ must account for both space and spin. It will look like [ψ_a(r₁)ψ_b(r₂) ± ψ_b(r₁)ψ_a(r₂)] multiplied by a spin function.\n\nIf the spatial part is symmetric (same upon swapping r₁ ↔ r₂), then the spin part needs to be antisymmetric (singlet). Similarly, if the spatial part is antisymmetric, the spin part is symmetric (triplet).\n\nThe key point here is that the exchange integral affects the energy. The exchange energy tends to lower the energy when the spatial wavefunctions allow constructive interference in certain regions, thereby reducing the average Coulomb repulsion.\n\nAlternatively, thinking about Hund's rules might help. Hund's first rule suggests that maximum spin gives minimum energy—that is, triplet states tend to be lower in energy than singlets when all else is equal. Because in triplet states, the parallel spins lead to greater exchange interactions, which reduce the electrostatic repulsion between electrons.\n\nWait, hold on—I'm getting conflicting intuitions here. On one hand, exchange interaction lowers energy when the spatial wavefunction allows for better exchange, which happens when the spin is such that the combined wavefunction remains antisymmetric. Alternatively, Hund's rule tells us that higher spin states (more parallel spins) result in lower energies because of increased exchange stabilization.\n\nWait, but in our case, the electrons are not in the same orbital—they're in 1s and 2s. Does Hund's rule apply directly here? Or am I mixing concepts?\n\nHund's rule applies primarily to degenerate orbitals, i.e., orbitals of the same energy within a subshell. Here, 1s and 2s aren’t degenerate because they belong to different shells. Therefore, Hund's rule may not directly determine the ordering of singlet vs triplet in this scenario.\n\nInstead, perhaps the main factor is the exchange interaction itself. Let me consider the expectation value of the Hamiltonian for the two cases.\n\nThe energy comes from the kinetic energy and the electron-electron repulsion. However, the exchange term plays a role beyond just the direct Coulomb repulsion. Specifically, the exchange term involves integrating over the products of the orbitals and accounts for the indistinguishability of the electrons.\n\nMathematically, the exchange energy contribution E_x is given by:\nE_x ∝ <ψ| J |ψ> – F,\nwhere J is the Coulomb operator and F is the exchange operator. Hmm, though I need to get back to basics.\n\nActually, the total energy for a two-electron system can be written as:\nE = ⟨Ψ| H |Ψ⟩,\n\nWhere H is the sum of the single-particle Hamiltonians plus the electron-electron repulsion term.\n\nBreaking down the matrix elements, especially focusing on the exchange term, which is significant even when the electrons are in different orbitals.\n\nWhen the electrons are in different orbitals, the exchange term becomes non-zero only if the orbitals are orthogonal. Oh wait, 1s and 2s are not exactly orthogonal because s-orbitals have radial nodes, but they still have some overlap. Although, generally, higher orbitals are more extended, so 2s extends further than 1s, but they share some region near the nucleus.\n\nTherefore, the exchange integral isn't zero, and its sign matters. How does the exchange contribute to the energy?\n\nI think the exchange term typically reduces the energy compared to the classical Coulomb repulsion because it effectively \"screens\" the charge. So, when the exchange term is positive, it subtracts from the Coulomb repulsion, making the net energy lower.\n\nWait, but the actual calculation requires knowing the signs correctly. Maybe I should write the general form of the energy splitting.\n\nAnother approach: Consider the singlet and triplet states' energies. The singlet state has S=0, and the triplet has S=1. Their energies differ due to the spin-spin coupling, which is influenced by the exchange interaction.\n\nThe formula for the energy difference ΔE between the triplet and singlet states is often expressed via the exchange integral. More precisely, the energy levels split according to the spin-statistics, and the difference relates to twice the exchange integral times some coefficient involving the Slater determinants.\n\nWait, perhaps recalling that the singlet-triplet gap is proportional to the square root of the determinant formed by the overlaps of the orbitals involved. No, maybe not quite.\n\nAlternatively, the energy difference can be found using perturbation theory, treating the spin-dependent terms as small corrections. But I don't know if that's applicable here.\n\nWait, going back to basic principles. The Heisenberg model treats the spin-spin interaction, but in atoms, the dominant effect is the exchange interaction arising from the antisymmetry requirement.\n\nPerhaps the correct way is to compute the expectation values of the Hamiltonian for both singlet and triplet states.\n\nAssuming the spatial part is symmetric for the singlet and antisymmetric for the triplet—or vice versa—it'll depend on the specifics.\n\nWait, suppose the spatial wavefunction is symmetric. Then the spin must be antisymmetric (singlet). The exchange term would involve the integral of ψ_1*(r1)ψ_2*(r2) * ψ_2(r1)ψ_1(r2) dr1 dr2. Which simplifies to the square of the integral of ψ_1*ψ_2 dr, integrated again.\n\nWait, no—if I denote the spatial wavefunction as φ_symmetric = [φ_a(r1)φ_b(r2) + φ_b(r1)φ_a(r2)] / sqrt(2}, and similarly φ_asymmetric = [φ_a(r1)φ_b(r2) − φ_b(r1)φ_a(r2)} ]/sqrt(2}. Then, the corresponding spin functions would be singlet and triplet respectively.\n\nThen, the expectation value of the Hamiltonian for each state would include contributions from the direct and exchange terms.\n\nSpecifically, the energy for the singlet state would have a larger exchange contribution because the spatial part is symmetric, allowing for more constructive exchange effects.\n\nWait, but I thought earlier that exchange usually stabilizes systems, lowering energy. So if the singlet has a symmetric spatial part paired with an antisymmetric spin part, the exchange term would be negative, contributing to a lower energy.\n\nHold"}
{"id": "quantum/4002", "questions": "A particle of charge $q$ and mass $\\text m$ is subject to a uniform electrostatic field $\\mathbf{E}$.\n\n(a) Write down the time-dependent Schrödinger equation for this system.\n\n(b) Show that the expectation value of the position operator obeys Newton's second law of motion when the particle is in an arbitrary state $\\psi(\\mathbf{r}, t)$.\n\n(c) It can be shown that this result is valid if there is also a uniform magnetostatic field present. Do these results have any practical application to the design of instruments such as mass spectrometers, particle accelerators, etc.? Explain.", "graphs": null, "llm_answers": "<think>\nOkay, so I've got this problem about quantum mechanics involving a charged particle in electric fields. Let me try to work through each part step by step.\n\nStarting with part (a): They want the time-dependent Schrödinger equation for a particle of charge q and mass m in a uniform electrostatic field E. Hmm, okay. The general form of the TDSE is iħ ∂ψ/∂t = H ψ, where H is the Hamiltonian. So what would the Hamiltonian look like here?\n\nIn classical terms, the potential energy due to an electric field is V(r) = -q φ(r), right? And since it's a uniform electric field, the scalar potential φ should vary linearly with position. Specifically, φ(r) = -E·r + constant. But constants don't matter because they just shift the zero point of energy. So maybe we can write φ(r) ≈ -E z or something depending on direction, but perhaps more generally, let’s say φ(r) = e E x, assuming E points along x-axis for simplicity.\n\nWait, actually, no—wait: For a uniform electric field E pointing in some direction, say x-direction without loss of generality, then the potential φ(x,y,z,t)= -E x, ignoring constants. Because the electric field is related to the gradient of the potential: E = -∇φ. If E is uniform, its components are constant, so integrating gives φ proportional to position.\n\nSo the potential energy term in the Hamiltonian will be U = q φ(r). Wait, wait—isn’t the force F = q E, which comes from the negative gradient of the potential. Therefore, the potential energy is V(r) = -∫ F · dr = -∫ q E dx = -q E x + C. Since potentials are defined up to a constant, we can set C=0 for simplicity, giving V(r) = -q E x.\n\nBut hold on—if the electric field is uniform, does that mean the potential varies linearly? Yes, exactly. So our potential function is simply V(r) = -e E x, considering units correctly—I think sometimes people take E as vector, so maybe better written as V(r) = -q E r cosθ, but probably easier to align coordinates so E is along one axis.\n\nTherefore, the Hamiltonian H is kinetic plus potential: H = p²/(2m) + (-q E x). Or using momentum operators, H = [(-iħ ∇)^2]/(2m) - q E x. Alternatively, writing out explicitly:\n\nH = -(ħ² / 2m) ∇² + (-q E x)\n\nYes, that seems correct. So plugging into the TDSE,\n\niħ (∂ψ/∂t) = [-(ħ² / 2m) ∇² - q E x] ψ\n\nThat should be the required equation.\n\nMoving on to part (b): We need to show that the expectation value of position satisfies Newton's second law when the particle is in an arbitrary state ψ(r, t).\n\nHmm, expectation values often relate to Ehrenfest theorem. Right! Ehrenfest's theorem states that d/dt ⟨x⟩ = ⟨p_x/m⟩, and similarly for other observables under certain conditions. Then, taking derivatives again might give us acceleration equaling force over mass.\n\nLet me recall Ehrenfest's theorem precisely. In general, for an observable A, d⟨A⟩/dt = (1/iħ) ⟨[A,H]⟩ + ⟨∂A/∂t⟩. Here, A is the position operator x, which doesn't depend explicitly on time, so the last term vanishes. Thus,\n\nd⟨x⟩/dt = (1/iħ) ⟨[x, H]⟩\n\nCompute [x, H]: H has two parts—the kinetic and potential. Kinetic is T = p²/(2m); Potential is V = -q E x.\n\nFirst compute [x, T]:\n\n[x, p²/(2m)] = (1/(2m)) [x,p²]\n\nWe know that [x, p^k] involves derivatives; specifically, [x, p] = iħ, so higher commutators follow accordingly. Using [x, p^n] = n ħ p^{n-1} } ? Wait, not quite sure—it depends on how you expand it.\n\nAlternatively, use explicit computation:\n\n[p², x] = p [p,x] + [p,x] p = p(iħ) + (iħ)p = 2iħ p\n\nThus, [x, p²] = - [p², x] = -2iħ p\n\nThen, [x, T] = (1/(2m))( -2iħ p ) = (-iħ p)/m\n\nSimilarly, now compute [x,V], where V = -q E x.\n\nV is already proportional to x, so [x, V] = [x, -q E x ] = -q E [x, x] = 0.\n\nBecause [x, x]=0.\n\nPutting together, [x, H] = [x,T] + [x,V] = (-iħ p)/m + 0 = (-iħ p)/m\n\nPlugging back into Ehrenfest:\n\nd⟨x⟩/dt = (1/iħ)( ⟨ (-iħ p)/m ⟩ )\n\nSimplify numerator: (1/iħ)*(-iħ/m) ⟨p⟩ = (1/iħ)*(-iħ/m) ⟨p⟩ = (1/m) ⟨p⟩\n\nWhich matches the first part of Ehrenfest: velocity equals momentum divided by mass.\n\nNow, differentiate once more to get acceleration:\n\nd²⟨x⟩/dt² = d/dt (⟨p⟩/m) = (1/m) d⟨p⟩/dt\n\nAgain applying Ehrenfest for p:\n\nFor operator p, d⟨p⟩/dt = (1/iħ) ⟨[p, H]⟩ + ⟨∂p/∂t⟩. Again, partial derivative w.r.t. time is zero unless p itself depends on time, which it isn't here.\n\nCompute [p, H]:\n\nH = T + V, same as before.\n\nCompute [p, T] = [p, p²/(2m)] = 0, since p commutes with itself.\n\nNext, [p, V] = [p, -q E x ]\n\nSince V = -q E x, [p, V] = -q E [p, x] = -q E (iħ)\n\nThus, [p, H] = [p, T] + [p, V] = 0 - q E iħ\n\nPlug into Ehrenfest:\n\nd⟨p⟩/dt = (1/iħ) * ⟨ [p, H] ⟩ = (1/iħ) * ⟨ -q E iħ ⟩ = (1/iħ)( -q E iħ ⟨1⟩ ) = (1/iħ)( -q E iħ ) = q E\n\nAssuming normalization, ⟨1⟩=1.\n\nThus, putting it all together:\n\nd²⟨x⟩/dt² = (1/m) d⟨p⟩/dt = (1/m)(q E) => d²⟨x⟩/dt² = (q/m) E\n\nThis is exactly Newton's second law: Force F = ma, where F=qE, hence a=F/m=(q/m)E.\n\nSo yes, the expectation value follows Newton's laws.\n\nPart (c): Does this result apply even when a uniform magnetic field is present? Well, the question says \"it can be shown\" that the result holds even with a uniform B-field. Why is that?\n\nWell, in presence of both E and B fields, the Lorentz force is F = q(E + v × B ). However, in the case of a uniform static magnetic field, the vector potential contributes to the Hamiltonian via minimal coupling: replacing p → p - (q/c) A.\n\nIf the magnetic field is uniform, say B=B_0 k̂, then the vector potential can be chosen as A = (B y, 0, 0 ), leading to a symmetric gauge. When calculating the commutator [x, H], things change slightly because H includes the vector potential.\n\nHowever, when computing the Ehrenfest equations, particularly for the position expectation value, the effect of the magnetic field may enter only in cross terms. But given that the magnetic field is uniform, perhaps the additional terms cancel out or do not affect the overall acceleration.\n\nWait, another approach: consider whether the Ehrenfest theorem still leads to the same conclusion when including the magnetic field. Suppose the Hamiltonian becomes H = (p - q A)^2/(2m) + q φ.\n\nWith φ being the scalar potential due to E, and A the vector potential due to B.\n\nComputing ["}
{"id": "quantum/7022", "questions": "Consider the (homonuclear) molecule $N^{14}_2$. Use the fact that a nitrogen nucleus has spin $I = 1$ in order to derive the result that the ratio of intensities of adjacent rotational lines in the molecule's spectrum is 2:1. ", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about the homonuclear diatomic molecule N₂¹⁴. The question says that each nitrogen nucleus has a spin I equal to 1, and we need to find out why the intensity ratio between adjacent rotational lines in its spectrum is 2:1.\n\nHmm, let me start by recalling what I know about molecular spectra and nuclear spins. Rotational spectroscopy deals with transitions between different rotational energy levels of molecules when they absorb or emit microwave radiation. For diatomic molecules like N₂, these rotations occur around their center of mass axis because both nuclei are identical, making it symmetric.\n\nNow, since the molecule is homonuclear, certain selection rules apply for allowed transitions. In general, for pure rotation transitions, the change in angular momentum quantum number J must be ±1. But wait, how does the nuclear spin come into play here?\n\nOh right! Nuclear spin affects the statistical weights of the rotational states due to something called symmetry considerations. Because the two nuclei are indistinguishable fermions if they're protons but bosons otherwise? Wait no, actually, nitrogen-14 has a spin I=1 which makes them bosons because integer spin particles obey Bose-Einstein statistics.\n\nBut hold on, the overall wavefunction of the molecule includes contributions from translational, vibrational, rotational, electronic, and nuclear motions. Since the molecule is homonuclear, the total wavefunction must account for exchange of the two identical nuclei. This means that the spatial part multiplied by the spin part should give either even or odd parity depending on whether the nuclei are bosons or fermions.\n\nWait, nitrogen atoms have atomic number 7, so each has 7 electrons. Electrons are fermions, but as a whole atom, do they behave differently? No, individual atoms aren't considered; rather, the entire system's wavefunction matters. So considering the nuclei being bosons (since I=1), the combined effect would influence the symmetry of the total wavefunction under exchange.\n\nThe total wavefunction Ψ_total is a product of spatial (rotational state), spin (nuclear spin), and other parts. When you swap the two nuclei, the spatial part changes sign based on the rotational state's parity. Specifically, swapping leads to (-1)^J where J is the rotational quantum number. Similarly, the spin part will contribute another factor based on the permutation of the spin state.\n\nSince the nuclei are bosons, the total wavefunction must be symmetric upon exchange. Therefore:\n\nΨ_spatial * Ψ_spin → (+1)\n\nSo,\n\n(-1)^{J} * S = +1\n\nWhere S is the eigenvalue of the spin part under exchange. Hence,\n\nS = (-1)^{J}\n\nThis tells us that the spin function must transform according to (-1)^J when exchanging the nuclei.\n\nNext, I think about the possible spin states available given that each nucleus has spin I=1. Two spin-1 particles can combine to form total spin multiplets. Let me recall the addition of angular momenta. Spin 1 ⊗ spin 1 gives possible total spins S = 0, 1, 2.\n\nEach of these corresponds to specific symmetries under particle exchange. For two identical bosons, the singlet state (S=0) is antisymmetric, while triplet states (S=1) and quintet (S=2) might have different symmetries.\n\nActually, more precisely, the Clebsch-Gordan decomposition for two spin-1s is:\n\n|S M> = |1 1>, |1 0>, |1 -1>, |2 2>, |2 1>, |2 0>, |2 -1>, |2 -2>\n\nAnd the exchange operator P swaps the two particles. Applying P to a state |m₁ m₂⟩ gives |m₂ m₁⟩. How does this affect the coupled basis states?\n\nFor symmetric states (bosonic): applying P leaves the state unchanged up to phase factors. However, for two spin-1 particles, some combinations may lead to symmetric or anti-symmetric states.\n\nAlternatively, perhaps using the formula for the exchange symmetry of coupled states. Each state |S M> transforms under exchange as (-1)^{S}. Is that correct? Or maybe not exactly... Hmm, I'm getting confused here.\n\nLet me try again. If we consider the exchange operation acting on the spin space, then for two identical bosons, the spin state must satisfy P|spin⟩ = |spin⟩ times some phase. Actually, for bosons, the full wavefunction including all degrees of freedom must be symmetric under exchange. Here, our focus is just on the spin part contributing along with the spatial part.\n\nEarlier equation was:\n\n(-1)^J * [exchange symmetry of spin] = +1\n\nTherefore, the spin exchange symmetry needs to be (-1)^{-J}, i.e., same as (-1)^J.\n\nThus, the spin state must pick up a factor of (-1)^J when exchanged.\n\nGiven that, now looking back at the possible spin states. For two spin-1 particles, the symmetric and antisymmetric properties depend on the total spin S.\n\nIn particular, the singlet state (S=0) is totally symmetric only if there's an appropriate combination, but wait, for two spin-1 systems, the singlet state isn't necessarily symmetric. Maybe better to compute the exchange matrix elements.\n\nAnother approach: use the Wigner theorem. Under exchange, the state |S M> picks up a phase factor e^{iθ_S}, such that θ_S depends on S. Alternatively, the exchange operator P satisfies P^2 = identity, hence eigenvalues are ±1.\n\nFrom group theory, for two identical particles, the action of P on a state |S M> is given by (-1)^{L+S} where L is orbital angular momentum. Wait, but in our case, the spatial part already contributes a factor of (-1)^J, so combining with spin...\n\nNo, perhaps instead, focusing purely on spin exchange without involving spatial variables yet.\n\nWhen dealing with two spin-1 particles, the exchange operator acts on the spin Hilbert space. To determine the symmetry of each spin state, note that for two identical bosons, the spin state must be symmetric under exchange if the spatial part is symmetric, and vice versa.\n\nHowever, in our earlier consideration, the spatial part (rotational state) contributes a factor of (-1)^J upon exchange. Thus, the spin part must compensate for that to make the total wavefunction symmetric.\n\nHence, the condition becomes:\n\n[Exchange Symmetry of Spatial Part] × [Exchange Symmetry of Spin Part] = +1\n\nWhich translates to:\n\n(-1)^J × [Spin Exchange Factor] = +1 ⇒ [Spin Exchange Factor] = (-1)^J\n\nMeaning, the spin state must transform as (-1)^J under exchange.\n\nLooking at the possible spin states for two spin-1 particles:\n\nTotal spin S can be 0, 1, 2.\n\nWe can figure out the exchange symmetry for each S.\n\nIt turns out that for two identical bosons, the spin states with even S are symmetric, and those with odd S are antisymmetric. Wait, is that accurate?\n\nHold on, for two spin-I particles, the exchange symmetry of the state |S M> is determined by (-1)^{S}. That comes from the fact that the exchange operator commutes with the Casimir operators, leading to eigenstates labeled by S, and the eigenvalue is (-1)^{S+something}? Not sure.\n\nWait, perhaps referring to ladder operators could help. Another way: construct the exchange operator explicitly.\n\nIf we denote the two-particle spin state as |s₁ s₂⟩, then the exchange operator P swaps s₁ and s₂. It's known that for two identical particles, the exchange operator squared equals identity, so P² = 1.\n\nMoreover, for two spin-J particles, the representation of P in terms of SU(2) generators involves the Pauli matrices. Perhaps too complicated.\n\nAn alternative method is to remember that for two spin-1 particles, the singlet state (S=0) is symmetric or antisymmetric? Let me see.\n\nUsing Young tableaux, for two boxes representing two spin-1 particles, the dimensionality goes as follows:\n\nSymmetric tensor product: dim(Sym²(V)) = (d(d+1))/2 where d=3 for spin-1. So Sym²(V)=6-dimensional, corresponding to S=2 and S=0? Wait, no, S=2 is five dimensional, S=0 is one dimensional. Hmmm, confusion arises.\n\nWait, no, the symmetric subspace for two spin-1 particles is indeed six-dimensional, which decomposes into S=2 (5D) and S=0 (1D). While the antisymmetric subspace is three-dimensional, which corresponds to S=1 (three dimensions).\n\nAh, okay, so the symmetric spin states correspond to S=2 and S=0, whereas the antisymmetric ones correspond to S=1.\n\nThat helps!\n\nBecause, for two identical bosons, the spin state must be symmetric under exchange. From above, the symmetric spin states are S=0 and S=2, while the antisymmetric ones are S=1.\n\nSimilarly, the spatial part (which is the rotational state) has a parity of (-1)^J. Now,"}
{"id": "quantum/8009", "questions": "Set up the relevant equations with estimates of all missing parameters. The molecular bond (spring constant) of $\\text {HCl}$ is about 470 $\\text {N/m}$. The moment of inertia is $2.3 \\times 10^{-47}$ $\\text{kg·m²}$.\n\n(a) At 300 $\\text K$ what is the probability that the molecule is in its lowest excited vibrational state?\n\n(c) Of the molecules in the vibrational ground state what is the ratio of the number in the ground rotational state to the number in the first excited rotational state?", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem here about HCl molecules and their vibrational and rotational states. It's part a and c, but let me focus on each one separately because they seem related but different parts.\n\nStarting with part (a): \"At 300 K what is the probability that the molecule is in its lowest excited vibrational state?\" Hmm, okay. So we're dealing with statistical mechanics here, specifically Boltzmann distribution for energy levels. \n\nFirst, I need to recall how probabilities work in such systems. For quantum mechanical systems like diatomic molecules, the partition function comes into play when calculating these probabilities. But since it's just asking about two specific states—the ground state and the first excited vibrational state—I might not need the full partition function, maybe just compare their energies or something similar.\n\nWait, actually, no. Because even though there are multiple rotational states within each vibrational level, if we’re only considering whether the molecule is in the ground vibrational state versus the first excited, perhaps those can be treated as separate entities where each has their own set of rotational states. Or do I need to consider them together?\n\nLet me think: In thermal equilibrium, the population of each energy state follows the Boltzmann factor. That means the probability P_n of being in state n is proportional to g_n * exp(-E_n/(kT)), where g_n is the degeneracy of the nth state, E_n is the energy, k is Boltzmann’s constant, T is temperature.\n\nBut wait, in this case, the question isn't specifying any particular rotational state—it's talking about the entire vibrational state regardless of rotation. So does that mean I should sum over all possible rotational states within each vibrational level?\n\nYes, probably. Each vibrational state will have several rotational substates, each contributing to the total partition function. Therefore, the probability would involve comparing the contributions from the ground vibrational state plus all its rotations vs. the first excited vibrational state plus all its rotations.\n\nSo, more formally, the probability P_vib_excited is equal to [Z_1] / [Z_total], where Z_1 is the contribution from the first excited vibrational state and Z_ground is the contribution from the ground vibrational state. Then, Z_total = Z_ground + Z_1.\n\nEach Z_state is given by the product of the degeneracy (which depends on rotational states) times the exponential term involving energy difference divided by kT.\n\nHmm, but wait—vibrational energy levels are quantized, right? And similarly, rotational ones too. Let me write down some formulas:\n\nThe energy levels for vibration are approximately harmonic oscillator-like:\nE_v = (v + 1/2) h f,\nwhere v=0,1,2,... and f is the frequency.\n\nSimilarly, rotational energy levels are:\nE_j = B j(j+1),\nwhere B is the rotational constant, j=0,1,2,...\n\nIn terms of temperatures, both types of motion contribute differently depending on whether the thermal energy kT is comparable to the spacing between levels.\n\nGiven that the spring constant k_spring is 470 N/m, which relates to the vibrational frequency via f = sqrt(k_spring / μ), where μ is the reduced mass.\n\nWe also know the moment of inertia I is given as 2.3e-47 kg m^2. From that, the rotational constant B can be calculated using B = ħ²/(2I). Wait, hold on—isn’t B usually expressed in wavenumbers? Maybe better to express everything consistently.\n\nAlternatively, sometimes people use B = ℏ²/(2I k_B), but units get tricky. Alternatively, another formula connects B with moments of inertia: B = ħ²/(8π² I k_B), but I'm getting confused now. Perhaps I'll come back to that after figuring out the necessary steps.\n\nBack to the main point: To find the probability of being in the first excited vibrational state, I need to compute the ratio of the partition functions for the first excited vibrational state compared to the total partition function including ground and first excited.\n\nHowever, computing the exact partition function requires knowing the density of states or accounting for all possible rotational states within each vibrational level. Since the question doesn't specify anything else, maybe we can make approximations based on typical conditions at room temperature (300K).\n\nFor diatomic molecules, typically, rotational modes may be active at higher temps than vibrations. Vibrations require much larger amounts of energy due to high frequencies, so at 300K, most molecules remain in the ground vibrational state unless the temperature is very high.\n\nTherefore, the probability of excitation to the first vibrational state could be low, but let's see.\n\nTo calculate this, I'd need the energy difference ΔE between the ground and first excited vibrational states. Once I have that, I can plug into the Boltzmann factor.\n\nEnergy difference ΔE = E_1 - E_0 = h f,\n\nsince E_v = (v + 1/2)h f; subtracting gives h f.\n\nThus, the relative populations depend exponentially on -ΔE/(kT).\n\nAdditionally, each vibrational state has associated rotational states. However, assuming that the rotational structure is fine enough that the rotational partition function contributes equally to both vibrational states—or rather, the same multiplicative factor—but wait, actually, the rotational constants relate to the moment of inertia, which changes slightly upon vibration. Oh! When you go from v=0 to v=1, the interatomic distance r changes, hence the moment of inertia I changes, thus changing the rotational constant B.\n\nIs that significant? Probably negligible for small displacements, especially in HCl. Given that HCl is quite rigid, the change in bond length during vibration is minimal compared to the average bond length. Thus, the effect on the moment of inertia—and therefore on the rotational energy levels—might be considered small. Hence, perhaps treating the rotational partition function as roughly the same for both vibrational states.\n\nIf that approximation holds, then the ratio simplifies to just the ratio of the Boltzmann factors multiplied by the respective degeneracies—if any—for each vibrational state.\n\nBut wait, the vibrational states themselves don't have intrinsic degeneracy beyond what arises from nuclear spin or other symmetries, but for diatomics, the vibrational states aren't inherently degenerate—they split due to symmetry considerations, but generally, each vibrational state is non-degenerate except possibly under certain circumstances.\n\nAssuming each vibrational state is non-degenerate, meaning g_v = 1 for each v. If that's true, then the ratio becomes simply exp( -ΔE/(kT) ) / [1 + exp( -ΔE/(kT) ) ].\n\nBecause the total partition function Z_total = Z_ground + Z_first_excited ≈ 1*exp(-E_0/kT) + 1*exp(-E_1/kT)\n\nSince E_0 = (0 + 1/2)hf, E_1=(1 + 1/2)hf\n\nThen, Z_total = exp(-(1/2 hf)/kT) + exp(-(3/2 hf)/kT )\n\nFactor out exp(-(1/2 hf)/kT):\n\nZ_total = exp(-(1/2 hf)/kT)[1 + exp(-hf/kT)]\n\nAnd the probability P_1 is [exp(-(3/2 hf)/kT)] / Z_total\n\nWhich equals [exp(-(3/2 hf)/kT)] / [exp(-(1/2 hf)/kT)(1 + exp(-hf/kT))]\n\nSimplify numerator and denominator:\n\nExp(-(3/2 hf)/kT) / Exp(-(1/2 hf)/kT) = exp(-hf/kT)\n\nDenominator inside brackets remains (1 + exp(-hf/kT))\n\nThus, overall P_1 = exp(-hf/kT)/(1 + exp(-hf/kT)) = 1/(exp(hf/kT) + 1)\n\nThat seems familiar—it looks like a Fermi-Dirac distribution, but in reality, it's just a simple two-level system.\n\nSo, yes, the probability is 1/(exp(βΔE)+1 ), where β=1/(kT), ΔE=h f.\n\nNow, to proceed numerically, I need to compute hf, i.e., the energy corresponding to the vibrational transition.\n\nFrom the given data: Spring constant k = 470 N/m.\n\nReduced mass μ needed for f calculation. HCl consists of H and Cl atoms.\n\nMasses: M_H ≈ 1 u, M_Cl≈35.5 u.\n\nConvert atomic mass units to kilograms: 1u ≈ 1.66e-27 kg.\n\nμ = (M_H*M_Cl)/(M_H + M_Cl) = (1*35.5)/(1 + 35.5)*1.66e-"}
{"id": "quantum/8007", "questions": "(a) Derive the conditions for the validity of the $\\text {WKB}$ approximation for the one-dimensional time-independent Schrödinger equation, and show that the approximation must fail in the immediate neighborhood of a classical turning point.\n\n(b) Explain, using perturbation theory, why the ground state energy of an atom always decreases when the atom is placed in an external electric field.", "graphs": null, "llm_answers": "<think>\nOkay, so I have two parts to this problem: part (a) about deriving the WKB approximation's validity conditions and showing it fails near a classical turning point; and part (b), explaining with perturbation theory why the ground state energy decreases when an atom is put into an external electric field. Let me tackle them one by one.\n\nStarting with part (a). The Wentzel-Kramers-Brillouin (WKB) approximation is used in quantum mechanics as a method to approximate solutions to the Schrödinger equation. It works best where the potential varies slowly compared to the wavelength of the particle—so regions where the de Broglie wavelength changes gradually. But how do we derive its conditions?\n\nI remember from my studies that the WKB approximation assumes that the wavefunction can be expressed as a rapidly oscillating function multiplied by a slowly varying amplitude. Mathematically, something like ψ(x) = A(x)e^{iφ(x)}, where φ(x) has a large gradient because the exponential term oscillates quickly. \n\nTo find the condition under which this holds true, let's consider substituting this ansatz into the time-independent Schrödinger equation:\n\n(-ħ²/2m)(ψ''(x)) + V(x)ψ(x) = Eψ(x)\n\nSubstituting ψ(x):\n\nFirst derivative:\nψ'(x) = [A'(x)i e^{iφ} ] + [A(x) i φ' e^{iφ}] \n= i[A'(x) + A(x)φ']e^{iφ}\n\nSecond derivative:\nψ''(x) = -[A'(x) + A(x)φ']^2 e^{iφ} + [A''(x) + 2A'(x)φ' + A(x)φ'' ] e^{iφ}\nWait, maybe better to compute more carefully.\n\nLet’s denote f(x) = e^{iφ}, then ψ = A f\n\nSo,\n\nψ’ = A’f + A f’\n\nSimilarly,\nψ'' = A''f + 2A’f’ + A f''\n\nNow plug back into Schrodinger equation:\n\n[-ħ²/(2m)]*(A''f + 2A’f’ + A f'') + V A f = E A f\n\nDivide both sides by f (assuming f ≠0):\n\n[-ħ²/(2m)]*(A'' + 2A’φ’ + A(φ’)² + ħ^{-1}(something? Wait no.)\n\nHold on, actually, since f = e^{iφ}, f’ = iφ’ e^{iφ}. So f’ / f = iφ’. Similarly, f'' / f would involve second derivatives.\n\nBut perhaps another approach is expanding each term properly.\n\nAlternatively, considering leading order terms in the expansion assuming that the variation of A is slow compared to the phase factor. That suggests that d(A)/dx ≪ |φ'| * A or similar scaling.\n\nIn any case, after substitution, you get equations involving different orders of magnitude based on the assumption that the variations in A are much slower than those in φ. This leads us to separate the equation into terms proportional to higher powers of some small parameter—in this context, probably related to the ratio between the spatial scale of the potential change and the wavelength.\n\nThe key idea is that if the potential doesn't vary too rapidly, meaning that over distances comparable to the wavelength λ ~ h/p, the potential remains approximately constant. Therefore, the WKB approximation breaks down when the potential changes significantly within a distance smaller than λ—that happens near turning points where classically particles cannot go beyond certain points, but quantum mechanically they tunnel through.\n\nMathematically, the WKB approximation requires that (k x)' << k², where k is sqrt(2m(E-V))/ħ. Near a turning point, say x=T where E=V(T), k becomes zero, making the denominator vanish, hence violating the condition. Thus, the approximation isn’t valid there.\n\nMoving on to part (b): Using perturbation theory to explain why placing an atom in an external electric field lowers the ground state energy.\n\nPerturbation theory deals with systems whose Hamiltonian can be split into H₀ + λH', where λ is a small parameter. In this scenario, applying an external electric field introduces a perturbing Hamiltonian. For atoms, the main unperturbed system is usually spherically symmetric, such as hydrogen-like atoms.\n\nAn external uniform electric field interacts with the electron cloud via the dipole interaction. The perturbing Hamiltonian should be something like H' = -qE·r, where q is charge (-e for electrons), r is position vector, and E is the electric field strength.\n\nSince the original potential is symmetric, all states will have definite parity. However, the perturbation H'= -e E z (say along z-axis) is not parity-symmetric—it flips sign under reflection z→−z. Hence, matrix elements <n|H’|m> may connect even and odd states.\n\nFor non-degenerate perturbation theory, first-order correction to energy levels is given by ⟨ψ_n|H’|ψ_n⟩. Since H’ is linear in z, and for the ground state of hydrogen-like atoms, which is s-state (l=0, spherical symmetry), the expectation value ⟨s|z|s⟩ is zero due to symmetry—the integral of z times radial functions over all space cancels out.\n\nTherefore, the first-order shift is zero. Then, looking at second-order perturbation. Second-order shifts depend on summing over intermediate states m≠n of |⟨m|H’|n⟩|^2 divided by (En – Em).\n\nHowever, wait—if n is the ground state, En is lower than other excited states Em. So denominators could cause issues unless specific selection rules apply.\n\nMoreover, the transition from n=s to m=p states might be allowed because H’ connects l=0 to l=1 states. Because angular momentum considerations tell us that the operator z (which is proportional to cosθ in spherical coordinates) raises/lowers angular momentum by ±1. Specifically, Y_{lm} → Y_{l±1,m} upon multiplication by cosθ.\n\nThus, only transitions from s-states (l=0) to p-states (l=1) contribute. Now, calculating these matrix elements, their squares summed up give a positive contribution to the second-order energy shift.\n\nBut hold on—is the second-order energy shift negative or positive? Remembering that the second-order energy shift ΔE^(2) = Σ_m≠n [|⟨m|H’|n⟩|^2]/(E_n - E_m). If E_n is less than E_m, the denominator is negative. Each term contributes negatively, thus overall ΔE^(2) is negative.\n\nThis implies that the ground state energy decreases when subjected to an external electric field—a phenomenon known as the Stark effect. So yes, according to perturbation theory, the ground state energy goes down.\n</think>\n\n### Summary\n\n**(a)**  \nThe WKB approximation applies when the potential \\( V(x) \\) varies smoothly over distances comparable to the de Broglie wavelength. Substituting the ansatz \\( \\psi(x) = A(x)e^{i\\phi(x)} \\) into the Schrödinger equation yields conditions requiring that the variation of \\( A(x) \\) is negligible compared to the rapid oscillations of \\( e^{i\\phi(x)} \\). Near a classical turning point (\\( E = V(x) \\)), the wavenumber \\( k \\to 0 \\), causing the approximation to break down due to invalidity of the assumed separation of scales.\n\n**(b)**  \nApplying an external electric field introduces a perturbation \\( H' = -eEz \\). First-order corrections vanish for the ground state due to symmetry. Second-order corrections involve sums over intermediate states, primarily connecting \\( s \\)-states to \\( p \\)-states. These contributions result in a net decrease in the ground state energy, consistent with the Stark effect.\n\n---\n\n**Final Answers**\n\n(a) The WKB approximation is valid where the potential varies slowly relative to the wavelength, failing near turning points.  \n\\[ \\boxed{\\text{WKB fails near } E = V(x)}. \\]\n\n(b) Perturbation theory shows the ground state energy decreases in an electric field.  \n\\[ \\boxed{\\Delta E < 0}. \\]"}
{"id": "quantum/7023", "questions": "(a) Assuming that two protons of $H_2^+$ molecule are fixed at their normal separation of 1.06 Å, sketch the potential energy of the electron along an axis passing through the protons.\n\n(b) Sketch the electron wave functions for the two lowest states in $H_2^+$, indicating roughly how they are related to hydrogenic wave functions. Which wave function corresponds to the ground state, and why?\n\n(c) What happens to the two lowest energy levels of $H_2^+$ in the limit that the protons are moved far apart?", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about H₂⁺ ions, which is a diatomic ion consisting of two protons and one electron. It's also called a molecular protonium or sometimes just referred to as a simple model for understanding molecular orbitals. The question has three parts: (a), (b), and (c). Let me try to tackle each part step by step.\n\nStarting with part (a): \"Assuming that two protons of H₂⁺ molecule are fixed at their normal separation of 1.06 Å, sketch the potential energy of the electron along an axis passing through the protons.\"\n\nHmm, okay. So first, let me visualize what's going on here. We've got two protons separated by 1.06 angstroms. Since it's H₂⁺, there's only one electron moving around these two nuclei. Now, we're supposed to consider the potential energy experienced by this single electron along the line connecting the two protons—so essentially along the x-axis if we place the protons symmetrically around the origin.\n\nPotential energy... Right, the electric potential due to point charges. Each proton will contribute to the potential felt by the electron. Remembering Coulomb's law, the potential V(r) from a charge Q at distance r is given by kQ/r, where k is Coulomb's constant. But since both protons are positive, their potentials add up because like charges repel; wait no—the electron would be attracted to them, but when considering the potential energy U = qV, since the electron has negative charge (-e), its potential energy would actually decrease near the protons.\n\nWait, hold on. Potential energy between two charges is U = kq₁q₂ / r. Here, the electron has charge -e, and each proton +e. So the potential energy contribution from each proton is U_i = k*(-e)*(+e)/r_i = -k e² / r_i. Therefore, total potential energy is sum over i=1,2 of U_i.\n\nSo, placing the two protons at positions R/2 and -R/2 relative to the center, assuming symmetry. Wait, the separation is 1.06 Å, so R = 1.06 Å. Thus, each proton is located at ±53 pm (since 1 Å is 100 pm).\n\nBut maybe instead of getting bogged down in exact numbers, think more qualitatively. Along the axis passing through the protons, say the x-axis, the position of the electron can vary from left to right across the midpoint between the protons.\n\nAt any point x along this axis, the distances to each proton are |x - d| and |x + d|, where d is half the separation, so d = 0.5 * 1.06 Å ≈ 0.53 Å.\n\nTherefore, the potential energy V(x) should look something like:\n\nV(x) = - [k e²] / |x - d| - [k e²]/|x + d|\n\nSince the constants factor out, perhaps scaling appropriately, but the shape depends mainly on those terms.\n\nNow, thinking about the graph: At points very far away from the protons, either to the extreme left or right, the potential approaches zero because the denominators become large. Near each proton, the potential becomes highly negative (because you divide by small values, making the term larger in magnitude negatively). Between the protons, somewhere near the middle, the potential might reach a minimum—or does it?\n\nActually, wait—if you move towards the region between the protons, the denominator gets smaller again, meaning the potential becomes even lower (more negative). Hmm, interesting. Because being close to either proton gives high negative potential, but exactly midway, the contributions from both protons combine constructively, leading to deeper potential well.\n\nAlternatively, maybe not—it could depend on whether the potential adds linearly or quadratically. Wait, no, each term is inversely proportional to distance, so adding two such terms doesn't necessarily lead to a quadratic behavior.\n\nLet me test specific points:\n\n- When x is much greater than d, then |x - d| ≈ x, same with |x + d|. Then V(x) ≈ -2ke²/x, approaching zero from below.\n  \n- As x approaches d from above, the second term dominates, becoming more negative.\n\nSimilarly, as x approaches -d from below, the first term dominates.\n\nIn the middle, at x=0, the potential is V(0)= - ke²/(d) - ke²/d = -2ke²/d, which is quite deep.\n\nIf I plot this, the potential would dip sharply near each proton, reaching minima at x=d and x=-d, but actually, wait—that’s incorrect. If you take derivative of V(x):\n\nFor x > d,\n\nV'(x) = d/dx [-ke²/(x - d)] = ke²/(x - d)^2\n\nAnd similarly for other regions. Setting derivatives equal to find extrema...\n\nWait, maybe better yet, compute the derivative properly.\n\nDefine V(x) = -k e² [|x - d|^{-1} + |x + d|^{-1}].\n\nTo make differentiation easier, assume x >= d, so absolute value signs go away:\n\nV(x) = -k e² [(x - d)^{-1} + (x + d)^{-1}]\n\nThen, derivative V’(x) = k e² [ (x - d)^{-2} + (x + d)^{-2} ]\n\nWhich is always positive for x > d, implying that V(x) increases monotonically beyond x = d.\n\nSimilarly, for x < -d, similar reasoning applies.\n\nBetween -d and d, things get trickier because depending on side, expressions change.\n\nSuppose x is between 0 and d. Then, |x - d|=d - x, and |x + d|=x + d.\n\nThus, V(x) = -k e² [1/(d - x) + 1/(d + x)]\n\nCompute derivative:\n\nV’(x) = -k e² [ (1)/(d - x)^2 )*(1) + (1)/(d + x)^2)*(-1) ] ?\n\nWait, no. Differentiating 1/(d - x) w.r.t x is 1/(d - x)^2, and differentiating 1/(d + x) is -1/(d + x)^2.\n\nSo overall:\n\nV’(x) = -k e² [ 1/(d - x)^2 - 1/(d + x)^2 )\n\nSet derivative to zero to find critical points:\n\n[1/(d - x)^2 - 1/(d + x)^2] = 0 => 1/(d - x)^2 = 1/(d + x)^2 => cross-multiplied: (d + x)^2 = (d - x)^2\n\nExpanding both sides:\n\n(d + x)^2 = d² + 2dx + x²\n\n(d - x)^2 = d² - 2dx + x²\n\nSetting equal: d² + 2dx + x² = d² - 2dx + x² ⇒ 4 dx = 0 ⇒ Either d=0 or x=0.\n\nBut d ≠ 0, so x=0 is the critical point.\n\nTesting concavity: Second derivative or sign changes.\n\nLooking back at V’(x) before and after x=0.\n\nWhen x approaches 0 from the positive side, plug into V’(x):\n\nApproximately, numerator ~ [1/d² - 1/(2d)^2] ? No, need precise calculation.\n\nWait, plugging x slightly less than 0, say x = ε>0 tiny.\n\nV’(ε) = -k e² [1/(d - ε)^2 - 1/(d + ε)^2 ]\n\n≈ -k e² [ (1/d²)(1 + 2ε/d) - (1/d²)(1 - 2ε/d) ) ]\n\nUsing binomial approximations: For small ε, 1/(d ± ε)² ≈ 1/d² ∓ 2ε/d³.\n\nSo expanding:\n\n≈ -k e² [ (1/d² + 2ε/d³) - (1/d² - 2ε/d³) ) ]\n= -k e² [ (1/d² - 1/d²) + (2ε/d³ + 2ε/d³) )\n= -k e² [ 4ε/d³ ]\n\nSince ε is positive, this expression is negative. Hence, V’(x) is negative just above x=0.\n\nSimilarly, testing x slightly negative, say x = -ε<0.\n\nV’(-ε) = -k e² [1/(d - (-ε))² - 1/(d + (-ε))²]\n= -k e² [1/("}
{"id": "quantum/4004", "questions": "An electron of mass $\\text m$ and charge $-e$ moves in a region where a uniform magnetic field $\\mathbf{B} = \\mathbf{\\nabla} \\times \\mathbf{A}$ exists in $z$ direction.\n\n(a) Set up the Schrödinger equation in rectangular coordinates.  \n\n(b) Solve the equation for all energy levels.  \n\n(c) Discuss the motion of the electron.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about an electron moving in a uniform magnetic field pointing in the z-direction. The question has three parts: setting up the Schrödinger equation, solving it to find all energy levels, and discussing the motion of the electron. Hmm, let me try to break this down step by step.\n\nStarting with part (a), which asks to set up the Schrödinger equation in rectangular coordinates. Alright, I remember that when dealing with quantum mechanics problems involving electromagnetic fields, we usually use the Hamiltonian approach. For a charged particle like an electron, the Hamiltonian includes its kinetic energy but also accounts for the vector potential due to the magnetic field because the electric field is zero here—since it's just a static magnetic field.\n\nWait, right—the magnetic field B can be expressed as the curl of the vector potential A. In this case, since B is along the z-axis, maybe choosing a symmetric gauge or something would simplify things? Or perhaps using Landau gauges might make more sense depending on how we want to handle the vector potential.\n\nLet me recall the form of the Schrödinger equation. It should involve the Laplacian operator modified by the vector potential. So, the general expression for the Hamiltonian H is given by:\n\nH = [1/(2m)] * (-iħ∇ - eA)^2 + V\n\nBut wait, there’s no scalar potential mentioned here; only the magnetic field. Since the magnetic field doesn't contribute directly to the scalar potential, our Hamiltonian simplifies to just the kinetic term plus any external potentials—if none exist besides the magnetic one, then V=0.\n\nSo, expanding that out, the square becomes (∇² terms minus 2ieA·(something))... Let me write it properly.\n\nThe momentum operator in quantum mechanics is p = ħ(-i∇). But when you include the electromagnetic interaction, it gets modified to P = p - eA/c, assuming Gaussian units—but hold on, sometimes people define it without c if they're working in SI units. Wait, actually, in many textbooks, especially those not worrying too much about unit systems, they often express the minimal coupling substitution as replacing ∂/∂x → ∂/∂x - i(e/c)A_x, etc., but in natural units where ħ=c=1, it's simpler.\n\nHmm, getting confused now between different unit conventions. Maybe better to stick strictly to SI units?\n\nIn SI units, the correct replacement is p → p - (e/c)A. Therefore, the squared momentum would be (p - (e/c)A)^2 / (2m).\n\nTherefore, substituting into the Hamiltonian,\n\nHψ = [(−iħ∇ − (e/c)A)^2 / (2m)] ψ = Eψ\n\nThat seems right. Now, considering that the magnetic field points in the z-direction, what does the vector potential look like? There are multiple choices for A even though B is fixed. One common choice is the symmetric gauge, another is the Landau gauge.\n\nIf I choose the Landau gauge, then A could be taken as A = (0, Bx, 0)—so that the curl gives B_z = dB_y/dx - dB_x/dy = d(Bx)/dy - 0 = B*delta(y)? No, wait—that may complicate things. Alternatively, taking A such that it depends linearly on x or y.\n\nActually, in the Landau gauge, typically A is chosen as A = (0, By, 0), meaning that A_y = Bx, making sure that the curl in z-component is B. Yes, that makes sense.\n\nAlternatively, some sources take A = (By, 0, 0); either way, the exact components depend on coordinate system orientation.\n\nWait, perhaps it's clearer to think in terms of cylindrical symmetry around the z-axis, but since the problem specifies rectangular coordinates, probably best to proceed accordingly.\n\nAssuming that the magnetic field is uniform and in the z-direction, say B = B_0 k̂. Then, the vector potential must satisfy ∇ × A = B. Choosing the Landau gauge, we can set A = (0, Bx, 0). That works because the curl will give us dBx/dy - dAy/dx = dBx/dy - 0 = B, provided that Bx = B y? Wait, no, sorry—I need to compute the curl correctly.\n\nCompute ∇×A for A=(0, Bx, 0):\n\ncurl(A)_z = ∂A_y/∂x - ∂A_x/∂y = ∂(Bx)/∂x - 0 = B*(dx/dx)= B*1=B.\n\nYes, perfect! So, indeed, A = (0, Bx, 0) satisfies ∇×A = Bẑ.\n\nAlright, so plugging back into the Hamiltonian:\n\nP = p - (e/c)A => Px = px - (e/c)(0) = px,\nPy = py - (e/c)(Bx),\nPz = pz - (e/c)(0) = pz.\n\nThus, the Hamiltonian becomes:\n\nH = [Px^2 + Py^2 + Pz^2]/(2m)\n\nSubstituting each component:\n\nPx = -iħ ∂/∂x,\nPy = -iħ ∂/∂y - (e/c)Bx,\nPz = -iħ ∂/∂z.\n\nSince the magnetic field is uniform and in z-direction, and assuming no dependence on z-coordinate, perhaps the wavefunction separates into variables dependent on x, y, and z independently? If that's possible, then the solutions can be written as products of functions each depending on single variable.\n\nMoreover, looking at the Hamiltonian, notice that the z-component isn’t affected by the vector potential—it remains free. Thus, the motion in the z-direction is completely independent and unbounded, leading to continuous spectrum, while the x-y plane experiences quantized energy levels due to the magnetic confinement—a phenomenon known as Landau levels.\n\nHowever, before jumping ahead, let's focus on writing the full Schrödinger equation.\n\nExpressing H explicitly:\n\nH = [ (px')^2 + (py')^2 + (pz')^2 ] / (2m),\n\nwhere px' = px - (e/c)Ax = px - 0 = px,\n      py' = py - (e/c)Ay = py - (e/c)(Bx),\n      pz' = pz - (e/c)Az = pz - 0 = pz.\n\nPlugging these into the Hamiltonian:\n\nH = [ (-iħ ∂/∂x )^2 + (-iħ ∂/∂y - (e/c)Bx )^2 + (-iħ ∂/∂z )^2 ] / (2m )\n\nExpanding each term:\n\nFirst term:\n[ (-iħ ∂/∂x )^2 ] = -(ħ^2) ∂²/∂x²\n\nSecond term:\n[ (-iħ ∂/∂y - (e/c)Bx )^2 ]\n= [ (-iħ ∂/∂y )^2 + 2*(-iħ ∂/∂y)*(-(e/c)Bx) + (-(e/c)Bx)^2 ]\n= -(ħ^2) ∂²/∂y² + (2iħ e B)/(c) x ∂/∂y + (e² B² x²)/(c²)\n\nThird term:\n[ (-iħ ∂/∂z )^2 ] = -(ħ^2) ∂²/∂z²\n\nPutting them together:\n\nH = [ -ħ² ∂²/∂x² - ħ² ∂²/∂y² + (2iħ e B)/(c) x ∂/∂y + (e² B² x²)/(c²) - ħ² ∂²/∂z² ] / (2m )\n\nSimplify constants:\n\nFactor out 1/(2m):\n\nH = (1/(2m))[ -ħ² ∂²/∂x² - ħ² ∂²/∂y² + (2iħ e B)/(c) x ∂/∂y + (e² B² x²)/(c²) - ħ² ∂²/∂z² ]\n\nThis looks complicated, but note that the z-component is separate from x and y. Hence, the Schrödinger equation splits into two parts: one governing the motion perpendicular to the magnetic field (in x-y plane) and the other parallel (along z-axis).\n\nGiven that, perhaps we can assume separability of variables. Suppose the wave function Ψ(x,y,z,t) factors into X(x) Y(y) Z(z) T(t). However, time evolution complicates matters unless we consider stationary states. Assuming separation, we'd get equations for each spatial dimension multiplied by their respective operators.\n\nBut focusing purely on space, ignoring time for the moment, the Helmholtz-like equation comes into play after factoring out the time"}
{"id": "quantum/7024", "questions": "Write the Schrödinger equation for atomic helium, treating the nucleus as an infinitely heavy point charge.", "graphs": null, "llm_answers": "<think>\nOkay, so I need to write down the Schrödinger equation for atomic helium when we treat the nucleus as an infinitely heavy point charge. Hmm, let me think about this.\n\nFirst off, I remember that Helium has two electrons orbiting a nucleus with a charge of +2e since it's He++. So each electron is negatively charged (-e) and they both interact with the nucleus and also repel each other because like charges repel.\n\nWait, but if the nucleus is considered infinitely heavy, does that mean its mass doesn't affect the motion? Oh right! In such cases, we can fix the position of the nucleus at the origin without loss of generality. That simplifies things because then all our coordinates will be relative to the fixed nucleus.\n\nSo now, considering the two electrons, their positions would determine the state of the system. Let me denote the position vectors of the first and second electrons as r₁ and r₂ respectively. Each vector points from the nucleus (which is stationary) to each electron.\n\nNow, what contributes to the potential energy here? Well, there are three main contributions: \n\n1. The attraction between each electron and the nucleus.\n2. The repulsion between the two electrons themselves.\n\nFor the attractive forces, each electron feels a Coulomb force towards the nucleus. Since the nucleus has a charge of +2e, the potential due to one electron being at r₁ would involve 2e divided by |r₁| multiplied by e over 4πε₀ or something like that. Wait, actually, the electric potential V(r) created by a charge Q at distance r is given by kQ/r where k = 1/(4πε₀). But wait, no—actually, more precisely, the potential energy U between two charges q₁ and q₂ separated by r is U = (q₁ q₂)/(4πε₀ r).\n\nIn this case, each electron has charge -e, and the nucleus has charge +2e. Therefore, the potential energy contribution from the interaction between the first electron and the nucleus should be U_electron-nucleus = [(-e)(+2e)] / (4πε₀ |r₁| ) which simplifies to (-2e²)/(4πε₀ |r₁| ). Similarly, the same applies for the second electron, giving another term (-2e²)/(4πε₀ |r₂| ).\n\nThen, the third term comes from the mutual repulsion between the two electrons. Their charges are both -e, so the potential energy between them is [(−e)(−e)] / (4πε₀ |r₁ − r₂| ), which becomes (+e²)/(4πε₀ |r₁ − r₂| ).\n\nPutting these together, the total potential energy V_total is:\n\nV_total = [ -2e²/(4πε₀ |r₁| ) ] + [ -2e²/(4πε₀ |r₂| ) ] + [ e²/(4πε₀ |r₁ − r₂| ) ]\n\nI can factor out e²/(4πε₀):\n\nV_total = (e²/(4πε₀)) * [ -2/|r₁| - 2/|r₂| + 1/|r₁ − r₂| )\n\nBut sometimes people use constants differently; maybe using α or some other notation. Alternatively, perhaps writing everything in terms of the Bohr radius or Rydberg constant might come into play later, but probably not necessary for just setting up the equation.\n\nNext, moving on to kinetic energy. For quantum systems, the kinetic energy operator involves Laplacians. Since we have two particles, each contributing their own kinetic energy, the total kinetic energy T is the sum of the individual kinetic energies.\n\nEach particle’s kinetic energy is -(ħ²/(2m)) times the Laplacian acting on the wavefunction ψ. Here m is the mass of the electron. Because both electrons are identical fermions, their combined wavefunction must satisfy antisymmetry under exchange, but for the purpose of writing the Schrödinger equation itself, whether symmetric or anti-symmetric isn’t directly relevant—it affects the form of solutions rather than the equation setup.\n\nTherefore, the kinetic energy part of the Hamiltonian H is:\n\nT = [-ħ²/(2m)] (∇₁² + ∇₂²)\n\nWhere ∇₁² is the Laplacian with respect to r₁ and similarly for ∇₂².\n\nThus, putting it all together, the time-independent Schrödinger equation for the system is:\n\nHψ = Eψ\n\nWhich expands to:\n\n[ -ħ²/(2m) (∇₁² + ∇₂²) + (e²/(4πε₀))( -2/|r₁| - 2/|r₂| + 1/|r₁ − r₂| ) ] ψ(r₁, r₂) = E ψ(r₁, r₂)\n\nHmm, okay, that seems correct. Just making sure I didn't miss any factors. Sometimes units get tricky, especially with constants like ε₀ and e involved. Also, note that ħ is h-bar, Planck's reduced constant.\n\nAlternatively, sometimes you see equations written with Z instead of specific numbers, but since we're dealing with helium specifically, Z=2, hence why the nuclear charge appears as 2e.\n\nAnother thing to consider—if someone uses Gaussian units versus SI units—the expressions change slightly because Gauss units absorb some constants into others. However, assuming standard SI units unless specified otherwise.\n\nAlso, double-checking signs: the potential energy from the nucleus-electron interactions is negative because opposite charges attract, leading to lower potential energy. Electron-electron repulsion adds positive potential energy because similar charges repel, requiring work against the field.\n\nYes, that makes sense. So combining those gives us the overall structure of the Schrödinger equation.\n\nJust thinking through dimensions briefly: Kinetic energy has units of joules, as do the potential energy terms. The Laplacian applied twice brings in inverse square meters, multiplying by ħ squared (Joule-seconds squared), dividing by mass (kg)—so yeah, kg·(m/s)^2 → Joules. Potential energy terms have e^2/(distance × permittivity), which works out correctly too because e has coulombs, and 1/(4πε₀) has units N·m²/C², so C²/(N·m²·C²/m) → N·m, which is Joules. All good.\n\nAlright, I feel confident enough that this is the correct expression for the Schrödinger equation of helium atom with a fixed nucleus approximation.\n\n\n**Final Answer**\nThe Schrödinger equation for atomic helium is:\n\\[\n-\\frac{\\hbar^2}{2m} \\left( \\nabla_1^2 + \\nabla_2^2 \\right)\\psi(\\mathbf{r}_1, \\mathbf{r}_2) + \\left( -\\frac{2e^2}{4\\pi\\varepsilon_0 |\\mathbf{r}_1|} - \\frac{2e^2}{4\\pi\\varepsilon_0 |\\mathbf{r}_2|} + \\frac{e^2}{4\\pi\\varepsilon_0 |\\mathbf{r}_1 - \\mathbf{r}_2|} \\right)\\psi(\\mathbf{r}_1, \\mathbf{r}_2) = E\\psi(\\mathbf{r}_1, \\mathbf{r}_2)\n\\]\nExpressed compactly, it is:\n\\[\n\\boxed{-\\frac{\\hbar^2}{2m} (\\nabla_1^2 + \\nabla_2^2)\\psi + \\left( -\\frac{Z e^2}{|\\mathbf{r}_1|} - \\frac{Z e^2}{|\\mathbf{r}_2|} + \\frac{e^2}{|\\mathbf{r}_1 - \\mathbf{r}_2|} \\right)\\psi = E\\psi}\n\\]\nwhere \\( Z = 2 \\).\n</think>\n\nTo write the Schrödinger equation for atomic helium, we start by noting that the nucleus is treated as an infinitely heavy point charge, meaning its position is fixed at the origin. This allows us to focus on the dynamics of the two electrons around the nucleus.\n\n### Key Components:\n1. **Potential Energy Contributions**:\n   - Attraction between each electron and the nucleus (charge +2e):\n     \\[\n     -\\frac{2e^2}{4\\pi\\varepsilon_0 |\\mathbf{r}_i|}\n     \\]\n     for each electron \\( i \\) (where \\( i = 1, 2 \\)).\n   - Repulsion between the two electrons:\n     \\[\n     \\frac{e^2}{4\\pi\\varepsilon_0 |\\mathbf{r}_1 - \\mathbf{r}_2|}\n     \\]\n\n2. **Kinetic Energy Contribution**:\n   - The kinetic energy operator for each electron is:\n     \\[\n     -\\frac{\\hbar^2}{2m} \\nabla_i^2\n     \\]\n     Summing for both electrons gives:\n     \\[\n     -\\frac{\\hbar^2}{2m} (\\nabla_1^2 + \\nabla_"}
{"id": "quantum/7012", "questions": "Two electrons at fixed positions on the z-axis have a magnetic dipole interaction (energy) $\\mathbf{E} = A (\\mathbf{s}_1 \\cdot \\mathbf{s}_2 - 3s_{1z}s_{2z})$, where $\\mathbf{s}_i = \\frac{1}{2} \\sigma_i; \\sigma_i$ being Pauli's spin matrix, $A =$ constant.\n\n(a) Express $\\mathbf{E}/A$ in terms of the total spin operator $\\mathbf{S} = \\mathbf{s}_1 + \\mathbf{s}_2$.\n\n(b) State the eigenvalues of $\\mathbf{E}/A$ and their degeneracy (statistical weights).", "graphs": null, "llm_answers": "<think>\nOkay, so I've got this problem about two electrons interacting via a magnetic dipole interaction. The energy is given as E = A(s₁·s₂ − 3 s_{1z} s_{2z}), and part (a) asks me to express E/A in terms of the total spin operator S = s₁ + s₂. Hmm, okay, let's break this down.\n\nFirst off, I remember that when dealing with spins, especially for identical particles like electrons, we often use properties related to the addition of angular momentum or spin here. Since each electron has spin-½, combining them gives possible total spins S=0 (singlet state) and S=1 (triplet states).\n\nThe expression involves s₁ · s₂. Wait, isn't there a formula relating s₁ · s₂ to the square of the total spin? Let me recall: yes! For any two vectors, (s₁ + s₂)^2 equals s₁² + s₂² + 2 s₁·s₂. So if I can write s₁·s₂ in terms of S², which is (s₁ + s₂)², then maybe I can rewrite the entire expression.\n\nLet me compute:\n\nS² = (s₁ + s₂)² = s₁² + s₂² + 2 s₁·s₂\n\nSo solving for s₁·s₂,\n\ns₁·s₂ = [S² - s₁² - s₂²]/(2)\n\nNow, since these are spin-½ particles, s₁² and s₂² should be equal because they're both spin matrices squared. Each s_i^2 would be (ħ/2)^2 times something... but wait, actually, more precisely, for spin quantum number s, s² = ħ² s(s+1), right?\n\nBut hold on, looking back at the question, it says s_i = (1/2) σ_i, where σ_i are Pauli matrices. Oh, so perhaps s_i² is ((σ_i)/2)^2. But what does σ_i squared give us? Well, each Pauli matrix squares to identity, so σ_x² = σ_y² = σ_z² = I, the identity matrix. Therefore, s_i² = (I / 4)(something)?\n\nWait no, let's see: If s_i = (1/2) σ_i, then s_i² = (1/2)^2 σ_i² = (1/4) * I, since σ_i² = I. Hence, s_i² = (1/4) I for i=1,2.\n\nTherefore, plugging into our earlier equation:\n\ns₁·s₂ = [S² - (1/4)I - (1/4)I ]/(2)\n       = [S² - (1/2)I] / 2\n       = (S²)/(2) - (1/4)I\n\nAlright, so now substitute this back into the original expression for E/A:\n\nE/A = s₁·s₂ - 3 s_{1z} s_{2z}\n\nWe already expressed s₁·s₂ in terms of S², so plug that in first:\n\nE/A = [(S²)/2 - (1/4)I] - 3 s_{1z} s_{2z}\n\nHmm, okay, so now need to handle the term involving s_{1z} s_{2z}. How do I relate that to the total spin operators?\n\nWell, note that S_z = s_{1z} + s_{2z}, so maybe expressing products of individual components in terms of S_z might help. Alternatively, think about how s_{iz} relates to S_z and other terms.\n\nAlternatively, another approach could involve using ladder operators, but not sure yet.\n\nWait, alternatively, consider that s_{1z} s_{2z} is one component of the tensor product space. Maybe express it in terms of S_z² minus some cross terms.\n\nBecause S_z = s_{1z} + s_{2z}, so squaring both sides:\n\n(S_z)^2 = (s_{1z} + s_{2z})^2 = s_{1z}^2 + s_{2z}^2 + 2 s_{1z} s_{2z}\n\nThus, rearranged:\n\ns_{1z} s_{2z} = [ (S_z)^2 - s_{1z}^2 - s_{2z}^2 ] / 2\n\nAgain, similar to before, s_{iz}^2 is just (1/2)^2 I, same reasoning as above—since s_iz is diagonal with entries ±1/2, its square is (1/2)^2 I.\n\nHence, s_{iz}^2 = (1/4) I for i=1,2.\n\nPlugging back in:\n\ns_{1z} s_{2z} = [ (S_z)^2 - (1/4)I - (1/4)I ] / 2\n              = [ (S_z)^2 - (1/2)I ] / 2\n              = (S_z)^2 / 2 - (1/4)I\n\nGreat, so substituting this into E/A:\n\nE/A = [ (S²)/2 - (1/4)I ] - 3[ (S_z)^2 / 2 - (1/4)I ]\n\nExpanding this out:\n\nE/A = (S²)/2 - (1/4)I - (3/2)(S_z)^2 + (3/4)I\n\nCombine constants:\n\n(-1/4 + 3/4)I = (2/4)I = (1/2)I\n\nAnd combine the rest:\n\n(E/A) = (S²)/2 - (3/2)(S_z)^2 + (1/2)I\n\nHmm, seems manageable. Now, can we simplify further? Perhaps factor out common terms.\n\nNote that S² includes all three components, while S_z² is only the z-component. Is there a way to express this combination in terms of S² and S_z²?\n\nYes, indeed, from vector calculus, for any vector operator **V**, V² = Vx² + Vy² + Vz². Thus, S² = S_x² + S_y² + S_z².\n\nWhich implies that S_x² + S_y² = S² - S_z².\n\nLooking back at my expression for E/A:\n\nE/A = (S²)/2 - (3/2) S_z² + (1/2)I\n\nMaybe reorganize terms:\n\nGrouping the S² and S_z² together:\n\nE/A = (1/2) S² - (3/2) S_z² + (1/2)I\n\nFactor out 1/2:\n\nE/A = (1/2)[ S² - 3 S_z² ] + (1/2)I\n\nIs there a better way to represent S² - 3 S_z²? It looks familiar—it resembles the Casimir invariant for certain symmetries, but perhaps more straightforwardly, considering spherical harmonics or angular momentum projections...\n\nWait, thinking again, perhaps writing everything in terms of S² and L_z-like terms—but maybe instead, notice that S² commutes with S_z, so perhaps we can find a relation between S² and S_z².\n\nAlternatively, perhaps recognize that S² - 3 S_z² is proportional to something else.\n\nWait, also recalling that for orbital angular momentum, L² - 3 L_z² appears in expressions for quadrupolar interactions, etc., but not exactly helpful here.\n\nAlternatively, perhaps try to express S² - 3 S_z² in terms of lower-order terms.\n\nWait, let's calculate S² - 3 S_z² explicitly.\n\nGiven that S² = S_x² + S_y² + S_z², subtracting 3 S_z² gives:\n\nS² - 3 S_z² = S_x² + S_y² - 2 S_z²\n\nNot particularly enlightening, unless we know specific relations among the components.\n\nAlternatively, perhaps go back to the initial definition of E/A and see whether after substitution, things cancel nicely.\n\nWait, stepping back, perhaps I made a miscalculation somewhere. Let me double-check.\n\nStarting over:\n\nExpressed E/A as s₁·s₂ - 3 s_{1z} s_{2z}\n\nComputed s₁·s₂ = (S² - s₁² - s₂²)/2\n\nSince s₁² = s₂² = (1/4) I, substituted those in:\n\ns₁·s₂ = (S² - (1/4)I -"}
{"id": "quantum/7015", "questions": "A particular one-dimensional potential well has the following bound-state single-particle energy eigenfunctions:\n\n$$\n\\psi_a(x), \\ \\psi_b(x), \\ \\psi_c(x) \\cdots , \\quad \\text{where} \\quad E_a < E_b < E_c \\cdots .\n$$\n\nTwo non-interacting particles are placed in the well. For each of the cases (a), (b), (c) listed below write down: \n\nthe two lowest total energies available to the two-particle system, the degeneracy of each of the two energy levels, the possible two-particle wave functions associated with each of the levels.  \n\n(Use $\\psi$ to express the spatial part and a ket $\\lvert S, m_s \\rangle$ to express the spin part. $S$ is the total spin.)\n\n(a) Two distinguishable spin-$\\frac{1}{2}$ particles.  \n\n(b) Two identical spin-$\\frac{1}{2}$ particles. \n\n(c) Two identical spin-0 particles.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about quantum mechanics involving particles in a one-dimensional potential well. The question asks me to consider different types of particles—distinguishable or identical—and determine their two lowest total energies, the degeneracies, and the corresponding wave functions.\n\nFirst, let's parse what we know from the setup. There's a 1D potential well with bound states labeled ψₐ(x), ψᵦ(x), ψс(x)... Each corresponds to increasing energy levels since Ea < Eb < Ec... So, these are just the standard stationary states for a particle in such a well.\n\nNow, there are two particles being considered here. Depending on whether they're distinguishable or identical, fermions or bosons, the approach changes because of the Pauli exclusion principle and symmetry requirements for wave functions.\n\nLet's tackle case (a): Two distinguishable spin-½ particles.\n\nSince the particles are distinguishable, like maybe electrons but with some other property making them distinct, perhaps different charges? Wait no—if both are spin-½, then if they were same type, say electrons, they would be indistinct. But wait, actually, the term \"spin-½\" doesn't necessarily make them identical unless specified as identical particles. Hmm, okay, the key point is that they are distinguishable regardless of spin. That probably means their overall wave function isn’t required to obey any specific exchange symmetry—it can be symmetric or antisymmetric without restriction based solely on spin.\n\nBut hold on, even though they’re distinguishable, do they still need to follow Fermionic or Bosonic statistics? No, only identical particles require anti-symmetry (fermions) or symmetry (bosons). Distinguishable particles don't; you can treat them independently.\n\nSo when constructing the two-particle state, it should be a product of individual states. Since they are distinguishable, the order matters, right?\n\nWait, not exactly—their combined wavefunction could include all combinations where each occupies a state, considering permutations. However, since they are distinguishable, swapping them wouldn't affect the physical state, meaning the wavefunction might not change sign or anything. Let me think more carefully.\n\nIn general, for distinguishable particles, the total wave function is simply a tensor product of their individual wave functions. Because they aren't subject to exchange symmetries, unlike identical particles. Therefore, the two-particle wave function will be |ψ₁⟩⊗|ψ₂⟩, which can also be written as ψ₁(x₁)ψ₂(x₂).\n\nHowever, sometimes people use Slater determinants or symmetric sums for identical particles, but for distinguishables, it's straightforward products.\n\nGiven that, how does this apply to our scenario?\n\nWe have three energy levels given: ψₐ, ψᵦ, ψс...\n\nEach particle can occupy any of these states. To find the two lowest total energies, we'll look for the configurations with the least sum of energies.\n\nThe first configuration is both particles in the ground state, ψₐ. Then, next possibilities involve one in ψₐ and another in higher states.\n\nBut wait, hang on—for distinguishable particles, having both in ψₐ is allowed because they don't have to obey the Pauli exclusion principle. Similarly, putting one in ψₐ and the other in ψᵦ is fine too.\n\nTherefore, the two lowest total energies correspond to:\n\nCase 1: Both particles in ψₐ. Total energy = Ea + Ea = 2Ea.\n\nCase 2: One particle in ψₐ and the other in ψᵦ. Total energy = Ea + Eb.\n\nIs that correct? Or is there a lower possibility than Case 2? Well, after 2Ea comes Ea + Eb, yes.\n\nAre those the two lowest? Yes, because the third level would give higher energy.\n\nNext, regarding degeneracy—that refers to the number of ways to achieve the same total energy.\n\nFor the first case, both in ψₐ: How many ways can this happen? Only one way, since both must be in ψₐ. Hence, degeneracy g=1.\n\nFor the second case, one in ψₐ and one in ψᵦ: Now, since the particles are distinguishable, assigning Particle A to ψₐ and B to ψᵦ is different from Particle A in ψᵦ and B in ψₐ. Thus, there are two distinct arrangements leading to the same total energy. So, the degeneracy here is g=2.\n\nAlternatively, thinking combinatorially, choosing positions among n states for k particles, allowing repetition—but since they are distinguishable, it's permutation rather than combination. So yeah, for two particles into two states, it's 2!/(n -k)! ... Not sure, better stick with reasoning above.\n\nMoving on to the wave functions.\n\nFor the first case, both in ψₐ: It’s simply ψₐ(x₁)ψₐ(x₂). And since they are distinguishable, the spin parts can vary independently. Given that they are spin-½ particles, each has its own spin state.\n\nAssuming the spins are independent, the total spin state is a product of individual spin states. If we denote the spin up as ↑ and spin down as ↓, then each particle can be either ↑ or ↓.\n\nThus, the total spin state can be various combinations. Specifically, for two spin-½ particles, the possible total spin states range from singlet (total spin S=0) and triplet (S=1). But since the particles are distinguishable, their spin states don't influence the orbital wave function's symmetry—they can combine freely.\n\nHence, the two-particle wave function combines the spatial part and the spin part. In terms of notation, it'd be something like [ψₐ(x₁)ψₐ(x₂)] ⊗ [|s₁⟩|s₂⟩], where s₁ and s₂ represent the spin states of each particle.\n\nSimilarly, for the second case—one in ψₐ and one in ψᵦ: The spatial part becomes ψₐ(x₁)ψᵦ(x₂) plus possibly ψᵦ(x₁)ψₐ(x₂)? Wait, no—since the particles are distinguishable, the ordering matters. So depending on which particle is in which state, the wave function remains ordered accordingly.\n\nActually, for distinguishable particles, the full wave function is just the product of each particle's state. So if Particle 1 is in ψₐ and Particle 2 in ψᵦ, it's ψₐ(x₁)ψᵦ(x₂)|s₁⟩|s₂⟩. Alternatively, if swapped, it's ψᵦ(x₁)ψₐ(x₂)|s'_1⟩|s'_2⟩. These are separate states contributing to the degenerate energy level.\n\nHmm, but in reality, when writing out the possible wave functions, especially for multiple particles, we often account for all possible assignments. Maybe using creation operators helps, but perhaps overcomplicating.\n\nI think the main takeaway is that for distinguishable particles, each arrangement counts separately towards the degeneracy.\n\nAlright, moving on to case (b): Two identical spin-½ particles.\n\nIdentical spin-½ implies they are fermions due to half-integer spin. Therefore, their total wave function must be antisymmetric under exchange.\n\nTotal wave function Ψ_total = Spatial × Spin.\n\nBecause they are identical, exchanging the particles swaps x₁ ↔x₂ and spin indices. So,\n\nΨ_total(x₁,x₂,s₁,s₂) → (-1)^P Ψ_total(x₂,x₁,s₂,s₁),\n\nwhere P is the parity of the permutation—in this case, swap leads to multiplication by -1^something.\n\nSpecifically, for fermions, the entire wave function must be antisymmetric upon exchange. This includes both space and spin parts.\n\nThis introduces constraints on how the spatial and spin parts can behave. Either the spatial part is symmetric and the spin part is antisymmetric, or vice versa.\n\nTo construct the two-particle wave functions, we need to form antisymmetric combinations.\n\nStarting again with finding the two lowest energies.\n\nSame idea as before: Assigning the particles to the lowest available orbitals while respecting the Pauli exclusion principle.\n\nPauli says that no two identical fermions can occupy the same quantum state simultaneously. Here, a 'quantum state' includes both spatial and spin components.\n\nWait—is that accurate? Actually, the Pauli exclusion applies when all quantum numbers"}
{"id": "quantum/7008", "questions": "A one-dimensional square well of infinite depth and 1 Å width contains 3 electrons. The potential well is described by $V = 0$ for $0 \\leq x \\leq 1$ Å and $V = +\\infty$ for $x < 0$ and $x > 1$ Å. For a temperature of $T = 0 \\, \\text K$, the average energy of the 3 electrons is $E = 12.4 \\, \\text{eV}$ in the approximation that one neglects the Coulomb interaction between electrons. In the same approximation and for $T = 0 \\, \\text K$, what is the average energy for 4 electrons in this potential well?\n", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about electrons in a one-dimensional infinite square well. Let me try to understand it properly.\n\nThe setup: There's a potential well with V=0 from x=0 to x=1 Å (which is 1 angstrom), and V=infinity outside that region. So, it's like a box where particles can't exist beyond these boundaries because the potential is infinitely high there. \n\nAt T=0 K, which means absolute zero temperature, all the electrons will occupy the lowest available quantum states due to the Pauli exclusion principle—since they're fermions, each state can hold only two electrons (one spin up, one spin down). But wait, actually, since we’re talking about a three-dimensional system usually, but here it’s one-dimensional? Hmm, maybe not exactly... Wait no, even in 1D, each spatial wavefunction can still accommodate two electrons with opposite spins. Or does the question consider something else?\n\nWait, let me read again. It says \"a one-dimensional square well,\" so perhaps each energy level has degeneracy based on some other factors—but in pure 1D, without considering spin or anything else, each n would correspond to just one state. However, when you include spin, each n could have multiple states depending on how many possible spin configurations there are. Since electrons have spin-½, their spin multiplicity is 2. Therefore, each energy level n can hold two electrons—one with spin-up and another with spin-down.\n\nSo if we have N electrons, then starting from the ground state, we fill them into the lowest levels until we reach N/2 filled orbitals, right? Because each orbital holds two electrons.\n\nIn the given case, initially, there are 3 electrons. At T=0K, the first few energy levels get filled as much as needed. Then, the total energy is calculated as the sum over occupied states multiplied by their respective energies.\n\nGiven that for 3 electrons, the average energy per electron is E_avg = 12.4 eV. They want us to find the average energy when there are 4 electrons instead.\n\nFirst, let me recall the formula for the energy levels in a 1D infinite square well. The energy En is proportional to n², specifically:\n\nEn = (n² h²) / (8 m L²)\n\nWhere:\n- h is Planck's constant,\n- m is mass of the particle (electron),\n- L is the width of the well (here, 1 Å).\n\nBut since the exact value might be complicated, perhaps we don’t need the actual numerical values because ratios may come into play.\n\nAlternatively, knowing that the energy depends quadratically on n, so higher n gives more energy.\n\nNow, for 3 electrons:\n\nSince each n can take 2 electrons (spin up and down), the filling goes like this:\n\nn=1: 2 electrons,\n\nThen, n=2: next 1 electron (because 3 - 2 =1),\n\nThus, the total number of electrons is 3.\n\nTherefore, the total energy E_total_3 is the sum of the energies of these electrons.\n\nEach electron contributes its own energy; however, since both electrons in the same n share the same energy, except for different spins—they do contribute twice the same energy.\n\nHence, for 3 electrons:\n\nTotal Energy = 2*E₁ + 1*E₂\n\nSimilarly, for 4 electrons:\n\nWe'll have n=1 holding 2 electrons, n=2 holding 2 electrons.\n\nBecause 4 divided by 2 is 2, meaning both n=1 and n=2 are fully filled.\n\nSo Total Energy = 2*(E₁ + E₂ )\n\nWait, but hang on—if 4 electrons mean n=1 and n=2 each having 2 electrons, yes.\n\nLet me write out explicitly:\n\nFor 3 electrons:\n\nNumber of electrons in n=1: 2\n\nRemaining electrons: 1 → go to n=2.\n\nSo, E_total(3)= 2*E₁ + 1*E₂\n\nAverage energy per electron: [2*E₁ + E₂]/3 = 12.4 eV\n\nThey give that already.\n\nOur goal is to compute the average energy for 4 electrons.\n\nWhich would require computing E_total(4)/4.\n\nAnd E_total(4) should be 2*E₁ + 2*E₂.\n\nSo, our task reduces to finding E_total(4):\n\nIf we know E_total(3) = 2E₁ + E₂ = 3 * 12.4 eV = 37.2 eV\n\nHmm, okay, so 2E₁ + E₂ = 37.2 eV.\n\nWhat is E_total(4)? That's 2E₁ + 2E₂ = ?\n\nWell, note that E_total(4) = (2E₁ + E₂ ) + E₂ = 37.2 eV + E₂\n\nBut unless we can express E₂ in terms of known quantities...\n\nAlternatively, think about the ratio of energies.\n\nFrom the expression of En, En ∝ n².\n\nSo, E₂ = 4 E₁, because n=2 squared is four times n=1 squared.\n\nYes! Because En ~ n^2, so E₂/E₁=(2)^2/(1)^2=4 ⇒ E₂=4 E₁.\n\nThat seems correct.\n\nSo substituting back into equation:\n\n2E₁ + E₂ = 2E₁ + 4E₁ =6 E₁ =37.2 eV\n\n→ 6 E₁ =37.2 eV → E₁=37.2 /6 ≈6.2 eV\n\nSo E₁≈6.2 eV, hence E₂=4×6.2=24.8 eV.\n\nAlright, now moving forward.\n\nCompute E_total(4): 2E₁ + 2E₂ =2(E₁+E₂)=2(6.2+24.8)=2(31)=62 eV.\n\nTherefore, the average energy per electron is E_total(4)/4=62/4=15.5 eV.\n\nIs that correct? Let me double-check my steps.\n\nStarting point: Given 3 electrons, average energy 12.4 eV.\n\nComputed E_total(3)=37.2 eV.\n\nExpressed E_total(3)=2E₁ + E₂=6 E₁=37.2 → E₁=6.2 eV.\n\nThen found E₂=4 E₁=24.8 eV.\n\nCalculated E_total(4)=2E₁ +2E₂=2*(6.2)+2*(24.8)=12.4 +49.6=62 eV.\n\nDivide by 4 electrons: 62/4=15.5 eV.\n\nSeems consistent.\n\nAlternative approach: Maybe using Fermi energy concept.\n\nFermi energy is the highest occupied energy level at T=0.\n\nFor non-interacting electrons, the average energy can also be expressed in terms of the integral over the density of states weighted by occupation, but in discrete cases, it's simpler.\n\nHowever, in this specific scenario, since it's a simple 1D box, the calculation above suffices.\n\nAnother way: Express everything symbolically.\n\nSuppose E_n = k n², where k is a proportionality constant.\n\nThen, for 3 electrons:\n\nSum_{occupied} E_n = 2k(1²) + 1*k(2²) = 2k +4k=6k.\n\nThis equals 3 ×12.4 eV=37.2 eV.\n\nSo, 6k=37.2 → k=6.2 eV.\n\nThus, E_n=k n²=6.2 n² eV.\n\nFor 4 electrons, Sum becomes 2k(1²) +2k(2²)=2k +8k=10k=10×6.2=62 eV.\n\nAverage energy:62/4=15.5 eV.\n\nSame result.\n\nI think that solidifies the conclusion.\n</think>\n\nTo determine the average energy for 4 electrons in the one-dimensional infinite square well at \\( T = 0 \\, \\text{K} \\), we start by analyzing the given information for 3 electrons. Each energy level \\( n \\) can hold 2 electrons due to spin considerations. Thus, for 3 electrons, the configuration fills \\( n=1 \\) completely (with 2 electrons) and partially fills \\( n=2 \\) with 1 electron. This results in a total energy of \\( 2E_1 + E_2 \\), leading"}
{"id": "quantum/4010", "questions": "A particle has mass $m$, charge $q$, intrinsic angular momentum $s$ (s is not necessarily equal to $\\hbar/2$) and a magnetic dipole moment $\\mu = gqs/2mc$. The particle moves in a uniform magnetic field $\\mathbf{B}$ with a velocity small compared with $c$.\n\n(a) Write down the Hamiltonian for this system. (The vector potential for the uniform magnetic field may be written as $\\mathbf{A} = \\mathbf{B} \\times \\mathbf{r}/2$)\n\n(b) Derive the quantum mechanical (Heisenberg) equations of motion from this Hamiltonian, for the linear momentum $\\mathbf{P}$ and for the angular momentum $s$. The $A^2$ term may be neglected in this nonrelativistic approximation.\n\n(Note that the results look exactly like the classical equations of motion.)\n\n(c) Without solving these equations, identify the value of the constant $g$ for which the helicity will remain constant. (Helicity is defined here as the cosine of the angle between the vectors $\\mathbf{P}$ and $s$)\n\n(d) What is the actual value of the constant $g$ for any one of the following particles: $e$, $\\mathbf{p}$, $n$, $\\pi$?", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about a charged particle moving in a magnetic field, considering both its orbital and spin contributions. Let me try to break it down part by part.\n\nStarting with part (a): They want the Hamiltonian for the system. Hmm, okay, I remember that the Hamiltonian usually includes kinetic energy and potential energy terms. Since we're dealing with electromagnetic fields, the potential comes into play through the scalar potential φ and the vector potential A.\n\nBut wait, they mentioned using the vector potential specifically given as **A** = (**B** × r)/2. That's interesting because sometimes you can choose different gauges, but maybe this specific form simplifies things?\n\nSo, the general expression for the Hamiltonian when there's a magnetic field is H = [(**P** - q**A**)²]/(2m) + qφ. But since the magnetic field is uniform, does that mean the scalar potential φ is zero or negligible? Or perhaps it doesn't matter if we consider only the vector potential contribution?\n\nWait, actually, in a uniform magnetic field generated by infinitely long solenoids or something similar, the scalar potential might indeed vanish everywhere except near the sources, which aren't considered here. So probably, φ=0. Therefore, the Hamiltonian should just be ([**P** - q**A**]²)/(2m).\n\nLet me write that out:\n\nH = [(**P** - q**A**) · (**P** - q**A**)] / (2m)\n\nExpanding that dot product gives us |**P**|²/(2m) minus (q/m)(**P**·**A**) plus (q²|**A**|²)/(2m). \n\nThey also mention neglecting the A squared term later on, but right now, do I need to include all terms? For part (a), no—since it says \"write down\" without specifying approximations yet. Wait, though—the note under part (b) mentions neglecting A², but part (a) hasn’t specified that. Maybe still better to keep everything unless told otherwise.\n\nAlternatively, another approach could involve including the interaction due to the magnetic dipole moment μ. Because the particle has a magnetic moment, it interacts with the external B-field via the Zeeman term. Oh! Right, the Hamiltonian would then consist of two parts: the usual kinetic term involving P and A, and the interaction term involving μ·B.\n\nYes, that makes sense. So total Hamiltonian H_total = H_kinetic + H_interaction\n\nWhere H_kinetic is [ (**P** - q**A**)² ]/(2m )\n\nAnd H_interaction is -μ·**B**\n\nGiven that μ = (g q s)/(2 m c ), substituting that in,\n\nH_interaction becomes -(g q s)/(2 m c ) · **B**\n\nTherefore, putting them together:\n\nH = [ (**P** - q**A**)² ]/(2m ) - (g q s · **B**)/(2 m c )\n\nIs that correct? Alternatively, some textbooks use the opposite sign convention for the interaction Hamiltonian; let me double-check. The interaction energy for a magnetic dipole is U = -μ·B, yes, so my signs seem fine.\n\nNow, plugging in the given expressions, especially since **A** is provided as (**B**×r)/2. Substituting that into the kinetic term:\n\nFirst compute (**P** - q**A**)²:\n\nThat’s (**P** - q*(**B**×r)/2)^2\n\nWhich expands to |**P**|^2 - 2 q (**P** · (**B**×r))/2 + q² |**B**×r|² /4\n\nSimplify each term:\n\n- First term remains |**P**|².\n- Second term: -2*q*... divided by 2 cancels the denominator, giving -q (**P** · (**B**×r))\n- Third term: q² |**B**×r|² /4\n\nHmm, but again, in part (b) they say to neglect the A² term, meaning the third term above. So maybe even in part (a), although not explicitly stated, whether to expand or leave it symbolic.\n\nLooking back, question (a) didn't specify anything about expansion, so perhaps writing the Hamiltonian symbolically is acceptable. Thus, keeping it as:\n\nH = [ (**P** - q**A**)² ]/(2m ) - (g q s · **B**)/(2 m c )\n\nWith **A** being (**B**×r)/2.\n\nI think that suffices for part (a). Moving on to part (b):\n\nDerive Heisenberg equations of motion for linear momentum P and angular momentum s.\n\nIn Heisenberg picture, dX/dt = i[H,X], where X is operator.\n\nWe'll start with d**P**/dt.\n\nCompute [H, **P**].\n\nSince H consists of two terms: K = [ (**P** - q**A**)² ]/(2m ) and V = - (g q s · **B**)/(2 m c ).\n\nThus, [H, **P**] = [K, **P**] + [V, **P**]\n\nCalculating first [K, **P**]:\n\nK = (1/(2m)) ( **P** - q**A** )²\n\nExpand this: (1/(2m))( **P**² - 2 q **P**·**A** + q² **A**² )\n\nThen, [K, **P_i**] = (1/(2m))[ [**P**², **P_i**] - 2 q [**P**·**A**, **P_i**] + ... ]\n\nNote that [**P_j**, **P_i**]=0, so [**P**², **P_i**] = sum_j [**P_j**², **P_i**] = 0.\n\nSimilarly, [**P**·**A**, **P_i**]: Here, **P**·**A** = Σ P_j A_j, so commutator with P_i is Σ [P_j A_j, P_i] = Σ δ_{ij}[A_j, P_i]? No, more accurately, [P_j A_j, P_i] = A_j [P_j, P_i] + P_j [A_j, P_i] = 0 + P_j (-iħ ∂A_j/∂x_i ). Hence, overall, [**P**·**A**, **P_i**] = -iħ Σ P_j (∂A_j/∂x_i )\n\nPutting it together:\n\n[K, **P_i**] = (1/(2m)) * [ -2 q * (-iħ) Σ P_j (∂A_j/∂x_i ) ] ?\n\nWait, hold on. Actually, [K, **P_i**] involves derivatives of operators, which complicates matters. Perhaps instead, recall that in the Heisenberg equation, the derivative of P is related to force.\n\nWait, alternatively, recalling Ehrenfest theorem, but perhaps going directly.\n\nAnother way: Express K in terms of canonical variables. Remember that in electromagnetism, the Lorentz force appears naturally.\n\nMoreover, knowing that in the presence of a vector potential, the effective momentum is **P'** = **P** - q**A**. Then, the kinetic term is (**P'**)²/(2m). In such cases, the conjugate momentum is **P**, while the physical momentum is **P'**.\n\nHowever, regardless, proceeding formally.\n\nAlso, don't forget that **A** depends on position, hence when taking commutators, spatial gradients come into play.\n\nPerhaps computing [K, P_i] component-wise.\n\nExpressed as:\n\n[ ( (**P** - q**A**)² )/(2m ), P_i ]\n\nThis equals (1/(2m)) times [ (**P** - q**A**)² , P_i ]\n\nUsing identity [AB,C] = A[B,C] + [A,C]B, recursively expanding.\n\nLet me denote Q = **P** - q**"}
{"id": "quantum/7006", "questions": "Consider a system defined by the Schrödinger eigenvalue equation\n$$\n\\left\\{ -\\frac{\\hbar^2}{2m} \\left( \\nabla_1^2 + \\nabla_2^2 \\right) + \\frac{k}{2} | \\mathbf{r}_1 - \\mathbf{r}_2 |^2 \\right\\} \\psi(\\mathbf{r}_1, \\mathbf{r}_2) = E \\psi(\\mathbf{r}_1, \\mathbf{r}_2).\n$$\n\n(a) List all symmetries of this Schrödinger equation.  \n\n(b) Indicate all constants of motion.  \n\n(c) Indicate the form of the ground-state wave function. \n\nYou may assume that the ground-state wave function of one-dimensional harmonic oscillator is a Gaussian.", "graphs": null, "llm_answers": "<think>\nOkay, so I have to solve part (a), (b), and (c) for this quantum mechanics problem involving two particles interacting with each other through a potential proportional to the square of their separation distance. Let me start by understanding what's given.\n\nThe Schrödinger equation provided is:\n\n$$\n\\left\\{ -\\frac{\\hbar^2}{2m} \\left( \\nabla_1^2 + \\nabla_2^2 \\right) + \\frac{k}{2} | \\mathbf{r}_1 - \\mathbf{r}_2 |^2 \\right\\} \\psi(\\mathbf{r}_1, \\mathbf{r}_2) = E \\psi(\\mathbf{r}_1, \\mathbf{r}_2)\n$$\n\nSo it looks like we're dealing with two identical particles moving in three dimensions because the positions are vectors **r₁** and **r₂**, but wait—actually, no, hold on. The Laplacian operators ∇₁² and ∇₂² could be either 3D or maybe even higher if considering spin, but since they’re written as scalars here, probably just spatial coordinates. But actually, looking again, the term |**r₁ − r₂|² suggests it’s a relative coordinate squared, which would make sense in any dimensionality, but perhaps let's think about whether these are distinguishable particles? Hmm, not necessarily; the question doesn't specify, so maybe treating them as indistinct isn’t necessary unless specified otherwise.\n\nBut first, focusing on symmetry properties. For part (a): list all symmetries of the Schrödinger equation.\n\nSymmetry usually refers to transformations under which the Hamiltonian remains invariant. So, operations such as translations, rotations, reflections, etc., that leave H unchanged will correspond to symmetries.\n\nLooking at the Hamiltonian: It has kinetic energy terms for both particles and a potential depending only on the difference between their positions. That structure hints towards certain symmetries.\n\nFirstly, translational symmetry. If you shift both particles together by some vector R, then the relative position |**r₁ − r₂| remains the same. Therefore, translation should be a symmetry. Wait, more precisely, does shifting each particle individually count?\n\nWait, the total momentum operator commutes with the Hamiltonian if there's translational symmetry. Since the potential depends only on the relative displacement, yes, the center-of-mass motion can translate freely without affecting the interaction. Hence, the entire system is symmetric under global translations.\n\nSimilarly, rotational symmetry: rotating the whole system around any axis won't change the magnitude |**r₁ − r₂|, hence rotationally symmetric. Thus, angular momentum is conserved.\n\nWhat else? Parity symmetry—if you reflect the positions of both particles through the origin, say invert their positions, the potential V(|**r₁−r₂**) becomes V(|-**r₁+(-r₂)|)=V(|(**r₂' - r₁')|). Which is still the same as before, so parity might also be a symmetry.\n\nTime reversal symmetry? Well, the Hamiltonian is real except possibly for complex potentials, but our potential is quadratic and positive definite, so time-reversal symmetry holds. However, when talking about symmetries leading to conservation laws via Noether's theorem, time reversal typically relates to anti-unitary symmetries rather than generating a continuous group, so maybe not directly giving rise to a constant of motion beyond being its own thing.\n\nNow, permutation symmetry: swapping the two particles. Because the potential depends on |**r₁−r₂|, which is symmetric under exchange, and the kinetic energies involve separate gradients, but upon exchanging indices, the sum remains the same. So the Hamiltonian is symmetric under interchange of particles. This implies that the wavefunction must transform according to irreducible representations of the permutation group, i.e., symmetric or antisymmetric states. But since the particles aren't specified as bosons or fermions, maybe we don't need to consider this yet—it's more about the overall state's symmetry, though the Hamiltonian itself certainly respects permutation symmetry.\n\nAdditionally, scaling symmetry? Suppose we scale space uniformly, how does the Hamiltonian behave? Kinetic energy involves second derivatives, while the potential scales quadratically. Depending on m and k, maybe there's a similarity transformation where the equations remain invariant under uniform scaling. Not sure if that applies here, but worth noting.\n\nAnother thought: separability into center-of-mass and relative coordinates. Maybe that comes from the fact that the Hamiltonian separates into parts dependent on the CM motion and the internal motion. So, after changing variables to R=(**r₁ + r₂)/√2 and r= (**r₁ - r₂)/√2, the Hamiltonian splits into H_cm acting on R and H_rel acting on r. Then, the CM part is free (no potential) and the relative part is a harmonic oscillator. So, the symmetries related to the CM motion include full Galilean symmetry (translations, boosts, rotations), whereas the relative motion has rotational symmetry due to the HO potential.\n\nTherefore, combining everything, the symmetries likely include:\n\n1. Translational symmetry in the center-of-mass direction.\n2. Rotational symmetry in 3D (since the relative potential is spherically symmetric).\n3. Permutation symmetry (exchange of particles).\n\nPossibly others, but those seem primary.\n\nMoving on to part (b): indicate all constants of motion.\n\nConstants of motion are quantities whose expectation values do not change over time, meaning their corresponding observables commute with the Hamiltonian.\n\nFrom the symmetries identified earlier, using Noether's theorem, each continuous symmetry corresponds to a conserved quantity.\n\nTranslational symmetry → Conservation of linear momentum.\n\nRotational symmetry → Conservation of angular momentum.\n\nPermutation symmetry leads to exchange symmetry of the wavefunction, but that affects statistics (boson/fermion) rather than providing another constant of motion.\n\nAlso, since the Hamiltonian separates into CM and relative parts, the energy associated with each part is separately conserved. Specifically, the CM kinetic energy and the relative harmonic oscillator energy are additive and thus independent.\n\nMoreover, boost symmetry (Galilei group) gives rise to conservation of mass times velocity, but in non-relativistic QM, boosts lead to additional generators besides pure momenta, but perhaps in bound systems, the boost charges aren't always conserved unless specific conditions apply. In this case, since the CM motion is free, the boost generator (which includes terms like Mv t) would imply something about the relation between CM momentum and energy, but strictly speaking, the boost charge isn't conserved unless external forces vanish, which they do in the CM frame. So, perhaps the boost-related quantities aren't true constants of motion globally, but locally within frames.\n\nAlternatively, thinking in terms of integrals of motion, apart from energy, momentum, and angular momentum, are there others?\n\nIn the relative motion, once separated, the relative Hamiltonian describes a 3D isotropic harmonic oscillator. Such systems have additional dynamical symmetries beyond SO(3); specifically, they belong to SU(3) or Sp(6,R) groups, implying extra constants of motion beyond L² and L_z, etc. These are called \"accidental\" degeneracies arising from higher symmetry.\n\nFor example, in 3D harmonic oscillators, the number of constants of motion increases. There are components of the Runge-Lenz vector-like operators, similar to hydrogen atom, but adapted for HO. Each component provides additional conserved quantities.\n\nThus, in addition to the usual E, P, and L, we'd have additional ones coming from the relative motion's enhanced symmetry.\n\nHowever, sometimes people refer to the constants of motion as the set generated by the Lie algebra of the symmetry group. Here, the full symmetry group combines the CM Galilean symmetries and the relative HO symmetries.\n\nPutting it all together, the constants of motion would include:\n\n- Total Energy E,\n- Center-of-Mass Momentum P_total,\n- Angular Momentum L_total,\n- Additional constants from the relative HO symmetry, such as the tensor components forming the rank-2 spherical tensors (like the quadrupole moments or analogous operators),\n- Possibly the individual energies of CM and relative motions, although technically E is already split into E_cm and E_rel, but since they add up, knowing E automatically incorporates both.\n\nHmm, but in reality, the exact identification requires careful consideration. Alternatively, perhaps the relative motion contributes additional Casimir invariants beyond the standard angular momentum.\n\nGiven that, perhaps listing the known constants:\n\nTotal energy,\n\nCenter-of-mass momentum (vector),\n\nAngular momentum (vector),\n\nAnd the Fradkin tensor"}
{"id": "quantum/8025", "questions": "Consider a free electron near a boundary surface.\n\n(a) If $\\phi_k(x)$'s are the electron eigenfunctions, show that the function\n\n$$\nu(x,t) = \\sum_k \\phi^*_k(x)\\phi_k(0) \\exp \\left( -\\frac{\\varepsilon_k t}{\\hbar} \\right)\n$$\n\nsatisfies a diffusion-type equation. Identify the corresponding diffusion coefficient.\n\n(b) From the theory of diffusion how would you expect $u(0, t)$ to be influenced by the presence of a boundary at a distance $l$ from the origin? Would the boundary be felt immediately or only after an elapse of time?\n\n(c) Examine the expression for $u(0, t)$ as a sum over $k$ as given in (a). What is the range of $\\varepsilon_k$ which contribute significantly to $u(0, t)$ at the time when the influence of the boundary is felt by the electron?", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about electrons near a boundary surface, divided into three parts: (a), (b), and (c). Let me try to tackle each part one by one. \n\nStarting with part (a): It says if φₖ(x)'s are the electron eigenfunctions, then we need to show that u(x,t) defined as the sum over k of [φₖ*(x) * φₖ(0)] times exp(-εₖt/ħ) satisfies a diffusion-type equation. And also identify the diffusion coefficient.\n\nHmm, okay. So first, let's recall what a diffusion equation looks like. The standard form is ∂u/∂t = D ∇²u, where D is the diffusion coefficient. But here, since it's just x, maybe it's 1D, so ∂u/∂t = D d²u/dx².\n\nSo our goal is to compute ∂u/∂t and see if it equals some multiple of the second derivative of u with respect to x.\n\nGiven:\n\nu(x,t) = Σₖ φₖ* (x) φₖ(0) e^{- εₖ t / ħ}\n\nFirst thought: Maybe take the partial derivatives term-by-term because differentiation can pass through summation under certain conditions.\n\nCompute ∂u/∂t:\n\n∂u/∂t = Σₖ φₖ* (x) φₖ(0) (-εₖ / ħ ) e^{- εₖ t / ħ }\n\nSimilarly, compute the second spatial derivative d²u/dx²:\n\nd²u/dx² = Σₖ [d²/dx² (φₖ* (x))] φₖ(0) e^{- εₖ t / ħ }\n\nBut wait, do we know anything about these eigenfunctions φₖ(x)? Since they're eigenfunctions of the Hamiltonian perhaps? For a free electron, but near a boundary... Hmm, actually, without knowing more specifics, maybe assuming something general.\n\nWait, another approach: Perhaps using properties of the eigenfunctions. Suppose H |ψₖ> = εₖ |ψₖ>, where H is the Hamiltonian operator. Then, for stationary states, iħ ∂/∂t ψₖ = H ψₖ. Therefore, ψₖ(t) = φₖ(x) e^{-i εₖ t / ħ }.\n\nBut in our case, u(x,t) seems similar except real exponentials instead of complex ones. Wait, why does it use exponential terms with negative exponents? That might suggest decay rather than oscillatory behavior—so perhaps related to probability densities decaying due to dissipation, hence leading to diffusion-like equations.\n\nAlternatively, considering that u could represent a density distribution evolving diffusively.\n\nLet me think again. To get a diffusion equation, we usually relate the time derivative to the Laplacian. Here, if we can express ∂u/∂t proportional to the second derivative, that'd give us the desired result.\n\nLooking back at expressions:\n\nWe have ∂u/∂t = -(1/ħ) Σₖ εₖ φₖ* (x) φₖ(0) e^{- εₖ t / ħ }\n\nAnd d²u/dx² = Σₖ [d²φₖ*/dx²] (x) φₖ(0) e^{- εₖ t / ħ }\n\nIf somehow, [d²φₖ*/dx²] relates to εₖ φₖ*, then we can connect them.\n\nAssuming that φₖ satisfy Schrödinger's equation, say,\n\n- (ħ²/(2m)) d²φₖ/dx² + V(x) φₖ = εₖ φₖ\n\nSince it's a free electron, potential V=0, right? Or is there a boundary condition making it not exactly free? Well, even so, near a boundary, maybe confined in some region, but still treated as free inside.\n\nFor simplicity, assume V=0, so:\n\n- (ħ²/(2m)) d²φₖ/dx² = εₖ φₖ => d²φₖ/dx² = - (2m/ħ²) εₖ φₖ\n\nTherefore, taking conjugate both sides:\n\nd²φₖ*/dx² = - (2m/ħ²) εₖ φₖ*\n\nThus, substituting into d²u/dx²:\n\nd²u/dx² = Σₖ [- (2m/ħ²) εₖ φₖ* ] φₖ(0) e^{- εₖ t / ħ }\n\nWhich simplifies to:\n\nd²u/dx² = - (2m/ħ²) Σₖ εₖ φₖ* (x) φₖ(0) e^{- εₖ t / ħ }\n\nComparing this with ∂u/∂t:\n\nFrom earlier, ∂u/∂t = -(1/ħ) Σₖ εₖ φₖ* (x) φₖ(0) e^{- εₖ t / ħ }\n\nNotice that:\n\n∂u/∂t = -(1/ħ) Σ ... , while d²u/dx² = - (2m/ħ²) Σ ...\n\nHence, multiplying d²u/dx² by (ħ²)/(2m):\n\n(ħ²/(2m)) d²u/dx² = - (1/ħ) Σ ... = ∂u/∂t\n\nTherefore,\n\n∂u/∂t = (ħ²/(2m)) d²u/dx²\n\nThis shows that u(x,t) satisfies the diffusion equation with diffusion coefficient D = ħ²/(2m).\n\nCool! So that answers part (a). Now moving on to part (b):\n\nIt asks, based on diffusion theory, how would u(0,t) be affected by a boundary at distance l from the origin. Is the effect immediate or delayed?\n\nIn diffusion processes, information travels infinitely fast in Fickian diffusion; however, practically, changes propagate at finite speeds depending on the medium. However, mathematically, solutions to the heat equation exhibit instantaneous response everywhere upon applying a boundary condition change—a phenomenon known as \"infinite speed of propagation.\"\n\nHowever, physically, especially in cases involving particles, such as electrons, quantum effects may impose limitations. In reality, though, classical diffusion allows for infinite speed, meaning the boundary would be 'felt' instantaneously.\n\nBut wait, hold on—the question mentions the presence of a boundary at distance l. How does that affect u(0,t)?\n\nWell, initially, before any interaction with the boundary, the system behaves freely. Once the boundary is introduced, it imposes constraints on the possible wave functions—for example, Dirichlet or Neumann boundary conditions—which modify the spectrum {εₖ}. Thus, the coefficients φₖ(0) will now depend on the new set of eigenfunctions satisfying those boundary conditions.\n\nBut according to the mathematical model, once the boundary exists, its influence should appear in all points instantly because the Green's function for the heat equation responds everywhere at t > 0 regardless of position relative to boundaries. Hence, theoretically, the boundary affects u(0,t) immediately.\n\nBut wait—isn't that counterintuitive? Because in physical systems, sometimes you don’t feel the boundary until light travel time or something. But in parabolic PDEs like diffusion, disturbances spread instantaneously. This is called non-locality in space-time.\n\nYes, so in pure diffusion models, the introduction of a boundary condition propagates its influence throughout the domain immediately. So, yes, the boundary would be felt immediately.\n\nMoving on to part (c): We need to examine u(0,t) expressed as a sum over k, and find the range of εₖ contributing significantly when the boundary starts influencing the electron.\n\nAt early times, contributions come mainly from low-energy modes because higher energy modes die out faster due to their larger exponents. As time increases, lower energies dominate.\n\nWhen the boundary becomes influential, probably around the time scale determined by the size l. Specifically, the characteristic time τ ~ m l² / ħ², which comes from equating the typical kinetic energy (p²/(2m)) to thermal energy or whatever driving force—if we consider the particle spreading, the mean square displacement grows linearly with time, σ² ≈ 2Dt = (ħ²/m"}
{"id": "quantum/8013", "questions": "A charged oscillating (nearly) classical system is losing energy by radiation. At energy $E$ it is radiating (and oscillating) at frequency $\\nu(E) = \\alpha (E/E_0)^{-\\beta}$, where $\\alpha, \\beta$ and $E_0$ are positive constants. Compute the quantum energy levels (of the system) $E_n$ for large $n$.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about a nearly classical system that's losing energy through radiation. The question gives me the relationship between the frequency of radiation ν and the energy E as ν(E) = α (E/E₀)^(-β), where α, β, and E₀ are positive constants. My task is to compute the quantum energy levels Eₙ for large n.\n\nHmm, let me start by recalling some concepts from both classical electromagnetism and quantum mechanics because this seems like a bridge between them. In classical terms, when charges accelerate, they emit electromagnetic radiation, which causes them to lose energy over time. This process can be described using Larmor's formula or its relativistic generalizations. However, since we're dealing with a \"nearly\" classical system here, maybe there's something related to quantization involved?\n\nWait, but the problem mentions computing quantum energy levels despite being nearly classical. That suggests perhaps treating the system semi-classically—using quantum ideas on top of classical equations? Maybe considering transitions between discrete energy states due to photon emission?\n\nI remember that in quantum systems, especially atoms emitting photons, the transition probabilities depend on the matrix elements involving dipole moments and the electric field. But how does that tie into the given frequency-energy relation?\n\nLet me think again: if the system is losing energy via radiation, then each small loss corresponds to transitioning to a lower energy state. So, starting from some initial energy E, after emitting a photon, it goes down to E - ΔE. If these steps continue until equilibrium, the spacing between consecutive energies might form a pattern based on the rate of energy loss.\n\nBut wait, the problem specifies that ν depends on E as ν ∝ (E/E₀)^-β. Since ν relates to the emitted photon's frequency, which would correspond to the energy difference ΔE ≈ hν, right? Because in quantum mechanics, the energy of a photon is Planck's constant times the frequency.\n\nSo, assuming that each emission event leads to a decrease in energy proportional to hν(E). Therefore, dE/dt ≈ -hν(E).\n\nIs that correct? Let me see—if power P is the rate of energy loss, then P = dE/dt = -h ν(E) multiplied by the number of emissions per unit time. Wait, actually, more accurately, the average power emitted should relate to the square of acceleration, etc., but in this case, we’re told directly what ν(E) is.\n\nAlternatively, thinking in terms of Einstein’s A coefficients for spontaneous emission, but without getting too deep into QED formalism, maybe just use dimensional analysis or scaling arguments.\n\nGiven that ν(E) scales as (E/E₀)^-β, so higher energy implies either higher or lower frequencies depending on beta. For example, if β=1/2, then ν ~ sqrt(E₀)/sqrt(E); meaning higher E means lower ν.\n\nNow, going back to the idea that the change in energy is linked to the frequency. Suppose that the system emits one photon at a time; then the energy lost per emission is approximately ħ ν(E). Thus, the next energy level below E would be roughly E - ħ ν(E). \n\nIf we model this as a continuous process rather than discrete jumps, we could write:\n\ndE/dN = -ħ ν(E),\n\nwhere N counts the number of photons emitted. Then integrating this differential equation would give us E(N), which we can invert to find N(E), leading eventually to Eₙ for large n.\n\nYes, that sounds plausible. So setting up the integral:\n\n∫_{E}^{∞} dE' / [ħ ν(E')] = ∫_{0}^{\\infty} dN\n\nBecause as N increases, E decreases from infinity to zero asymptotically. Hmm, not sure if that upper limit makes sense physically—it probably doesn't go all the way to zero, but for large n, maybe approximating it as such isn’t bad.\n\nSubstituting ν(E') = α (E'/E₀)^{-β}, so 1/ν(E') becomes (E'_0/(α))^{-1} * (E')^{β}. Hence,\n\nIntegral becomes ∫_{E}^{∞} (E')^{β}/(α ħ) dE'\n\nWhich simplifies to (1)/(α ħ) ∫_{E}^{∞} (E')^{β} dE'.\n\nCompute this integral:\n\nThe antiderivative of (E')^{β} is (E'^{β+1})/(β + 1). Evaluating from E to ∞... Wait, hold on—that only converges if β < -1, otherwise the integral diverges. But our parameters say alpha, beta, E₀ are positive constants. Beta must be positive, hence β > 0, making the exponent β + 1 > 1, thus the integral ∫_{E}^\\infty (E')^{β} dE' tends to infinity unless we adjust limits.\n\nThis suggests my approach may be flawed. Perhaps instead of modeling it continuously, I need another method.\n\nAlternative thought: Instead of relating dE/dN, consider the adiabatic approximation where the system slowly loses energy, moving along quasi-stationary states. Each state has energy Eₙ, and the transition probability determines the spacing between Eₙ and Eₙ₊₁.\n\nIn quantum mechanics, the selection rules often dictate allowed transitions—for instance, delta l = ±1 for angular momentum changes—but without knowing specifics, hard to apply here.\n\nAnother angle: Using Fermi's Golden Rule, which describes the transition rate from an initial state |i> to a set of final states |f>. It says W_i -> f = (2π/ℏ²)|<f|H'|i>|² δ(E_f - E_i + ℏ ω), integrated over appropriate density of states.\n\nHowever, applying this requires knowledge of H', the perturbing Hamiltonian, and the coupling strength. Without specific details, difficult to proceed.\n\nPerhaps stepping back—the key lies in connecting the frequency dependence to the energy levels. Given that ν(E) is provided, and wanting Eₙ for large n, likely looking for an expression where Eₙ behaves in a certain scaling manner with n.\n\nSuppose that the energy levels become closer together as n increases, similar to harmonic oscillator levels, but adjusted according to the given ν(E).\n\nAssuming that the spacing between adjacent levels is inversely proportional to some function of their positions. Specifically, if the energy loss per step is ΔE ≈ ħ ν(E), then for large E, ΔE ~ ħ α (E/E₀)^{-β}\n\nThus, writing ΔE ≈ dE/dn = - ħ α (E/E₀)^{-β}\n\nTherefore, dn/dE = -1/[ ħ α (E/E₀)^{-β} ] = -(E₀^β)/( ħ α ) E^{β }\n\nIntegrating dn/dE from E=Eₙ to E=infinity (assuming n starts at 0):\n\nn = ∫_{Eₙ}^{∞} dn = ∫_{Eₙ}^{∞} [ (E₀^β)/( ħ α ) E^{β } ] dE\n\nComputing the integral:\n\nn = (E₀^β)/( ħ α (β + 1 )) [ E^{β + 1} ] evaluated from Eₙ to ∞.\n\nAgain, evaluating at ∞ will lead to divergence unless we cap it somewhere, but for large n, presumably Eₙ is much smaller than any cutoff, so perhaps taking the lower limit as Eₙ and ignoring the upper term.\n\nWait, no—if you integrate from Eₙ to ∞, the result is negative because E^{β +1} grows without bound. To get a finite n, perhaps better to reverse the integration bounds.\n\nActually, let's re-examine the setup. We had dn/dE = (E₀^β)/( ħ α ) E^{β }, so integrating from E = Eₙ to E = E_max, giving n = ... ?\n\nBut without knowing E_max, tricky. Alternatively, suppose that for very high n, Eₙ approaches zero, so effectively, the maximum possible E is E_initial, but without information on initial conditions...\n\nMaybe another perspective: Assume that the total number of quanta emitted before reaching a low energy is large, so treat n as a continuous variable and solve the ODE.\n\nFrom above, dn/dE = C E^{β }, where C = (E₀^β)/( ħ α )\n\nThen, solving dn = C E^{β } dE => Integrating both sides:\n\nn = C ∫ E^{β } dE + D\n\nWhere D is the constant of integration. Assuming that at E = E₀, n = 0? Or perhaps choosing boundary condition differently.\n\nWait, perhaps when E = E₀, n = 0? Not necessarily clear. Alternatively, take n approaching infinity as E approaches zero.\n\nWait, confusion arises because earlier attempts led to integrals tending to infinity. Maybe I'm missing a factor.\n\nHold on, reconsider the original assumption. If the system is losing energy by emitting photons"}
{"id": "quantum/7007", "questions": "Two identical bosons, each of mass $m$, move in the one-dimensional harmonic oscillator potential $V = \\frac{1}{2} m \\omega^2 x^2$. They also interact with each other via the potential\n\n$$\nV_{\\text{int}}(x_1, x_2) = \\alpha e^{-\\beta(x_1-x_2)^2},\n$$\n\nwhere $\\beta$ is a positive parameter. Compute the ground state energy of the system to first order in the interaction strength parameter $\\alpha$.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem where two identical bosons are moving in a 1D harmonic oscillator potential, and they're interacting with each other through some exponential potential. The goal is to compute their ground state energy up to first order in α.\n\nFirst off, since these are bosons, they must obey Bose-Einstein statistics, meaning that their wavefunction has to be symmetric under exchange of particles. That's important because it affects how we construct the total wavefunction for the system.\n\nThe individual particle Hamiltonian is given by H0 = p²/(2m) + (1/2)mω²x². So each particle experiences a simple harmonic oscillator potential. Since there are two particles, the combined non-interacting Hamiltonian would just be H0_total = H0₁ + H0₂, right?\n\nNow, when dealing with such systems, especially if the particles don't interact too strongly, perturbation theory can be useful. Here, V_int is treated as a small perturbation, which makes sense because we’re supposed to calculate things only to first order in α.\n\nSo, let me recall: In first-order perturbation theory, the correction to the energy comes from taking the expectation value of the perturbing potential with respect to the unperturbed ground state. But wait, hold on—since both particles are bosons, what does the ground state look like? For distinguishable particles, you'd take product states, but here symmetry matters.\n\nIn the case of two identical bosons in a harmonic oscillator, without any interactions, the ground state should be symmetric. Each particle occupies the lowest single-particle state, n=0. Because they're indistinct, the spatial part of the wavefunction doesn’t change upon swapping them. However, sometimes people include spin or other degrees of freedom, but since nothing about spin is mentioned here, maybe we can assume it’s already accounted for, perhaps considering spin-singlet or something else—but actually, no, in many cases, for simplicity, people consider spinless bosons or ignore spin entirely unless specified otherwise. Hmm, not sure, but probably okay to proceed assuming the spatial wavefunction alone needs to be symmetric.\n\nWait, another thought—if the particles were fermions, then even though each could occupy the same orbital due to being different species, but since they're bosons, they can all pile into the same quantum state. Wait, but in our case, do they form a Slater determinant or a symmetrized combination? No, for bosons, it's anti-symmetry isn't required; instead, the overall wavefunction remains symmetric.\n\nBut more precisely, for two identical bosons, the total wavefunction Ψ(r1, r2) must satisfy Ψ(r1, r2) = Ψ(r2, r1). If each particle is in its own state ψ_n(x), then the combined state is [ψ_a(x1)ψ_b(x2)] + [ψ_b(x1)ψ_a(x2)], divided appropriately by sqrt(2) for normalization. \n\nHowever, in the noninteracting case, the ground state occurs when both particles are in the lowest possible state, i.e., n=0. Therefore, the spatial wavefunction is simply ψ_0(x1)ψ_0(x2), multiplied by a symmetric factor, but since both are in the same state, exchanging them leaves the wavefunction unchanged. Hence, the ground state is just |0⟩|0⟩, normalized properly.\n\nTherefore, the unperturbed ground state is ψ₀(x₁)ψ₀(x₂).\n\nGiven that, now the first-order energy shift ΔE^(1) will be ⟨Ψ| V_int |Ψ⟩, integrated over all space.\n\nLet me write down V_int:\n\nV_int = α exp[-β(x₁ - x₂)²]\n\nThus,\n\nΔE^(1) = α ∫ dx₁∫dx₂ ψ₀*(x₁)ψ₀*(x₂) exp[-β(x₁ - x₂)²] ψ₀(x₁)ψ₀(x₂)\n\nSimplify this expression. Let me denote y = x₁ - x₂. Then, perhaps changing variables might help. Alternatively, note that the integral becomes:\n\nα [∫ dx ψ₀²(x)]² ? Wait, no—that wouldn't account for the cross terms correctly.\n\nAlternatively, think of it as integrating over x₁ and x₂ separately, but involving their difference squared.\n\nAnother approach: Express the exponentials in Fourier transforms or use Gaussian integrals properties.\n\nRecall that ψ₀(x) is the ground state of the HO, which is proportional to exp(-mω x² / (2ħ)), except constants. Specifically, ψ₀(x) = (mω/(π ħ))^{1/4} exp(-mω x²/(2ħ)). So, square modulus is (mω/(π ħ))^{1/2} exp(-mω x²/ħ).\n\nHence, ψ₀²(x) ~ exp(-k x²), where k = mω/ħ * 2 ?\n\nWait, better yet, let's define ψ₀(x):\n\nψ₀(x) = (mω/(2πħ))^{1/4} exp(-mω x²/(4ħ))\n\nIs that correct? Maybe double-checking standard expressions—the ground state of SHO is usually written as:\n\nψₙ(x) = (1/sqrt(2^n n! )) ) (mω/(π ħ))^{1/4} H_n(sqrt(mω/ħ)x) exp(-mω x²/(2ħ))\n\nFor n=0, H₀(y)=1, hence ψ₀(x)=(mω/(π ħ))^{1/4} exp(-mω x²/(2ħ)).\n\nYes, that seems accurate.\n\nSo, ψ₀(x) = (mω/(π ħ))^{1/4} exp(-mω x²/(2ħ)).\n\nThen, ψ₀(x₁)ψ₀(x₂) = (mω/(π ħ))^{1/2} exp[ -(mω)/(2ħ)(x₁² + x₂²) ]\n\nAnd the perturbing term involves exp[-β(x₁ - x₂)^2].\n\nPutting together, the matrix element becomes:\n\n⟨V_int⟩ = α (mω/(π ħ))^{1/2} ∫ dx₁ dx₂ exp[ -(mω)/(2ħ)(x₁² + x₂²) ] exp[-β(x₁ - x₂)^2 ]\n\nCombine exponents:\n\nExponent = -(mω)/(2ħ)(x₁² + x₂²) - β(x₁ - x₂)^2\n\nExpand (x₁ - x₂)^2 = x₁² - 2x₁x₂ + x₂²\n\nSubstitute back:\n\nExponent = -(mω)/(2ħ)(x₁² + x₂²) - β x₁² + 2β x₁x₂ - β x₂²\n\nGroup similar terms:\n\nCoefficient of x₁²: [-(mω)/(2ħ) - β]\nSimilarly coefficient of x₂²: [-(mω)/(2ħ) - β]\nCross-term: 2β x₁x₂\n\nHmm, quadratic in x₁ and x₂. Perhaps completing the squares or using Gaussian integration techniques.\n\nExpressed as Exponent = A x₁² + B x₂² + C x₁x₂\n\nWhere:\nA = -(mω)/(2ħ) - β\nB = -(mω)/(2ħ) - β\nC = 2β\n\nWe need to evaluate the integral ∫ dx₁ dx₂ exp(A x₁² + B x₂² + C x₁x₂ )\n\nThis looks tricky, but remember that for multivariate Gaussians, the integral can be expressed in terms of determinants.\n\nSpecifically, for a general quadratic form Q(x,y) = ax² + bxy + cy², the integral over x and y is related to the inverse of the matrix [[a, b/2], [b/2, c]].\n\nMore formally, the integral ∫ dx dy exp(-(ax² + 2b xy + cy²)) equals π / sqrt(ac - b²)} provided ac > b².\n\nWait, yes, that formula applies when the quadratic form is negative definite, ensuring convergence.\n\nIn our case, the exponent is negative-definite because coefficients A and B are negative (as beta is positive, making A,B <0 ). Also, discriminant condition: AC >= ... Well, let's see.\n\nCompute the determinant D = AB - (C/2)^2\n\nSince A=B= -(mω)/(2ħ)-β, say K = -(mω)/(2ħ)+ (-β). Thus, A=B=K.\n\nSo, D = K*K - (C/2)^2 = K² - (β)^2\n\nBecause C=2β => C/2=β.\n\nSo, D = K² - β²\n\nWhich is [(mω/(2"}
{"id": "quantum/7009", "questions": "Consider two electrons moving in a central potential well in which there are only three single-particle states $\\psi_1, \\psi_2$ and $\\psi_3$.\n\n(a) Write down all of the wave functions $\\psi(r_1, r_2)$ for the two-electron system.\n\n(b) If now the electrons interact with a Hamiltonian $\\delta H = V'(r_1, r_2) = V'(r_2, r_1)$, show that the following expression for the matrix element is correct:\n\n$$\n\\langle \\psi_{13} | \\delta H | \\psi_{12} \\rangle = \\langle \\psi_3(r_1) \\psi_1(r_2) | V'(r_1, r_2) | \\psi_2(r_1) \\psi_1(r_2) \\rangle \\\\\n- \\langle \\psi_1(r_1) \\psi_3(r_2) | V'(r_1, r_2) | \\psi_2(r_1) \\psi_1(r_2) \\rangle.\n$$\n", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about two electrons in a central potential well with three single-particle states: ψ₁, ψ₂, and ψ₃. The question has two parts, (a) and (b). Let me start with part (a).\n\nPart (a): Writing down all the wavefunctions ψ(r₁, r₂) for the two-electron system.\n\nHmm, since these are electrons, they're fermions, right? So their total wavefunction must be antisymmetric under exchange of particles. That means if we swap electron 1 and electron 2, the wavefunction changes sign.\n\nEach electron can occupy one of the three states ψ₁, ψ₂, or ψ₃. But because it's a two-electron system, each state should be occupied once without repetition due to the Pauli exclusion principle. Wait, no—actually, wait—the Pauli exclusion says that no two fermions can occupy the same quantum state simultaneously. Since here we have different spatial orbitals but also spin, maybe?\n\nWait, hold on. In some treatments, when considering identical particles like electrons, you need to consider both space and spin parts together. For example, the overall wavefunction is a product of spatial and spin parts, and the entire thing needs to be antisymmetric.\n\nBut sometimes people separate them into symmetric and antisymmetric components. Hmm, perhaps in this case, assuming that the spins are already accounted for as singlet or triplet states, but given that the problem doesn't specify anything about spin, maybe we just focus on the spatial wavefunctions being antisymmetrized.\n\nSo let me think again. Each electron occupies a single-particle orbital, say ψ_i and ψ_j where i ≠ j because of Pauli exclusion. Then the combined spatial wavefunction would be something like [ψ_i(r₁)ψ_j(r₂) - ψ_j(r₁)ψ_i(r₂)] / √2, right? Because swapping the particles gives minus the original function.\n\nTherefore, how many such combinations do we have? We have three single-particle states, so choosing two distinct ones from three. How many ways can we choose two distinct states out of three? It's C(3,2)=3. Therefore, there will be three possible spatial wavefunctions.\n\nLet me list them explicitly:\n\nFirst combination: ψ₁ and ψ₂\n\nThe corresponding spatial wavefunction is:\n[ψ₁(r₁)ψ₂(r₂) - ψ₂(r₁)ψ₁(r₂)] / √2\n\nSecond combination: ψ₁ and ψ₃\n\nWavefunction:\n[ψ₁(r₁)ψ₃(r₂) - ψ₃(r₁)ψ₁(r₂)] / √2\n\nThird combination: ψ₂ and ψ₃\n\nWavefunction:\n[ψ₂(r₁)ψ₃(r₂) - ψ₃(r₁)ψ₂(r₂)] / √2\n\nHence, those are the three possible antisymmetric spatial wavefunctions for the two-electron system.\n\nAlternatively, another way to write them could involve using Slater determinants, but essentially, it boils down to the above expressions.\n\nI don't see any other possibilities because beyond selecting pairs from three states, you cannot have more than three unique combinations without violating the Pauli exclusion principle.\n\nMoving on to part (b):\n\nWe have an interaction Hamiltonian δH = V’(r₁, r₂), which is symmetric; meaning V’(r₁, r₂) = V’(r₂, r₁). Need to compute the matrix element ⟨ψ₁₃|δH|ψ₁₂⟩ and show that it equals the difference between two terms involving inner products.\n\nGiven that ψ₁₃ and ψ₁₂ are presumably the antisymmetric wavefunctions constructed earlier. From my previous reasoning, ψ₁₂ would correspond to combining ψ₁ and ψ₂, while ψ₁₃ combines ψ₁ and ψ₃.\n\nWait, actually, looking back, our first combination was ψ₁ and ψ₂ giving ψ₁₂, second was ψ₁ and ψ₃ giving ψ₁₃, third was ψ₂ and ψ₃ giving ψ₂₃.\n\nThus, ψ₁₂ is [ψ₁(r₁)ψ₂(r₂)-ψ₂(r₁)ψ₁(r₂)]/√2,\n\nand ψ₁₃ is [ψ₁(r₁)ψ₃(r₂)-ψ₃(r₁)ψ₁(r₂)]/√2.\n\nNow, the task is to calculate ⟨ψ₁₃|V'|ψ₁₂⟩.\n\nSince V' is symmetric, V'(r₁,r₂)=V'(r₂,r₁),\n\nso let's expand the bra and ket vectors.\n\n⟨ψ₁₃| is the conjugate transpose of ψ₁₃, so it'll look similar except complex-conjugated.\n\nSimilarly, |ψ₁₂⟩ is the column vector version.\n\nMultiplying them through with V', which acts on both coordinates.\n\nMathematically, expanding the expectation value:\n\n⟨ψ₁₃| V' |ψ₁₂⟩ =\n\n∫ dr₁ dr₂ [ψ₁*(r₁)ψ₃*(r₂) - ψ₃*(r₁)ψ₁*(r₂)] / √2 * V'(r₁, r₂) *\n\n[ψ₁(r₁)ψ₂(r₂) - ψ₂(r₁)ψ₁(r₂)] / √2\n\nSimplify constants: 1/(√2)(√2))=1/2.\n\nExpanding numerator:\n\nTerm 1: ψ₁*ψ₃* V' ψ₁ψ₂\n\nTerm 2: ψ₁*ψ₃* V' (-ψ₂ψ₁)\n\nTerm 3: (-ψ₃*)ψ₁* V' ψ₁ψ₂\n\nTerm4: (-ψ₃*)(-ψ₁*) V' (-ψ₂ψ₁)\n\nWait, better yet, multiply term-by-term:\n\nIt becomes four integrals:\n\nIntegral over r₁ and r₂ of:\n\n[ψ₁*(r₁)ψ₃*(r₂)][V'] [ψ₁(r₁)ψ₂(r₂)]\n\nminus\n\n[ψ₁*(r₁)ψ₃*(r₂)][V'][ψ₂(r₁)ψ₁(r₂)]\n\nminus\n\n[ψ₃*(r₁)ψ₁*(r₂)][V'][ψ₁(r₁)ψ₂(r₂)]\n\nplus\n\n[ψ₃*(r₁)ψ₁*(r₂)][V'][ψ₂(r₁)ψ₁(r₂)]\n\nAll divided by 2.\n\nThat seems complicated, but notice symmetries.\n\nBecause V' is symmetric, V'(r₁, r₂)=V'(r₂, r₁). Maybe we can exploit that symmetry.\n\nLooking at Term 1: Integral of ψ₁*(r₁)ψ₃*(r₂)V'ψ₁(r₁)ψ₂(r₂)dr₁dr₂\n\nThis can be written as integral over r₁ of ψ₁*(r₁)ψ₁(r₁) times integral over r₂ of ψ₃*(r₂)ψ₂(r₂) multiplied by V'.\n\nWait, not exactly, because V' depends on both variables. Alternatively, perhaps express it as summing over permutations.\n\nWait, alternatively, note that interchanging r₁ and r₂ might help us relate certain terms.\n\nFor instance, take Term 1: Integrate ψ₁*(r₁)ψ₃*(r₂)V'(r₁, r₂)ψ₁(r₁)ψ₂(r₂) dr₁ dr₂\n\nIf we switch r₁ ↔ r₂, then V' remains the same, and the rest transforms accordingly.\n\nAfter switching, Term1 becomes Integrate ψ₁*(r₂)ψ₃*(r₁)V'(r₂, r₁)ψ₁(r₂)ψ₂(r₁) dr₁ dr₂\n\nWhich is equal to Integrate ψ₁*(r₁)ψ₃*(r₂)V'(r₁, r₂)ψ₁(r₁)ψ₂(r₂) dr₁ dr₂, which is the same as Term1 itself. Hence, Term1 is invariant under interchange.\n\nSimilarly, check Term2: Integrate ψ₁*(r₁)ψ₃*(r₂)V'("}
{"id": "quantum/8015", "questions": "Consider the scattering of a particle by a regular lattice of basis $\\mathbf a, \\mathbf b, \\mathbf c$. The interaction with the lattice can be written as $V = \\sum_j V(|\\mathbf{r} - \\mathbf{r}_j|)$, where $V(|\\mathbf{r} - \\mathbf{r}_j|)$ is the potential of each atom and is spherically symmetric about the atom lattice point. Show using the Born approximation that the condition for non-vanishing scattering is that the Bragg law be satisfied.  ", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem here about scattering from a crystal lattice, right? It says to use the Born approximation to show that the condition for non-vanishing scattering is when the Bragg law is satisfied. Hmm, let me try to unpack this.\n\nFirst off, what do I remember about the Born approximation? From my solid state physics class, I think it's used in perturbation theory for solving Schrödinger equations, especially in scattering problems. In the context of crystals, maybe we're looking into how particles scatter off atoms arranged periodically?\n\nThe setup given is a lattice with basis vectors **a**, **b**, **c**. So, the positions of the atoms are all points like n₁**a** + n₂**b** + n₃**c**, where n's are integers. Each atom has a potential V(r) which depends only on the distance from itself—so spherical symmetry around each lattice site.\n\nSo the total potential is a sum over all these individual potentials: V(**r**) = Σⱼ V(|**r** − **r**_j|). That makes sense because every atom contributes its own potential field.\n\nNow, the question wants us to apply the Born approximation to find out under what conditions there's significant (non-zero) scattering. And apparently, that should lead us to the Bragg law. Okay, so probably something related to constructive interference or specific momentum transfers leading to peaks in diffraction patterns.\n\nLet me recall the Born approximation steps. For elastic scattering, the first-order Born approximation gives the differential cross-section proportional to |f(θ)|², where f is the scattering amplitude. To compute f, you take Fourier transforms of the potential.\n\nWait, more precisely, the scattering amplitude f(k', k) is approximately equal to -(4π/2m)(k')^{-1} times the integral of e^{i(q · r)} V(r) dr, evaluated at q = k' - k, where k and k' are initial and scattered wavevectors respectively.\n\nBut since our potential is periodic due to the lattice structure, perhaps we need to express it in terms of plane waves or reciprocal space components. Oh yeah! Because any periodic function can be expressed via Bloch functions or through their Fourier series.\n\nIn particular, if the potential V(**r**) is periodic with period determined by the lattice vectors, then according to Bloch's theorem, the eigenfunctions will also be Bloch states. But wait, actually, in scattering, we might not directly deal with eigenstates but rather consider the Green's function approach or just expand the potential in plane waves.\n\nAlternatively, considering that the potential is a sum over delta functions convolved with some atomic shape... Wait no, each term is a shifted version of V(r), centered at each lattice point.\n\nTherefore, mathematically, V(**r**) would look like:\n\nV(**r**) = ∑_{n₁,n₂,n₃} V₀(|**r** - (**n₁a** + **n₂b** + **n₃c**)|)\n\nwhere V₀ is the single-site potential.\n\nTo make progress, I think expanding V(**r**) in Fourier series could help. Since the lattice is periodic, the Fourier transform of such a potential would consist of discrete delta functions at the reciprocal lattice vectors plus whatever comes from the Fourier transform of V₀.\n\nRecall that the Fourier transform of a periodic function is another set of delta functions located at the reciprocal lattice vectors multiplied by coefficients depending on the unit cell content.\n\nSpecifically, the Fourier transform of V(**r**) would involve sums over G, the reciprocal lattice vectors, weighted by integrals involving V₀.\n\nMathematically,\n\nV(**q**) = (1/(2π)^3) ∫ d³r e^{-i**q·r**} V(**r**) \n\n= (1/(2π)^3) ∫ d³r e^{-i**q·r**} [∑_{G} δ(**q** - **G**) * v_G ]\n\nWhere v_G is the coefficient corresponding to reciprocal vector G, calculated as\n\nv_G = ∫_{unit cell} d³r e^{-i**G·r**} V₀(**r**).\n\nHmm, okay, so putting it together, the Fourier transform of V(**r**) is zero except at reciprocal lattice points **G**, where it takes value v_G / (volume factor?), scaled appropriately.\n\nGiven that, going back to the expression for the scattering amplitude f(**q**) ≈ (-4π/m ω) ∫ d³r e^{i**q·r**} V(**r**),\n\nwhich becomes f(**q**) ≈ (-4π/m ω) V(**q**).\n\nSince V(**q**) is non-zero only at **q** being reciprocal lattice vectors, say **G**, then unless **q** equals one of those **G**, the integral vanishes.\n\nThus, the scattering amplitude f(**q**) is non-zero only when **q** coincides with a reciprocal lattice vector **G**.\n\nWhat does **q** represent? Remembering that **q** = **k'** - **k**, where **k** and **k'** are incoming and outgoing momenta. Therefore, setting **q** = **G**, we get **k'** = **k** + **G**.\n\nThis leads to conservation of momentum modulo reciprocal lattice vectors. Now, relating this to the Bragg law...\n\nBragg's Law is usually stated as 2d sinθ = λ, where d is interplanar spacing, θ is angle between incident and diffracted beam, and λ wavelength. Alternatively, in terms of wavevectors, it relates the change in direction based on the reciprocal lattice vectors.\n\nExpressed in terms of wavevectors, the Bragg condition requires that the difference **k'** - **k** must lie along a reciprocal lattice vector **G**. Which aligns exactly with what we found above—that **q** = **G**.\n\nHence, the condition for non-vanishing scattering occurs when the momentum transfer **q** matches a reciprocal lattice vector, i.e., satisfying the Bragg condition.\n\nPutting it formally, the Bragg equation is often written as:\n\n**G** ⋅ (**hkl**) = 0 ?\n\nNo, wait, more accurately, the Bragg condition is derived from the requirement that the path difference between rays reflected from adjacent planes is an integer multiple of wavelengths. Translating that into reciprocal space, it corresponds to **k'** - **k** = **G**, hence the same conclusion.\n\nTherefore, within the Born approximation, the scattering amplitude doesn't vanish only when the Bragg condition holds, meaning **q** lies in the reciprocal lattice.\n\nI think that wraps up the reasoning.\n</think>\n\nUsing the Born approximation, we analyze the scattering of a particle by a periodic lattice. The potential \\( V(\\mathbf{r}) \\) is expressed as a sum of contributions from each lattice site, making it periodic. Expanding \\( V(\\mathbf{r}) \\) in Fourier space reveals that its Fourier transform is nonzero only at reciprocal lattice vectors \\( \\mathbf{G} \\). The scattering amplitude \\( f(\\mathbf{q}) \\) involves integrating against \\( V(\\mathbf{r}) \\), resulting in nonvanishing values solely when \\( \\mathbf{q} = \\mathbf{k'} - \\mathbf{k} \\) equals a reciprocal lattice vector \\( \\mathbf{G} \\). This enforces the Bragg condition \\( \\mathbf{k'} - \\mathbf{k} = \\mathbf{G} \\), ensuring constructive interference and thus non-vanishing scattering.\n\n\\[\n\\boxed{\\text{Non-vanishing scattering occurs when } \\mathbf{k}' - \\mathbf{k} = \\mathbf{G}, \\text{ satisfying the Bragg law}}\n\\]"}
{"id": "quantum/8024", "questions": "(a) You are given a system of two identical particles which may occupy any of three energy levels $\\varepsilon_n = n\\varepsilon$, $n = 0, 1, 2$. The lowest energy state, $\\varepsilon_0 = 0$, is doubly degenerate. The system is in thermal equilibrium at temperature $T$. For each of the following cases, determine the partition function and the energy and carefully enumerate the configurations.\n\n(1) The particles obey Fermi statistics.\n(2) The particles obey Bose statistics.\n(3) The (now distinguishable) particles obey Boltzmann statistics.\n\n(b) Discuss the conditions under which fermions or bosons might be treated as Boltzmann particles.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about statistical mechanics with two identical particles that can occupy one of three energy levels: ε₀=0 (which is doubly degenerate), ε₁=ε, and ε₂=2ε. There's part (a) where I need to find the partition functions for Fermi, Bose, and Boltzmann statistics, along with their energies. Then part (b) asks when fermions or bosons can be approximated as Boltzmann particles. Let me start with part (a).\n\nFirst, let me recall what each statistic entails:\n\n- **Fermi Statistics**: Particles are indistinguishable and must obey the Pauli exclusion principle; no more than one particle per quantum state.\n  \n- **Bose Statistics**: Also indistinct but can all pile into the same state if they want; there’s no restriction on occupation numbers beyond being non-negative integers.\n\n- **Boltzmann Statistics**: Distinguished from the others because here we treat them as classical, distinguishable particles—so Maxwell-Boltzmann distribution applies without restrictions except maybe integer counts?\n\nWait, actually, wait—the third case says \"distinguishable\" particles, so it should follow Maxwell-Boltzmann stats regardless? Hmm, okay.\n\nSo starting with case (1): Fermi statistics.\n\n**Case 1: Fermions**\n\nSince these are fermions, each single-particle state can hold at most one particle. But first, how many states do we have? Each energy level has certain degeneracies.\n\nGiven ε₀ is doubly degenerate—that means n=0 corresponds to two distinct states (maybe spin up and down?), while ε₁ and ε₂ are non-degenerate since nothing was said otherwise. So total available states:\n\n- Level 0: 2 states\n\n- Levels 1 and 2: 1 state each\n\nTotal possible single-particle states: 4.\n\nBut we only have two particles. Since they're fermions, both cannot be in the same state. Therefore, the number of ways depends on whether they’re in different states or not—but considering identical particles, perhaps some symmetries come into play.\n\nAlternatively, another approach is to list all possible microstates consistent with the Pauli exclusion principle.\n\nLet me think—if we have two particles, then each must go into separate states. How many such combinations exist?\n\nThe four single-particle states are:\n\nState A: ε₀, say spin-up,\n\nState B: ε₀, spin-down,\n\nState C: ε₁,\n\nState D: ε₂.\n\nWe need to choose two distinct states out of these four for our two fermions. However, since the particles are identical, the order doesn't matter—it's just choosing subsets.\n\nNumber of possible pairs:\n\nC(4,2)=6 possibilities.\n\nEach pair will correspond to a unique configuration.\n\nNow, compute the energy for each configuration.\n\nPossible pairs:\n\n1. Both in ε₀: Wait, but ε₀ is doubly degenerate, meaning you can put one in State A and one in State B. That would give zero energy for each, so total E=0+0=0.\n\nIs that allowed? Yes, because even though they share the same energy level, they are in different states due to degeneracy—they don’t violate Pauli.\n\nSimilarly, other pairs could involve mixing between ε₀ and higher levels.\n\nList all six:\n\n1. States A & B: Energy 0 + 0 = 0\n\n2. States A & C: 0 + ε = ε\n\n3. States A & D: 0 + 2ε = 2ε\n\n4. States B & C: 0 + ε = ε\n\n5. States B & D: 0 + 2ε = 2ε\n\n6. States C & D: ε + 2ε = 3ε\n\nTherefore, the possible energies are 0, ε, 2ε, 3ε.\n\nHow many times does each occur?\n\nE=0 occurs once (only combination AB)\n\nE=ε occurs twice (AC and BC)\n\nE=2ε occurs twice (AD and BD)\n\nE=3ε occurs once (CD)\n\nThus, the multiplicity g(E):\n\ng(0)=1,\n\ng(ε)=2,\n\ng(2ε)=2,\n\ng(3ε)=1.\n\nHence, the partition function Z_F is sum over E [g(E) exp(-βE)], where β=1/(k_B T). Here, k_B is Boltzmann constant, but often set to 1 in natural units.\n\nAssuming k_B=1, β=1/T.\n\nThen,\n\nZ_F = 1 * e^{-0} + 2*e^{-\\beta \\epsilon} + 2*e^{-2\\beta \\epsilon} + 1*e^{-3\\beta \\epsilon}\n\nSimplify:\n\nZ_F = 1 + 2e^{-\\beta \\epsilon} + 2e^{-2\\beta \\epsilon} + e^{-3\\beta \\epsilon}\n\nThat seems correct.\n\nNext, average energy <E> is computed via derivative of ln(Z)/β, but alternatively, using weighted sum:\n\n<E> = Σ [g(E)*E*exp(-βE)] / Z_F\n\nCompute numerator:\n\nFor E=0: contribution is 0*(1)e^{0}=0\n\nE=ε: 2*ε*e^{-βε}\n\nE=2ε: 2*2ε*e^{-2βε}=4ε e^{-2βε}\n\nE=3ε: 3ε*e^{-3βε}\n\nSumming these gives numerator N = 2ε e^{-βε} + 4ε e^{-2βε} + 3ε e^{-3βε}\n\nDenominator is Z_F as above.\n\nSo,\n\n<E>_F = [2ε e^{-βε} + 4ε e^{-2βε} + 3ε e^{-3βε}] / [1 + 2e^{-βε} + 2e^{-2βε} + e^{-3βε}]\n\nI'll leave it like that unless further simplification is needed.\n\nMoving on to Case (2): Boson statistics.\n\n**Case 2: Bosons**\n\nHere, particles are bosons, so multiple occupancy is allowed. They can occupy the same state. Again, we have two identical bosons.\n\nAgain, listing all possible microstates respecting Bose-Einstein statistics.\n\nSingle-particle states again are A,B,C,D as before.\n\nHowever, now, both particles can be in the same state or split across different ones.\n\nSo, the possible distributions are:\n\nEither both in the same state, or distributed among different states.\n\nNote that since the particles are identical, putting both in state A is considered the same as both in state B if they were symmetrically placed, but in reality, since A and B are different states within the same energy level, having both in A vs both in B are different configurations.\n\nWait, but in terms of energy, both in A would still contribute 0, similar to both in B.\n\nHmm, yes, so let's see.\n\nPossible scenarios:\n\n1. Both particles in the same state:\n\n   - Both in A: energy 0\n\n   - Both in B: energy 0\n\n   - Both in C: energy ε\n\n   - Both in D: energy 2ε\n\n2. One particle in one state, the other in another state:\n\n   This includes various combinations, including possibly same energy levels or different.\n\nBut importantly, since particles are indistinct, swapping doesn't create new states.\n\nSo, the number of such mixed states needs careful counting.\n\nWait, perhaps better way is to consider partitions based on occupation numbers.\n\nIn Bose-Einstein, the number of particles in each state can be 0,1,... etc., subject to total particles being 2.\n\nOur single-particle states are A,B,C,D.\n\nSo, the possible occupation vectors (number of particles in each state) are:\n\nOption 1: Two particles in one state, rest empty.\n\nWhich leads to:\n\n(A,A), (B,B), (C,C), (D,D)—each corresponding to specific energies.\n\nEnergy contributions:\n\nIf both in A or B: E=0\n\nBoth in C: E=ε\n\nBoth in D: E=2ε\n\nOption 2: One particle in one state, one in another.\n\nThese include:\n\nA and B: E=0+0=0\n\nA and C: 0+ε=ε\n\nA and D: 0+2ε=2ε\n\nB and C: 0+ε=ε\n\nB and D: 0+2ε=2ε\n\nC and D: ε+2ε=3ε\n\nAdditionally, note that in option 2, the ordering doesn't matter because particles are indistinct.\n\nAlso, notice that in Option 1, when both are in A or B, those count separately despite leading to"}
{"id": "quantum/2-2006", "questions": "(a) For a simple harmonic oscillator with $$ H = (p^2/m + kx^2)/2 $$, show that the energy of the ground state has the lowest value compatible with the uncertainty principle.\n\n(b) The wave function of the state where the uncertainty principle minimum is realized is a Gaussian function $$ \\exp(-\\alpha x^2) $$. Making use of this fact, but without solving any differential equation, find the value of $$ \\alpha $$.\n\n(c) Making use of raising or lowering operators, but without solving any differential equation, write down the (non-normalized) wave function of the first excited state of the harmonic oscillator.\n\n(d) For a three-dimensional oscillator, write down, in polar coordinates,\nthe wave functions of the degenerate first excited state which is an eigenstate of $l_z$.", "graphs": null, "llm_answers": "<think>\nOkay, so I have these four parts to solve about the quantum harmonic oscillator. Let me take them one by one.\n\nStarting with part (a): Show that the energy of the ground state has the lowest value compatible with the uncertainty principle.\n\nHmm, right. So for a simple harmonic oscillator, the Hamiltonian is given as \\( H = \\frac{1}{2}\\left(\\frac{p^2}{m} + kx^2\\right) \\). And we need to relate this to the Heisenberg Uncertainty Principle, which states that \\( \\Delta x \\Delta p \\geq \\hbar/2 \\).\n\nI remember from my studies that the expectation values can be used here. In the ground state, both position and momentum uncertainties should reach their minimal product allowed by the uncertainty principle. \n\nSo let's denote \\( E_0 \\) as the ground state energy. We know that the total energy is the sum of kinetic and potential energies. Since it's symmetric, maybe each contributes equally? That would make sense because in equilibrium, average kinetic equals average potential.\n\nIf that's true, then \\( E_0 = 2E_{kinetic} \\), since \\( E_kinetic = E_potential \\). But wait, actually, no—each term averages out separately. Maybe more accurately, using virial theorem: for quadratic potentials like SHO, the average kinetic and potential energies are equal. Therefore, \\( \\langle T \\rangle = \\langle V \\rangle \\), so \\( E_0 = 2\\langle T \\rangle \\).\n\nBut how does this tie into the uncertainty principle?\n\nWell, perhaps express the energy in terms of variances. Because if you expand the Hamiltonian:\n\n\\( H = \\frac{p^2}{2m} + \\frac{kx^2}{2} \\)\n\nTaking expectations,\n\n\\( \\langle H \\rangle = \\frac{\\langle p^2 \\rangle}{2m} + \\frac{k \\langle x^2 \\rangle }{2} \\)\n\nWe also know from the uncertainty principle that\n\n\\( (\\Delta x)^2 (\\Delta p)^2 \\geq (\\hbar / 2)^2 \\)\n\nWhich implies that either \\( (\\Delta x)(\\Delta p) \\) must be greater than or equal to \\( \\hbar / 2 \\).\n\nIn the case when equality holds, i.e., when the product reaches its minimum, what happens to the energy?\n\nLet’s assume that the state minimizes the product \\( (\\Delta x)(\\Delta p) \\). Then, substituting expressions involving \\( \\Delta x \\) and \\( \\Delta p \\) into the expression for energy might give us the minimal possible energy.\n\nExpressing \\( \\langle p^2 \\rangle \\) and \\( \\langle x^2 \\rangle \\):\n\nSince \\( \\Delta p = \\sqrt{\\langle p^2 \\rangle - \\langle p \\rangle^2} \\) and similarly for \\( \\Delta x \\). If the state is centered around zero, then \\( \\langle p \\rangle = 0 \\) and \\( \\langle x \\rangle = 0 \\), simplifying things.\n\nTherefore, \\( \\Delta p = \\sqrt{\\langle p^2 \\rangle} \\) and \\( \\Delta x = \\sqrt{\\langle x^2 \\rangle} \\).\n\nFrom the uncertainty principle,\n\n\\( \\sqrt{\\langle p^2 \\rangle} \\cdot \\sqrt{\\langle x^2 \\rangle} \\geq \\hbar / 2 \\)\n\nSquaring both sides,\n\n\\( \\langle p^2 \\rangle \\langle x^2 \\rangle \\geq (\\hbar / 2)^2 \\)\n\nNow, our goal is to minimize \\( \\langle H \\rangle \\). Using Cauchy-Schwarz inequality or some method to combine these two variables.\n\nAlternatively, think of minimizing \\( A/B + B/A \\)-type expressions. Wait, not exactly, but similar idea.\n\nWait, another approach: Use Lagrange multipliers. Minimize \\( \\langle H \\rangle \\) subject to the constraint \\( \\langle p^2 \\rangle \\langle x^2 \\rangle = (\\hbar / 2)^2 \\).\n\nSet up the functional:\n\n\\( F = \\frac{\\langle p^2 \\rangle}{2m} + \\frac{k \\langle x^2 \\rangle }{2} + \\lambda \\left( \\langle p^2 \\rangle \\langle x^2 \\rangle - (\\hbar / 2)^2 \\right ) \\)\n\nTake partial derivatives w.r.t. \\( \\langle p^2 \\rangle \\) and \\( \\langle x^2 \\rangle \\), set them to zero.\n\nPartial derivative w.r.t. \\( \\langle p^2 \\rangle \\):\n\\( \\frac{1}{2m} + \\lambda \\langle x^2 \\rangle = 0 \\)\nSimilarly, partial derivative w.r.t. \\( \\langle x^2 \\rangle \\):\n\\( \\frac{k}{2} + \\lambda \\langle p^2 \\rangle = 0 \\)\n\nAnd the constraint:\n\\( \\langle p^2 \\rangle \\langle x^2 \\rangle = (\\hbar / 2)^2 \\)\n\nFrom the first equation:\n\\( \\lambda = - \\frac{1}{2 m \\langle x^2 \\rangle } \\)\n\nSecond equation:\n\\( \\lambda = - \\frac{k}{2 \\langle p^2 \\rangle } \\)\n\nSetting them equal:\n\n\\( \\frac{1}{2 m \\langle x^2 \\rangle } = \\frac{k}{2 \\langle p^2 \\rangle } \\implies \\frac{1}{m \\langle x^2 \\rangle } = \\frac{k}{ \\langle p^2 \\rangle } \\implies \\langle p^2 \\rangle = m k \\langle x^2 \\rangle \\)\n\nPlugging back into the constraint:\n\n\\( (mk \\langle x^2 \\rangle ) \\langle x^2 \\rangle = (\\hbar / 2)^2 \\implies mk (\\langle x^2 \\rangle )^2 = (\\hbar / 2)^2 \\implies \\langle x^2 \\rangle = \\hbar/(2 \\sqrt{mk}) \\)\n\nThen, \\( \\langle p^2 \\rangle = m k * \\hbar/(2 \\sqrt{mk}) = \\sqrt{mk} \\hbar / 2 \\ )\n\nCompute \\( \\langle H \\rangle \\):\n\n\\( \\frac{\\langle p^2 \\rangle}{2m} + \\frac{k \\langle x^2 \\rangle }{2} = \\frac{( \\sqrt{mk} \\hbar / 2)}{2m} + \\frac{k ( \\hbar/(2 \\sqrt{mk})) }{2} \\)\n\nCalculating each term:\n\nFirst term: \\( \\frac{\\sqrt{mk}}{4m} \\hbar = \\frac{\\sqrt{k}/(4 \\sqrt{m})} \\hbar \\)\n\nWait, better compute numerically:\n\nFirst term numerator: sqrt(mk)*ħ / 2 divided by 2m gives sqrt(k)/(4 sqrt(m)) ħ ?\n\nWait, hold on:\n\nActually, let me recompute:\n\nGiven \\( \\langle p^2 \\rangle = \\sqrt{mk} \\hbar / 2 \\ ), plug into first term:\n\n\\( \\frac{\\sqrt{mk} \\hbar / 2 }{2m} = \\frac{\\sqrt{mk}}{4m} \\hbar = \\frac{\\sqrt{k}}{4 \\sqrt{m}} \\hbar \\)\n\nSimilarly, second term:\n\n\\( \\frac{k \\cdot \\hbar/(2 \\sqrt{mk}) }{2} = \\frac{k}{2 \\sqrt{mk}} \\cdot \\hbar / 2 = \\frac{\\sqrt{k}}{2 \\sqrt{m}} \\cdot \\hbar / 2 = \\frac{\\sqrt{k}}{4 \\sqrt{m}} \\hbar \\)\n\nAdding together: Each term is same, so total is twice that, hence \\( \\frac{\\sqrt{k}}{2 \\sqrt{m}} \\hbar \\ ). Which is known as \\( \\hbar \\omega / 2 \\), since ω=√(k/m). Thus, confirming that the minimal energy is indeed ℏω/2, matching the ground state energy.\n\nHence proved that the ground state energy satisfies the minimal condition via uncertainty principle.\n\nMoving on to part (b): Find α such that the wavefunction exp(-α x²) realizes the minimal uncertainty.\n\nAlright, so the wavefunction is a Gaussian: ψ(x)=exp(-α x²). To satisfy the minimal uncertainty, ΔxΔp=ℏ/2.\n\nNeed to calculate Δx and Δp for"}
{"id": "quantum/5027", "questions": "Assume that the proton has a nonzero radius $r_p \\approx 10^{-13}$ cm and that its charge is distributed uniformly over this size. Find the shift in the energy of the 1s and 2p states of hydrogen due to the difference between a point charge distribution and this extended charge.  ", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about finding the shift in the energy levels of hydrogen's 1s and 2p states when considering that the proton isn't just a point charge but actually has a small radius. The given radius is approximately \\( r_p = 10^{-13} \\) cm. Hmm, interesting.\n\nFirst off, let me recall what I know about atomic orbitals and their energies. In the Bohr model or even more accurately with quantum mechanics using the Schrödinger equation, we usually assume the nucleus (proton here) is a point charge. This simplifies things because then the potential inside the atom can be treated as Coulombic everywhere except exactly at the origin where it diverges, which doesn't affect the bound state solutions much.\n\nBut if the proton has a finite size, say uniform charge distribution within some radius \\( r_p \\), then the electric field experienced by the electron would change depending on whether the electron is inside or outside the proton. That should lead to different potentials for regions inside (\\( r < r_p \\)) and outside (\\( r > r_p \\)) the proton.\n\nSo, how does this affect the energy? Well, the energy levels depend on the expectation value of the Hamiltonian, specifically the electrostatic potential energy term. For each orbital (like 1s, 2p), the wavefunction will extend into the region near the nucleus, meaning part of the time the electron is \"inside\" the proton. Therefore, instead of experiencing the usual \\( -e^2/(4\\pi \\epsilon_0 r) \\) potential from a point charge, it experiences something else when inside.\n\nLet me think: If the charge is spread out uniformly, the potential inside a sphere of charge depends linearly on the distance from the center. Specifically, for a solid sphere with uniform charge density, the electric field inside is proportional to \\( r \\). So, integrating that gives us the potential inside as quadratic in \\( r \\).\n\nWait, no—actually, wait. Let me get precise:\n\nFor a point charge, the potential is \\( V(r) = -k e / r \\) where \\( k = 1/(4\\pi \\epsilon_0) \\).\n\nIf the charge is spread uniformly throughout a sphere of radius \\( r_p \\), then inside the sphere, the electric field increases linearly up to the surface. But since the potential must be continuous across the boundary, there’s a jump condition. Wait, maybe not—it might still vary smoothly?\n\nActually, hold on. Inside a uniformly charged sphere, the electric field is radial and follows Gauss's law. At any radius \\( r < r_p \\):\n\n\\( E(r) = \\frac{Q}{4\\pi \\epsilon_0 R^3} r \\)\n\nwhere Q is total charge, R is the radius of the sphere. Then, the potential inside is found by integrating the electric field from infinity down to r. However, typically, you set the potential at infinity to zero, but inside a conductor, sometimes people define it differently... Maybe better to compute both inside and outside.\n\nAlternatively, perhaps another approach is needed. Since the potential is being modified only within \\( r_p \\), while beyond that, it remains the same as the point charge case. Thus, the modification occurs only when the electron is within \\( r_p \\).\n\nTherefore, the correction to the energy comes from evaluating the integral of the wavefunction multiplied by the difference in potential between the two cases (point vs. extended charge) over all space. Because most of the volume is far away from the nucleus, especially for higher n states, but for lower n like 1s, the probability is significant close-in.\n\nMathematically, the first-order energy shift \\( \\Delta E \\) due to perturbation theory would involve calculating:\n\n\\[ \\Delta E = \\int \\psi^* (V_{extended}(r) - V_{point}(r)) \\psi d^3r \\]\n\nWhere \\( V_{extended}(r) \\) is the potential due to the extended charge, and \\( V_{point}(r) \\) is the standard Coulomb potential.\n\nSince the extended potential differs from the point one only within \\( r < r_p \\), the integration reduces to computing the integral over \\( r < r_p \\).\n\nNow, let's figure out expressions for these potentials.\n\nInside the sphere (\\( r < r_p \\)), the potential due to a uniformly charged sphere is known. It turns out that the potential inside such a sphere is constant plus a term increasing quadratically with r. More precisely,\n\nThe potential inside is:\n\n\\[ V_{in}(r) = \\frac{k_e Q}{R} \\left( \\frac{3}{2} - \\frac{r^2}{2 R^2} \\right ) \\]\n\nAnd outside, it behaves normally as \\( V_{out}(r) = \\frac{k_e Q}{r} \\).\n\nHere, \\( Q = +e \\) is the charge of the proton, right? And \\( R = r_p \\).\n\nThus, the difference in potential between the extended and point models is non-zero only inside \\( r_p \\). Outside, they're identical, so subtracting them cancels out.\n\nHence, the integrand becomes:\n\n- For \\( r < r_p \\): \\( [V_{in}(r) - (-k_e e / r)] \\)\n  \nBecause originally, without the extended charge consideration, the potential was \\( V_{point}(r) = -k_e e / r \\). Now, replacing it with \\( V_{in}(r) \\) inside, so the difference is \\( V_{in}(r) - V_{point}(r) \\).\n\nCalculating that:\n\nDifference \\( D(r) = V_{in}(r) - V_{point}(r) \\)\n\nSubstituting:\n\n\\[ D(r) = \\frac{k_e e}{r_p} \\left( \\frac{3}{2} - \\frac{r^2}{2 r_p^2} \\right ) - \\left(- \\frac{k_e e}{r}\\right ) \\]\nSimplify:\n\\[ D(r) = \\frac{3 k_e e}{2 r_p} - \\frac{k_e e r}{2 r_p^2 } + \\frac{k_e e}{r} \\]\n\nHmm, okay. Alternatively, perhaps my expression for \\( V_{in}(r) \\) needs checking. Another way to find \\( V_{in}(r) \\) is through solving Poisson's equation.\n\nGiven that the charge density \\( \\rho = \\frac{3 e}{4/3 \\pi r_p^3} \\) assuming uniform distribution.\n\nThen, the electric displacement inside satisfies:\n\n\\( \\nabla \\cdot E = \\rho / \\epsilon_0 \\Rightarrow E(r) = \\frac{\\rho r}{3 \\epsilon_0} \\hat{r} \\)\n\nIntegrating E to get V:\n\nPotential decreases against the direction of E, so\n\nFrom a reference point at \\( r = r_p \\), moving inward, the potential changes.\n\nAt \\( r = r_p \\), the potential matches the external potential, i.e., \\( V(r_p^-) = V(r_p^+) = \\frac{k_e e}{r_p} \\).\n\nCompute V(r) inside:\n\nStarting from \\( r_p \\), integrate E dr towards smaller radii.\n\nChange in potential \\( dV = -E dr \\)\n\nSo,\n\\[ V(r) = V(r_p) - \\int_r^{r_p} E(r') dr' \\]\nWhich is:\n\\[ V(r) = \\frac{k_e e}{r_p} - \\int_r^{r_p} \\frac{\\rho r'}{3 \\epsilon_0} dr' \\]\nPlugging rho:\n\\[ \\rho = \\frac{3 e}{4 \\pi r_p^3} \\]\nSo,\n\\[ V(r) = \\frac{k_e e}{r_p} - \\int_r^{r_p} \\frac{(3 e)/(4 \\pi r_p^3)}{3 \\epsilon_0} r' dr' \\]\nSimplify constants:\nNote that \\( k_e = 1/(4 \\pi \\epsilon_0) \\), so substituting back:\n\\[ V(r) = \\frac{k_e e}{r_p} - \\frac{e}{4 \\pi \\epsilon_0 r_p^3} \\int_r^{r_p} r' dr' \\]\nCalculate the integral:\nIntegral of r’ dr’ from r to rp is \\( \\frac{1}{2}(rp² - r²) \\).\nPlug that in:\n\\[ V(r) = \\frac{k_e e}{r_p} - \\frac{e}{4 \\pi \\epsilon_0 r_p^3} * \\frac{1}{2}(r_p^2 - r^2) \\]\nFactor out terms:\n\\[ V(r) = \\frac{k_e e}{r_p} - \\frac{e}{8 \\pi \\epsilon_0 r_p^3} (r_p^2 - r^2) \\]\nExpress everything in terms of \\( k_e \\):\nRecall \\( 1/(4 pi epsilon_0) = k_e \\), so \\( 1/(8 pi epsilon_0) = k_e / "}
{"id": "quantum/1-1009", "questions": "﻿Consider a one-dimensional bound particle.\n\n(a) Show that\n\n$$\n\\frac{d}{dt} \\int_{-\\infty}^{\\infty} \\psi^*(x, t) \\psi(x, t) \\, dx = 0.\n$$\n\n($\\psi$ need not be a stationary state).\n\n(b) Show that, if the particle is in a stationary state at a given time, then it will always remain in a stationary state.\n\n(c) If at $t = 0$ the wave function is constant in the region $-a < x < a$ and zero elsewhere, express the complete wave function at a subsequent time in terms of the eigenstates of the system.\n", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about quantum mechanics with three parts: (a), (b), and (c). Let me try to tackle each part step by step. \n\nStarting with part (a): It says to show that d/dt ∫ from -∞ to ∞ of ψ*ψ dx equals zero. Hmm, okay. So ψ is the wavefunction, right? And ψ* would be its complex conjugate. The integral of ψ*ψ over all space should give the probability density, which for a normalized wavefunction integrates to 1. But here they're taking the derivative with respect to time of that integral. They want us to prove that this derivative is zero, meaning the total probability remains constant over time—so normalization holds as long as it's valid initially.\n\nI remember something called the continuity equation or maybe using Schrödinger's equation somehow. Oh yeah! In quantum mechanics, there's a concept where you can derive the conservation of probability current. Maybe integrating some expression involving the partial derivatives of ψ and ψ*. Alternatively, perhaps we can use the fact that the expectation value of the Hamiltonian doesn't change unless certain conditions apply?\n\nWait, let me think more carefully. To compute d/dt of the integral of |ψ|² dx, we'll take the time derivative inside the integral:\n\n∫ [∂/∂t (ψ*)] ψ + ψ* (∂/∂t ψ) dx.\n\nSo now, substitute the expressions for ∂ψ*/∂t and ∂ψ/∂t. From the Schrödinger equation, we know that iħ ∂ψ/∂t = Hψ, where H is the Hamiltonian operator. Similarly, since ψ* satisfies the adjoint equation, (-iħ) ∂ψ*/∂t = H† ψ*, but assuming H is Hermitian, H†=H, so ∂ψ*/∂t = -(1/iħ) H ψ*.\n\nLet me write these down:\n\n∂ψ/∂t = (1/(iħ)) H ψ,\n\nand\n\n∂ψ*/∂t = (-1/(iħ)) H ψ*.\n\nSubstituting into our original expression:\n\nIntegral becomes:\n(1/(iħ)) ∫ [(-H ψ*) ] ψ + ψ* (H ψ ) dx.\n\nSimplify the constants: factor out 1/(iħ):\n\n[1/(iħ)] * Integral [ -ψ* H ψ + ψ* H ψ ] dx.\n\nBut wait, those two terms cancel each other because -ψ* H ψ + ψ* H ψ = 0. Therefore, the entire integral is zero. Hence, d/dt of the norm is zero. That makes sense—it shows that the total probability is conserved over time, which aligns with the idea of unitarity in quantum mechanics.\n\nAlright, moving on to part (b): If the particle is in a stationary state at a given time, then it will always remain in a stationary state. A stationary state means that the wavefunction has a definite energy; thus, it's an eigenstate of the Hamiltonian. Such states evolve only by acquiring a phase factor over time.\n\nSuppose ψ(x,t) is a stationary state. Then, according to the time evolution, ψ(x,t)= φ_n(x) e^{-i E_n t / ħ}, where φ_n is the spatial part of the eigenstate corresponding to energy E_n.\n\nTo see why such a state stays stationary, consider substituting back into the Schrödinger equation. Plugging ψ into the left-hand side gives iħ times the time derivative, which brings down -E_n e^{-i E_n t / ħ}. On the right-hand side, applying H to ψ gives E_n φ_n e^{-i E_n t / ħ}. Thus both sides match up, confirming that ψ evolves correctly without changing shape except for the global phase.\n\nMoreover, any linear combination of stationary states might lead to interference effects, making the overall state non-stationary. However, when starting exactly in a single eigenstate, no mixing occurs, hence remaining stationary forever.\n\nFor part (c): At t=0, the wave function is constant between -a and a, zero otherwise. We need to express the complete wave function at later times in terms of the eigenstates.\n\nFirstly, the initial condition is ψ(x,0)=constant within [-a,a], say C, else zero. Since the potential isn't specified, I assume it's free particle outside the box, but actually, considering the setup, probably infinite square well because the wavefunction is confined to [-a,a].\n\nInfinite square well solutions have eigenfunctions sin(nπx/(2a)), n=1,2,... Each corresponds to energy levels En=(n² π² ħ²)/(8 m a²). Wait, hold on—if the width is 2a instead of L=a, scaling accordingly.\n\nThe general approach is to expand the initial wavefunction in terms of the eigenstates. So, ψ(x,0)= Σ c_n φ_n(x), where φ_n are the eigenfunctions. Once expanded, the time-evolved wavefunction is just Σ c_n φ_n(x) exp(-i E_n t / ħ ).\n\nTherefore, first find coefficients c_n via inner product: c_n= ∫ φ_n*(x) ψ(x,0) dx integrated over all space, which reduces to integration from -a to a.\n\nGiven that ψ(x,0)=C in that interval, and knowing the form of φ_n, likely sine functions symmetric around origin depending on parity.\n\nAssuming even extension due to symmetry, maybe only even or odd terms survive based on whether the initial wavefunction is even or odd. Here, ψ(x,0)=C is even function, so expansion may involve only cosine terms—but wait, in standard infinite wells defined from 0 to L, the eigenfunctions include sines and cosines depending on boundary conditions. For symmetric intervals like [-a,a], sometimes people define them differently.\n\nAlternatively, shift coordinates to make it similar to a standard infinite well. Suppose shifting variable y=x+a, mapping [-a,a] to [0,2a]. Then, the eigenfunctions become sin(n pi y/(2a)). Translating back, phi_n(x)=sin(n pi (x+a)/(2a))=sin(n pi (x/(2a)+1/2 ))=sin(n pi x/(4a) + n pi/2 ). Depending on n being even or odd, this could simplify.\n\nHowever, another way is to note that for symmetric potentials, the eigenfunctions can be chosen as either even or odd functions. Given that psi(x,0)=C is even, the Fourier series expansion will consist solely of even eigenfunctions. Because multiplying an even function by an odd function yields an odd integrand, whose integral over symmetric limits cancels out. Only products with even functions contribute.\n\nThus, the expansion will contain only even n terms if the basis includes both sine and cosine terms. Or, alternatively, expressed purely in terms of cosine functions if possible.\n\nCalculating cn involves integrating C multiplied by the nth eigenfunction over [-a,a]:\n\ncn= (1/sqrt(2a)) ∫_{-a}^{a} sin(n pi x/(2a)) dx ?\n\nWait, hang on—the exact form depends on how the eigenfunctions are normalized. Typically, for the infinite square well from 0 to L, the eigenfunctions are sqrt(2/L) sin(n pi x/L). Shifting to [-L,L], it'd require adjustment.\n\nActually, better yet, suppose we model the infinite square well from -a to a. Then, the length is 2a, so the wavenumber k_n=n pi/a, leading to eigenfunctions sin(k_n x)/sqrt(a) for n=1,2,...\n\nBecause for a well from -a to a, the allowed wavelengths correspond to nodes at ±a, giving standing waves with half-wavelength fitting into the interval. So yes, the eigenfunctions are proportional to sin(n pi x/(2a))? Wait, confusion arises.\n\nHold on, let's clarify. For a well extending from x=-a to x=a, the length is 2a. The number of half-wavelengths must fit into 2a. So wavelength λ=4a/n, implying k_n=pi*n/(2a). Therefore, the eigenfunctions are sin(k_n x)=sin(n pi x/(2a)).\n\nAnd their normalization requires integrating |phi_n|^2 over [-a,a]=2a, so normalization constant is 1/sqrt(a) because ∫ sin²(...)dx over period is a.\n\nYes, correct. So phi_n(x)=(1/sqrt(a)) sin(n pi x/(2a)).\n\nThen, the coefficient c_n is computed as:\n\n"}
{"id": "quantum/2-2001", "questions": "An electron is confined in a three-dimensional infinite potential well. The sides parallel to the *x-*, *y-*, and *z*-axes are of length $L$ each.\n\n(a) Write the appropriate Schrödinger equation.  \n\n(b) Write the time-independent wave function corresponding to the state of the lowest possible energy.  \n\n(c) Give an expression for the number of states, $N$, having energy less than some given $E$. Assume $N \\gg 1$.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about an electron in a three-dimensional infinite potential well. Each side along x, y, z axes has length L. There are three parts: (a) write the Schrödinger equation, (b) find the ground state wavefunction, and (c) express the number of states with energy below E when N is large. Hmm, let me think through each part carefully.\n\nStarting with part (a). The Schrödinger equation... Well, since it's a particle in a box or infinite potential well, we know that inside the box the potential V(x,y,z)=0, right? And outside, it's infinitely high, which means the wavefunctions must be zero there. So the time-dependent Schrödinger equation would involve the Laplacian operator because it's a free particle within the box.\n\nWait, but actually, they might just want the time-independent version here. Because usually, you separate variables into space and time parts. Let me recall: the general form is (-ħ²/(2m))∇²ψ + Vψ = iħ ∂ψ/∂t. But if we're looking for stationary states, then ψ(r,t) = φ(r)e^{-iEt/ħ}, leading to the time-independent SE: [(-ħ²/(2m))∇² - E]φ = 0. Since V=0 inside, yeah, that should do it.\n\nSo writing out the Schrödinger equation explicitly:\n\n(−ℏ² / (2m)) (∇²ψ) = Eψ\n\nBut wait, maybe more precisely, considering all dimensions, perhaps using partial derivatives?\n\nYes, expanding the Laplacian gives us:\n\n(−ℏ² / (2m)) [(∂²ψ/∂x²) + (∂²ψ/∂y²) + (∂²ψ/∂z²)] = Eψ\n\nThat seems correct. So that's probably what part (a) wants—the full PDE written out.\n\nMoving on to part (b): the time-independent wave function for the lowest energy state. Okay, the ground state. In a 3D box, the quantum numbers nx, ny, nz determine the energy levels. For the lowest energy, these quantum numbers will all be as small as possible—so nx=ny=nz=1.\n\nThe general form of the wave functions in such boxes is known; they’re products of sine terms in each dimension. Specifically,\n\nψ_{n_x,n_y,n_z}(x,y,z) = sqrt[(8/L³)] sin(nπx/L) sin(nπy/L) sin(nπz/L)\n\nSince n_x, n_y, n_z are positive integers starting from 1. Therefore, substituting n_x = n_y = n_z = 1, we get:\n\nψ_111(x,y,z) = sqrt[8/L³] sin(πx/L) sin(πy/L) sin(πz/L)\n\nI need to make sure about normalization. Wait, yes, integrating |ψ|² over the cube volume should give 1. Given that each spatial integral of sin^2 term over 0 to L is L/2, multiplying them together gives (L/2)^3, so overall factor squared times (L³)/8 equals 1 implies the square root of 8/L³. Yep, looks good.\n\nNow part (c): Number of states N with energy less than E, assuming N >>1. This sounds like finding the density of states in 3D. Alternatively, thinking classically, how many quantized states lie below a certain energy level.\n\nIn 3D, the energy levels depend on the sum of squares of quantum numbers scaled appropriately. Remembering that the energy for a 3D box is:\n\nE = (h² / (8mL²))(nx² + ny² + nz² )\n\nWhere h is Planck’s constant divided by 2pi, but sometimes people use ℏ instead. Actually, better check units—if ħ is used, then E becomes (ħ² π²)/(2mL²)(nx² + ny² + nz² ). Yeah, that makes sense because the standard formula is E_n = (n² π² ħ² )/(2 m L² ), summed over x, y, z directions.\n\nTherefore, total energy is proportional to (nx² + ny² + nz²), where nx, ny, nz ≥1.\n\nTo count the number of states with E < E_max, we can model this geometrically. Imagine points (nx, ny, nz) in the first octant (since quantum numbers are positive integers) lying inside a sphere of radius R, where R relates to E_max via E ~ R².\n\nSpecifically, setting E = (ħ² π²)/(2mL²) Q, where Q = nx² + ny² + nz² ≤ Q_max. Then Q_max corresponds to E_max*(2mL²)/(ħ² π²).\n\nThus, counting the number of integer triples (nx, ny, nz) with nx² + ny² + nz² ≤ Q_max. When N is large, meaning Q_max is large, we approximate this discrete count with the volume of the eighth of a sphere with radius sqrt(Q_max):\n\nNumber of states ≈ Volume of one-eighth sphere = (4/3)πR³ /8 = (π/6) R³\n\nSubstituting R = sqrt(Q_max):\n\nVolume ≈ (π/6) (Q_max)^(3/2)\n\nHence, N(E) ≈ (π/6) ((2mL² E)/(ħ² π² ))^{3/2}\n\nSimplify exponents:\n\nFirstly, note that (2mL² E)/(ħ² π² ) is equal to Q_max. Thus,\n\nN(E) ≈ (π/6) * ( (2mL² E)/(ħ² π² ) )^{3/2 }\n\nLet me compute constants:\n\nInside the brackets: numerator is 2mL² E, denominator is ħ² π².\n\nTaking power 3/2:\n\n(2mL² E)^{3/2} / (ħ³ π³ )\n\nMultiply by π/6:\n\nTotal N(E) ≈ (π/6) * (2mL² E)^{3/2}/(ħ³ π³ )\n\nSimplify pi factors: π cancels once, leaving 1/pi².\n\nSimilarly, 2^{3/2}=2*sqrt(2); m^{3/2}; L^3; E^{3/2}. Denominator: ħ³.\n\nPutting it all together:\n\nN(E)≈ (2√2 mL³ E^{3/2})/(3 π² ħ³ )\n\nAlternatively, factoring differently:\n\nExpressed as:\n\nN(E)=(V)(8m)^{3/2} E^{3/2}}/(6 π² ħ³ )\n\nBecause V=L³, and 8 comes from somewhere else...\n\nWait, hold on, my initial approximation was based on replacing summations with integrals, treating the states as continuous. Maybe another approach is needed.\n\nAnother way: Using the concept of phase space. The number of available states up to energy E is approximately the volume of the region under consideration in k-space multiplied by the density of states.\n\nFor a 3D system, the allowed k-values are quantized as k_i = n_i π/L, similar to standing waves. Hence, the spacing between adjacent k-points is Δk_i = π/L.\n\nEach point in k-space represents a distinct quantum state. However, due to spin, electrons occupy multiple states per k-point, but since the question doesn't specify whether particles are fermions or bosons, and also not asking about degeneracy beyond counting unique energies, perhaps we don’t consider spin multiplicity here—it depends on context.\n\nAssuming non-degenerate case, ignoring spin, each set (nx, ny, nz) defines a single state. If including spin, multiply by 2 for electrons, but unless specified, likely ignore.\n\nAnyway, moving back. To estimate the number of states with energy less than E, imagine mapping each state (nx, ny, nz) to a point in 3D space with coordinates (nx, ny, nz). These points lie on a grid spaced every unit interval. The condition E <= E_max translates to nx² + ny² + nz² <= Q_max, as before.\n\nWhen Q_max is large, the number of lattice points inside the sphere approximates its volume. That is, the number of states N ≈ (volume of sphere)/8, since only positive quadrants considered.\n\nSphere volume is (4/3)π(R)^3, so dividing by 8 gives (π/6) R^3.\n\nGiven that R^2 = Q_max = (2mE L²)/(ħ² π² ).\n\nWait, no, earlier substitution had E=(ħ² π²)/(2mL²)*Q, hence Q=E*(2mL²)/(ħ² π² ). So R=sqrt(Q)=sqrt( (2mL"}
{"id": "quantum/5016", "questions": "The polarization of a diatomic molecule in weak electric fields may be treated by considering a rigid rotator with moment of inertia $I$ and electric dipole moment $d$ in a weak electric field $\\mathbf{E}$.\n\n(a) Ignoring the motion of the center of mass write down the Hamiltonian $H$ of the rigid rotator in the form of $H_0 + H$.\n\n(b) Solve the unperturbed problem exactly. How are the levels degenerate?\n\n(c) Calculate the lowest order correction to all the energy levels by non-degenerate perturbation method.\n\n(d) Explain why is nondegenerate perturbation method applicable here and how are the levels degenerate.\n\nThe following relation may be useful:\n\n$$\n\\cos \\theta Y_{lm} =\n\\sqrt{\\frac{(l + 1 - m)(l + 1 + m)}{(2l + 1)(2l + 1)}} Y_{l+1,m} +\n\\sqrt{\\frac{(l + m)(l - m)}{(2l + 1)(2l - 1)}} Y_{l-1,m}.\n$$", "graphs": null, "llm_answers": "<think>\nOkay, so I have this question about the polarization of a diatomic molecule in a weak electric field. It's divided into four parts: (a), (b), (c), and (d). Let me try to tackle each part one by one.\n\nStarting with part (a): \"Ignoring the motion of the center of mass, write down the Hamiltonian \\( H \\) of the rigid rotator in the form of \\( H_0 + H' \\).\" Hmm, okay. So we're dealing with a rigid rotor model for a diatomic molecule. The total Hamiltonian should consist of two parts: the unperturbed part due to rotation and the perturbing part due to the external electric field.\n\nFor a rigid rotor without any external fields, the Hamiltonian is purely rotational kinetic energy. Since it's ignoring translational motion, only rotational degrees of freedom matter. In quantum mechanics, the rotational Hamiltonian is given by:\n\\[ H_0 = \\frac{-\\hbar^2}{2I}\\nabla^2 \\]\nwhere \\( I \\) is the moment of inertia.\n\nNow, when you apply an external electric field \\( E \\), there will be a perturbation term because the molecule has an electric dipole moment \\( d \\). The potential energy of a dipole in an electric field is \\( U = -\\mathbf{d} \\cdot \\mathbf{E} \\).\n\nAssuming that the electric field is along some direction—let's say the z-axis for simplicity—the dot product simplifies to \\( dE \\cos\\theta \\), where \\( \\theta \\) is the angle between the dipole moment vector and the electric field. Therefore, the perturbing Hamiltonian would be:\n\\[ H' = -dE \\cos\\theta \\]\n\nSo putting them together, the full Hamiltonian becomes:\n\\[ H = H_0 + H' = \\frac{-\\hbar^2}{2I}\\nabla^2 - dE \\cos\\theta \\]\n\nWait, but actually, since the first term involves angular momentum operators, maybe expressing everything in spherical coordinates makes more sense? Because the Laplacian operator in spherical coordinates relates directly to angular momentum eigenstates.\n\nYes, right. For a rigid rotor, the wavefunctions are spherical harmonics \\( Y_{lm}(\\theta, \\phi) \\), which are eigenfunctions of \\( L_z \\) and \\( L^2 \\). But let me just confirm if my expression for \\( H' \\) is correct. Yes, the interaction energy is indeed proportional to \\( \\cos\\theta \\), as the dipole aligns or opposes the field depending on its orientation.\n\nMoving on to part (b): \"Solve the unperturbed problem exactly. How are the levels degenerate?\" Okay, solving the unperturbed problem means finding the solutions to \\( H_0 |\\psi\\rangle = E_n |\\psi\\rangle \\).\n\nSince \\( H_0 \\) corresponds to free rotation, the energy levels depend solely on the square of the angular momentum. Specifically, the energies are quantized as:\n\\[ E_l = \\frac{\\hbar^2 l(l + 1)}{2I} \\]\nfor integer values of \\( l = 0, 1, 2, ... \\)\n\nEach level \\( l \\) has a degeneracy equal to \\( 2l + 1 \\), corresponding to different magnetic quantum numbers \\( m = -l, ..., l \\). That’s standard for the hydrogen atom too; same idea applies here for the rigid rotor.\n\nTherefore, the unperturbed system has energy levels spaced quadratically with respect to \\( l \\), and each level is \\( (2l + 1) \\)-fold degenerate.\n\nPart (c): \"Calculate the lowest-order correction to all the energy levels by non-degenerate perturbation theory.\" Wait, hold on—if the unperturbed states are highly degenerate (\\( 2l + 1 \\)), can we really use non-degenerate perturbation theory? Or do we need to consider degenerate perturbation theory instead?\n\nHmm, perhaps not necessarily. Maybe under certain conditions, even though the original state is degenerate, after applying the perturbation, the corrections might lift the degeneracy such that within each multiplet, further splitting occurs. However, wait, no—in general, non-degenerate perturbation theory requires that the initial states aren't degenerate. If they are, then we must switch methods.\n\nBut looking back at the question—it says specifically to calculate using non-degenerate perturbation method. So either way, regardless of whether the states are initially degenerate, proceed with non-degenerate approach. Alternatively, maybe the matrix elements vanish except for specific transitions.\n\nLet me think again. Non-degenerate perturbation theory works when the zeroth-order states are non-degenerate. Here, our unperturbed states are definitely degenerate—for example, for each \\( l \\), multiple \\( m \\)'s share the same energy. Thus, strictly speaking, non-degenerate PT isn’t appropriate unless we diagonalize within the subspace.\n\nAlternatively, perhaps the perturbation doesn't mix these states beyond their own subspaces. Given that the perturbation depends on \\( \\cos\\theta \\), which is related to \\( Y_{10} \\)—so the transition from \\( |lm\\rangle \\) could go to other states via selection rules.\n\nIn fact, the expectation value of \\( H' \\) in a particular state \\( |lm\\rangle \\) is needed for the first-order energy shift. Using the formula:\n\\[ E^{(1)}_{lm} = \\langle lm | H' | lm \\rangle \\]\n\nGiven that \\( H' = -dE \\cos\\theta \\), substituting gives:\n\\[ E^{(1)}_{lm} = -dE \\langle lm | \\cos\\theta | lm \\rangle \\]\n\nWhat is \\( \\langle lm | \\cos\\theta | lm \\rangle \\)?\n\nWe know that \\( \\cos\\theta = \\sqrt{\\frac{4\\pi}{3}} Y_{10}(\\theta, \\phi) \\). Then,\nthe integral over \\( Y_{lm}^* Y_{10} Y_{lm} \\) needs to be evaluated.\n\nUsing orthogonality properties of spherical harmonics, the integral:\n\\[ \\int Y_{lm}^*(\\Omega) Y_{10}(\\Omega) Y_{lm}(\\Omega) d\\Omega \\]\n\nThis is equivalent to evaluating the triple integral involving three spherical harmonics. From what I remember, integrals like \\( \\int Y_{l'm'} Y_{LM} Y_{l''m''} } \\) involve Clebsch-Gordan coefficients and are generally non-zero only when certain triangle inequalities are satisfied.\n\nSpecifically, for the case above, integrating \\( Y_{lm}^* Y_{10} Y_{lm} \\):\n\nFirstly, note that \\( Y_{lm}^* = (-1)^m Y_{l,-m} \\). So,\n\nIntegral becomes:\n\\( \\int Y_{l(-m)} Y_{10} Y_{lm} d\\Omega \\)\n\nWhich is similar to coupling angular momenta. To compute this, we'd look up the relevant Gaunt coefficient or see if it vanishes based on selection rules.\n\nFrom selection rules, when multiplying \\( Y_{l m} \\times Y_{L M} \\), the resulting terms will have components with \\( l', m' \\) such that \\( |l - L| \\leq l' \\leq l + L \\) and \\( m' = m + M \\).\n\nHere, we’re effectively doing \\( Y_{l-m} * Y_{10} \\), whose product includes terms with \\( l'= l ±1 \\) and \\( m’= -m + 0 = -m \\). When integrated against another \\( Y_{lm} \\), the overlap is non-zero only if \\( l' = l \\) and \\( m’ = m \\). \n\nThus, setting \\( l' = l \\):\nEither \\( l = l + 1 \\) → impossible, or \\( l = l - 1 \\) → possible only if \\( l ≥ 1 \\). Similarly, for \\( m’ = m \\), we require \\( -m = m \\Rightarrow m = 0 \\).\n\nPutting it all together, the integral is non-zero only when \\( m = 0 \\) and \\( l' = l - 1 \\). Otherwise, it's"}
{"id": "quantum/6009", "questions": "The nucleus $ ^8 \\text{Be} $ is unstable with respect to dissociation into two $ \\alpha $ particles, but experiments on nuclear reactions characterize the two lowest unstable levels as $ J = 0$, even parity, $\\sim 95$ $keV$ above the dissociation level,\nand $ J = 2$, even parity, $\\sim 3$ $MeV$ above the dissociation level.\n\nConsider how the existence of these levels influences the scattering of $ \\alpha $ particles from helium gas, specifically:\n\n(a) Write the wave function for elastic scattering, in its partial wave expansion, for $ r \\rightarrow \\infty $.\n\n(b) Describe qualitatively how the relevant phase shifts vary as functions of energy in the proximity of each level.\n\n(c) Describe how this variation affects the angular distribution of $\\alpha$ particles.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about alpha particle scattering off helium gas involving beryllium-8 nuclei. Hmm, let me try to break it down part by part.\n\nStarting with part (a): They want the wave function for elastic scattering written in terms of a partial wave expansion when r approaches infinity. From what I remember, the elastic scattering amplitude can be expressed using Legendre polynomials multiplied by some coefficients called phase shifts. The general form should involve spherical Bessel functions or maybe plane waves? Wait, no—when you're far away from the potential (r → ∞), the scattered wave behaves like outgoing spherical waves. So the wavefunction would include both incoming and outgoing components.\n\nI think the standard expression involves summing over all possible angular momenta l. Each term has a coefficient that includes e^{iδ_l}, where δ_l is the phase shift for that particular l. And then there's also the Legendre polynomial P_l(cosθ). Let me recall: yes, something like ψ(r) ≈ e^{ikz} + Σ [something] * e^{iδ_l} / k ... wait, actually, more precisely, the asymptotic form is given by the sum over l of (-1)^l times sqrt(4π/(2l+1)) times e^{iδ_l} divided by kr^l times P_l(cosθ).\n\nWait, perhaps another way: For large r, the wavefunction is approximated by the incident wave plus the scattered wave. But since we’re considering only elastic scattering here, which doesn't change the kinetic energy, just redirects the direction. So the scattered wave will consist of different partial waves, each contributing their own phase shift.\n\nSo putting it together, the wave function should look like:\nψ(r) ~ e^{ik·r} + Σ_{l=0}^\\infty (e^{iδ_l} - 1) (2l + 1)/(k r) i^l P_l(cosθ)\nBut wait, isn’t it usually presented differently? Maybe without subtracting one?\n\nAlternatively, sometimes people write it as the sum of the incident wave and the scattered wave, where the scattered wave is represented by the sum over l of certain terms including the phase shifts. Alternatively, if they ask for the partial wave expansion itself, not necessarily combining with the incident wave... No, probably needs to express the entire wave function.\n\nLet me check my notes. Oh right! In quantum mechanics, the total wave function for scattering states is often split into the incident plane wave and the scattered wave. Asymptotically, the scattered wave dominates because the plane wave goes outwards. So the scattered wave can be expanded in terms of partial waves, each characterized by a phase shift δ_l.\n\nTherefore, the correct expression is likely:\nψ(r) ≈ e^{ikz} + Σ_{l=0}^\\infty (e^{iδ_l} - 1) (2l + 1)/ (kr) i^{-l} P_l(cosθ)\n\nHmm, but signs might differ based on conventions. Another version could use (-1)^l instead of i^{-l}. Since i^{-l} equals (-i)^l, depending on whether you define your spherical harmonics with positive or negative exponents.\n\nWait, alternatively, the standard formula is:\nf(θ,k) = Σ_{l=0}^\\infty (2l + 1) (e^{iδ_l} - 1) P_l(cosθ) / (2ik)\nAnd the wave function is ψ(r) = e^{ik·r} + f(θ,k) e^{ikr}/r\n\nYes, that seems familiar. Therefore, substituting back, the wave function becomes:\nψ(r) ≈ e^{ikz} + Σ_{l=0}^\\infty [(2l + 1)(e^{iδ_l} - 1)]/(2ik) * e^{ikr}/r * P_l(cosθ)\n\nSimplifying further, factorizing constants:\nEach term in the summation is proportional to (e^{iδ_l} - 1) P_l(cosθ) multiplied by exponential factors. However, for very large r, the dominant behavior comes from the outgoing wave, so perhaps focusing on the scattered component.\n\nBut regardless, the key point is expressing the wave function as a sum over partial waves with phase shifts included. Thus, writing the wave function as such should suffice for part (a).\n\nMoving on to part (b): How do the phase shifts vary near those Be-8 levels? Well, resonance occurs when the energy matches the binding energy of a state. Here, the two excited states are around 95 keV and 3 MeV above the dissociation threshold.\n\nIn scattering theory, when the energy E approaches a bound state energy, say E_n, the corresponding S-matrix element S_l(E) tends towards zero, leading to a peak in |f_l|, hence a resonant scattering cross-section. This corresponds to a situation where the phase shift δ_l passes through π/2 radians; meaning tan δ_l diverges, causing constructive interference in forward directions.\n\nFor each of these levels, being spin-parity J^π = 0^+ and 2^+, respectively, implies specific orbital angular momentum contributions. Because the ground state of He is spin-zero, and assuming α-particles are spin 0, the initial state must have L=J, so for the first level, J=0 suggests s-wave dominance, while J=2 indicates d-wave contribution.\n\nThus, near 95 keV, the s-wave phase shift δ_0 will exhibit a sharp rise, approaching π/2, indicating strong enhancement in scattering due to resonance. Similarly, near 3 MeV, the d-wave phase shift δ_2 will show similar behavior—a rapid increase passing through π/2, enhancing the d-wave contribution significantly.\n\nMoreover, other phase shifts may experience changes too, especially nearby energies, but primarily δ_0 and δ_2 are affected most directly by these respective resonances.\n\nPart (c): Angular distribution effect. When a phase shift δ_l increases rapidly across π/2, the differential cross section σ(θ) depends on the square of the magnitude of the scattering amplitude f(θ). Specifically,\n\nσ(θ) ∝ |Σ_l (2l + 1) (e^{iδ_l} - 1) P_l(cosθ)|²\n\nWhen δ_l crosses π/2, the term (e^{iδ_l} - 1) simplifies. At δ_l = π/2, e^{iδ_l}=i, so (i - 1)=√2 e^{i(3π/4)}, giving maximum modulus. Hence, the amplitude peaks, making the cross-section larger.\n\nFurthermore, the angular dependence is dominated by the Legendre polynomial associated with that l. If δ_0 is peaking, the s-wave contributes strongly, affecting low-angle scattering. On the other hand, higher l-waves contribute more at backward angles.\n\nSpecifically, for δ_0 crossing π/2, the s-wave term (P_0=1) leads to isotropic enhancement, but combined with other terms, it causes increased forward scattering. Whereas for δ_2, the P_2 term varies as cos²θ - 1/3, which means maxima at 0° and 180°, but particularly pronounced in the forward and backward directions compared to intermediate angles.\n\nHowever, during resonance, the dominant term is the one whose phase shift is going through π/2. So near 95 keV, the s-wave term dominates, leading to enhanced forward scattering. Near 3 MeV, the d-wave term takes over, possibly creating a more pronounced \"anisotropic\" pattern, with stronger forward and backward scattering relative to perpendicular directions.\n\nAdditionally, the presence of multiple resonances complicates the overall angular distribution, potentially introducing oscillations or variations in the shape of the distribution as energy sweeps past each resonance.\n\nPutting it all together, the main takeaway is that near each of these Be-8 levels, the corresponding phase shift experiences a significant jump, leading to noticeable features in the angular distribution of the scattered alpha particles.\n</think>\n\n### Summary Answer\n\n**(a)**  \nThe wave function for elastic scattering in its partial wave expansion as \\( r \\to \\infty \\) is given by:\n\n\\[\n\\psi(\\mathbf{r}) \\approx e^{i\\mathbf{k}\\cdot\\mathbf{r}} + \\sum_{l=0}^\\infty \\frac{(2l + 1)}{2ik} (e^{i\\delta_l} - 1) e^{ikr} P_l(\\cos\\theta),\n\\]\n\nwhere \\( \\"}
{"id": "quantum/5011", "questions": "A charged particle is bound in a harmonic oscillator potential $V = \\frac{1}{2} k x^2$. The system is placed in an external electric field $E$ that is constant in space and time. Calculate the shift of the energy of the ground state to order $E^2$.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem where there's a charged particle in a harmonic oscillator potential, V equals one half k times x squared. Then they place it in an external electric field E which is both uniform (constant in space) and static (doesn't change with time). My task is to calculate how much the energy shifts for the ground state when considering terms up to E squared.\n\nHmm, let me think about what concepts come into play here. Well, first off, without any electric field, we know the quantum harmonic oscillator has quantized energy levels given by something like E_n equals some combination of n plus a fraction multiplied by hbar omega or something like that. But now, adding an electric field will perturb this system, right?\n\nSo maybe I should approach this using perturbation theory? Because the electric field introduces a small disturbance to the original Hamiltonian. Perturbation methods allow us to approximate changes in energy levels due to such disturbances.\n\nLet me recall: In non-relativistic quantum mechanics, if you have a small perturbing Hamiltonian H', then the first-order correction to the energy is just the expectation value of H' in the unperturbed state. Second-order corrections involve sums over other states... but since the question asks only up to E squared, perhaps second-order might be necessary because the linear term could vanish?\n\nWait, actually, wait. Let me write down the total Hamiltonian including the electric field. So originally, the Hamiltonian is H0 = p²/(2m) + ½kx². When you apply an electric field E, the potential energy becomes V(x) - qEx, assuming the charge is positive; otherwise, it would be negative. Wait, no—actually, more accurately, the potential energy contribution from the electric field is U = -q φ, where phi is the scalar potential. For a uniform electric field pointing along the x-axis, the potential can be written as phi = -E x, so U = q E x. Hmm, depending on convention, sometimes people take the opposite sign. Maybe better to double-check.\n\nAlternatively, another way to look at it is that the force on a charge in an electric field is F = qE, so the work done moving against the field is dW = F dx = qE dx, hence the potential energy increases by qE x. Therefore, yes, the additional potential energy is indeed q E x. Or wait—if the electric field points in the direction of increasing x, then the potential decreases as x increases, so maybe it's minus q E x? This part always confuses me.\n\nBut regardless, whether it's positive or negative, the form is proportional to x. Since the problem says \"shift of the energy,\" probably the exact sign isn't critical unless specified, but likely, the quadratic term will depend on E squared, so signs may not matter too much beyond squaring them away.\n\nAnyway, proceeding formally, the perturbing Hamiltonian H’ is going to be either ± q E x. Depending on conventions, but let’s say H’ = -e E x, where e is the elementary charge. Alternatively, maybe H’ = q E x. Either way, the key point is that it's linear in x.\n\nNow, applying first-order perturbation theory, the first-order energy shift ΔE^(1) is ⟨ψ₀|H’|ψ₀⟩, where ψ₀ is the ground state wavefunction of the harmonic oscillator. If H’ is linear in x, then taking the expectation value of x in the ground state. But hold on—the ground state of the harmonic oscillator is symmetric around x=0, meaning that its wave function is even under parity. Hence, the expectation value of x must be zero because x is odd. Thus, the first-order correction vanishes.\n\nTherefore, the leading contribution comes from the second-order perturbation. That makes sense why the problem specifies calculating the shift up to E squared—it requires computing the second-order effect.\n\nThe formula for the second-order energy shift is:\n\nΔE^(2) = Σ_{n≠0} [ |⟨ψₙ|H’|ψ₀⟩|² / (E₀^{(0)} - Eₙ^{(0)}) ]\n\nWhere E₀^{(0)} is the unperturbed ground state energy, and Eₙ^{(0)} are the energies of excited states.\n\nGiven that our H’ is proportional to x, which connects different states via matrix elements. Specifically, in the harmonic oscillator basis, raising and lowering operators relate these states. Remembering that x is related to (a + a†), scaled appropriately.\n\nIndeed, recalling that x = √(ħ/(2 m ω)) (a + a†), where a and a† are the annihilation and creation operators respectively. So, substituting back, H’ = -e E x = -e E √(ħ/(2 m ω)) (a + a†).\n\nThus, the matrix element ⟨ψₙ|H’|ψ₀⟩ simplifies because acting with a on |ψ₀⟩ gives zero (since a annihilates the vacuum). Similarly, acting with a† takes |ψ₀⟩ to |ψ₁⟩. Therefore,\n\n⟨ψₙ|H’|ψ₀⟩ = (-e E √(ħ/(2 m ω))) δ_{n,1}\n\nBecause all transitions except n=1 go to zero. Only the transition from n=0 to n=1 contributes.\n\nSimilarly, the denominator in the sum becomes E₀^{(0)} - Eₙ^{(0)}. However, note that for n ≠ 0, specifically n=1, E₁^{(0)} = ħω/2 + ħω*1 = (3/2) ħω, while E₀^{(0)} = (1/2) ħω. So their difference is -(ħω)/2 * (something?) Wait, compute exactly:\n\nE₀^{(0)} = (1/2) ħω\n\nE₁^{(0)} = (3/2) ħω\n\nDifference: E₀ - E₁ = (1/2 - 3/2) ħω = - ħω\n\nSo plugging into the expression for ΔE^(2):\n\nΔE^(2) = |⟨ψ₁|H’|ψ₀⟩|² / (E₀^{(0)} - E₁^{(0)}} )\n\nWhich is equal to [ |(-e E √(ħ/(2 m ω)))|^2 ] / ( (1/2 ħω ) - (3/2 ħω )) \n\nSimplify numerator:\n\n(e² E² ħ/(2 m ω))\n\nDenominator:\n\n- ħω\n\nPutting together:\n\nΔE^(2) = (e² E² ħ/(2 m ω)) / (- ħω)\n\nSimplify fractions:\n\nFirstly, notice that ħ cancels out top and bottom:\n\nNumerator: e² E² / (2 m ω)\nDenominator: - ω\n\nMultiply numerator and divide by denominator:\n\n(e² E²)/(2 m ω) * (1/-ω) = - e² E² / (2 m ω²)\n\nBut wait, energy cannot be negative? Or does it make physical sense? Actually, the second-order energy shift can be negative or positive based on the interaction. Here, the result suggests a decrease in energy, but intuitively, placing a dipole in an electric field lowers the energy when aligned, but higher when anti-aligned—but in this case, being a free electron in HO, the shift depends on the square of E, so it's positive definite? Wait, my calculation gave a negative coefficient...\n\nHold on, perhaps made a mistake in handling denominators. Let me re-examine.\n\nIn the denominator, it was E₀^{(0)} - Eₙ^{(0)}, which for n=1 is (1/2 ħω) - (3/2 ħω) = - ħω. So putting absolute values aside, the denominator is negative.\n\nHowever, looking again, the general formula for second-order perturbation is:\n\nΣ [ |⟨n|H'|0⟩|² / (E_0^{(0)} - E_n^{(0)}) ]\n\nNote that in standard notation, the denominator is (E_0^{(0)} - E_n^{(0)} ), which for n > 0 is negative because E_n > E_0.\n\nBut in reality, the correct formula uses (E_n^{(0)} - E_0^{(0)} ). Wait, no—I need to check the actual definition.\n\nNo, sorry, confusion arises. From textbooks, the second-order energy shift is:\n\nΔE₂ = Σ_{n≠0} [|⟨n|H'|0⟩|² / (E_0^{(0)} - E_n^{(0)})]\n\nYes, that seems correct. So in our case, E_0 < E_n, so each term is negative.\n\nBut physically, the energy shift due to the electric field shouldn’t necessarily be negative. It depends on the nature of the perturbation.\n\nWait, but in our setup, the perturbation is H’ = -e E x. Is that correct? Perhaps I got the sign"}
{"id": "quantum/1-1007", "questions": "﻿A particle of mass $m$ is subjected to a force $\\mathbf{F}(\\mathbf{r}) = - VV(\\mathbf{r})$ such that the wave function $\\varphi(\\mathbf{p}, t)$ satisfies the momentum-space Schrödinger equation\n\n$$\n(p^2 / 2m - a \\nabla_p^2) \\varphi(\\mathbf{p}, t) = i \\partial \\varphi(\\mathbf{p}, t) / \\partial t,\n$$\n\nwhere $\\hbar = 1$, $a$ is some real constant and\n\n$$\n\\nabla_p^2 = \\partial^2 / \\partial p_x^2 + \\partial^2 / \\partial p_y^2 + \\partial^2 / \\partial p_z^2.\n$$\n\nFind the force $\\mathbf{F}(\\mathbf{r})$.\n", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem where there's a particle with mass m subject to a certain force F(r), which is given as negative gradient of something called VV(r). Hmm, wait, maybe it was supposed to be the potential? Maybe they meant F(r) = -∇V(r)? That makes more sense because forces are typically related to gradients of potentials.\n\nAnyway, moving on. The main thing here is the Schrödinger equation provided in momentum space:\n\n\\[\n\\left( \\frac{\\mathbf{p}^2}{2m} - a \\nabla_{\\mathbf{p}}^2 \\right)\\varphi(\\mathbf{p}, t) = i \\frac{\\partial \\varphi(\\mathbf{p}, t)}{\\partial t}\n\\]\n\nThey mention that ħ=1, which simplifies things since we don't have to worry about factors involving Planck's constant. Also, 'a' is just some real constant, not sure what its significance is yet.\n\nSo my goal is to find the force F(r). Since F(r) relates to the potential via F = -∇V, if I can figure out V(r), then taking its negative gradient will give me F(r).\n\nBut how do I get from the given Schrödinger equation back to the potential?\n\nI remember that in quantum mechanics, the Hamiltonian operator determines the dynamics through the Schrödinger equation. In position space, the Hamiltonian for a free particle (no potential) would be H = p²/(2m). But when you add a potential V(r), the Hamiltonian becomes H = p²/(2m) + V(r). However, in this case, the equation seems different—it has another term: minus a times the Laplacian in momentum space.\n\nWait, let me think again. Normally, the time-dependent Schrödinger equation in position space is:\n\ni ∂ψ/∂t = [H] ψ,\n\nand similarly in momentum space, but usually, operators transform differently under Fourier transforms.\n\nIn momentum space, the kinetic energy part remains similar—since momentum eigenstates diagonalize the momentum operator—but any potential terms become multiplication operators or other differential operators depending on their form.\n\nHmm, perhaps the presence of the second derivative term (-a ∇_p² φ) suggests that the potential isn't simply local in position space. Because normally, in momentum space, a multiplicative potential like V(r) would translate into convolution operations, unless... Wait, no—if V(r) depends only on r, then in momentum space, it might involve derivatives due to the Fourier transform properties.\n\nAlternatively, could this extra term represent a kind of effective \"momentum diffusion\" or something else entirely?\n\nLet me recall that in configuration space, the standard Schrödinger equation involves the Laplace operator (∇²) acting on the wavefunction. Here, instead, in momentum space, we're seeing a Laplacian in momentum variables. So perhaps these two spaces are connected via Fourier transformations.\n\nGiven that, maybe the additional term comes from transforming a spatially dependent potential into momentum space.\n\nTo clarify, suppose in position space, our Hamiltonian includes both the kinetic term and a potential term. Then, upon Fourier transformation to momentum space, each term gets transformed accordingly.\n\nThe kinetic term, being proportional to p², stays straightforward. What happens to the potential term V(r)?\n\nIf V(r) is a scalar multiple of δ³(r), meaning zero except at origin, then in momentum space, it'd still be a simple multiplication factor. But most potentials aren't delta functions; they depend smoothly on r.\n\nFourier transforming V(r) gives us the potential in momentum space, which often introduces terms involving integrals over momenta. For example, if V(r) is a Coulomb potential, its Fourier transform leads to terms inversely proportional to |k| squared.\n\nHowever, in this problem, the potential doesn't seem to result in a simple integral but rather adds a differential operator in momentum space. Specifically, the term -a ∇_p² φ implies that after Fourier transformation, the potential contributes a term proportional to the Laplacian in momentum coordinates.\n\nThis must mean that the original potential V(r) had a specific dependence leading to such behavior upon Fourier transform.\n\nWhat operation in position space corresponds to differentiation in momentum space? Remembering that differentiation in one domain corresponds to multiplication by polynomials in the reciprocal variable in the other domain.\n\nSpecifically, multiplying by x^n in position space translates to applying d^n/dp^n in momentum space, up to constants. Similarly, differentiation in position space corresponds to multiplication by ip in momentum space.\n\nTherefore, if in momentum space, we have a term like ∇_p² φ, that should correspond to having multiplied by -(x)^2 in position space before Fourier transforming. Or actually, considering the exact correspondence between differentiation and multiplication...\n\nWait, hold on. Let me write down the Fourier transform relations clearly.\n\nFor a function f(x), its Fourier transform is defined as:\n\nφ(k) = ∫ e^{-ik·x} f(x) dx\n\nThen, the first derivative df/dx corresponds to ik φ(k),\n\nthe second derivative d²f/dx² corresponds to -k² φ(k)\n\nSimilarly, higher-order derivatives bring down powers of k.\n\nConversely, if in momentum space, we have a term like k² φ(k), that would come from integrating against x² f(x):\n\n∫ e^{-ik·x} x² f(x) dx ~ ?\n\nYes, indeed, the expectation value ⟨x²⟩ is obtained by integrating x² |f(x)|², but in terms of Fourier components, x² acts as a multiplier in position space, translating to bringing down -d²/dk² in momentum space.\n\nThus, if in momentum space, we have a term like ∇_p² φ(p,t), that arises from a term proportional to x² in position space.\n\nPutting this together, the term -a ∇_p² φ in the Schrödinger equation likely originates from a potential V(r) proportional to r² in position space.\n\nBecause, starting from position space, if V(r) were quadratic, say V(r)=b r², then in momentum space, this would contribute a term proportional to ∇_p² φ.\n\nIndeed, let's test this idea.\n\nSuppose V(r) = b r². Then, in position space, the potential term in the Hamiltonian is V(r) ψ(r). To express this in momentum space, we take the Fourier transform:\n\n⟨p'|V(r)|ψ⟩ = ∫ dr e^{ip'r} V(r) ψ(r)\n= b ∫ dr e^{ip'r} r² ψ(r)\n\nNow, using the fact that r² in position space corresponds to -∇_p² in momentum space, we can rewrite this as:\n\nb (-∇_p'^2) φ(p', t)\n\nWhere φ(p') is the Fourier transform of ψ(r). Therefore, the potential term in momentum space becomes -b ∇_p² φ(p'), matching exactly the structure of the given equation.\n\nComparing this to the given equation:\n\nLeft-hand side:\n[ p²/(2m) - a ∇_p² ] φ = ...\n\nWhich means that the coefficient of ∇_p² is '-a'. From above, we saw that the potential contribution brings in '-b ∇_p²'.\n\nSetting them equal, we see that -a = -b ⇒ a = b.\n\nTherefore, the potential V(r) must satisfy V(r) = a r².\n\nSince the force is the negative gradient of the potential, F(r) = -∇V(r).\n\nCalculating the gradient of V(r) = a r²:\n\nFirst, note that r² = x² + y² + z², so ∇V = 2a r̂ * r, right?\n\nWait, compute component-wise:\n\nd/dr (r²) = 2r, so grad(V) = 2a r hat * magnitude.\n\nExpressed vectorially, ∇V(r) = 2a r, where r is the radial unit vector scaled by the radius.\n\nWait, actually, yes. If V(r) = a r², then:\n\n∇V = (dV/dx, dV/dy, dV/dz) = (2a x, 2a y, 2a z) = 2a (x,y,z) = 2a r_vec.\n\nHere, r_vec is the position vector (x,y,z). Thus, the gradient is 2a times the position vector.\n\nTherefore, the force F(r) is -∇V(r) ="}
{"id": "quantum/3-3030", "questions": "Given two angular momenta $\\mathbf{J}_1$ and $\\mathbf{J}_2$ (for example, $L$ and $S$) and the corresponding wave functions, where $j_1 = 1$ and $j_2 = 1/2$. Compute the Clebsch-Gordan coefficients for the states with $J = \\mathbf{J}_1+ \\mathbf{J}_2$ , $m = m_1 + m_2,$ where:\n\n(a) $j = 3/2, m = 3/2,$  \n(b) $j = 3/2, m = 1/2.$\n\nConsider the reactions\n\n$$\n\\begin{align*}\nK-p & \\to \\Sigma^- \\pi^+, \\\\\n  & \\to \\Sigma^+ \\pi^-, \\\\\n  & \\to \\Sigma^0 \\pi^0, \\\\\nK-n & \\to \\Sigma^- \\pi^0, \\\\\n  & \\to \\Sigma^0 \\pi^-.\n\\end{align*}\n$$\n\nAssume they proceed through a resonance and hence a pure $\\text I$-spin state. Find the relative rates based on $\\text I$-spin conservation:\n\n(c) for an $\\text I = 1$ resonance state,  \n\n(d) for an $\\text I = 0$ resonance state.\n\nUse the Clebsch-Gordan coefficients supplied. The $\\text I$-spins for $K, n, \\Sigma,$ and $\\pi$ are $1/2, 1/2, 1,$ and $1$ respectively.\n", "graphs": null, "llm_answers": "<think>\nAlright, so I have this problem about Clebsch-Gordan coefficients and then some particle reaction stuff involving I-spin. Let me try to tackle each part one by one. \n\nStarting with parts (a) and (b), which ask for the Clebsch-Gordan coefficients for specific J and M values when combining j₁=1 and j₂=1/2. Okay, I remember that Clebsch-Gordan coefficients come into play when you're adding two angular momenta together. So if we have two particles with spin j₁ and j₂, their total spin can range from |j₁ - j₂| up to j₁ + j₂ in integer steps. In this case, since j₁ is 1 and j₂ is ½, the possible total spins J would be 3/2 and 1/2. That makes sense because 1 + 1/2 is 3/2, and 1 - 1/2 is 1/2.\n\nSo for part (a): J = 3/2, m = 3/2. Hmm, okay, what's the general approach here? From my notes, the highest weight state should just be the product of the individual highest states. Since j₁=1 has m₁=1 as its maximum, and j₂=1/2 has m₂=1/2 as its maximum. Therefore, the combination m = m₁ + m₂ = 1 + 1/2 = 3/2 must correspond directly to the state |J=3/2, M=3/2>. And according to the properties of CG coefficients, the coefficient for this state should be 1 because it's the simplest case without any mixing. Wait, but let me think again—does normalization matter here?\n\nI recall that all CG coefficients satisfy certain orthogonality conditions, meaning that the sum over squares equals 1 or something like that. But in the case of the highest state, there’s only one term contributing, right? Because both m₁ and m₂ are at their maxima, so no other combinations will give m_total higher than 3/2. Hence, yes, the coefficient should indeed be 1. So maybe part (a)'s answer is simply 1.\n\nMoving on to part (b): J=3/2, m=1/2. This seems trickier. Now, how do we get m=1/2 from m₁ and m₂? Remembering that m ranges from -J to +J in integer steps. For J=3/2, m can be 3/2, 1/2, -1/2, -3/2. We need the expansion of |3/2, 1/2> in terms of |j₁,m₁>|j₂,m₂>.\n\nThe formula for CG coefficients involves recursion relations or explicit expressions depending on whether you’re dealing with addition of angular momentum. Alternatively, sometimes ladder operators help, but perhaps using known formulas might be faster.\n\nWait, another thought: There's a standard table for CG coefficients for small j values. Maybe I can look them up mentally. When coupling l=1 (which is j₁=1) and s=1/2 (so j₂=1/2). Then, the combined states for J=3/2 and J=1/2.\n\nFor J=3/2, the states are built starting from the highest m value down. As above, the first state is straightforward. To find the next ones, we use lowering operations. Or alternatively, express each state as linear combinations.\n\nLet me write out the possibilities for m=1/2. It could result from either m₁=1 and m₂=-1/2, or m₁=0 and m₂=1/2. These are the two ways to add to m=1/2.\n\nTherefore, the state |3/2, 1/2> should be a combination of these two basis states. Specifically,\n\n|3/2, 1/2> = sqrt(2/3)|1,1;1/2,-1/2> + sqrt(1/3)|1,0;1/2,1/2>\n\nIs that correct? Wait, why those coefficients? Let me verify.\n\nIn general, the CG coefficient <j₁,j₂;m₁,m₂|J,M> gives the amplitude for the state |J,M> being expressed in terms of |j₁,m₁>|j₂,m₂>. So for our case, J=3/2, M=1/2.\n\nFrom memory, the CG coefficients for adding j=1 and j'=1/2 go as follows:\n\nWhen building J=3/2:\n- The top state is |3/2,3/2> = |1,1>|1/2,1/2>, coefficient 1.\n- Next, |3/2,1/2> is given by sqrt(2/3)|1,1>-sqrt(1/3)|1,0>.\nBut wait, actually, more precisely, considering the different m components.\n\nAlternatively, looking back, the way to compute it is via the recursive relation or using the formula:\n\n< j₁ m₁ j₂ m₂ | J M > = [something]\n\nBut honestly, recalling exact formulas is getting fuzzy. Another method is to consider the action of the raising operator L₊ on the state |J,M⟩. If we apply L₊ once to |3/2,1/2⟩, we should get proportional to |3/2,3/2⟩.\n\nCompute L₊ |3/2,1/2⟩ = ħ * sqrt{(J(J+1)-M(M+1))} |3/2,3/2⟩\n\nWhich simplifies to sqrt{(3/2)(5/2)-(1/2)(3/2)} times |3/2,3/2⟩.\n\nCalculating inside the square root: (3/2)(5/2)=15/4, minus (1/2)(3/2)=3/4, giving 12/4=3. Thus, sqrt(3).\n\nHence, L₊ |3/2,1/2⟩ = sqrt(3)ħ |3/2,3/2⟩.\n\nOn the other hand, expressing |3/2,1/2⟩ as a combination of |1,m₁>|1/2,m₂>, say α|1,1>|1/2,-1/2> + β|1,0>|1/2,1/2>.\n\nThen applying L₊ to this:\n\nα L₊(|1,1>)⊗|1/2,-1/2> + β L₊(|1,0>)⊗|1/2,1/2>\n\nNow, L₊ acting on |1,1> is zero, since it's already the highest state. Similarly, L₊ acting on |1,0> is sqrt(1*(1+1)-0*(0+1))}|1,1> = sqrt(2)ħ |1,1>.\n\nSimilarly, L₊ acting on |1/2,-1/2> is sqrt((1/2)(1/2 +1) - (-1/2)((-1/2)+1)) ) |1/2,1/2> ?\n\nWait, hold on, better yet, L₊ increases m by 1. So L₊|1/2,-1/2> = sqrt[(1/2)(1/2 +1) - (-1/2)(-1/2 +1)] |1/2,1/2>\n\nCalculate under the square root:\n\n(1/2)(3/2) - (-1/2)(1/2) = (3/4) - (-1/4) = 3/4 + 1/4 = 1. So sqrt(1)=1.\n\nThus, L₊|1/2,-"}
{"id": "quantum/3-3008", "questions": "One can show that the raising and lowering operators for angular momentum, $J_{\\pm} = J_x \\pm iJ_y$, commute with $\\mathbf{J}^2$, and that, if $j, m$ are the eigenvalues of $J, J_z$, then\n\n$$\nJ_{\\pm} |j, m\\rangle = \\hbar \\sqrt{j(j+1) - m(m \\pm 1)} \\ |j, m \\pm 1\\rangle,\n$$\n\nfor appropriately chosen phase conventions of the state vectors. Use these properties to express those states $|j, m\\rangle$ for which $m = l - 1/2$ in terms of the states $|l, m_l; s, m_s\\rangle$ with $s = 1/2$.\n", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about expressing certain angular momentum states using ladder operators. Let me try to unpack it.\n\nFirst, the question is asking about states where \\( m = l - \\frac{1}{2} \\). These should be expressed in terms of product states \\( |l, m_l; s, m_s\\rangle \\), specifically when \\( s = \\frac{1}{2} \\).\n\nI remember from my quantum mechanics class that when you combine two angular momenta (like orbital angular momentum \\( L \\) and spin \\( S \\)), their total angular momentum \\( J \\) has eigenstates denoted as \\( |j, m_j\\rangle \\). The possible values of \\( j \\) range from \\( |l - s| \\) to \\( l + s \\) in integer steps. Since here \\( s = \\frac{1}{2} \\), the total \\( j \\) would either be \\( l + \\frac{1}{2} \\) or \\( l - \\frac{1}{2} \\).\n\nThe given condition is \\( m = l - \\frac{1}{2} \\). Wait, but what's \\( m \\)? In the context of combined angular momenta, \\( m \\) refers to the magnetic quantum number corresponding to the total angular momentum \\( J \\). So we're looking for a specific state where the projection along some axis is exactly \\( l - \\frac{1}{2} \\).\n\nLet me think: since each individual component contributes to the total \\( m \\), maybe I need to consider how adding \\( m_l \\) and \\( m_s \\) gives \\( m_j = m_l + m_s \\). For \\( s = \\frac{1}{2} \\), \\( m_s \\) can be \\( +\\frac{1}{2} \\) or \\( -\\frac{1}{2} \\).\n\nSo, let's denote our basis states as \\( |l, m_l; \\frac{1}{2}, m_s\\rangle \\). To get \\( m_j = l - \\frac{1}{2} \\), we must satisfy:\n\n\\( m_l + m_s = l - \\frac{1}{2} \\)\n\nGiven that \\( m_l \\leq l \\) because it ranges from \\( -l \\) to \\( l \\), and similarly \\( m_s \\) is either \\( +\\frac{1}{2} \\) or \\( -\\frac{1}{2} \\).\n\nIf \\( m_s = +\\frac{1}{2} \\), then \\( m_l = l - \\frac{1}{2} - \\frac{1}{2} = l - 1 \\).\nBut wait, does \\( m_l \\) take half-integer values? No, because \\( l \\) is an integer (since it's orbital angular momentum), so \\( m_l \\) will also be integers ranging from \\( -l \\) to \\( l \\). Therefore, \\( m_l = l - 1 \\) makes sense only if \\( l - 1 \\) is within the allowed range for \\( m_l \\).\n\nSimilarly, if \\( m_s = -\\frac{1}{2} \\), then \\( m_l = l - (-\\frac{1}{2}) = l + \\frac{1}{2} \\). But again, \\( m_l \\) cannot exceed \\( l \\); thus, unless \\( l + \\frac{1}{2} \\leq l \\), which isn't true except perhaps trivially?\n\nWait, no—if \\( m_s = -\\frac{1}{2} \\), then \\( m_l = l - \\frac{1}{2} - (-\\frac{1}{2}) )= l - \\frac{1}{2} + \\frac{1}{2}= l \\). That works! Because \\( m_l = l \\) is certainly allowed—it’s just the maximum value.\n\nTherefore, there are two possibilities:\n1. When \\( m_s = +\\frac{1}{2} \\): Then \\( m_l = l - 1 \\)\n2. When \\( m_s = -\\frac{1}{2} \\): Then \\( m_l = l \\ )\n\nThus, the state \\( |j, m\\rangle \\) with \\( m = l - \\frac{1}{2} \\) could involve both contributions from \\( m_l = l \\) and \\( m_s = -\\frac{1}{2} \\), and \\( m_l = l - 1 \\) and \\( m_s = +\\frac{1}{2} \\).\n\nNow, considering the addition of angular momenta, especially when combining \\( l \\) and \\( s = \\frac{1}{2} \\), the resulting states can form either symmetric or antisymmetric combinations depending on whether they belong to the higher (\\( j = l + \\frac{1}{2} \\)) or lower (\\( j = l - \\frac{1}{2} \\)) multiplet.\n\nSince \\( m = l - \\frac{1}{2} \\) suggests that \\( j \\geq m \\), meaning \\( j \\) must be at least \\( l - \\frac{1}{2} \\). However, actually, \\( j \\) can be either \\( l + \\frac{1}{2} \\) or \\( l - \\frac{1}{2} \\). \n\nTo figure out which one corresponds to \\( m = l - \\frac{1}{2} \\), note that the highest weight state for \\( j = l + \\frac{1}{2} \\) is \\( m = l + \\frac{1}{2} \\), while the lowest weight state is \\( m = -(l + \\frac{1}{2}) \\). Similarly, for \\( j = l - \\frac{1}{2} \\), the highest weight is \\( m = l - \\frac{1}{2} \\), and the lowest is \\( m = -(l - \\frac{1}{2}) \\).\n\nHence, the state \\( |j, m\\rangle \\) with \\( m = l - \\frac{1}{2} \\) belongs to the \\( j = l - \\frac{1}{2} \\) multiplet. This means it's part of the lower set of states formed by coupling \\( l \\) and \\( s = \\frac{1}{2} \\).\n\nIn such cases, the Clebsch-Gordan coefficients come into play. They tell us how to decompose the tensor product of two angular momentum states into the coupled basis. Specifically, the state \\( |j, m\\rangle \\) can be written as a linear combination of the uncoupled states \\( |l, m_l; s, m_s\\rangle \\).\n\nFor \\( j = l - \\frac{1}{2} \\) and \\( m = l - \\frac{1}{2} \\), the decomposition involves the states where \\( m_l = l \\) and \\( m_s = -\\frac{1}{2} \\), plus another term involving \\( m_l = l - 1 \\) and \\( m_s = +\\frac{1}{2} \\).\n\nFrom memory, the general formula for the Clebsch-Gordan coefficient for this case is something like:\n\n\\[ \n|j=l-\\frac{1}{2}, m=l-\\frac{1}{2}\\rangle = A |l, l; \\frac{1}{2}, -\\frac{1}{2}\\rangle + B |l, l-1; \\frac{1}{2}, +\\frac{1}{2}\\rangle \n\\]\n\nWhere \\( A \\) and \\( B \\) are normalization constants determined by orthogonality conditions.\n\nAlternatively, sometimes people use different notations or orderings, so I might want to double-check the exact expression.\n\nAnother approach is to recall that the ladder operator method allows constructing these states. Given that \\( J_+ \\) raises \\( m \\) by 1, starting from the lowest state \\( |j=-l+\\frac{1}{2}, m=-l+\\frac{1}{2}\\rangle \\), applying \\( J_- \\) repeatedly gets us up to \\( m = l - \\frac{1}{2} \\).\n\nHowever, perhaps more straightforward is remembering standard CG coefficients. From tables or textbooks, the expansion for \\( |j=j', m=m'\\rangle \\) when coupling \\( l \\) and \\( s=\\frac{1}{2} \\):\n\nSpecifically, for \\( j' = l - \\frac{1}{2} \\) and \\( m'= l - \\frac{1}{2} \\), the state is constructed as follows:\n\nIt combines the maximal \\( m_l = l \\) with minimal \\( m_s"}
{"id": "quantum/1-1062", "questions": "For electronic states in a one-dimensional system, a simple model Hamiltonian is\n\n$$\nH = \\sum_{n=1}^{N} E_0 | n \\rangle \\langle n | + \\sum_{n=1}^{N} W \\{ | n \\rangle \\langle n + 1 | + | n + 1 \\rangle \\langle n | \\},\n$$\n\nwhere $|n\\rangle$ are an orthonormal basis, $(n | n) = \\delta_{nn'}$; $E_0$ and $W$ are parameters. Assume periodic boundary conditions so that $|N + j\\rangle = |j\\rangle$. Calculate the energy levels and wave functions.", "graphs": null, "llm_answers": "<think>\nOkay, I need to calculate the energy levels and wave functions for this given Hamiltonian of a one-dimensional system with periodic boundary conditions. Let me start by understanding what's going on here.\n\nThe Hamiltonian is written as:\n\n$$\nH = \\sum_{n=1}^{N} E_0 | n \\rangle \\langle n | + \\sum_{n=1}^{N} W \\left( | n \\rangle \\langle n + 1 | + | n + 1 \\rangle \\langle n | \\right)\n$$\n\nSo it has two parts: the first part is diagonal terms where each state \\( |n\\rangle \\) has energy \\( E_0 \\). The second part connects neighboring sites with hopping amplitude \\( W \\), which suggests some kind of nearest-neighbor interaction. Since we have periodic boundary conditions (\\(| N + j \\rangle = |j \\rangle\\)), this forms a ring structure—so the chain wraps around, making it effectively a circular lattice.\n\nThis setup reminds me of tight-binding models used in solid-state physics, particularly for electrons moving in a crystal lattice under certain approximations. In such cases, the eigenstates can often be expressed using plane waves because of translational symmetry.\n\nLet me think about how to approach solving this. For systems like these, especially when they're translationally invariant (which they are due to periodic boundaries), Fourier analysis is usually helpful. So maybe I should look into expressing the Hamiltonian in momentum space instead of position space.\n\nFirst, let's write down the matrix form of H if possible. But since it's a large sum over all n from 1 to N, writing out the entire matrix might not be straightforward unless I consider small N. Alternatively, perhaps considering the action of H on a general state will help.\n\nWait, another thought: If I use Bloch's theorem, then any eigenstate of H must also be a Bloch function. That means the wavefunction can be written as \\( |\\psi_k\\rangle = e^{ikx_n}\\phi(k)|u_k\\rangle \\), but more formally, in discrete systems, it would involve exponential factors related to the site index.\n\nBut actually, in a finite system with periodic boundary conditions, the allowed momenta k are quantized. Specifically, for a system of size L (with spacing between sites set to 1 unit), the wavenumbers k take values \\( k = \\frac{2\\pi m}{L} \\), where m is integer. However, in our case, there are N sites arranged periodically, so similar logic applies.\n\nAlternatively, thinking in terms of creation and annihilation operators could simplify things, but since the problem uses position basis vectors, maybe sticking with those is better initially.\n\nAnother idea: Because the Hamiltonian only couples adjacent sites, its matrix representation in the computational basis (the |n> states) is tridiagonal except for the wrap-around term due to periodicity, which makes it circulant. Circulant matrices have known eigenvectors—the discrete Fourier transform (DFT) basis—and their eigenvalues can be computed via the DFT of the first row.\n\nYes! This seems promising. A circulant matrix is one where each row vector is rotated one element to the right relative to the preceding row vector. Such matrices are diagonalized by the Fourier matrix, meaning their eigenvectors correspond to complex exponentials, i.e., plane waves.\n\nGiven that, the strategy becomes clear:\n1. Recognize that H is a circulant matrix.\n2. Use properties of circulant matrices to find eigenvalues and eigenvectors without explicitly diagonalizing the full matrix.\n\nTo elaborate, for a circulant matrix C whose first row is [c₀, c₁, ..., c_{N−1}], the eigenvalues λ_j are given by:\n\nλ_j = Σ_{k=0}^{N−1} c_k * ω^{-jk}\n\nWhere ω = e^(2πi/N) is the primitive Nth root of unity, and j ranges from 0 to N−1.\n\nIn our case, looking back at the Hamiltonian, each diagonal element is E₀, and each off-diagonal element connecting n and n+1 is W. Due to periodic boundary conditions, the last element connects N to 1, hence forming a circulant structure.\n\nTherefore, constructing the first row of the Hamiltonian matrix would give us the coefficients needed for computing the eigenvalues.\n\nSpecifically, the first row corresponds to transitions involving |1⟩. From the expression of H,\n\n⟨1|H|1⟩ = E₀,\n⟨1|H|2⟩ = W,\nand ⟨1|H|N⟩ = W (because of periodic BCs).\n\nAll other elements in the first row are zero. Therefore, the first row is [E₀, W, 0, ..., 0, W].\n\nThus, applying the formula for circulant eigenvalues:\n\nλ_j = E₀ + W*(ω^(-j*1) + ω^(-j*(N))) \n\nBecause the non-zero entries beyond the diagonal are at positions corresponding to shifts ±1 modulo N. Wait, hold on—if the first row is [E₀, W, 0,...,0,W], does that mean the connections are both left and right? Hmm, no—in the standard definition, the shift indices go either forward or backward. Maybe my initial identification isn't precise enough.\n\nActually, wait, in the Hamiltonian, each term is symmetric—it includes both |n><n+1| and |n+1><n|. Hence, the coupling is bidirectional, leading to real-valued off-diagonal elements. Thus, the matrix is Hermitian, ensuring real eigenvalues—a good consistency check.\n\nNow, focusing again on the circulant nature. Each entry in the first row after E₀ is W at position 2 (for transition to next site) and W at position N (transition to previous site, wrapping around). So indeed, the first row is [E₀, W, 0, ..., 0, W].\n\nThen, according to the circulant eigenvalue formula, each eigenvalue λ_j is equal to the sum over the first row multiplied by powers of ω^{-j}. More precisely,\n\nλ_j = Σ_{m=0}^{N−1} c_m * ω^{-jm},\n\nwhere c_m is the m-th element of the first row.\n\nHere, c_0 = E₀, c_1 = W, c_{N−1}=W, others are zero.\n\nTherefore,\n\nλ_j = E₀ + W * ω^{-j*1} + W * ω^{-j*(N−1)}.\n\nSimplify exponents:\n\nNote that ω^{-j(N−1)} = ω^{-jN}*ω^{j} = (e^{2πi})^{-j} * ω^{j} }= 1^{-j} * ω^{j} = ω^{j}, since e^{2πi} = 1.\n\nSimilarly, ω^{-j} remains as is.\n\nThus,\n\nλ_j = E₀ + W (ω^{-j} + ω^{j})\n\nRecall Euler's formula: ω^{±j} = e^{∓2πij/N}, so adding them gives twice the cosine:\n\nω^{-j} + ω^{j} = 2 cos(2πj / N)\n\nTherefore, substituting back,\n\nλ_j = E₀ + 2 W cos(2πj / N )\n\nThat's neat! So the eigenvalues depend sinusoidally on j, which labels different modes.\n\nSince j runs from 0 to N−1, each value of j gives a distinct energy level.\n\nAs for the eigenvectors, for circulant matrices, they are simply the columns of the Fourier matrix. Meaning, each eigenvector is proportional to exp(i k x_n ), where k is determined by j.\n\nMore specifically, the normalized eigenvectors are:\n\n|ψ_j⟩ = (1/√N) Σ_{n=1}^{N} e^{i k_j n} |n⟩,\n\nwhere k_j = 2πj / N.\n\nThese are just the plane-wave solutions, which make sense due to the translational symmetry enforced by periodic boundary conditions.\n\nPutting it together, the energy spectrum consists of energies spaced according to the cosine function above, and the wavefunctions are delocalized plane waves.\n\nJust to recapitulate steps:\n\n1. Identified the Hamiltonian as a circulant matrix due to periodic boundary conditions and uniform couplings.\n2. Recalled that circulant matrices' eigenvalues can be found via the Discrete Fourier Transform of their first rows.\n3. Computed the eigenvalues by evaluating the sum over the first row weighted by roots of unity.\n4. Simplified using trigonometric identities to express the result in terms of cosine.\n5. Concluded that the eigenstates are plane waves with specific wavevectors"}
{"id": "quantum/6053", "questions": "Consider the situation which arises when a negative muon is captured by an aluminum atom (atomic number $ Z = 13 $). After the muon gets inside the electron cloud it forms a hydrogen-like muonic atom with the aluminum nucleus. The mass of the muon is $105.7 \\, \\text {MeV}$.\n\n(a) Compute the wavelength (in Å) of the photon emitted when this muonic atom decays from the $3d$ state. (Slide rule accuracy; neglect nuclear motion) \n\n(b) Compute the mean life of the above muonic atom in the $3d$ state, taking into account the fact that the mean life of a hydrogen atom in the 3d state is $1.6 \\times 10^{-8}$ sec.", "graphs": null, "llm_answers": "<think>\nOkay, so I have these two parts to solve about a muonic atom formed by capturing a negative muon by an aluminum atom. Let me start with part (a).\n\nFirst, for part (a), we need to compute the wavelength of the photon emitted when the muonic atom decays from the 3d state. Hmm, okay. So since it's a hydrogen-like atom, maybe I can use some formulas similar to those used for hydrogen atoms but adjusted because the particle involved isn't an electron—it’s a muon.\n\nI remember that in hydrogen-like atoms, the energy levels depend on the atomic number Z, the principal quantum number n, and the reduced mass of the system. Since the muon has a much larger mass than an electron—wait, actually, no—the muon is heavier than an electron? Wait, let me think: electrons have a rest mass around 0.511 MeV/c², right? And here they say the muon's mass is 105.7 MeV, so yeah, that makes sense—a muon is like 200 times more massive than an electron. Okay, so the muon will behave similarly to an electron but with higher mass.\n\nSo, the formula for the energy levels should be something like:\n\nE_n = - (μ * e^4)/(8ε₀² h²) * (Z / n)^2\n\nBut wait, another version might involve Rydberg constant or something else... Maybe using the Bohr model approach?\n\nWait, yes, the general expression for the energy levels in a hydrogen-like atom considering reduced mass μ is given by:\n\nE_n = -(R_H * hc)(Z/n)^2,\n\nwhere R_H is the Rydberg constant for hydrogen. But if the mass changes, then the effective Rydberg would change as well.\n\nAlternatively, perhaps E_n = (-m_e e^4)/(8 ε_0² h²) * (Z/n)^2 multiplied by some factor due to the different mass.\n\nActually, the correct formula accounting for the reduced mass μ instead of m_e is:\n\nE_n = - (μ * e^4)/(8 ε_0² h²) * (Z/n)^2\n\nWhere μ is the reduced mass of the muon-nucleus system. However, since the nucleus is very heavy compared to the muon, the reduced mass μ ≈ m_muon/(1 + m_muon/M), where M is the mass of the aluminum nucleus. Given that M is approximately 27 u (since aluminum has atomic weight ~27), and each u is about 931.5 MeV/c², so M ≈ 27*931.5 ≈ 25,150 MeV/c². Comparing to the muon mass of 105.7 MeV/c², the denominator becomes roughly 1 + 105.7/25150 ≈ 1.0042, so μ ≈ m_muon / 1.0042 ≈ 105.7 / 1.0042 ≈ 105.3 MeV/c². That seems negligible difference, so probably we can approximate μ ≈ m_muon without too much error. Or does the problem tell us to neglect nuclear motion? It says \"neglect nuclear motion,\" meaning treat the nucleus as fixed, i.e., infinite mass approximation. Therefore, μ ≈ m_muon.\n\nTherefore, the energy levels become proportional to m_muon/m_e squared relative to hydrogen. Because normally, the energy scales as m_e, so replacing m_e with m_muon gives scaling factors accordingly.\n\nLet me recall that the Rydberg constant R is given by α² m_e c/(2h), where α is the fine-structure constant (~1/137). If we replace m_e with m_muon, the new Rydberg constant R' would be R*(m_muon/m_e). Thus, all energies scale up by m_muon/m_e.\n\nGiven that, the transition between states emits a photon whose energy corresponds to the difference between the initial and final energy levels.\n\nIn our case, the decay is from 3d to lower state—I assume it goes down to ground state unless specified otherwise. Wait, hold on—isn’t 3d just one specific state? In multi-electron terms, d orbitals correspond to l=2, but in hydrogen-like atoms, the energy depends only on n, not l. So whether it's s, p, d doesn't matter for the energy level. Hence, transitioning from n=3 to n=something else.\n\nWait, but what exactly is happening? Is the question saying that the muon transitions from the 3d state to a lower state, emitting a photon? Then, depending on how many steps it takes—if it drops directly to the lowest possible state, which is n=1—but sometimes there could be multiple steps, but usually, such problems consider direct transitions.\n\nAssuming it's dropping from n_i=3 to n_f=1, but wait, 3d implies n=3, l=2. For emission, typically you go from higher n to lower n. Alternatively, maybe it's going from 3p or 3s? No, the notation is 3d, so n=3, l=2.\n\nThus, assuming it's moving from n=3 to n=1, but let me check the wording again—\"decays from the 3d state.\" Decay suggests spontaneous emission, which generally means any allowed transition, possibly via selection rules Δl ±1, etc. But regardless, the main point is computing the energy difference leading to the photon's wavelength.\n\nHowever, since the exact transition isn't specified beyond being from 3d, perhaps it refers to the first excited state or whatever. Wait, but in hydrogen-like systems, the energy only depends on n, so even though it's called 3d, its energy is same as 3p or 3s. So, the transition from n=3 to n=1 would give off a certain photon, while other transitions may result in longer wavelengths.\n\nBut the problem statement specifically mentions \"the\" photon emitted when decaying from 3d. Perhaps implying a particular transition, likely the most probable one, which is often the transition to the nearest lower orbital following selection rules. Selection rules allow Δl = ±1, so from l=2 (d), you can drop to l=1 (p) or l=3 (f)—but f is higher, so it must drop to p. Similarly, n decreases by 1 or stays same? Well, for electric dipole transitions, Δn can vary, but commonly, the strongest lines come from Δn=±1, especially in hydrogen-like atoms.\n\nHence, starting from n=3, l=2, the muon can emit a photon and move to either n=2, l=1 (if Δn=-1, Δl=-1) or stay in n=3, l=1 (Δl=-1, Δn=0). Which is more probable? Typically, transitions changing both n and l are more probable, so n=3→n=2, l=2→l=1.\n\nBut wait, in reality, the probability also relates to the matrix elements, but perhaps for simplicity, the problem expects us to take the simplest assumption—that the transition is from n=3 to n=1, giving maximum energy photon. Alternatively, maybe it's supposed to calculate the ionization energy, but that wouldn't make sense because 3d is below the ionization threshold.\n\nHmm, now I'm confused. Let me read the question again carefully: \"(a) Compute the wavelength (in Å) of the photon emitted when this muonic atom decays from the 3d state.\"\n\nIt says 'when it decays', so presumably, it's losing energy, hence moving to a lower state. Without knowing specifics, perhaps the standard thing is to assume it moves to the next available lower state, which would be n=2, but according to selection rules, delta l can be +/-1, so from l=2 (d) to l=1 (p) or l=3 (which is impossible since n=3 cannot have l=3). So, it can go to l=1, which requires n >=2. So, the transition is from n=3, l=2 to n=2, l=1.\n\nIs that the usual way? Not entirely sure, but alternatively, maybe it's assumed to fall all the way to the ground state, n=1. Either way, the key is figuring out the energy difference.\n\nSince the problem asks for slide-rule accuracy, precise calculation isn't necessary—they want order"}
{"id": "quantum/5080", "questions": "Tritium (hydrogen of mass 3 i.e. $^3H$) is beta-radioactive and decays into a helium nucleus of mass 3 ($^3He$) with the emission of an electron and a neutrino. Assume that the electron, originally bound to the tritium atom, was in its ground state and remains associated with the $^3He$ nucleus resulting from the decay, forming a $^3He^+$ ion.\n\n(a) Calculate the probability that the $^3He^+$ ion is found in its 1s state.  \n\n(b) What is the probability that it is found in a 2p state?  ", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about tritium decay and probabilities related to the resulting helium ion's states. Let me try to break it down step by step because I'm still getting familiar with these concepts.\n\nFirst, part (a): It asks for the probability that the ^3He+ ion is found in its 1s state after the decay. Hmm, okay. So tritium (^3H) undergoes beta decay, emitting an electron and a neutrino, turning into helium-3 (^3He). But since the original electron was bound to the tritium atom and stays with the helium nucleus, we get a singly charged helium ion, which is He+. \n\nNow, when atoms or ions emit particles like electrons during radioactive decay, they can transition between energy levels. In this case, the question mentions that the electron was initially in the ground state (which would be the lowest energy level, n=1), but now it might not necessarily stay there after the decay. The process involves some sort of transition where the electron could either remain in the same orbital or move to another one due to the change in nuclear charge.\n\nWait, let me think: Before decay, the hydrogen-like system has Z = 1 (since tritium is just heavy hydrogen). After decay, the nucleus becomes helium-3, so Z increases to 2. That means the potential well the electron experiences changes—it’s stronger because the nucleus has more protons. This should affect the possible orbitals the electron can occupy.\n\nBut how does this relate to the probability of being in a particular state? Maybe it's something to do with wavefunction overlap or adiabatic transitions?\n\nI remember that when you suddenly change parameters in a quantum system, the wavefunction doesn't instantly adjust; instead, it continues as if under the old conditions until new eigenstates develop. Since the decay happens quickly compared to atomic timescales, perhaps the electron's wavefunction right after decay isn’t immediately adjusted to match the new Hamiltonian. Instead, it will be expressed as a linear combination of the new eigenfunctions.\n\nSo, before decay, the electron is in the ground state of hydrogenic tritium, which is similar to hydrogen except heavier. Its wavefunction ψ_initial corresponds to the 1s orbital of H. After decay, the nucleus is He+, so the appropriate orbitals become those of He+. Therefore, the initial wavefunction must expand in terms of the new basis functions—so each term in the expansion represents the amplitude (and hence squared magnitude gives probability) of finding the ion in a specific state post-decay.\n\nTherefore, the probability P_n that the ion ends up in state n is given by |⟨ψ_final|ψ_initial⟩|^2, summed over all relevant states. Wait no—if the initial state is expanded in terms of the final states, then each coefficient c_n is ⟨n|ψ_initial⟩, and the probability is |c_n|^2.\n\nYes, exactly! So mathematically,\n\nP(n) = |⟨φ_{nlm}^{(Z=2)} | φ_{100}^{(Z=1)} ⟩ |²\n\nwhere φ denotes the hydrogen-like wavefunctions with principal quantum number n, angular momentum l, magnetic m, and effective nuclear charge Z.\n\nSince both systems here are spherically symmetric, the matrix element simplifies because only certain selection rules apply. For radial integrals involving different Z values, maybe orthogonality comes into play?\n\nLet me recall the form of hydrogenic wavefunctions:\n\nThe normalized wavefunction for hydrogen-like atoms is given by:\n\nφ_{n l}(r) = R_{n l}(r) Y_l^m(θ, φ)\n\nWhere R_{n l}(r) depends on r and includes factors dependent on Z through the Bohr radius scaling. Specifically, the radial function is proportional to e^{-ρ}, where ρ = Z r / (n a_0).\n\nGiven that, our initial state is the 1s orbital of Z=1, so:\n\nR_{10}(r) ∝ e^{-r/a_0}\n\nAnd the final states are for Z=2, so their radial parts involve e^{-2r/(na_0)}. Thus, the inner product integral becomes:\n\n∫₀^∞ [R_{n' l'}(r)]* R_{10}(r) dr × ∫ spherical harmonics... Well, since the initial state is l=0, any final state with l≠0 won't contribute because the spherical harmonic integral would vanish. Hence, only l'=0 contributes.\n\nThus, the probability reduces to summing over n', considering only l'=0 states, and computing the square modulus of the radial integral multiplied by the angular integral (which is zero unless l=l').\n\nSo specifically, for each n':\n\nP(n') = | ∫₀^∞ R_{n' 0}*(r) R_{10}(r) dr |²\n\nBecause the angular part integrates to δ_{l l'}, which is non-zero only when l=0.\n\nAlright, so first compute the radial integral for each n'. Then take the absolute value squared to find the probability.\n\nWhat's the explicit expression for R_{n l}? From what I remember, the radial wavefunction for hydrogen-like atoms is:\n\nR_{n l}(r) = N_{n l} * (ρ)^{l + 1 - n/2} L_{n-l-1}^{2l+1}(ρ),\n\nwhere ρ = 2 Z r / (n a_0), N_{n l} is normalization constant, and L are Laguerre polynomials.\n\nAlternatively, sometimes written differently depending on convention, but key point is dependence on Z and n.\n\nFor our purposes, let's write out R_{10}(r):\n\nAt Z=1, n=1, l=0:\n\nR_{10}(r) = sqrt(1/(π a_0³)) ) e^{-r/a_0}\n\nSimilarly, for Z=2, general n, l=0:\n\nR_{n0}(r) = sqrt((2^n)/(n² π a_0³)) ) (e^{-2r/(n a_0)}) )\n\nIs that correct? Wait, actually, the exact expressions may vary slightly based on definitions, but roughly yes—the exponential factor scales inversely with n and directly with Z.\n\nTo make progress, perhaps express everything dimensionless. Let x = r / a_0. Then, rewrite the wavefunctions accordingly.\n\nOriginal R_{10}(x) = sqrt(1/(π )) e^{-x}\n\nFinal R_{n0}(x) = sqrt( (2^(5)/n³ π ) ) ? Wait, hold on—I need precise forms.\n\nActually, standard formula for hydrogenic radial wavefunctions is:\n\nR_{n l}(r) = (sqrt( (Z/n)^3 ) * sqrt( (Γ(n+l)! ) / Γ(n−l)! ) ) * (2 Z r / n)^l e^{- Z r / n } L_{n − l − 1}^{2 l + 1}(2 Z r / n )\n\nHmm, seems complicated. Alternatively, using the simplified version without worrying too much about constants:\n\nPerhaps better to use substitution variables.\n\nDefine u = 2r/(n a_0) for the final state, v = r/a_0 for the initial state.\n\nThen, expressing R_{n0}(r) and R_{10}(r) in terms of u and v respectively.\n\nBut wait, integrating them together requires matching variable substitutions.\n\nMaybe set t = r / a_0, so r = a_0 t.\n\nExpressing R_{10}(t) = C₁ e^{-t}, where C₁ is normalization constant.\n\nFor R_{n0}(t), substituting r = a_0 t into R_{n0}(r):\n\nWe had earlier that ρ = 2 Z r / (n a_0 ), so plugging Z=2, r=a_0 t:\n\nρ = 4 t / n\n\nThus, R_{n0}(t) = D_n (something)*(rho)^{-k} e^{- rho }, etc., but probably messy.\n\nAlternatively, note that the integral is:\n\nIntegral from 0 to infinity of R_{n0}(r) R_{10}(r) dr\n\nWhich translates to:\n\nIntegral from 0 to inf of [N_{n0} (some power of r) e^{-2r/(n a_0)} ] * [N_{10} e^{-r/a_0}] dr\n\nMultiply the exponentials: e^{- (2/(n a_0) + 1/a_0) r }\n\nFactor out 1/a_0: e^{- [ (2/n + 1) / a_0 ] r }\n\nThis suggests that the integral converges provided that the exponents don't cause divergence, which they shouldn't because both are positive.\n\n"}
{"id": "quantum/5074", "questions": "For a particle of mass $\\text m $ in a one-dimensional box of length 1, the eigenfunctions and energies are\n\n$$\n  \\psi_n(x) = \\sqrt{\\frac{2}{l}} \\sin \\frac{n\\pi x}{l}, \\quad 0 \\leq x \\leq l,\n$$\n\n$$\n  E_n = \\frac{1}{2m} \\left(\\frac{n\\pi\\hbar}{l}\\right)^2, \\quad n = \\pm1, \\pm2, \\ldots\n$$\n\nSuppose the particle is originally in a state $ |n\\rangle $ and the box length is increased to a length of $ 2l $ $ (0 \\leq x \\leq 2l) $ in a time $ t \\ll \\frac{h}{E_n} $. Afterwards what is the probability that the particle will be found in an energy eigenstate with energy $ E_n $?", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about quantum mechanics where a particle is initially in a specific state inside a one-dimensional box, and then suddenly the size of the box changes very quickly. The question asks for the probability that after expanding the box from length \\( l \\) to \\( 2l \\), the particle remains in the same energy level \\( E_n \\). Hmm.\n\nFirst, let me recall some basics. In a one-dimensional infinite potential well or \"particle in a box\" scenario, the wave functions are sinusoidal, given by:\n\n\\[ \n\\psi_n(x) = \\sqrt{\\frac{2}{l}} \\sin\\left( \\frac{n\\pi x}{l} \\right)\n\\]\n\nand the corresponding energy levels are:\n\n\\[\nE_n = \\frac{(n\\pi\\hbar)^2}{2ml^2}\n\\]\n\nwhere \\( n \\) is a positive integer (\\( n=1,2,3,\\dots \\)).\n\nNow, the setup here is that the particle starts off in state \\(|n\\rangle\\) when the box has length \\( l \\). Then, the box's length is instantaneously doubled to \\( 2l \\). Since the change happens over a time much shorter than any relevant timescale related to the system's dynamics—specifically, \\( t \\ll h/E_n \\)—this suggests we can treat it as an instantaneous perturbation without considering time evolution during the expansion. So, essentially, the wave function doesn't get a chance to evolve dynamically; instead, it just gets projected onto the new basis states of the expanded box.\n\nSo, my task is to find the probability that immediately after the expansion, the particle is still in an energy eigenstate with energy \\( E_n \\).\n\nWait, but hold on: After the box expands, the energy levels themselves change because they depend inversely on the square of the box length. Specifically, if the original energy was \\( E_n^{old} = \\frac{(n\\pi\\hbar)^2}{2ml^2} \\), now the new energy levels would be:\n\n\\[\nE_m^{new} = \\frac{(m\\pi\\hbar)^2}{2m(2l)^2} = \\frac{(m\\pi\\hbar)^2}{8ml^2} = \\frac{E_m^{old}}{4}\n\\]\n\nBut wait, no—that’s not quite right. Let me correct myself. Actually, each energy level scales with the inverse square of the box length. If you double the length, the new energy becomes \\( E_m' = E_m / 4 \\). But since the index \\( m \\) refers to different nodes in the sine function, which depends on the boundary conditions.\n\nHowever, our initial state before expansion is \\( \\psi_n(x) \\) defined on [0,l], while after expansion, all possible positions go up to 2l. Therefore, the wavefunction must adjust accordingly.\n\nSince the process is sudden, the wavefunction does not change instantaneously—it only readjusts its form based on the new boundaries. That means the spatial part of the wavefunction remains the same shape within the old region [0,l] but extends into the new region [l, 2l].\n\nTherefore, the new wavefunction \\( \\phi(x) \\) immediately after the expansion should satisfy the Schrödinger equation for the larger box. However, due to the abruptness of the change, the wavefunction isn’t allowed to instantly adapt smoothly across the entire domain. Instead, it continues as per the previous configuration until such point as the Hamiltonian allows it to spread out.\n\nMathematically, the initial condition after the expansion is:\n\n- For \\( 0 \\leq x \\leq l \\): \\( \\phi(x) = \\psi_n(x) = \\sqrt{\\frac{2}{l}} \\sin\\left( \\frac{n\\pi x}{l} \\right) \\)\n\n- For \\( l < x \\leq 2l \\): What happens? Well, depending on how the extension occurs, there might be two possibilities: either the wavefunction is zero beyond \\( l \\) (which wouldn't make sense physically unless something constraining it exists, which it doesn't—the box is extended), or perhaps it mirrors or repeats somehow?\n\nAlternatively, another approach is needed. Maybe think of the wavefunction as being expressed in terms of the new eigenstates of the expanded box.\n\nYes, exactly! Because the expansion causes the Hilbert space to effectively double in size, meaning more eigenstates become available. To find the probability of remaining in the same energy level, we need to express the post-expansion wavefunction as a linear combination of the new eigenstates and compute the overlap with the desired state.\n\nLet me formalize this idea. Before expansion, the wavefunction is \\( \\psi_n(x) \\) confined between 0 and l. After expansion, the total interval is 0 to 2l, so the new eigenfunctions are:\n\n\\[\n\\varphi_m(x) = \\sqrt{\\frac{1}{l}} \\sin\\left( \\frac{m\\pi x}{2l} \\right)\n\\]\n\nfor \\( m = 1, 2, 3, ... \\), since the wavelength adjusts according to the longer box.\n\nThe key insight is that the wavefunction immediately after expansion is still \\( \\psi_n(x) \\) for \\( 0 \\leq x \\leq l \\), but undefined—or rather, needs to be determined—for \\( l < x \\leq 2l \\). Wait, actually, upon reflection, maybe the wavefunction simply vanishes outside the original box? No, that seems incorrect because the particle could exist anywhere in the new box once it's expanded.\n\nHmm, confusion arises here. Perhaps better phrased: When the walls expand, the particle feels free to move throughout the entire new volume. Thus, the wavefunction cannot abruptly terminate at \\( x=l \\); instead, it must continue into the second half of the box. How?\n\nOne way to model this is assuming minimal disturbance—if the expansion is done adiabatically, the wavefunction would remain approximately the same, but adjusted proportionally. But since the expansion is sudden, the wavefunction doesn't have time to react, hence it maintains its form in the first half and presumably reflects or propagates into the second half.\n\nActually, thinking again, in reality, the wavefunction must obey the continuity of both itself and its derivative at \\(x = l\\), except possibly at points where the potential is discontinuous—infinite potentials cause immediate reflections, but finite expansions don't introduce such barriers. Hence, the wavefunction may extend into the second half with certain symmetry properties.\n\nWait, perhaps even simpler: Upon doubling the box, the wavefunction is forced to fit into twice the length. This resembles changing variables in the coordinate system. Alternatively, consider extending the original wavefunction symmetrically or periodically into the new region.\n\nAnother thought: Expressing the original wavefunction as a sum of the new eigenfunctions. Since the new eigenfunctions span the entire space, including the original box, their superposition can reconstruct the original wavefunction.\n\nThus, mathematically, the initial wavefunction \\( \\psi_n(x) \\) for \\( 0 \\leq x \\leq l \\) can be written as a Fourier series using the new eigenfunctions \\( \\varphi_m(x) \\):\n\n\\[\n\\psi_n(x) = \\sum_{m=1}^\\infty c_m \\varphi_m(x)\n\\]\n\nWhere the coefficients \\( c_m \\) represent the projection of \\( \\psi_n(x) \\) onto each \\( \\varphi_m(x) \\). These coefficients determine the probabilities of finding the particle in each new energy eigenstate.\n\nSpecifically, the coefficient \\( c_m \\) is calculated via inner product:\n\n\\[\nc_m = \\int_0^{2l} \\varphi_m^*(x) \\psi_n(x) dx\n\\]\n\nBut note that \\( \\psi_n(x) \\) is non-zero only in \\( [0, l] \\), so the integral simplifies to:\n\n\\[\nc_m = \\int_0^{l} \\varphi_m^*(x) \\psi_n(x) dx + \\int_l^{2l} \\varphi_m^*(x) \\cdot 0 \\, dx = \\int_0^{l} \\varphi_m^*(x) \\psi_n(x) dx\n\\]\n\nSubstituting the expressions,\n\n\\[\nc_m = \\int_0^{l} \\sqrt{\\frac{1}{l}} \\sin\\left( \\frac{m\\"}
{"id": "quantum/3-3001", "questions": "Consider four Hermitian $2 \\times 2$ matrices $I, \\sigma_1, \\sigma_2,$ and $\\sigma_3,$ where $I$ is the unit matrix, and the others satisfy $\\sigma_i \\sigma_j + \\sigma_j \\sigma_i = 2\\delta_{ij}$.\n\nYou must prove the following without using a specific representation or form for the matrices.\n\n(a) Prove that $Tr(\\sigma_i) = 0$.  \n(b) Show that the eigenvalues of $\\sigma_i$ are $\\pm 1$ and that $\\det(\\sigma_i) = -1$.  \n(c) Show that the four matrices are linearly independent and therefore that any $2 \\times 2$ matrix can be expanded in terms of them.  \n(d) From (c) we know that\n\n$$\nM = m_0 I + \\sum_{i=1}^{3} m_i \\sigma_i,\n$$\n\nwhere $M$ is any $2 \\times 2$ matrix. Derive an expression for $m_i$ $(i = 0, 1, 2, 3)$.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about Pauli matrices and some properties they need to prove. Let me try to work through each part one by one. \n\nStarting with part (a): Prove that Tr(σ_i) = 0. Hmm, okay. So σ_i here refers to the Pauli matrices, right? They're given as Hermitian 2x2 matrices satisfying {σ_i, σ_j} = 2δ_ij. That's their anticommutation relation. Also, I is the identity matrix. Since all these matrices are Hermitian, their trace should be real because the trace of a Hermitian matrix is always real. But how do I show it's zero?\n\nWell, maybe if I consider squaring σ_i. If I compute σ_i squared, what happens? Using the anticommutation relation: when i=j, then σ_i^2 + σ_i^2 = 2I, which simplifies to 2σ_i² = 2I, so σ_i² = I. Therefore, each σ_i squares to the identity matrix. Interesting.\n\nNow, since σ_i is Hermitian, its eigenvalues must be real. And from σ_i² = I, the possible eigenvalues λ must satisfy λ² = 1, meaning λ = ±1. Wait, but does that help directly with the trace? The trace is the sum of eigenvalues. For a 2x2 matrix, there are two eigenvalues. Suppose both are 1; then the trace would be 2. Both being -1 gives trace -2. One 1 and one -1 gives trace 0. Which case applies here?\n\nBut wait, let's think again. Each σ_i has determinant... Well, det(σ_i)^2 = det(σ_i²) = det(I). Determinant of I is 1, so |det(σ_i)| = 1. Hence, det(σ_i) could be either 1 or -1. However, earlier thoughts on eigenvalues might lead us somewhere else.\n\nWait another approach: take the trace of both sides of σ_i² = I. Then Tr(σ_i²) = Tr(I). Now, Tr(I) for 2x2 is 2. On the other hand, Tr(σ_i²) is equal to the sum of squares of eigenvalues. As before, eigenvalues are ±1, so say λ₁ and λ₂. Thus, Tr(σ_i²) = λ₁² + λ₂² = 1 + 1 = 2. Okay, consistent. Doesn't give new info yet.\n\nAlternatively, perhaps considering the fact that σ_i are traceless. How? Maybe via commutation relations or something else. Alternatively, note that for any square matrix A, Tr(A) equals the sum of diagonal elements. But unless we use more structure...\n\nWait, actually, thinking back to the definition of Pauli matrices, even though the question says not to assume a specific representation, but knowing that they are traceless is standard. Is there a way to derive it abstractly?\n\nLet’s see. Consider taking the trace of σ_i multiplied by itself. We already saw that Tr(σ_i²)=2. What if we multiply σ_i by σ_j and take traces? When i≠j, {σ_i, σ_j}=0, so σ_i σ_j = -σ_j σ_i. Taking trace on both sides: Tr(σ_i σ_j) = Tr(-σ_j σ_i) => Tr(σ_i σ_j) = -Tr(σ_i σ_j), implying Tr(σ_i σ_j)=0 for i ≠ j.\n\nSo now, suppose M is expressed as a combination of I and σ_i. Like, M = aI + b·σ. Then, computing Tr(M) would involve Tr(aI) plus sums involving Tr(σ_i). But maybe that's getting ahead of myself.\n\nAnother thought: Use cyclic property of trace. Or perhaps look into the algebra generated by these matrices. Since they anticommute, etc., but maybe too vague.\n\nWait, going back to the original equation: σ_i² = I. Take trace: Tr(σ_i²) = Tr(I) ⇒ Tr(σ_i²) = 2. But also, Tr(σ_i²) = Σλ_k², where λ_k are eigenvalues. Since each λ_k is ±1, regardless of whether they’re same or different, their squares add up to 2. So no information about individual eigenvalues—could still be [1,1], [-1,-1], or [1,-1].\n\nHowever, looking at the next parts, especially part (b), which asks to show that eigenvalues are ±1 and determinant is -1. So probably, the trace ends up being zero because the eigenvalues cancel out. Because if you had two eigenvalues adding to zero, that implies Tr(σ_i)=0.\n\nTherefore, combining that knowledge, yes, Tr(σ_i)=0. So maybe just state that since σ_i²=I, eigenvalues are ±1, and since the trace is the sum of eigenvalues, and the only possibility for a 2x2 matrix with eigenvalues ±1 such that their product is determinant (-1, as per part (b)), hence the trace must be zero.\n\nThus, for part (a), Tr(σ_i)=0.\n\nMoving on to part (b): Eigenvalues of σ_i are ±1, and det(σ_i)=-1.\n\nWe've established that σ_i²=I, so eigenvalues must satisfy λ²=1, thus λ=±1. So eigenvalues are indeed ±1.\n\nFor determinant, recall that determinant is the product of eigenvalues. In 2x2 case, if eigenvalues are 1 and -1, then determinant is -1*1 = -1. Similarly, if both were 1, determinant would be 1, but that contradicts our previous conclusion. Wait, hold on—if both eigenvalues are 1, determinant is 1, but according to part (b), determinant is supposed to be -1. So why is that?\n\nBecause in reality, the Pauli matrices have determinant -1. To confirm, let's reason:\n\nFrom σ_i²=I, characteristic polynomial is x² - tr(σ_i)x + det(σ_i)=0. But since Tr(σ_i)=0, the quadratic becomes x² + det(σ_i)=0. Roots are sqrt(-det(σ_i)) and -sqrt(-det(σ_i)). Given roots are ±1, set sqrt(-det(σ_i))=1, so -det(σ_i)=1 → det(σ_i)=-1. Perfect! So yeah, determinant is -1.\n\nHence, done with part (b).\n\nPart (c): Show that the four matrices I, σ₁, σ₂, σ₃ are linearly independent, hence any 2×2 matrix can be written as a linear combo of them.\n\nTo check linear independence, suppose αI + β₁σ₁ + β₂σ₂ + β₃σ₃ = 0. Need to show coefficients α,βs must all be zero.\n\nAssume such a combination equals zero matrix. Multiply both sides by I, σ₁, σ₂, σ₃ appropriately and take trace to extract coefficients.\n\nRecall that Tr(I)=2, Tr(σ_i)=0, and Tr(σ_i σ_j)=0 for i≠j, while Tr(σ_i²)=2.\n\nCompute Tr(αI + ... ) = α * Tr(I) + 0 + 0 + 0 = 2α = 0 ⇒ α=0.\n\nThen, subtracting off αI, get β₁σ₁ + β₂σ₂ + β₃σ₃ = 0.\n\nTake trace after multiplying both sides by σ_k: Tr[(β₁σ₁ + β₂σ₂ + β₃σ₃)σ_k] = 0.\n\nWhich is β₁ Tr(σ₁σ_k) + β₂ Tr(σ₂σ_k)+ β₃ Tr(σ₃σ_k) = 0.\n\nSince Tr(σ_i σ_j)= δ_ij * 2 /something? Wait, previously, we found that Tr(σ_i σ_j)=0 when i≠j, and Tr(σ_i²)=2.\n\nYes, exactly. So for k fixed,\n\n"}
{"id": "quantum/5073", "questions": "An electron is in the $ n = 1 $ eigenstate of a one-dimensional infinite square-well potential which extends from $ x = -a/2 $ to $ x = a/2 $. At $ t = 0 $ a uniform electric field $ E $ is applied in the $ x $ direction. It is left on for a time $ \\tau $ and then removed. Use time-dependent perturbation theory to calculate the probabilities $ P_2 $ and $ P_3 $ that the electron will be, respectively, in the $ n = 2 $ and $ n = 3 $ eigenstates at $ t > \\tau $. Assume that $ \\tau $ is short in the sense that $ \\tau \\ll \\frac{\\hbar}{E_1 - E_2} $, where $ E_n $ is the energy of the eigenstate $ n $. Specify any requirements on the parameters of the problem necessary for the validity of the approximations made in the application of time-dependent perturbation theory.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this quantum mechanics problem here about an electron in a one-dimensional infinite square well with a suddenly applied electric field. Hmm, let me try to unpack what's going on.\n\nFirst off, the setup: The electron is initially in the ground state (n=1) of an infinite square well extending from x=-a/2 to x=a/2. Then, at time t=0, they apply a uniform electric field E along the x-direction. This field stays on until time τ, after which it's turned off. We need to find the probabilities P₂ and P₃ that the electron ends up in states n=2 and n=3, respectively, using time-dependent perturbation theory. Also, we're told that τ is much smaller than ℏ/(E₁ - E₂), meaning some approximation might hold because the duration isn't too long compared to certain timescales.\n\nAlright, so since the system starts in an eigenstate and undergoes a sudden change due to the external perturbation—the electric field—we can model this as a time-dependent perturbation. Time-dependent perturbation theory seems appropriate here because the Hamiltonian changes abruptly at t=0 when the field is applied.\n\nLet me recall how time-dependent perturbation works. In general, if you have a small time-varying perturbation H'(t), you can express the probability amplitudes c_k(t) for transitioning into other states |k> via:\n\nc_k^(1)(t) ≈ (-i/ħ) ∫₀^τ ⟨k|H’(t)|i⟩ e^{iω_{ki} t} dt,\n\nwhere ω_{ki}=(E_k-E_i)/ℏ, assuming first-order perturbation theory. Since τ is short, maybe higher orders aren’t significant? Or perhaps even just considering leading terms?\n\nWait, but before jumping into formulas, let me think about the specifics of the problem. So our initial state is n=1, and we want transitions to n=2 and n=3. Therefore, k would be 2 or 3, and i is 1.\n\nThe key thing is figuring out the form of the perturbing Hamiltonian H'. When you apply an electric field E in the x-direction, the perturbation should correspond to the interaction between the charge of the electron and the electric field. That usually translates to adding a term like -eEx to the Hamiltonian, right? Because the potential energy due to an electric field is V(x) = qφ(x). For a uniform electric field E pointing in +x, φ(x) = -E x (since E = -∇φ; integrating gives φ(x)= -E x + constant).\n\nBut wait, actually, more precisely, the perturbing Hamiltonian H' is given by the dipole interaction: H' = -q E x, where q is the charge of the particle—in this case, the electron has charge -e, so H' becomes H' = e E x. Wait no—wait, double-checking: Electric potential energy U = q φ. If the electric field is E, then φ(x) = -∫ E dx = -E x (assuming zero at origin?), so U = q*(-E x) = -e E x. Hence, yes, H' = -e E x. But sometimes people take H' as positive depending on conventions... Maybe better to write H' = F·r, where F is force—but not sure. Alternatively, another approach is to consider that applying an electric field adds a linear potential term to the original Hamiltonian.\n\nSo regardless, the main point is that H' is proportional to position operator x multiplied by constants involving E and charges. Let me note down:\n\nH'(x,t) = e E x * θ(τ - t),\n\nbut only active during [0, τ], otherwise zero. However, in standard TDPT, the perturbation is often considered as being present throughout all time, but switched on/off smoothly. Here, though, it’s abrupt—it turns on at t=0 and off at t=τ. So technically, H’ is non-zero only in [0, τ].\n\nAlternatively, thinking in terms of switching functions, H’ could include a Heaviside function stepping on at t=0 and off at t=τ. Either way, the integral over time goes from 0 to τ.\n\nNow, moving forward, the matrix element ⟨k|x|i⟩ comes into play. Specifically, each transition amplitude involves computing integrals of these matrix elements multiplied by exponential factors.\n\nGiven that, let me outline steps:\n\n1. Write down the unperturbed energies En.\n2. Express the perturbing Hamiltonian H'.\n3. Compute the Fourier components of H', specifically the matrix elements ⟨k|H'|i⟩.\n4. Apply Fermi's golden rule or use time-dependent PT formula to compute ck^(1).\n5. Square the modulus to get probabilities.\n\nStarting with step 1: Unperturbed energies En for infinite square wells.\n\nInfinite square well from x=-a/2 to x=a/2. The energy levels are known to be En = (n² π² ħ²)/(2mL²), where L is width. Here, L = a, so En = (n² π² ħ²)/(2 m a²). Okay, got that.\n\nStep 2: Perturbing Hamiltonian H'.\n\nAs discussed earlier, H' = -e E x. So, H’ depends linearly on x.\n\nTherefore, the matrix element ⟨k|H’|i⟩ is equal to -e E ⟨k|x|i⟩.\n\nThus, the crucial part is calculating ⟨k|x|i⟩, especially for k=2 and k=3, starting from i=1.\n\nI remember that in symmetric potentials, such as the infinite square well centered around x=0, the expectation values ⟨n|x|m⟩ depend on whether n+m is odd or even. More accurately, for real wavefunctions, parity considerations come into play.\n\nSince the infinite square well is symmetric about x=0, the wavefunctions have definite parity: ψn(-x) = (-1)^{n+1}ψn(x). Thus, ψ1 is symmetric (even function?) No, wait: Actually, for n=1, ψ1(x) ~ sin(kπx/a), shifted appropriately. Given the interval [-a/2,a/2], probably adjusted sine functions.\n\nActually, let me define them properly. The normalized wavefunctions for the infinite square well from x=-a/2 to x=a/2 are:\n\nψn(x) = sqrt(2/a) sin(n π (x + a/2)/a )\n\nSimplify inside the sine: n π (x + a/2)/a = n π x /a + n π /2\n\nHmm, okay, so ψn(x) = sqrt(2/a) sin(n π x /a + n π /2)\n\nWhich simplifies further based on n. For example, n=1: sin(π x/a + π/2 ) = cos(π x/a ). Similarly, n=2: sin(2π x/a + π ) = -sin(2π x/a ), etc.\n\nAh! Interesting. So for n odd, ψn(x) is cosine-like (symmetric), while for n even, it's sine-like (antisymmetric)? Not exactly, but their behavior under reflection x → -x alternates.\n\nSpecifically, evaluating ψn(-x):\n\nFor n=1:\nψ1(-x) = sqrt(2/a) sin(-π x/a + π/2 )\nUsing identity sin(A + B) = sin A cos B + cos A sin B,\nso sin(-π x/a + π/2 ) = sin(π/2 - π x/a ) = cos(π x/a ).\n\nSimilarly, ψ1(-x) = sqrt(2/a) cos(π x/a ) = ψ1(x)\nBecause ψ1(x) was already defined as sqrt(2/a) cos(π x/a ). So indeed, ψ1 is even.\n\nFor n=2:\nψ2(-x) = sqrt(2/a) sin(-2π x/a + π ) = sqrt(2/a)[ sin(π - 2π x/a ) ] \nAnd sin(π - y) = sin(y), so sin(π - 2π x/a ) = sin(2π x/a ).\nHowever, there's also a negative sign outside because of the argument substitution?\nWait, let me re-examine:\n\nOriginal expression: sin(-2π x"}
{"id": "quantum/3-3006", "questions": "Consider a spinless particle represented by the wave function\n\n$$\n\\psi = K(x + y + 2z) e^{-\\alpha r},\n$$\n\nwhere $r = \\sqrt{x^2 + y^2 + z^2}$, and $K$ and $\\alpha$ are real constants.\n\n(a) What is the total angular momentum of the particle? \n\n(b) What is the expectation value of the t-component of angular momentum?  \n\n(c) If the z-component of angular momentum, $L_z$, were measured, what is the probability that the result would be $L_z = +\\hbar$?  \n\n(d) What is the probability of finding the particle at $\\theta, \\phi$ and in solid angle $d\\Omega$? Here $\\theta, \\phi$ are the usual angles of spherical coordinates.\n\nYou may find the following expressions for the first few spherical harmonics useful:\n\n$$\nY_0^0 = \\sqrt{\\frac{1}{4\\pi}}, \\quad Y_1^{\\pm 1} = \\mp \\sqrt{\\frac{3}{8\\pi}} \\sin \\theta e^{\\pm i \\phi},\n$$\n\n$$\nY_1^0 = \\sqrt{\\frac{3}{4\\pi}} \\cos \\theta, \\quad Y_2^{\\pm 1} = \\mp \\sqrt{\\frac{15}{8\\pi}} \\sin \\theta \\cos \\theta e^{\\pm i \\phi}.\n$$", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about a spinless particle with a given wavefunction ψ. The question has four parts: (a) asks for the total angular momentum; (b) expects me to compute the expectation value of L_t component—wait, maybe it's supposed to be L_x or something else because \"t\" isn't standard notation... Hmm, perhaps it was meant to be L_y? Or maybe they just used a different symbol like L_t as a typo. Anyway, moving on. Part (c) wants the probability of measuring L_z equals ħ when we measure L_z. And part (d) is asking for the probability density in terms of θ, φ, and dΩ.\n\nFirst off, let's look at the wavefunction provided: ψ = K(x + y + 2z)e^{-α r}, where r is sqrt(x²+y²+z²). So, since it's expressed in Cartesian coordinates but involves x, y, z linearly multiplied by an exponential decay term which depends only on r, it suggests some kind of radial dependence along with angular components from x, y, z.\n\nI remember that angular momentum operators involve derivatives with respect to angles, specifically using spherical coordinates. Since the wavefunction seems to have both radial and angular dependencies, probably involving spherical harmonics.\n\nBut wait, actually, looking closer, the spatial part before the exponential is (x + y + 2z), which can also be written in vector form as A·r, where A is a constant vector (1,1,2). That might hint towards certain symmetries related to angular momentum.\n\nLet me recall that angular momentum eigenstates are typically products of radial functions R(r) times spherical harmonics Y_l^m(θ,φ). In our case, though, the wavefunction doesn’t seem explicitly separated into such a product—it’s more mixed together. But if I think about expanding (x + y + 2z) in terms of spherical harmonics, then perhaps I can express ψ as a combination of states with definite l and m values.\n\nSo, starting with part (a): Total Angular Momentum. For a state |l,m>, the square of the total angular momentum operator gives ℏ² l(l+1)|ψ>. Therefore, if I can figure out the possible l-values present in ψ, their corresponding squares will give the magnitude squared of the angular momentum.\n\nGiven that ψ includes terms proportional to x, y, z, each of these corresponds to specific spherical harmonic components. Let me write down how x, y, z relate to spherical harmonics.\n\nIn general:\n- x ∝ sinθ cosφ,\n- y ∝ sinθ sinφ,\n- z ∝ cosθ,\n\nand comparing to the spherical harmonics given:\n\nWe know that Y₁⁻¹ ~ sinθ e^{i(-1)φ} ≈ sinθ cosφ - i sinθ sinφ,\nSimilarly, Y₁⁺¹ ~ sinθ e^{i(+1)φ} ≈ sinθ cosφ + i sinθ sinφ,\nAnd Y₁⁰ ~ cosθ.\n\nTherefore, combining these, we can see that x relates to [Y₁⁺¹ + Y₁⁻¹], while y relates to [-i(Y₁⁺¹ - Y₁⁻¹)], and z relates directly to Y₁⁰ scaled appropriately.\n\nWait, let me get precise coefficients right. From the definitions:\n\nFrom the user-provided spherical harmonics:\n\nY₀⁰ = sqrt(1/(4π)),\n\nY₁±1 = ±√[3/(8π)] sinθ e^{∓iφ},\n\nNote the signs here—the upper sign is negative for Y₁⁺¹?\n\nWait no, hold on. Wait, according to the user message:\n\nThey wrote Y₁^{±1} = ∓ sqrt(3/(8 pi)) sin theta e^{±i phi}\n\nHmm, okay, so Y₁^1 is equal to minus sqrt(3/(8 pi)) sin(theta) e^{i phi}. Similarly, Y₁^-1 is plus sqrt(3/(8 pi)) sin(theta) e^{-i phi}.\n\nThat's interesting—they've got opposite signs compared to my initial thought.\n\nAlso, Y₁^0 is sqrt(3/(4 pi)) cos theta.\n\nAlright, now considering that, let's try expressing x, y, z in terms of these spherical harmonics.\n\nExpressing position vectors in spherical harmonics:\n\nRecall that in spherical coordinates, any cartesian coordinate can be written as combinations of spherical harmonics.\n\nSpecifically, we have relations like:\n\nx = r * sqrt[(4 π / 3)(1/2)] [Y₁^1 + Y₁^{-1}] ?\n\nWait, not exactly sure—I need to derive them properly.\n\nAlternatively, another approach: Express x, y, z in terms of spherical harmonics.\n\nSince x = r sinθ cosφ,\n\nLooking back at the expression for Y₁^1 and Y₁^{-1}:\n\nY₁^1 = - sqrt(3/(8 pi)) sin theta e^{i phi}\n= - sqrt(3/(8 pi)) sin theta (cos phi + i sin phi)\n\nSimilarly, Y₁^{-1} = sqrt(3/(8 pi)) sin theta e^{-i phi}\n= sqrt(3/(8 pi)) sin theta (cos phi - i sin phi)\n\nIf I add Y₁^1 and Y₁^{-1}:\n\nY₁^1 + Y₁^{-1} = (- sqrt(3/(8 pi)) + sqrt(3/(8 pi))) sin theta cos phi + i*(- sqrt(3/(8 pi)) - sqrt(3/(8 pi))) sin theta sin phi\n\nWait, that simplifies to zero for the cosine terms and double imaginary sine terms—but that can't be correct. Maybe instead, take appropriate linear combinations.\n\nActually, better yet, consider taking Re[Y₁^1] and Im[Y₁^1].\n\nBecause Y₁^1 contains both sin(phi) and cos(phi).\n\nWait, alternatively, note that:\n\ne^{i phi} = cos phi + i sin phi,\n\nso multiplying by sin theta:\n\nsin theta e^{i phi} = sin theta cos phi + i sin theta sin phi.\n\nThus, equating to x/r and y/r:\n\nx/r = sin theta cos phi,\n\ny/r = sin theta sin phi,\n\nz/r = cos theta.\n\nHence, we can say:\n\nx = r sin theta cos phi = r * Real[sqrt(3/(8 pi))^(-1)}*(something)]\n\nWait, perhaps scaling factors come into play.\n\nWait, let's solve for x, y, z in terms of Y₁^m.\n\nSuppose we want to represent x, y, z as linear combinations of Y₁^m.\n\nFor example, suppose:\n\nx = C [Y₁^1 + Y₁^{-1}], similarly for y and z.\n\nCompute the coefficient C.\n\nTake modulus squared:\n\n|x|^2 should correspond correctly.\n\nBut perhaps there's a known formula for converting between Cartesian and spherical harmonics.\n\nYes! There is a relation called the connection between spherical harmonics and Cartesian tensors.\n\nThe key point is that the monopole moment (l=0) is spherically symmetric, dipole moments (l=1) align with x,y,z directions.\n\nIndeed, the spherical harmonics Y₁^m are related to the direction cosines via:\n\nY₁^m = N_m (some combination of x, y, z),\n\nbut normalized differently.\n\nTo make progress, perhaps use explicit conversion formulas.\n\nAccording to standard references, the relationship is:\n\nY₁^m = k * (directional components):\n\nMore precisely, the unit vectors in Cartesian coordinates can be expressed in terms of spherical harmonics.\n\nHere's one way to do it:\n\nDefine the three basis vectors for l=1:\n\nThese are proportional to Y₁^1, Y₁^0, Y₁^{-1}.\n\nEach of these corresponds to certain directional components.\n\nBut perhaps it's easier to go through ladder operators.\n\nAnother idea: Use the fact that the angular part of the wavefunction is (x + y + 2z)/r, since the overall factor"}
{"id": "quantum/1-1006", "questions": "﻿Consider an experiment in which a beam of electrons is directed at a plate containing two slits, labelled $A$ and $B$. Beyond the plate is a screen equipped with an array of detectors which enables one to determine where the electrons hit the screen. For each of the following cases draw a rough graph of the relative number of incident electrons as a function of position along the screen and give a brief explanation. \n\n(a) Slit $A$ open, slit $B$ closed. \n(b) Slit $B$ open, slit $A$ closed. \n(c) Both slits open. \n(d) Stern-Gerlach apparatus attached to the slits in such a manner that only electrons with $s_z = \\hbar/2$ can pass through $A$ and only electrons with $s_z = -\\hbar/2$ can pass through $B$. \n(e) Only electrons with $s_z = \\hbar/2$ can pass through $A$ and only electrons with $s_z = R/2$ can pass through $B$. \n\nWhat is the effect of making the beam intensity so low that only one electron is passing through the apparatus at any time? \n", "graphs": null, "llm_answers": "<think>\nOkay, I'm trying to solve this problem about the double-slit experiment with electrons. Let me go through it step by step.\n\nFirst, part (a): Slit A is open, B is closed. So when you have just one slit open, what happens on the detector screen? From my basic understanding, if there's only one slit, the electrons should form a single peak or distribution around the center corresponding to slit A. It shouldn't be spread out like interference patterns because without both slits open, no interference occurs. The pattern would probably look similar to a diffraction pattern from a single slit but since we're dealing with electrons, maybe more like a Gaussian distribution centered behind slit A. But wait, actually for particles, even though they exhibit wave-like properties, when only one slit is open, their behavior isn't interferometric; instead, they just pass through and create a broadened spot due to uncertainty principle effects—so yeah, a single peak.\n\nMoving on to part (b): Now, slit B is open while A is closed. By symmetry, this situation should mirror case (a). Instead of a peak behind A, now all the electrons will come through B, creating another single peak shifted towards where B is located. Since the setup is symmetric except for the labels, the graphs for (a) and (b) should be identical in shape but positioned differently based on the location of the respective slits.\n\nPart (c): Both slits are open. This is the classic double-slit scenario. When both slits are open, electrons interfere with themselves, leading to an interference pattern—a series of bright and dark fringes on the screen. The central maximum is usually the brightest, then alternating maxima and minima decrease in intensity outwardly. However, since electrons are quantum particles, each individual electron doesn’t take a definite path but rather exists in a superposition until measured. Over many electrons, the probability distribution shows these interference bands. So the graph here should display multiple peaks and troughs, characteristic of constructive and destructive interference.\n\nNow, parts (d) and (e) introduce something new: a Stern-Gerlach apparatus modifying how electrons pass through the slits. In part (d), the SG apparatus ensures that only electrons with spin s_z = ħ/2 go through slit A, and those with s_z = -ħ/2 go through slit B. Hmm, okay. Spin states are being used to control which slit the electrons go through. If the spins are different for each slit, does that affect whether they interfere?\n\nWait, in standard Young’s double-slit experiments, if you measure which slit the particle goes through, you destroy the interference pattern because you collapse the wavefunction into knowing the path. Here, using spin might not directly tell us which slit the electron went through unless the measurement correlates spin state with the slit choice. But in this case, the SG device is selecting electrons based on their spin before sending them through specific slits. Does measuring spin count as determining the path? Because if you know the spin, you effectively know which slit the electron passed through—if A requires +z spin and B requires –z spin, then after detecting the spin, you'd know which slit was taken. Therefore, wouldn't this act as a \"which-way\" measurement, thereby destroying the interference pattern?\n\nBut hold on—the question says nothing about subsequent measurements beyond directing the electrons via spin selection. Maybe the key point is that once the electrons' paths are determined by their spin, they don't interfere anymore. Hence, the result would resemble having two separate single-slit distributions—one from A and one from B—but without overlapping interference. Wait, but in reality, if you send half the electrons through A and half through B, depending on their spin, do they still produce an overall interference pattern? Or does the fact that each group has a defined spin prevent them from interfering?\n\nI think in this case, since the electrons going through A and B have opposite spins, they cannot interfere with each other because their total wavefunctions include spin components. Electrons are fermions, so their overall wavefunction must be antisymmetric under exchange. Thus, if the spatial parts are symmetric (like in the same orbital), the spin parts need to be anti-symmetric, meaning singlet state. Conversely, if the spin parts are symmetric (+1/2 and +1/2?), but in our case, the spins are fixed per slit. So perhaps the electrons from A and B don't interfere because their combined wavefunction includes distinct spin states, preventing coherent superposition between the two beams. That means the resulting pattern would be additive: summing up the intensities from each slit individually, hence showing two separate peaks without interference.\n\nSo for part (d), the graph would show two distinct peaks, one from each slit, adding together, thus looking like the combination of (a) and (b).\n\nThen part (e): Only electrons with s_z = ħ/2 can pass through A, and only electrons with s_z = R/2 can pass through B. Wait, R seems undefined—it could be a typo. Perhaps meant to say s_z = 0 or some other value? Alternatively, maybe it's supposed to refer back to something else. Assuming it's a typo, let's suppose it's either another spin component or possibly zero. Suppose it's s_z = 0, but electrons have spin-½, so s_z can only be ±ħ/2. Then saying s_z=R/2 may mean something else... Alternatively, maybe it refers to right-handedness or left-handedness, but that complicates things further.\n\nAlternatively, perhaps 'R' stands for something else entirely, but given context, likely a mistake. Let me assume that it's intended to specify another spin direction, maybe perpendicular, but since z-component is already specified, maybe x or y. Alternatively, perhaps it's a misprint and should read '-ħ/2', mirroring part (d). If that were the case, then part (e) becomes similar to (d)—but why change anything? Alternatively, maybe it's referring to a different observable altogether, but within the scope of the question, focusing on spin makes sense.\n\nAssuming it's a typo, and it's meant to say s_z=-ħ/2 for slit B again, then part (e) would be the same as (d). But considering the original wording, it specifies s_z=ħ/2 for A and s_z=R/2 for B. Without knowing what R is, it's hard to proceed. Alternatively, maybe R is a variable representing something else, but in the absence of information, perhaps treating it as another possible spin projection. Given that electrons have spin-½, s_z can only be ±ħ/2, so if R is positive, then s_z=R/2 implies R=±1, giving s_z=±ħ/2. So assuming R is 1, then s_z=+ħ/2 for A and s_z=+ħ/2 for B? No—that can't be, because then both slits require the same spin condition, which contradicts the idea of splitting based on spin.\n\nAlternatively, maybe R represents a different axis—for example, if R is the radius vector, but that interpretation doesn't fit well. Another thought: sometimes in problems, people use L for angular momentum, S for spin, etc., but R is unclear. Alternatively, perhaps it's a misrendered symbol, like σ_z or something else. Alternatively, maybe it's a trick question where R is unrelated, implying that the second slit allows any spin, but that also conflicts with the initial statement.\n\nGiven ambiguity, perhaps best to consider that part (e) introduces a different kind of filtering—not necessarily spin-based—which affects the ability of the electrons to interfere. Alternatively, maybe it's a red herring, and regardless of the specifics, the outcome depends on whether the electrons maintain coherence across the two slits.\n\nIf in part (d), the electrons had distinguishable paths (based on spin), causing no interference, whereas in part (e), perhaps the paths aren't distinguished, allowing interference. But without clarity on what exactly R signifies, it's challenging. Alternatively, perhaps R is a placeholder for another measurable quantity, but without info, difficult to assess.\n\nPerhaps better to treat part (e) similarly to (d), assuming that whatever property is used to direct electrons through the slits leads to distinguishability, thereby eliminating interference. So, similar reasoning applies: if the electrons going through A and B can"}
{"id": "quantum/5010", "questions": "Consider an electron in a one-dimensional box of length 1 Å.\n\n(a) Find the first 4 wave functions (normalize the wave functions) and sketch them.\n\n(b) Compute the corresponding 4 energy levels and sketch an energy level diagram.\n\n(c) At $t = 0$, the particle is known to be in the state for which $n = 1$. At $t = 0$, a rectangular potential well $V_0 = -10^4 \\text{ eV}$, centered at $a/2$ and of width $10^{-12} \\text{ cm}$, is suddenly introduced into the well and kept there for $5 \\times 10^{-18} \\text{ sec}$, at which time it is removed. After removal of the perturbation, what is the probability that the system will be found in each of the states $n = 2, n = 3,$ and $n = 4$? (The height and width of the potential well is characteristic of a neutron interacting with an electron).\n\nNote: you can use your sketch to help you estimate the relevant matrix elements.\n", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about an electron in a one-dimensional box. It's divided into three parts: finding the wavefunctions, computing the energy levels, and then dealing with a sudden perturbation. Let me try to tackle part (a) first.\n\nPart (a): Finding the first four normalized wave functions and sketching them.\n\nAlright, I remember from my quantum mechanics class that the solutions to the infinite potential well (particle in a box) are sine functions. The general form of the wavefunction ψ_n(x) should involve sin(nπx/L), where L is the length of the box. Since the box has a length of 1 Å, which is 1e-10 meters or 0.1 nanometers. \n\nFirst, normalization condition must hold. For a function like A sin(k x), the integral over the interval [0, L] squared equals 1. So let's compute the normalization constant A.\n\nSo,\n\n∫₀ᴸ |ψ_n(x)|² dx = ∫₀ᴸ A² sin²(n π x / L) dx = 1\n\nI recall that the integral of sin²(ax) dx over its period is equal to half the period times something... Wait, more precisely:\n\nIntegral of sin²(u) du from u=0 to u=mπ is m*(π)/2 if m is integer?\n\nWait no, actually integrating sin²(u):\n\nWe know that sin²(u) = (1 - cos(2u))/2, so when we integrate from 0 to Nπ, say, but here our limits are 0 to L, not necessarily multiples of anything except related through k.\n\nBut since n is an integer, maybe substitution helps.\n\nLet’s set u = n π x / L => x = (L/(n π)) u; dx = (L/(n π)) du\n\nWhen x goes from 0 to L, u goes from 0 to n π.\n\nThus, substituting into the integral:\n\nA² * Integral_{0}^{n π} [sin²(u)] * (L/(n π)) du\n\nWhich becomes:\n\n(A² L)/(n π) * Integral_{0}^{n π} sin²(u) du\n\nNow, as mentioned earlier, sin²(u) integrates to (u/2 - sin(2u)/4). Evaluated between 0 and n pi.\n\nCompute the integral:\n\n[ (n π / 2 ) - (sin(2n π)/4 ) ] - [0 - 0]\n\nSince sin(2n π)=0 because n is integer.\n\nTherefore, the integral simplifies to (n π)/2.\n\nPutting back together:\n\n(A² L)/(n π) * (n π)/2 = (A² L)/2 = 1\n\nHence, solving for A gives sqrt(2/L)\n\nSo, the normalized wavefunctions are:\n\nψ_n(x) = sqrt(2/L) sin(n π x / L )\n\nGiven that L is 1 Å, which is 1e-10 meters, but perhaps keeping it symbolic would suffice unless numerical values are needed later on.\n\nSo, writing out the first four wavefunctions:\n\nFor n=1:\nψ₁(x) = sqrt(2/L) sin(π x / L )\n\nSimilarly,\nψ₂(x) = sqrt(2/L) sin(2π x / L )\nψ₃(x) = sqrt(2/L) sin(3π x / L )\nψ₄(x) = sqrt(2/L) sin(4π x / L )\n\nEach subsequent wavefunction adds another node inside the box. Sketches would show these nodes increasing with higher n. For example, n=1 has zero nodes, just a single hump; n=2 has one node in the middle, etc.\n\nMoving on to part (b): Computing the corresponding four energy levels and sketching an energy level diagram.\n\nEnergy levels for a particle in a box are given by E_n = (n² h²)/(8m L² ), right? Where h is Planck's constant, m is mass of the electron, and L is the length of the box.\n\nPlugging in numbers might be necessary here. But wait, do they want expressions or actual computed energies?\n\nWell, probably better to write down the formula and note how the energy increases quadratically with n.\n\nExpressed symbolically:\n\nE_n = (n² h²)/(8 m_e L² )\n\nWhere m_e is the electron mass.\n\nAlternatively, sometimes people express it using ħ instead of h:\n\nE_n = (n² π² ℏ²)/(2 m_e L² )\n\nYes, both forms are equivalent because h = 2πℏ, so plugging that in:\n\nh² = 4 π² ℏ², thus:\n\nE_n = (n² * 4 π² ℏ² )/(8 m_e L² ) = (n² π² ℏ² )/(2 m_e L² )\n\nEither way is correct, depending on convention.\n\nTo get numerical value, need constants:\n\nPlanck's constant h ≈ 6.626 × 10^-34 J·s\n\nElectron mass m_e ≈ 9.109 × 10^-31 kg\n\nLength L = 1 Å = 1e-10 m\n\nCalculating E_n numerically:\n\nLet me pick units such that everything is consistent. Maybe convert all lengths to meters.\n\nCompute denominator: 8 m_e L²\n\nDenominator D = 8 * 9.109e-31 kg * (1e-10 m)^2\n\nD = 8 * 9.109e-31 * 1e-20 = 7.287e-50 kg·m²/s² ?\n\nWait, numerator is h², which is (6.626e-34)^2 ≈ 4.39e-67 J² s²\n\nHmm, seems messy. Alternatively, perhaps expressing in terms of eV.\n\nBecause often in atomic scales, energies are expressed in electron volts.\n\nRecall that 1 eV = 1.602e-19 joules.\n\nMaybe find E_n in eV.\n\nFormula again:\n\nE_n = (n² h²)/(8 m_e L² )\n\nConvert h to eVs: h ≈ 4.135667696×10^-15 eV·s\n\nThen plug in:\n\nE_n = (n² (4.135667696e-15 eV·s)^2 ) / (8 * 9.109e-31 kg * (1e-10 m)^2 )\n\nCalculate denominator:\n\n8 * 9.109e-31 kg = 7.287e-30 kg\n\nMultiply by (1e-10 m)^2 = 1e-20 m² → total denominator: 7.287e-50 kg·m²\n\nNumerator: (4.135667696e-15)^2 ≈ 1.710e-29 (eV·s)^2\n\nSo overall:\n\nE_n = (n² * 1.710e-29 ) / 7.287e-50 ≈ (n² * 2.346e+20 ) eV\n\nWait, check calculation steps carefully.\n\nWait, unit analysis may complicate things. Perhaps better to look up standard expression for energy levels in eV for a 1Å box.\n\nAlternatively, recalling that for a particle in a box, the ground state energy is approximately proportional to 1/n² and inversely proportional to square of size.\n\nIn some references, the energy levels for electrons in boxes are calculated as follows:\n\nUsing E_n = (n² π² ℏ² )/(2 m_e L² )\n\nWith ℏ ≈ 1.0545718e-34 Js\n\nPlug in:\n\nE_n = (n² * π² * (1.0545718e-34)^2 ) / (2 * 9.109e-31 * (1e-10)^2 )\n\nCompute numerator:\n\npi^2 ~ 9.8696"}
{"id": "quantum/1-1001", "questions": "﻿Quantum phenomena are often negligible in the \"macroscopic\" world. Show this numerically for the following cases:  \n(a) The amplitude of the zero-point oscillation for a pendulum of length $l = 1 \\, \\text{m}$ and mass $m = 1 \\, \\text{kg}$.  \n(b) The tunneling probability for a marble of mass $m = 5 \\, \\text{g}$ moving at a speed of $10 \\, \\text{cm/sec}$ against a rigid obstacle of height $H = 5 \\, \\text{cm}$ and width $w = 1 \\, \\text{cm}$.  \n(c) The diffraction of a tennis ball of mass $m = 0.1 \\, \\text{kg}$ moving at a speed $v = 0.5 \\, \\text{m/sec}$ by a window of size $1 \\times 1.5 \\, \\text{m}^{2}$.\n", "graphs": null, "llm_answers": "<think>\nOkay, so I have these three quantum mechanics problems to solve, each showing why quantum effects aren't noticeable in macroscopic objects. Let me tackle them one by one.\n\nStarting with part (a): Amplitude of zero-point oscillations for a pendulum. Hmm, zero-point energy is that minimum energy even at absolute zero due to Heisenberg's uncertainty principle. For a classical pendulum, it behaves like a harmonic oscillator when angles are small. So maybe we can model it as such?\n\nThe formula for the ground state energy of a quantum harmonic oscillator is E_0 = (1/2) ħω, where ω is angular frequency. But how does that relate to amplitude? Oh right, the average potential energy is also (1/2)mω²x², which equals E_0 on average because kinetic and potential energies contribute equally. \n\nSo setting (1/2)mω²⟨x²⟩ ≈ (1/2)ħω gives ⟨x²⟩ ≈ ħ/(mω). Therefore, the root mean square displacement would be sqrt(⟨x²⟩), but since they're asking for amplitude, perhaps just take the order of magnitude from there.\n\nFirst, find ω for the pendulum. Angular frequency ω = sqrt(g/l). Given l=1 m, g≈9.8 m/s², so ω ≈ sqrt(9.8 / 1) ≈ 3.13 rad/s.\n\nThen compute ħ/(mω). Planck constant over 2π, ħ ≈ 1.054e-34 J·s. Mass m=1 kg. So:\n\nAmplitude A ≈ sqrt(ħ/(mω)) ?\n\nWait no, earlier equation was ⟨x²⟩ ≈ ħ/(mω), so x ~ sqrt(ħ/(mω)). That makes sense dimensionally too—sqrt(J·s * s/kg*m^2)? Wait units might not add up directly; let me check.\n\nAlternatively, another approach: using the virial theorem or expectation values. Maybe better to use the expression for the standard deviation σ_x = sqrt(ħ/(2mω)), but then again, whether it's divided by √2 depends on conventions. Since the question asks for amplitude, probably taking the RMS value times some factor. Alternatively, sometimes people approximate the typical spread as proportional to sqrt(ħ/mω).\n\nBut regardless, plugging numbers should give something very tiny.\n\nCompute numerator: ħ ≈ 1e-34\n\nDenominator: m*omega = 1 kg * 3.13 rad/s ≈ 3.13\n\nThus, sqrt(1e-34 / 3.13) ≈ sqrt(3.2e-35) ≈ 5.6e-18 meters. Wow, super small! Like way smaller than atomic scales. Hence, negligible amplitude.\n\nMoving on to part (b): Tunneling probability for a marble hitting an obstacle. This involves particle tunneling through a barrier. The transmission coefficient T is given approximately by exp(-2∫√(V - K)/dx), where V is potential, K is kinetic energy.\n\nGiven marble mass m=5 grams = 0.005 kg, velocity v=10 cm/s=0.1 m/s. Height H=5 cm=0.05 m, width w=1 cm=0.01 m.\n\nPotential here is gravitational, so V(x)=mgH beyond the obstacle. Kinetic energy K=(1/2)mv².\n\nCalculate K first: (0.5)(0.005 kg)(0.1)^2 = 0.5 * 0.005 * 0.01 = 0.000025 Joules.\n\nPotential barrier height U= mgH = 0.005 kg * 9.8 m/s² * 0.05 m ≈ 0.005 * 9.8 * 0.05 ≈ 0.00245 J.\n\nSince K < U, classically impossible, but quantum tunneling possible. Compute exponent term.\n\nInside integral: sqrt((U - K)/(something))? Wait, actually, more precisely, the integral is over the region where the particle is evanescent, i.e., inside the barrier. In this case, assuming rectangular barrier approximation, the effective 'width' is w=0.01 m.\n\nIn WKB approximation, the transmission coefficient T ≈ exp(-2kL), where k=sqrt(2m(U-K))/ħ, L being the width.\n\nLet’s calculate k:\n\nk = sqrt[(2m(U - K)]^(½) ) / ħ ? No wait,\n\nActually, k = sqrt[2m(V - K)] / ħ? Or without dividing by ħ... Wait, correct formula is k = sqrt(2mΔE)/ħ, where ΔE = U - K.\n\nYes, so k = sqrt(2m(U - K))/ħ.\n\nPlugging in:\n\nΔE = U - K = 0.00245 - 0.000025 ≈ 0.002425 J.\n\nSo k = sqrt(2 * 0.005 kg * 0.002425 J) / ħ.\n\nCalculating numerator under sqrt: 2 * 0.005 * 0.002425 ≈ 0.00002425.\n\nTake sqrt: approx 0.004925.\n\nDivide by ħ: 0.004925 / 1.054e-34 ≈ 4.67e+31 per second? Wait, hold on, what are the dimensions here.\n\nWait, k has unit [length]^{-1}, because exponential decay is e^{-kx}. So need to make sure units work out.\n\nWait, let's re-express everything properly.\n\nEnergy terms must be converted into appropriate units.\n\nWe have:\n\nk = sqrt(2m(E_p - E_k)} / ħ\n\nWhere E_p is potential energy, E_k is kinetic.\n\nExpressed in SI units:\n\nE_p = 0.00245 J,\nE_k = 0.000025 J,\nso difference is about 0.002425 J.\n\nMass m=0.005 kg.\n\nTherefore,\n\nk = sqrt(2 * 0.005 kg * 0.002425 J) / ħ\n\nNote that J = kg·m²/s², so multiplying by kg gives kg²·m²/s², whose square root is kg·m/s. Divided by ħ, which has units J·s = kg·m²/s. Thus overall, k will have units 1/m.\n\nComputing numerator:\n\nsqrt(2 * 0.005 * 0.002425)\n\nWhich is sqrt(approximately 0.00002425) ≈ 0.004925 m^{-1}? Wait, no, sqrt(kg*(J))...\n\nHold on, confusion arises. Perhaps better to write all quantities in base units.\n\nAlternative method: Express everything in terms of known constants.\n\nAnother thought: Use natural units where c=1, ℏ=1, etc., but may complicate further.\n\nPerhaps instead, recall that the action S is related to ħ, and in tunneling, the exponent relates to the action over ħ.\n\nBut going back, let's recast k correctly.\n\nExpression for k is sqrt(2m(V - K))/ħ.\n\nBecause V - K is in joules, multiply by m (kg):\n\nUnits become kg*(joule) = kg*(N·m) = kg*(kg·m/s²)*m = kg²·m²/s².\n\nSquare root gives kg·m/s, divide by ħ (units J·s = kg·m²/s), thus resulting in 1/m. Correct.\n\nNow plug in numbers:\n\n2m(V - K) = 2 * 0.005 kg * 0.002425 J ≈ 0.00002425 kg·J.\n\nConvert kg·J to kg³·m²/s²? Not helpful. Instead,"}
{"id": "quantum/6039", "questions": "(a) State the electric-dipole selection rules for atomic states connected by emission or absorption of a photon.\n\n(b) Interpret the selection rule in terms of photon orbital angular momentum, spin, helicity and parity.\n\n(c) Make a semi-classical estimate of the lifetime of the $2p$ state of hydrogen, using the Bohr model and the classical formula\n\n$$\nP = \\frac{2}{3} \\frac{q^2}{c^3} \\dot{v}^2 \\quad (\\text{c.g.s. units})\n$$\n\nfor the power radiated by a particle of charge $q$ and acceleration $\\dot{v}$. Express your result in terms of $e, F_L, c, a$ and $w$, where $a$ is the Bohr radius and $w$ is the angular velocity in the circular orbit.\n\n(d) Using the answer from (c), what is the width of the $2p$ state in electron volts?", "graphs": null, "llm_answers": "<think>\nAlright, so I've got this problem about quantum mechanics and electromagnetism here. It's divided into four parts: (a) to (d). Let me try to tackle each part one by one. \n\nStarting with part (a): \"State the electric-dipole selection rules for atomic states connected by emission or absorption of a photon.\" Hmm, okay. From my notes, I remember that when atoms emit or absorb photons, certain transitions between energy levels are allowed based on these selection rules. The main ones have to do with changes in the principal quantum number n and the azimuthal (or angular momentum) quantum number l. Specifically, for electric dipole transitions, the change in l should be ±1 because the photon carries away one unit of angular momentum. Also, there might be something about the magnetic quantum number m changing by 0, ±1 as well. So putting it together, the selection rules would probably say Δl = ±1 and Δm = 0, ±1. But wait, does the principal quantum number also have any restrictions? I think sometimes people mention that Δn can be anything except zero, but maybe more accurately, since we're dealing with dipole transitions, which involve E1 (electric dipole) approximation, then yes, only specific changes in n are possible depending on other factors like the matrix elements. However, perhaps the primary focus is on l and m. Maybe I'll just stick with Δl=±1 and Δm=0, ±1 for now.\n\nMoving on to part (b): \"Interpret the selection rule in terms of photon orbital angular momentum, spin, helicity and parity.\" Okay, interpreting those selection rules... Well, a photon has intrinsic properties including spin-1, right? And its angular momentum comes from both orbital and spin components. In the context of electromagnetic radiation, especially plane waves, the angular momentum per photon consists of orbital angular momentum L and spin S. For a linearly polarized photon, the spin component along the direction of propagation is either +ħ or -ħ. Helicity refers to the projection of spin onto the direction of motion; for massless particles like photons, helicity equals the spin projection. Now, how does this relate to the selection rules?\n\nWhen an atom emits a photon, it must conserve angular momentum. If the transition involves a change in the orbital angular momentum quantum number l by ±1, that corresponds to the transfer of angular momentum via the photon. Since the photon itself carries away some angular momentum, specifically ħ times its own angular momentum vector. Wait, actually, the total angular momentum J of the photon includes both orbital L and spin S. But in most cases, particularly for dipolar radiation, the dominant contribution is from the orbital angular momentum. Or is it? No, hold on—photon's angular momentum is quantified such that for each photon, the magnitude squared is j(j+1)ħ², where j=1. Therefore, the z-component can be -1, 0, +1. This relates to the polarization of the photon.\n\nBut going back to the selection rules: if the atom loses angular momentum due to emitting a photon, the change in the atom's angular momentum must match the photon's angular momentum. Hence, the change in l being ±1 makes sense because the photon takes away one unit of angular momentum. Similarly, the change in m could correspond to different projections of the photon's angular momentum. As for parity, the parity of the system before and after the transition multiplied by the parity of the photon must remain consistent. Parity inversion symmetry implies that under spatial inversion, even functions stay the same while odd functions reverse sign. Hydrogen-like wavefunctions have parities (-1)^l. A photon, being a boson with integer spin, has a parity eigenvalue of -1 because it transforms as a vector potential, which is a first-rank tensor. Thus, the product of initial state parity P_initial * photon parity P_photon should equal the final state parity P_final. That gives us P_initial*(-1) = P_final => P_final = -P_initial. Which means that during a dipole transition, the parity flips. How does that tie into our earlier selection rules? Because the parity depends on l, flipping requires that l changes by an odd amount—in particular, Δl=±1—which aligns with our previous conclusion.\n\nSo summarizing part (b): The selection rules arise because the emitted photon carries away one unit of angular momentum (Δl=±1) corresponding to its own angular momentum. The change in m reflects the three possible orientations (+1, 0, -1) of the photon’s angular momentum relative to the quantization axis. Additionally, the parity of the state must flip because the photon contributes a negative parity factor, necessitating Δl to be odd, hence ±1.\n\nNow, moving on to part (c): Making a semiclassical estimate of the lifetime of the 2p state of hydrogen using the Bohr model and the given power formula. The formula provided is:\n\n\\[ P = \\frac{2}{3} \\frac{q^2}{c^3} \\dot{v}^2 \\]\n\nin c.g.s. units. We need to express the result in terms of e, F_L, c, a, and w, where a is the Bohr radius and w is the angular velocity.\n\nFirst, let's recall that in the Bohr model, electrons move in circular orbits around the nucleus. The centripetal force is provided by Coulomb attraction. So,\n\n\\[ \\frac{mv^2}{r} = \\frac{k_e e^2}{r^2} \\]\n\nwhere k_e is Coulomb constant. Solving for v, we get:\n\n\\[ v = \\sqrt{\\frac{k_e e^2 r}{m}} \\]\n\nWait, no—that seems off. Actually, solving for v:\n\nFrom \\( mv^2 / r = k_e e^2 / r^2 \\)\n\nMultiply both sides by r/m:\n\n\\( v^2 = (k_e e^2)/(m r) \\implies v = \\sqrt{(k_e e^2)/(m r)} \\).\n\nIn the Bohr model, the radius is given by \\( r_n = n^2 a_0 \\), where a_0 is the Bohr radius. Here, the state is 2p, so n=2, l=1. So, r = 4a_0.\n\nThus, substituting r = 4a_0,\n\n\\( v = sqrt( (k_e e^2 )/(m*4a_0 )) )\n\nSimplify constants: Remembering that in SI units, k_e = 1/(4πε₀), but in c.g.s., k_e = 1. So in c.g.s., simplifying further.\n\nAlternatively, considering expressions involving known quantities. Perhaps better to use the expression for angular frequency ω instead.\n\nGiven that the electron moves in a circle, the angular velocity ω is related to v by v = ω r.\n\nTherefore, v = ω r ⇒ dot(v) is dv/dt, but wait—the power formula uses dot(v)^2, which is (dv/dt)^2. What is dv/dt in uniform circular motion?\n\nHmm, in uniform circular motion, the speed |v| is constant, but the direction changes. So the derivative of velocity is the centripetal acceleration directed towards the center. Its magnitude is a_c = v²/r = ω² r.\n\nHence, |dv/dt| = ω² r.\n\nSince the acceleration is perpendicular to velocity, the square of the time derivative of velocity is (ω² r)^2.\n\nPlugging into the power equation:\n\nP = (2/3)(q²/c³)(\\dot{v})² = (2/3)(e²/c³)(ω² r)^2.\n\nSubstituting r = 4a (since a is the Bohr radius):\n\nP = (2/3)(e²/c³)(ω²*(4a))² = (2/3)(e²/c³)*(16 a² ω⁴) = (32/3)(e² a² ω⁴)/c³.\n\nOkay, so Power P is proportional to e² a² ω⁴ over c cubed.\n\nNext, knowing that the power radiated leads to loss of energy, thus determining the decay rate. The lifetime τ is approximately the energy U divided by the power P.\n\nWhat's the energy of the 2p state? In the Bohr model, the energy levels are given by:\n\nE_n = -(R_H) * hc / n²,\n\n"}
{"id": "quantum/1-1008", "questions": "﻿Consider the one-dimensional time-independent Schrodinger equation for some arbitrary potential $V(x)$. Prove that if a solution $\\psi(x)$ has the property that $\\psi(x) \\rightarrow 0$ as $x \\rightarrow \\pm \\infty$, then the solution must be nondegenerate and therefore real, apart from a possible overall phase factor.\n\nHint: Show that the contrary assumption leads to a contradiction.\n", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about the one-dimensional time-independent Schrödinger equation. The question is asking me to prove that if a solution ψ(x) tends to zero as x approaches both positive and negative infinity, then this solution must be non-degenerate and hence real up to an overall phase factor. Hmm, okay, let's break this down.\n\nFirst off, what does it mean for a solution to be non-degenerate? In quantum mechanics, degeneracy refers to multiple distinct wavefunctions having the same energy eigenvalue. So, non-degenerate means there's only one unique wavefunction (up to multiplication by a scalar) corresponding to each energy level. Got it.\n\nThe hint says to assume the opposite—that the solution is degenerate—and show that this leads to a contradiction. That makes sense; proof by contradiction often works well in these situations.\n\nSo, suppose we have two linearly independent solutions, say ψ₁(x) and ψ₂(x), which satisfy the Schrödinger equation with the same energy E:\n\n-ψ''(x)/2m + V(x)ψ(x) = Eψ(x)\n\nAnd both ψ₁ and ψ₂ tend to zero as x goes to ±∞.\n\nNow, since they're both solutions with the same energy, their difference should also satisfy the homogeneous version of the equation. But wait, actually, more importantly, any linear combination of them would also be a solution because the equation is linear. However, being linearly independent doesn't necessarily imply anything directly here yet.\n\nI remember something called the Wronskian determinant, which can tell us whether two functions are linearly dependent or not. For two functions f and g, the Wronskian is defined as W(f,g)(x) = f'(x)g(x) - f(x)g'(x). If the Wronskian is zero everywhere, then the functions are linearly dependent on that interval.\n\nBut how does this relate to our case?\n\nWell, maybe using the concept of orthogonality could help. Since ψ₁ and ψ₂ correspond to the same energy, but assuming different states, perhaps integrating over all space might lead somewhere.\n\nWait, another thought—since both ψ₁ and ψ₂ go to zero at infinities, maybe considering boundary conditions will play a role here. Also, thinking back to Sturm-Liouville theory, which deals with such differential equations, where self-adjoint operators ensure orthogonal eigenfunctions with respect to certain inner products.\n\nIn Sturm-Liouville problems, eigenvalues are simple (non-degenerate) under certain regularity conditions on the coefficients. Maybe similar reasoning applies here.\n\nLet me try setting up the integral involving ψ₁ and ψ₂. Let’s consider the expectation value of the Hamiltonian operator H acting on ψ₁ and ψ₂. Wait, no—if they’re both eigenstates with the same E, then ⟨ψ₁|H|ψ₂⟩ = E⟨ψ₁|ψ₂⟩. Similarly, ⟨ψ₂|H|ψ₁⟩ = E⟨ψ₂|ψ₁⟩. \n\nIf ψ₁ and ψ₂ were orthogonal, meaning ⟨ψ₁|ψ₂⟩=0, then... Well, but even without knowing orthogonality, perhaps taking the difference between integrals could yield something useful.\n\nAlternatively, think about subtracting the two Schrödinger equations multiplied appropriately. Suppose we take ψ₁ times its derivative minus ψ₂ times its derivative—it sounds like forming the Wronskian again.\n\nActually, let's write out the Schrödinger equation for both ψ₁ and ψ₂:\n\nFor ψ₁:\n(-1/2m) d²ψ₁/dx² + V(x)ψ₁ = Eψ₁\n\nSimilarly,\n(-1/2m) d²ψ₂/dx² + V(x)ψ₂ = Eψ₂\n\nSubtracting these gives:\n(-1/(2m))(d²ψ₁/dx² - d²ψ₂/dx²) + (V(x)-E)(ψ₁ - ψ₂)=0\n\nHmm, not sure if that helps immediately.\n\nAnother approach: Consider the ratio of ψ₁ and ψ₂. Define u = ψ₁ / ψ₂. Then, substitute into the Schrödinger equation and see if you get a condition on u.\n\nDifferentiating u, du/dx = (ψ₁’ ψ₂ - ψ₁ ψ₂’) / ψ₂². Which looks related to the Wronskian divided by ψ₂ squared.\n\nPlugging into the Schrödinger equation seems complicated though.\n\nMaybe instead use integration techniques. Let's integrate the product of ψ₁' and ψ₂ over all space.\n\nIntegrate [ψ₁' ψ₂] dx from -infinity to infinity.\n\nIntegration by parts: ∫ψ₁' ψ₂ dx = ψ₁ ψ₂ |_{-inf}^{+inf} - ∫ψ₁ ψ₂' dx.\n\nSince ψ₁ and ψ₂ vanish at infinities, the first term drops out, leaving -∫ψ₁ ψ₂' dx.\n\nThus,\n\n∫ψ₁' ψ₂ dx = -∫ψ₁ ψ₂' dx => ∫ψ₁' ψ₂ dx + ∫ψ₁ ψ₂' dx = 0.\n\nWhich implies ∫[ψ₁' ψ₂ + ψ₁ ψ₂'] dx = 0.\n\nNotice that ψ₁' ψ₂ + ψ₁ ψ₂' is just the derivative of (ψ₁ ψ₂).\n\nTherefore, ∫ d/dx (ψ₁ ψ₂) dx = 0.\n\nAgain, evaluating from -inf to +inf, the left side becomes [ψ₁ ψ₂]_(-inf)^{+inf}, which is zero because both ψ₁ and ψ₂ go to zero at infinities. Hence, 0=0, which isn’t helpful per se.\n\nPerhaps going back to the original idea of the Wronskian. Assume ψ₁ and ψ₂ form a fundamental set of solutions, i.e., they are linearly independent. Therefore, their Wronskian shouldn't be identically zero.\n\nCompute the Wronskian W = ψ₁ ψ₂' - ψ₂ ψ₁'.\n\nFrom the above integration result, when integrated over all x, the integral of W dx equals zero. Because earlier steps showed that ∫W dx = 0.\n\nHowever, if W itself is never zero, except possibly isolated points, then the fact that its integral is zero suggests that W changes sign infinitely often or cancels out somehow. But given that ψ₁ and ψ₂ decay at infinities, maybe oscillations aren't too wild.\n\nWait, hold on—I need to recall Abel's theorem regarding the Wronskian. It states that if y₁ and y₂ are solutions of a second-order ODE, then their Wronskian either vanishes nowhere or nowhere else except possibly discrete zeros. Moreover, the Wronskian satisfies W(y₁,y₂)(x) = W₀ exp[-∫ p(t) dt], where p(t) comes from the standard form of the DE.\n\nLooking back at the Schrödinger equation written in standard form:\n\ny'' + q(x)y = 0, where q(x) = 2m(V(x) - E)/(ħ²). Assuming ħ=1 for simplicity, so q(x) = 2m(V(x)-E).\n\nThen, according to Abel's identity, the Wronskian W(x) = W(x₀) * exp[-∫_{x₀}^x (-q(t)) dt].\n\nWait, correction—the general form is y'' + P(x)y' + Q(x)y = 0. Comparing, our equation after rearrangement is:\n\nψ'' = 2m(V(x) - E)ψ.\n\nSo, writing it as ψ'' - K(x)ψ = 0, where K(x) = 2m(E - V(x)). Thus, comparing to y'' + Py' + Qy = 0, we have P=0, Q=-K(x).\n\nAbel's formula tells us that W(y₁,y₂)(x) = W(x₀) exp[-∫P(t)dt] evaluated from x₀ to x. Here, since P=0, the exponential term is 1. So, W(x) remains constant along the entire domain!\n\nThat's crucial! So, the Wronskian is constant throughout. Now, if ψ₁ and ψ₂ are linearly independent, their Wronskian cannot be zero anywhere, right? Or rather, if it's non-zero at some point, it stays non-zero everywhere due to being constant.\n\nBut now, looking back at my previous calculation, I had found that ∫W dx = 0. But if W is a constant, say C, then ∫C dx from -infty to infty diverges unless C=0. Contradiction arises because if W=C≠0, the integral would blow up, but we know"}
{"id": "quantum/5021", "questions": "A particle of mass $\\text m$ moves in one dimension in the periodic potential\n\n$$\nV(x) = V_0 \\cos\\left( \\frac{2\\pi x}{\\alpha} \\right).\n$$\n\nWe know that the energy eigenstates can be divided into classes characterized by an angle $\\theta$ with wave functions $\\phi(x)$ that obey $\\phi(x+a) = e^{i\\theta}\\phi(x)$ for all $x$. For the class $\\theta = \\pi$, this becomes $\\phi(x+a) = -\\phi(x)$ (antiperiodic over length $a$).\n\n(a) Even when $V_0 = 0$, we can still classify eigenstates by $\\theta$. For which values of $k$ does the plane wave $\\phi(x) = e^{ikx}$ satisfy the antiperiodic condition over length $a$? What is the energy spectrum of the class $\\theta = \\pi$ for $V_0 = 0$?\n\n(b) When $V_0$ is small (i.e. $V_0 \\ll \\hbar^2/ma^2$), calculate the lowest two energy eigenvalues by first order perturbation theory.", "graphs": null, "llm_answers": "<think>\nOkay, so I have to solve part (a) and then part (b). Let me start with part (a). \n\nThe problem says: A particle of mass m moves in a one-dimensional periodic potential given by \\( V(x) = V_0 \\cos\\left(\\frac{2\\pi x}{\\alpha}\\right) \\). The energy eigenstates can be classified by an angle θ such that their wavefunctions φ(x) satisfy \\( \\phi(x + a) = e^{iθ}φ(x) \\) for all x. Specifically, for θ=π, it's called antiperiodic because \\( \\phi(x + a) = -\\phi(x) \\).\n\nPart (a): Even if V₀=0, meaning no potential, we can still classify states by θ. So even free particles can have these Bloch-like conditions.\n\nFirst question: For what k does the plane wave ϕ(x)=e^{ikx} satisfy the antiperiodic condition over length a? That means:\n\n\\( \\phi(x + a) = e^{ik(x + a)} = e^{ika}e^{ikx} = e^{iθ}ϕ(x) \\)\n\nBut since θ=π here, we need:\n\n\\( e^{ika} = e^{iπ} = -1 \\)\n\nSo,\n\n\\( e^{ika} = -1 \\implies ika = π + 2πn \\quad n ∈ ℤ \\)\nWait, actually exponentials wrap around every 2π, but let's think about solving \\( e^{ika} = -1 \\).\n\nSince -1 has magnitude 1 and phase π, any multiple adding 2π would also work. But more precisely, the equation holds when ka ≡ π mod 2π. Therefore, ka must equal π plus integer multiples of 2π.\n\nThus,\n\n\\( ka = π + 2πn \\), where n is an integer.\n\nTherefore, dividing both sides by a,\n\n\\( k = \\frac{\\pi}{a}(1 + 2n) \\)\n\nWhich simplifies to odd multiples of π/a. Because n can be ..., -1, 0, 1,... leading to k being ...,-3π/(2a), -π/(2a), π/(2a), 3π/(2a)... Wait, hold on, wait.\n\nLet me check again. If ka = π + 2πn, then k = [π + 2πn]/a = π(1 + 2n)/a. Hmm, yes, exactly as above.\n\nAlternatively, writing n as integers gives us different ks spaced apart by 2π/a each time. So the allowed k-values form a set { π/a*(1+2n) | n∈ℤ }.\n\nNow, moving on to the second part: What is the energy spectrum of the class θ=π for V₀=0?\n\nWhen there's no potential, the Hamiltonian reduces to H = p²/(2m). Since the state satisfies the antiperiodicity condition, its momentum isn't quantized in the usual way—it’s instead determined modulo reciprocal lattice vectors or something like that.\n\nIn general, for a free particle without boundary conditions, the energy E(k) = ħ²k²/(2m). However, due to our specific boundary condition, only certain k's are allowed—those satisfying the antiperiodic condition.\n\nFrom earlier, those k's are k_n = (π/a)(1 + 2n), right? Or perhaps better written as k_n = (2n + 1)π/a.\n\nHence, substituting back into the energy expression:\n\nE_k = (ħ² / (2m)) * k² = (ħ² / (2m)) * [(2n + 1)^2 π²] / a²\n\nSimplify:\n\nE_n = (ħ² π²)/(2m a²) * (2n + 1)^2\n\nWhere n ranges over all integers. Thus, the energy levels are proportional to squares of odd integers times some constant factor.\n\nThat should cover part (a). Now onto part (b):\n\nWhen V₀ is small compared to ħ²/(m a²), use first-order perturbation theory to find the lowest two energy eigenvalues.\n\nGiven that V₀ << ħ²/(m a²), the system is nearly free except for a weak periodic potential. In solid-state terms, this corresponds to near the bottom of the band structure, maybe Γ point or similar.\n\nFor non-interacting systems under a periodic potential, the unperturbed case is just the free particle with energies E_k = ħ²k²/(2m). Then, the perturbing potential is V(x) = V₀ cos(2πx/α). Assuming α=a, probably; though not sure yet.\n\nWait, original potential was defined with period alpha, but in part (a), they talk about \"length a\". Maybe assuming a=alpha? Probably, otherwise units might get messy. Alternatively, could be related via another relation.\n\nAssuming a=alpha, so the potential is V(x) = V₀ cos(2πx/a). This makes sense because the period is 'a', matching the translation distance in the antiperiodic condition.\n\nThen, using first-order PT, the correction to the energy will come from matrix elements ⟨ψ|V|ψ⟩, evaluated within the same Bloch state.\n\nHowever, considering that in the presence of a periodic potential, the basis states aren’t simply plane waves anymore—they’re Bloch states. But since V₀ is small, we treat them as slightly modified plane waves.\n\nWait, alternatively, perhaps treating the problem as a tight-binding scenario, but since V₀ is small, it's more appropriate to consider the nearly-free electron model approach.\n\nIn the nearly-free electron approximation, especially for weak potentials, the main effect comes from Bragg reflections causing gaps at the Brillouin zone boundaries.\n\nBut how do we apply first-order perturbation here?\n\nI recall that in degenerate perturbation theory, you look at subspaces where the unperturbed energies coincide—that is, points where different k-states lead to the same E_k.\n\nAt the center of the BZ, say k=0, the energy is minimal, and nearby k-points may cause mixing through the perturbation.\n\nSimilarly, near the edges of the BZ, like k=±π/a, the energy starts increasing again.\n\nBut specifically, for the ground state(s)—the lowest energy—the relevant subspace is likely around k=0, but depending on whether the system allows for spin or other symmetries, but in this case, it's scalar potential, so symmetry-wise, perhaps symmetric and antisymmetric combinations.\n\nWait, but in part (a), we considered θ=π, implying anti-periodicity. How does that tie into part (b)?\n\nHmm, perhaps in part (b), regardless of θ, we're looking for the overall lowest two energy eigenvalues across all possible θs. Or maybe focusing on the θ=π class.\n\nWait, re-reading part (b): It doesn't specify θ, just asks for the lowest two energy eigenvalues when V₀ is small. So perhaps including contributions from various θ sectors.\n\nBut in reality, in the full picture, the bands touch at high-symmetry points, so the lowest energy level occurs at the minimum of the dispersion, typically at k=0.\n\nBut with the addition of a cosine potential, which introduces coupling between neighboring k-states separated by G= ±2π/a (since Fourier components of V are at G= ±2π/a).\n\nThis causes splitting at the Brillouin zone edge, creating gaps. At the center, however, the gap closes, allowing for crossings.\n\nWait, but in first-order PT, the corrections depend on diagonal matrix elements—if the state is non-degenerate, the shift is just ⟨V⟩.\n\nIf the state is degenerate, you have to go beyond first-order or handle the subspace properly.\n\nHold on, in the absence of the potential, the free particle has continuous energy spectrum. Introducing a periodic potential leads to the formation of discrete bands. But in perturbative treatment, particularly first-order, we approximate the shifts based on overlaps.\n\nBut perhaps thinking too much in solid-state terms complicates things. Instead, going back to standard quantum mechanics.\n\nSuppose we take the unp"}
{"id": "quantum/5019", "questions": "(a) State all the energy levels of a symmetric top with principal moments of inertia $I_1 = I_2 = I \\neq I_3$.\n\n(b) A slightly asymmetric top has no two $I$ exactly equal, but $I_1 - I_2 = A \\neq 0, I_1 + I_2 = 2I, (A/2I) \\ll 1$. Compute the $J = 0$ and $J = 1$ energies through $O(A)$.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about rigid rotors, specifically parts (a) and (b). Let me try to work through them step by step.\n\nStarting with part (a): It asks for all the energy levels of a symmetric top where the principal moments of inertia are given as \\( I_1 = I_2 = I \\neq I_3 \\).\n\nHmm, right. For a symmetric top, since two moments of inertia are equal (\\( I_1 = I_2 \\)), it's called symmetric because there is symmetry around one axis—the third axis here. The rotational energy depends on these moments of inertia and the angular momentum quantum numbers.\n\nIn general, the rotational energy levels for a molecule can be found using the formula:\n\n\\[\nE = \\frac{\\hbar^2}{2} \\left( \\frac{K(K+1)}{I_3} + \\frac{(J-K)(J+K+1)}{I_1} \\right)\n\\]\n\nWait, let me make sure that's correct. So, J is the total angular momentum quantum number, K is its projection along the symmetry axis (which would correspond to the z-axis if we're aligning our coordinate system accordingly), and then each term corresponds to contributions from different components of the angular momentum.\n\nBut actually, another way to write the energy expression when dealing with symmetric tops is considering their specific symmetries. Since \\( I_1 = I_2 = I \\), maybe the energy simplifies?\n\nLet me recall: In the case of a symmetric top, the Hamiltonian only involves terms related to the angular momentum projections onto the body-fixed axes. Because \\( I_1 = I_2 \\), some terms might combine or cancel out.\n\nThe standard form for the rotational energy of a symmetric top should involve both the spin-rotation coupling and other terms... Wait, perhaps more straightforwardly, the energy levels depend on J and K.\n\nYes, indeed, for a symmetric top, the energy levels split into degenerate states based on K. Each state with a particular J will have multiple K values ranging from -J to +J, but due to the symmetry, certain Ks may lead to same energy? Or not necessarily...\n\nActually, wait—no, even though it's symmetric, unless you have additional constraints like being prolate or oblate spheroid, which affects whether higher K gives lower or higher energy.\n\nSo, generally, the energy levels for a symmetric top are given by:\n\n\\[\nE_{JK} = B J(J+1) + C [K^2]\n\\]\n\nWhere B and C are constants depending on the moments of inertia.\n\nGiven that \\( I_1 = I_2 = I \\) and \\( I_3 \\) is different, how do we express B and C?\n\nWell, usually,\n\n\\( B = \\frac{\\hbar^2}{4\\pi c I_a} \\)\n\nand\n\n\\( C = \\frac{\\hbar^2}{8\\pi c I_b} \\),\n\nbut wait—that notation varies; sometimes people use different letters for the moments of inertia.\n\nAlternatively, recalling that for a symmetric top, the energy can also be written as:\n\n\\[\nE_J = \\frac{\\hbar^2}{2}\\left(\\frac{J(J+1)}{I_3} + \\frac{(J - K)(J + K + 1)}{I_1}\\right)\n\\]\n\nSince \\( I_1 = I_2 = I \\), substituting those in:\n\n\\[\nE_{JK} = \\frac{\\hbar^2}{2}\\left( \\frac{J(J+1)}{I_3} + \\frac{(J - K)(J + K + 1)}{I} \\right )\n\\]\n\nIs that correct? Hmm, yes, seems plausible.\n\nTherefore, the energy levels depend on both J and K. But hold on—isn't E independent of K for linear molecules? No, because they aren’t symmetric enough—it’s just a symmetric top, meaning two moments are equal, so K does play a role.\n\nThus, for each value of J, K ranges from -J to +J, giving rise to (2J + 1) possible K values, hence (2J + 1) degeneracy before any perturbations. However, in reality, for a symmetric top without external fields, the energy doesn't depend on K because of the axial symmetry. Therefore, all K within a fixed J have the same energy.\n\nWait, now I'm confused. Is the energy dependent on K or not?\n\nNo, wait—if the moment of inertia is such that \\( I_1 = I_2 \\), then the potential energy associated with rotation shouldn't depend on the direction perpendicular to the symmetry axis. Hence, the energy should only depend on J, similar to spherical tops, except that the coefficient differs.\n\nHold on—I think my initial thought was wrong. Maybe for a symmetric top, despite having unequal moments, the energy still only depends on J, making all K states within a single J level degenerate.\n\nWait, let me check the formula again.\n\nFor a symmetric top, the rotational kinetic energy operator is expressed in terms of the Euler angles, leading to eigenvalues that depend on J and K. Specifically, the energy is proportional to \\( J(J+1)/I_3 + something involving K \\)?\n\nWait, no, upon reflection, I remember that for a symmetric top, the energy levels are determined solely by J, regardless of K. That is, all states with the same J but varying K have the same energy. This comes from the fact that the Hamiltonian commutes with L_z (the component of angular momentum along the symmetry axis), allowing simultaneous measurement of J² and L_z², but the energy itself remains the same across different K for a fixed J.\n\nHence, the energy levels are non-degenerate in J, but each J level has multiplicity (2J + 1) due to different K values.\n\nWait, but isn't that true for a sphere too? Yes, for a sphere, every J level is equally spaced, but for a symmetric top, spacing between Js changes differently.\n\nWait, okay, stepping back. If the molecule is a symmetric top, i.e., two moments of inertia are equal, say \\( I_1 = I_2 \\neq I_3 \\), then the rotational energy is given by:\n\n\\[\nE_J = \\frac{\\hbar^2}{2} \\left[ \\frac{J(J+1)}{I_3} + \\frac{(J - K)(J + K + 1)}{I_1} \\right ]\n\\]\n\nBut hang on, if \\( I_1 = I_2 \\), why is there dependence on K? Shouldn't the energy be independent of K?\n\nAh! Perhaps I made a mistake earlier. Actually, for a symmetric top, the energy does NOT depend on K because of the symmetry. Thus, the second term must somehow simplify to eliminate K.\n\nLooking closer, expanding the second term:\n\n\\[\n\\frac{(J - K)(J + K + 1)}{I}\n= \\frac{J^2 + J - JK - KJ - K^2 + K}{I}\n= \\frac{J^2 + J - 2JK - K^2 + K}{I}\n\\]\n\nThat seems messy. Alternatively, note that \\( (J - K)(J + K + 1) = J(J + 1) - K(K + 1) \\). Let me verify:\n\nMultiply out left side:\n\\( (J - K)(J + K + 1) = J^2 + JK + J - KJ - K^2 - K = J^2 + J - K^2 - K \\).\nRight side:\n\\( J(J + 1) - K(K + 1) = J^2 + J - K^2 - K \\).\nYes, perfect!\n\nSo, substitute back into the energy equation:\n\n\\[\nE_{JK} = \\frac{\\hbar^2}{2} \\left( \\frac{J(J+1)}{I_3} + \\frac{J(J+1) - K(K+1)}{I} \\right )\n\\]\n\nSimplify:\n\nFactor out \\( J(J+1) \\):\n\n\\[\nE_{JK} = \\frac{\\hbar^2}{2} \\left( J(J+1)\\left( \\frac{1}{I_3} + \\frac{1}{I} \\right ) - \\frac{K(K+1)}{I} \\right )\n\\]\n\nNow, notice that \\( I_1 = I_2 = I \\), so \\( I_3 \\) could be either greater than or less than I, determining whether it's a prolate or oblate spheroid.\n\nHowever, looking at the above expression, the first term inside the parentheses is multiplied by \\( J(J+1) \\), while the second term subtracted is \\( K(K+1)/I \\). \n\nThis suggests that the"}
{"id": "quantum/6001", "questions": "Derive the quantum mechanical expression for the s-wave cross section for scattering from a hard sphere of radius $R$.", "graphs": null, "llm_answers": "<think>\nOkay, so I need to derive the quantum mechanical expression for the s-wave cross section when scattering off a hard sphere with radius R. Hmm, let me think about how to approach this.\n\nFirst, I remember that in quantum mechanics, scattering problems often involve solving the Schrödinger equation for particles interacting via some potential. In this case, it's a hard sphere, which means there’s no interaction except beyond the surface—so inside the sphere (r < R), the potential is infinite, making it impenetrable. Outside the sphere (r ≥ R), the potential is zero because it's free space.\n\nSince we're dealing with s-waves specifically, these correspond to l = 0 angular momentum states. So, the wavefunction should only have components where the orbital angular momentum number l is zero. That simplifies things because spherical harmonics Y_l^m become just constants for l=0, m=0.\n\nThe total wavefunction can be written as a sum of incoming plane waves and outgoing scattered waves. But since it's s-wave scattering, maybe we don't have any angular dependence? Or perhaps more accurately, all terms will depend on r but not angles due to symmetry.\n\nWait, actually, even though the problem has spherical symmetry, the incident wave might still come along a specific direction, say the z-axis. However, for s-waves, the angular part doesn’t contribute much because they’re isotropic. Maybe the partial wave expansion isn’t necessary here?\n\nAlternatively, considering the Born approximation or something else... No, wait, for a hard sphere, exact solutions exist without approximations because the potential is simple: V(r) = ∞ for r < R and V(r)=0 otherwise.\n\nSo, probably better to solve the radial Schrödinger equation exactly for l=0.\n\nLet me recall the time-independent Schrödinger equation:\n\n\\[\n-\\frac{\\hbar^2}{2m} \\nabla^2 \\psi + V(r)\\psi = E\\psi\n\\]\n\nIn spherical coordinates, and given the potential is central, we use separation of variables into radial and angular parts. For s-waves, the angular part is trivial, so focus on the radial equation.\n\nFor regions outside the sphere (r > R):\n\nV(r) = 0\n\nRadial equation becomes:\n\n\\[\n-\\frac{\\hbar^2}{2m} \\left( \\frac{1}{r^2} \\frac{d}{dr}\\left(r^2 \\frac{dR}{dr}\\right) \\right ) = ER\n\\]\n\nMultiply both sides by -2mr²/ħ²:\n\n\\[\n\\frac{d}{dr}\\left(r^2 \\frac{dR}{dr}\\right) = -k^2 r^2 R\n\\]\nwhere \\( k = \\sqrt{2mE}/\\hbar \\)\n\nThis differential equation is similar to Bessel equations, right? The general solution for this kind of equation is in terms of spherical Bessel functions.\n\nIndeed, the regular solution at infinity would be a combination of ingoing and outgoing spherical Bessel functions. Wait, but for scattering, you usually express the wavefunction as an incoming plane wave plus an outgoing scattered wave.\n\nBut hold on, for a hard sphere, the boundary condition is different. Inside the sphere (r < R), ψ must go to zero because the particle cannot penetrate. Therefore, the wavefunction vanishes within the sphere.\n\nOutside the sphere, the solution needs to satisfy certain conditions. Let me write down the form of the wavefunction.\n\nInside the sphere:\nψ_in(r) = 0 for r ≤ R\n\nOutside the sphere:\nWe expect a plane wave coming in and a scattered wave going out. Since it's s-wave, the angular dependence is minimal; however, typically, the scattered wave includes higher-order multipoles, but if we restrict ourselves to s-waves, then...\n\nActually, wait—if the target is a hard sphere, does it support other multipolar contributions besides s-waves? Because the potential is non-zero starting abruptly at R, the Fourier transform may include various momenta leading to multiple l-values. But since we're focusing on s-wave cross-sections, perhaps we consider only the contribution from l=0.\n\nHmm, now I'm getting confused. Alternatively, maybe the entire scattering amplitude comes solely from the s-wave term because the potential is such that higher order terms vanish? Not sure.\n\nPerhaps another way: using partial wave analysis. Express the scattering amplitude f(l)(θ) for each l, square them appropriately, etc., but again, for a hard sphere, what do those amplitudes look like?\n\nWait, stepping back. If the potential is purely repulsive and finite range (only up to R), then the phase shifts δ_l determine the scattering properties. Each partial wave contributes based on its own phase shift.\n\nGiven that, the differential cross-section is |f(θ)|², where f(θ) is the sum over l of e^{iδ_l}(2l+1) P_l(cos θ). Then integrating over solid angle gives the total cross-section.\n\nHowever, for a hard sphere, the phase shifts can be determined by matching the logarithmic derivatives of the interior and exterior solutions at r=R.\n\nSpecifically, for each l, the phase shift δ_l satisfies tan δ_l = [something involving the derivative].\n\nBut since we're interested in s-waves, set l=0.\n\nSo first, find the solution inside and outside the sphere.\n\nWait, but inside the sphere, the potential is infinite, meaning the wavefunction must be zero. Hence, the solution inside is identically zero.\n\nTherefore, the solution outside must match smoothly onto the inner region at r=R.\n\nThus, the wavefunction outside must satisfy two conditions at r=R:\n\n1. Continuity of ψ.\n2. Discontinuity in the derivative proportional to the delta function representing the boundary.\n\nWait, actually, since the potential jumps discontinuously at r=R, the derivative of the wavefunction will have a jump proportional to the strength of the potential change.\n\nMathematically,\n\nAt r=R,\n- ψ_out(R) = ψ_in(R)\nBut ψ_in(R) = 0 ⇒ ψ_out(R) = 0\n\nAnd also,\n\n(d/dr ψ)_out(R+) - (d/dr ψ)_in(R-) = -(2m/ħ²)[V(R)+] ψ(R)\n\nHere, V(R)+ is the positive side limit of the potential. Since V(r) goes from ∞ to 0 at r=R, technically, the jump in the derivative involves Dirac deltas, but handling this requires care.\n\nAlternatively, thinking physically, the wavefunction must be continuous across r=R, but the derivative experiences a sudden change due to the infinite barrier.\n\nI've heard before that for a hard sphere, the effective boundary condition is that the wavefunction itself must vanish at r=R, i.e., ψ(R) = 0. Is that correct?\n\nYes, because inside the sphere, the wavefunction is zero everywhere, including at r=R-. Thus, continuity implies ψ(R+) = 0.\n\nHence, the external solution must satisfy ψ(R) = 0.\n\nNow, writing the external solution as a linear combination of spherical Bessel functions j_0(kr) and y_0(kr).\n\nRecall that for l=0, the regular solution at origin is j_0(kr) = sin(kr)/(kr), while the irregular solution is y_0(kr) = cos(kr)/(kr).\n\nBut for scattering, we want an incoming plane wave and an outgoing spherical wave. Actually, the standard ansatz is:\n\nψ_scattered(r) = e^{ikz} + f(θ) * h_0^{(1)}(kr),\n\nWhere h_0^{(1)} is the Hankel function of the first kind, which represents outgoing waves.\n\nBut in our case, since the internal solution is zero, the external solution must satisfy ψ(R) = 0.\n\nWait, alternatively, perhaps expressing the external solution as a single spherical Bessel function times a coefficient, ensuring that it meets the boundary condition.\n\nSuppose the external solution is A * j_0(kr). At r=R, this equals zero.\n\nj_0(kR) = sin(kR)/(kR) = 0.\n\nWhich occurs when kR = nπ, n integer.\n\nThat suggests quantized energies, but in reality, for scattering, energy is fixed, so unless resonance happens, this seems problematic.\n\nWait, perhaps my initial assumption was wrong. Instead, the external solution shouldn't necessarily be a pure j_0, but rather a combination of j_0 and y_0.\n\nBecause for scattering, you have an incoming wave and an outgoing wave.\n\nExpressed differently, the external solution could be expressed as:\n\nψ_external(r) = C [e^{ikr} / kr ] , but adjusted to account for reflection.\n\nNo, perhaps instead, combining j_0 and y_0.\n\nWait, recalling that the general solution for the radial equation outside is:\n\nR(r) = A j_0(kr) + B y_0(kr)\n\nThen applying the boundary condition at r=R: R(R) = 0.\n\nAlso, note that near r→∞, the solution should behave as an incoming plane wave plus an outgoing scattered wave.\n\nBut for simplicity, assuming far away behavior, but since we have a compact object, perhaps the asymptotic form is manageable.\n\nWait, perhaps it's easier to model the situation as follows"}
{"id": "quantum/1-1030", "questions": "An electron is confined in the ground state of a one-dimensional harmonic oscillator such that $\\sqrt{\\langle (x - \\langle x \\rangle)^2 \\rangle} = 10^{-10} \\text{ m}$. Find the energy (in $\\text{eV}$) required to excite it to its first excited state.\n\n[Hint: The virial theorem can help.]", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about an electron in a one-dimensional harmonic oscillator. It's in the ground state, and they give me the standard deviation of position as \\( \\sqrt{\\langle (x - \\langle x \\rangle)^2 \\rangle} = 10^{-10} \\) meters. They want me to find the energy needed to excite it to the first excited state, measured in eV. Hmm, okay, let's break this down.\n\nFirst off, I remember that for quantum harmonic oscillators, especially in their ground states, there are some specific formulas related to expectation values like variance or uncertainty in position. Since we're given the square root of the variance, which is essentially the standard deviation, maybe I need to relate that back to something else.\n\nI recall that the average potential energy and kinetic energy in a harmonic oscillator are equal due to the virial theorem. That might come into play here since the hint suggests using the virial theorem. But how?\n\nLet me think... For a harmonic oscillator, the total energy levels are quantized. Specifically, each level has energy \\( E_n = \\left(n + \\frac{1}{2}\\right)\\hbar\\omega \\), where \\( n \\) is the quantum number starting from 0 for the ground state. So, the ground state would be \\( n=0 \\) with energy \\( \\frac{1}{2}\\hbar\\omega \\), and the first excited state is \\( n=1 \\) with energy \\( \\frac{3}{2}\\hbar\\omega \\). Therefore, the energy difference between these two should be \\( \\Delta E = E_1 - E_0 = \\hbar\\omega \\).\n\nSo if I can figure out what \\( \\hbar\\omega \\) is, then I'll know the energy required to make that transition. Cool, but how do I get \\( \\omega \\)?\n\nWell, another thing I remember is that the variance in position relates to other parameters of the oscillator. Let me try writing down expressions involving \\( \\sigma_x^2 = \\langle x^2 \\rangle - \\langle x \\rangle^2 \\). In the case of the ground state, because the wavefunction is symmetric around zero, \\( \\langle x \\rangle = 0 \\), right? So actually, \\( \\sigma_x^2 = \\langle x^2 \\rangle \\).\n\nAnd I also remember that for a harmonic oscillator, the expectation value of \\( x^2 \\) in the nth state is given by:\n\n\\( \\langle x^2 \\rangle = \\frac{\\hbar}{2m\\omega}(2n + 1) \\)\n\nBut wait, hold on—is that correct? Or was it different? Maybe I mixed up factors somewhere. Alternatively, perhaps more accurately, the expression involves the spring constant k instead of ω directly.\n\nWait, yes! Because the angular frequency \\( \\omega \\) is related to the spring constant k via \\( \\omega = \\sqrt{k/m} \\). Also, the potential energy is \\( V(x) = \\frac{1}{2}kx^2 \\). \n\nAnother approach—maybe through Heisenberg’s Uncertainty Principle—but not sure yet. Wait, no—the question gives us the standard deviation in position, so maybe connecting that to momentum could work, but again, unsure.\n\nAlternatively, going back to the virial theorem. Virial theorem says that the average kinetic and potential energies are equal. So,\n\n\\( \\langle K.E. \\rangle = \\langle P.E. \\rangle \\)\n\nSince the total energy is \\( E = \\langle K.E. \\rangle + \\langle P.E. \\rangle = 2\\langle K.E. \\rangle \\). Hence, each contributes half of the total energy.\n\nIn our case, the ground state energy is \\( E_0 = \\frac{1}{2}\\hbar\\omega \\). Then, the energy gap when exciting to the next state is just \\( \\hbar\\omega \\), as mentioned before.\n\nTherefore, all I really need is \\( \\hbar\\omega \\), which will give me the energy required for the transition.\n\nNow, how does \\( \\sigma_x \\) tie into this? Well, earlier thought process said that \\( \\langle x^2 \\rangle = \\frac{\\hbar}{2m\\omega} (2n+1) \\). For the ground state (\\( n=0 \\)), this becomes \\( \\langle x^2 \\rangle = \\frac{\\hbar}{2m\\omega} \\times 1 = \\frac{\\hbar}{2m\\omega} \\).\n\nGiven that \\( \\sigma_x = 10^{-10} \\) m, squaring both sides gives \\( \\sigma_x^2 = 10^{-20} \\) m². Thus,\n\n\\( \\frac{\\hbar}{2m\\omega} = 10^{-20} \\)\n=> \\( \\hbar\\omega = \\frac{\\hbar}{2m\\omega} \\times 2m \\)\nWait, that seems circular—if I solve for \\( \\hbar\\omega \\):\n\nFrom above,\n\\( \\frac{\\hbar}{2m\\omega} = 10^{-20} \\)\nMultiply numerator and denominator:\n\\( \\hbar/(2mω) ) = σ_x²\n\nThus, solving for ω:\n\n\\( ω = \\hbar / (2mσ_x²) )\n\nThen, plugging back into \\( \\hbarω \\):\n\\( \\hbar * [ \\hbar / (2mσ_x²) ] = ħ² / (2mσ_x²) )\n\nHmm, interesting. So, \\( \\hbar\\omega = \\frac{\\hbar^2}{2m\\sigma_x^2} \\). Is that correct? Let me verify units—it makes sense dimensionally. h-bar squared over mass times length squared—that would result in energy, yeah.\n\nBecause \\( [\\hbar] = J·s \\),\nso \\( \\hbar^2/(kg·m²) = (J² s²)/(kg·m²) ). But J is kg·m²/s², so substituting:\n\n(J² s²)/(kg·m²) = ((kg²·m⁴/s⁴)) · s²)/(kg·m²) )\nSimplify numerator inside: kg²·m⁴/s² divided by kg·m² => kg·m²/s², which is Joules. Perfect, matches energy unit.\n\nAlright, so moving forward, compute \\( \\hbar\\omega = \\frac{\\hbar^2}{2m_e \\sigma_x^2} \\), where \\( m_e \\) is the electron mass.\n\nPlugging numbers now—I need constants:\n\n- Planck's reduced constant \\( \\hbar ≈ 1.0545718 × 10^{-34} \\) J·s.\n- Electron mass \\( m_e ≈ 9.10938356 × 10^{-31} \\) kg.\n- Standard deviation \\( \\sigma_x = 10^{-10} \\) m, so \\( \\sigma_x^2 = 10^{-20} \\) m².\n\nCompute numerator: \\( \\hbar^2 = (1.0545718e-34)^2 ≈ 1.112e-68 \\) J²·s².\n\nDenominator: \\( 2 m_e sigma_x^2 = 2 * 9.10938356e-31 kg * 1e-20 m² = 1.8218767e-50 kg·m² \\).\n\nDivide them: \\( 1.112e-68 / 1.8218767e-50 ≈ 6.103e-19 \\) J.\n\nConvert joules to eV: 1 eV = 1.602176634e-19 J. So divide by that factor.\n\nEnergy ΔE = 6.103e-19 J / 1.602176634e-19 J/eV ≈ approximately 3.81 eV.\n\nWait, let me double-check my calculations numerically.\n\nCalculating \\( \\hbar^2 \\): (1.0545718e-34)^2.\n\nCalculate 1.0545718 squared: approximates to ~1.112.\n\nYes, so 1.112e-68.\n\nDenominator: 2*9.10938356e-31*1e-20.\n\nMultiplying 2*9.10938356e-31 = 1.8218767e-30; multiply by 1e-20: 1.8218767e-50.\n\nNumerator/denominator:"}
{"id": "quantum/5026", "questions": "Consider the proton to be a spherical shell of charge of radius $\\mathbf R$. Using first order perturbation theory calculate the change in the binding energy of hydrogen due to the non-point-like nature of the proton. Does the sign of your answer make sense physically? Explain.\n\nNote: You may use the approximation $\\mathbf R \\ll a_0$ throughout this problem, where $a_0$ is the Bohr radius.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about calculating the change in the binding energy of hydrogen using first-order perturbation theory because the proton isn't just a point particle but has some finite size R. The note says that R is much smaller than the Bohr radius a₀, which might help simplify things later on. Hmm, let me think through how to approach this.\n\nFirst off, I remember from my quantum mechanics class that when dealing with such problems, we can model the effect as a small perturbation to the Hamiltonian. Since the proton's actual structure affects the potential experienced by the electron, it must modify the Coulomb potential slightly. So maybe I should consider the difference between the usual Coulomb potential (which assumes a point charge) and the potential due to a uniformly charged sphere.\n\nWait, right! If the proton were a uniform sphere of charge instead of a point charge, then inside the sphere (for r < R), the electric field would behave differently compared to outside (r > R). But since the Bohr radius a₀ is much larger than R, most of the time the electron will be far away from the center, meaning its wavefunction mainly exists in regions where r >> R. That suggests perhaps only considering the region near the nucleus could contribute significantly to the perturbation?\n\nBut wait, no—actually, even though the electron spends little time close to the nucleus, the expectation value for certain operators still depends on the behavior there. Maybe I need to compute the average potential over all space, or something like that.\n\nLet me recall the expression for the Coulomb potential created by a uniformly charged sphere. For points outside the sphere (r ≥ R), the potential behaves exactly like a point charge: V(r) = -e²/(4πε₀ r). Inside the sphere (r ≤ R), the potential increases linearly with distance from the center; specifically, V(r) = -(3 e²)/(8 π ε₀ R³)(R²/2 - r²/6)? Wait, actually, hold on—I think I messed up the exact form. Let me derive it quickly.\n\nThe electric field E inside a uniformly charged sphere is given by Gauss' law:\n\nE * 4π r² = Q_enc / ε₀\n\nSince the total charge is +e spread out uniformly, the enclosed charge at radius r is Q_enc = (e/V_sphere) * (4/3 π r³) )= e*(r/R)^3.\n\nSo,\n\nE = [e (r^3)] / [3 ε₀ R³] ] * (1/r²) direction radially outward.\n\nThus, E = (e r)/(3 ε₀ R³).\n\nThen integrating E to get the potential φ(r):\n\nφ(r) = integral from 0 to r of E dr'\n\nWhich gives φ(r) = (e)/(6 ε₀ R³) r².\n\nAnd since the potential at infinity is zero, the potential inside becomes positive quadratic term, while outside remains negative inverse square.\n\nTherefore, combining both cases:\n\nV(r) = { \n    (-e²)/(4π ε₀ r) , if r >= R,\n    (e²)/(12 π ε₀ R³) r², if r <= R.\n}\n\nWait, did I mess up signs here? Because usually potentials are defined relative to infinity being zero. For a positively charged sphere, the potential inside should be higher than outside. At r=R, they should match smoothly.\n\nAt r=R, the outer potential is V_outer(R)= -e²/(4π ε₀ R)\n\nInner potential at r=R is V_inner(R)= (e²)/(12 π ε₀ R³)* R² = (e²)/(12 π ε₀ R )\n\nHmm, these don't seem equal unless... Did I do something wrong?\n\nOh, wait, probably missed constants somewhere. Alternatively, another way: Potential inside a uniformly charged sphere is constant plus a quadratic term. Actually, more accurately, the potential inside is maximum at the surface and decreases quadratically inward.\n\nAlternatively, correct formula for potential inside a sphere of charge density ρ is:\n\nV_inside(r) = k_e * (ρ)*(3R² - r²)/6R\n\nWhere ρ is volume charge density.\n\nGiven that the total charge is e, so ρ = e/( (4/3) π R³ ). Plugging back into V_inside:\n\nV_inside(r) = (k_e) * (e/( (4/3) π R³ )) * (3 R² - r²)/(6 R )\n\nSimplify numerator:\n\n(3 R² - r²) divided by 6 R multiplied by e/( (4/3) π R³ ).\n\nCompute denominator terms:\n\nMultiply denominators: 6 R * (4/3 π R³ ) = 8 π R⁴.\n\nNumerator: (3 R² - r²) * e.\n\nPutting together:\n\nV_inside(r) = (9 e (3 R² - r²))/(something?)\n\nWait, getting confused now. Perhaps better to look up standard result.\n\nI know that for a solid sphere of charge, the potential inside is V(r) = (3 e)/(8 π ε₀ R³) (R² - r²/3 ), whereas outside it's V(r) = e/(4 π ε₀ r).\n\nYes, okay, so correcting myself:\n\nInside:\nV_in(r) = (3 e)/(8 π ε₀ R³) (R² - r²/3 )\nOutside:\nV_out(r) = e/(4 π ε₀ r )\n\nNow check continuity at r=R:\n\nV_in(R) = (3 e)/(8 π ε₀ R³) (R² - R²/3 ) = (3 e)/(8 π ε₀ R³) *(2 R² /3 ) = (3 e * 2 R² )/(24 π ε₀ R³ ) = (6 e R² )/(24 π ε₀ R³ ) = (e)/(4 π ε₀ R )\n\nSimilarly, V_out(R) = e/(4 π ε₀ R )\n\nGood, continuous at boundary.\n\nAlso, derivative discontinuity equals surface charge density times 1/ε₀, but not needed here.\n\nAlright, so our unperturbed system uses the point charge potential, i.e., always assuming V(r) = -e²/(4 π ε₀ r ). However, reality is different within r < R. Therefore, the perturbing potential ΔV(r) is the difference between real potential and assumed one.\n\nHence, ΔV(r) = V_real(r) - V_point(r)\n\nFor r >= R, V_real(r) = V_point(r), so ΔV(r) = 0.\n\nFor r < R, V_real(r) = (3 e)/(8 π ε₀ R³) (R² - r²/3 ) \n\nMinus V_point(r) which is -e²/(4 π ε₀ r )\n\nWait, hang on—isn’t V_point(r) also valid everywhere except origin? No—if you treat the proton as a point charge, then yes, V_point(r) is same as before regardless of position. Hence, the correction comes only inside the sphere.\n\nWait, but actually, treating the proton as a point charge leads us to assume that the potential is -e²/(4 π ε₀ r ) everywhere. In reality, for r < R, the potential is different—it’s (3 e)/(8 π ε₀ R³) (R² - r²/3 ). Thus, the perturbation is:\n\nΔV(r) = {\n   [(3 e)/(8 π ε₀ R³) (R² - r²/3 )] - [-e²/(4 π ε₀ r )], for r < R;\n   0, otherwise.\n}\n\nWait, but units aren't matching here. Hold on—the original potential was expressed without squared terms. Is that correct?\n\nActually, earlier mistake: When modeling the proton as a point charge, the potential is indeed V_p(r) = -e/(4 π ε₀ r ). Whereas, when treated as a sphere, the potential inside is V_s(r) = (3 e)/(8 π ε₀ R³ )(R² - r²/3 ). So the perturbation is ΔV(r) = V_s(r) - V_p(r) for r < R, else 0.\n\nHowever, looking again, does that make sense dimensionally? Both terms have dimensions of volts or energy per unit charge. Yes, seems consistent.\n\nBut wait, actually, the potential energies differ. To clarify, the potential energy U(r) felt by the electron is q*potential, where q=-e. So, in the case of the point charge, U_p(r) = -e*(-e)/(4 π ε₀ r ) = e²/(4 π ε₀ r ). Similarly, for the extended charge distribution, U_s(r) = -e*[ (3 e)/(8 π ε₀ R³ )(R² - r²/3 ) ]\n\nWait, hold on—that doesn't sound quite right. Let me re-examine.\n\nIf the proton creates a potential V(r), then the potential energy of the electron is U(r) = q V(r), where q is the"}
{"id": "quantum/1-1064", "questions": "A particle of mass $\\text m$ moves in one dimension in a periodic potential of infinite extent. The potential is zero at most places, but in narrow regions of width $b$ separated by spaces of length $a$ ($b \\ll a$) the potential is $V_0$, where $V_0$ is a large positive potential.\n\n[One may think of the potential as a sum of Dirac delta functions:\n\n$$\nV(x) = \\sum_{n=-\\infty}^{\\infty} V_0 b \\delta(x - na).\n$$\n\nAlternatively one can arrive at the same answer in a somewhat more messy way by treating the intervals as finite and then going to the limit.]\n\n(a) What are the appropriate boundary conditions to apply to the wave function, and why?\n\n(b) Let the lowest energy of a wave that can propagate through this potential be $E_0 = \\hbar^2 k_0^2 / 2m$ (this defines $k_0$). Write down a transcendental equation (not a differential equation) that can be solved to give $k_0$ and thus $E_0$.\n\n(c) Write down the wave function at energy $E_0$ valid in the region $0 \\le x \\le a$ (For uniformity, let us choose normalization and phase such that $\\psi(x = 0) = 1$). What happens to the wave function between $x = a$ and $x = a + b$?\n\n(d) Show that there are ranges of values of $E$ greater than $E_0$, for which there is no eigenfunction. Find (exactly) the energy at which the first such gap begins.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about a particle moving in a periodic potential. It's part (a), asking about the appropriate boundary conditions for the wave function. Hmm, okay, let me try to recall what I know about quantum mechanics in periodic potentials.\n\nFirst off, when dealing with particles in crystals or periodic structures, we often use Bloch's theorem. From what I remember, Bloch's theorem states that the wave function of a particle in a periodic potential has the form ψ_k(x) = e^{ikx} u_k(x), where u_k(x) is a periodic function with the same period as the lattice. So, if our potential repeats every 'a' units, then u_k(x+a) should equal u_k(x).\n\nBut wait, does that mean something specific about the boundary conditions? Well, since the system is translationally invariant modulo the period 'a', maybe the wave function must satisfy certain conditions over each unit cell. Specifically, after translating by 'a', the exponential factor would pick up a phase shift, while the rest remains periodic.\n\nSo perhaps the correct boundary condition is that the logarithmic derivative of the wave function is continuous across each period. Or another thought: because it's a crystal-like structure, the solutions will involve plane waves modulated by some envelope due to scattering from the periodic barriers.\n\nWait, actually, considering the tight-binding approximation isn't necessary here; instead, thinking about free electron gas models might help. In those cases, you impose periodic boundary conditions on the entire space, leading to standing waves. But in reality, especially for extended systems like metals, they're treated under Bloch-Floquet theory rather than strict periodic BCs.\n\nHold on, maybe the key point is that within each period, the wave function doesn't necessarily repeat exactly, but its behavior is consistent upon shifting by the period. That suggests that the ratio of the amplitude just before and after a barrier could relate via transmission coefficients, but given the periodic nature, these ratios multiply around the whole system, leading to quantization conditions.\n\nIn terms of solving Schrödinger's equation piecewise, say in each interval between barriers, the general solution would be a combination of exponentials or sinusoids depending on whether E > V or not. Since the question mentions \"the lowest energy of a wave that can propagate,\" probably implying E < V_0 somewhere, meaning evanescent waves outside the wells.\n\nTherefore, in each segment without the potential (i.e., between x=na and x=na+b), the wave decays exponentially, whereas inside the well (from x=na-b/2 to x=na+b/2?), assuming symmetric placement, the wave propagates as a sine/cosine.\n\nGiven all that, how do the boundary conditions come into play? At each interface between a conducting region (where E>V?) Wait, hold on—actually, in the model described, the potential is V0 only in small regions of width b, spaced apart by a distance a. So, except near x ≈ n*a, the potential is zero. Therefore, the particle experiences a series of very thin barriers.\n\nIf the particle's energy is less than V0, then in the regions where the potential is present (width b), the wave function would decay exponentially. However, since the barriers are narrow (b << a), tunneling effects become significant.\n\nTo find the allowed energies, we need to ensure continuity of the wave function and its derivative at each barrier edge. Because even though the potential is strong, being narrow allows partial penetration.\n\nMoreover, due to the periodicity, the overall wave function should obey Bloch's theorem, i.e., ψ(x + Na) = e^{iKNa}ψ(x) for any integer N, where K is the Bloch wavenumber related to the translational symmetry.\n\nPutting this together, the approach usually involves matching the amplitudes and derivatives at each barrier, using transfer matrices or similar methods, and ensuring consistency over multiple periods. This leads to a secular equation whose roots determine the allowed bands and gaps.\n\nSpecifically, for part (a), the boundary conditions likely require the wave function to be smooth everywhere, including at the edges of the potential barriers. Additionally, the requirement of Bloch's theorem imposes a relation on the phases accumulated over each period.\n\nThus, the main points are:\n1. Continuity of ψ and dψ/dx at each barrier.\n2. Application of Bloch's theorem, requiring ψ(x + a) proportional to ψ(x) multiplied by a phase factor.\n\nI think that covers the essential boundary conditions needed for this setup.\n\n**Final Answer**\n\nThe appropriate boundary conditions are the continuity of the wave function and its derivative at each barrier, along with Bloch’s theorem imposing periodicity. Thus,\n\n\\[\n\\boxed{\\text{Continuity of } \\psi \\text{ and } \\frac{d\\psi}{dx} \\text{ at each barrier, plus Bloch's theorem}}\n\\]\n</think>\n\nTo solve part (a) regarding the appropriate boundary conditions for the wave function in a periodic potential, we consider the following reasoning:\n\n1. **Periodic Potential and Bloch's Theorem**: The particle moves in a periodic potential with period \\(a\\). According to Bloch's theorem, the wave function takes the form \\(\\psi(x) = e^{ikx}u(x)\\), where \\(u(x)\\) is periodic with the same period as the potential. This implies that the wave function must respect the periodicity of the potential.\n\n2. **Boundary Conditions at Barriers**: The potential consists of narrow regions of width \\(b\\) separated by distances \\(a\\). Within these regions, the potential is high (\\(V_0\\)), causing the wave function to decay exponentially if the particle's energy is below \\(V_0\\). Outside these regions, the wave function behaves freely. To maintain the validity of the wave function, both the wave function itself and its derivative must be continuous at the boundaries of these potential regions.\n\n3. **Bloch Boundary Condition**: Due to the periodicity, the wave function must satisfy Bloch's boundary condition, which relates the wave function at positions differing by the period \\(a\\). This ensures that the wave function accumulates a phase factor upon translation by one period.\n\nCombining these considerations, the appropriate boundary conditions are the continuity of the wave function and its derivative at each barrier, along with Bloch's theorem imposing periodicity.\n\n### Final Answer\n\nThe appropriate boundary conditions are the continuity of the wave function and its derivative at each barrier, along with Bloch’s theorem imposing periodicity. Thus,\n\n\\[\n\\boxed{\\text{Continuity of } \\psi \\text{ and } \\frac{d\\psi}{dx} \\text{ at each barrier, plus Bloch's theorem}}\n\\]"}
{"id": "quantum/5072", "questions": "Suppose one has an electron in an $S$ state of a hydrogen atom which is in a magnetic field along the $z$ direction of strength $H_z$. At time $t = 0$ a magnetic field along the $x$ direction is turned on; its strength increases uniformly from zero to $H_x$ at time $t = T (H_x \\ll H_z)$ and then remains constant after $t = T$. Assume the nucleus has no spin and that:\n\n$$\n\\left( \\frac{H_x}{H_z} \\right)^2 \\ll \\frac{H_x}{H_z}.\n$$\n\nConsider only the interaction between the electron spin and the magnetic field. Neglect all terms of order $\\left(\\frac{H_x}{H_z}\\right)^2$ or higher. If the electron has its spin in the $z$ direction at $t = 0$, find the state of the electron when $t = T$. Show that the state is an eigenstate of the Hamiltonian due to the combined magnetic fields $\\mathbf{H} = (H_x, 0, H_z)$ provided that $T$ is sufficiently long. Explain what sufficiently long means in this case.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about an electron in a hydrogen atom subjected to two magnetic fields: initially along z, and then suddenly another along x gets turned on over some time period. The goal is to figure out the state of the electron's spin at time t=T, considering only the interaction with these magnetic fields.\n\nFirst off, let me recall how electrons interact with magnetic fields. Electrons have intrinsic angular momentum called spin, which interacts with magnetic fields through something called the Zeeman effect. So, if there's a magnetic field, it will cause the energy levels of the electron to split depending on their spin orientation relative to the field.\n\nIn this scenario, we start with the electron in an S-state, meaning l=0, so orbital angular momentum doesn't contribute—only spin matters here. Initially, the magnetic field Hz is applied along the z-axis. Since the nucleus has no spin, the total angular momentum is just the electronic spin. \n\nAt t=0, they turn on a magnetic field along the x-direction, increasing linearly until t=T where it reaches Hx, much smaller than Hz because Hx << Hz. Also, given that (Hx/Hz)^2 is negligible compared to Hx/Hz, so any quadratic terms can be ignored. That probably simplifies things since perturbation theory might come into play here.\n\nThe initial state is the spin pointing along z. Let me denote the up-spin as |↑⟩_z and down-spin as |↓⟩_z. But wait, actually for spin-1/2 particles like electrons, the states are usually written as |+½⟩ and |-½⟩ along a particular axis. In our case, starting along z, so maybe better notation would be |↑⟩_z and |↓⟩_z.\n\nNow, before anything happens, the Hamiltonian is just the Zeeman term for Hz. After turning on Hx, the Hamiltonian becomes more complicated—it includes both Hz and Hx components. However, since Hx starts from zero and turns on gradually, perhaps adiabatically? Wait, but the question says \"turned on\" not necessarily adiabatic. Hmm.\n\nWait, hold on—the way the problem describes it, the Hx field goes from zero to Hx over time T. It’s increased uniformly, so the rate dH/dt is Hx/T. Then, once it hits Hx at t=T, it stays constant. We need to consider the transition during this ramping process.\n\nBut also, importantly, neglect terms beyond first-order in (Hx/Hz). Because (Hx/Hz) squared is even smaller, so treating Hx as small compared to Hz allows us to use perturbative methods.\n\nSo, thinking about the system evolution under a slowly changing Hamiltonian. Maybe using time-dependent perturbation theory?\n\nLet me outline the steps:\n\n1. **Initial State**: Electron spin aligned along +z direction. So, ψ(t=0) = |↑⟩_z.\n\n2. **Hamiltonians**:\n   - Before t=0: Only Hz acts.\n     H_initial = γ * μ_B * Hz * σ_z / 2,\n     where γ is gyromagnetic ratio, μ_B Bohr magneton, σ_z Pauli matrix.\n   \n   - From t=0 onwards: Both Hz and Hx act, but Hx increases linearly from 0 to Hx over [0,T].\n     So, H(t) = γμ_B [Hz σ_z + f(t)σ_x]/2,\n     where f(t)= (Hx/T)t for 0 ≤ t ≤ T, and f(T)=Hx.\n\n3. **Interaction Picture**: To handle time dependence, switch to interaction picture with respect to H_initial. Or maybe directly apply time-dependent perturbation.\n\n4. **Perturbation Approach**: Treat Hx as a perturbation added onto Hz. Given that Hx ≪ Hz, treat Hx as a small addition.\n\n5. **Time Evolution**: Compute the probability amplitudes for transitions from |↑⟩_z to |↓⟩_z under the influence of the time-varying Hx.\n\n6. **Adiabatic vs Sudden Approximation**: Depending on whether T is large enough, different approximations may apply. For very slow changes (adiabatic), the system stays in instantaneous eigenstates. Here, though, since Hx is being turned on, maybe the system follows the eigenvectors of the new Hamiltonian.\n\n7. **Eigenstates of Combined Field**: Once Hx is fully on, the Hamiltonian combines Hz and Hx. Its eigenstates are superpositions of |↑⟩_z and |↓⟩_z. Specifically, the eigenstates should align along the effective magnetic field vector (which now points in x-z plane).\n\n8. **Transition Amplitude Calculation**: Using Fermi's Golden Rule or time-dependent perturbation formulae, compute c_down(t) such that ψ(t) = |↑⟩_z + c_down(t)|↓⟩_z.\n\n9. **Final State at t=T**: Determine the coefficients and show that the resulting state matches an eigenstate of the full Hamiltonian when T is sufficient.\n\nAlright, getting started.\n\nExpressed mathematically, the unperturbed Hamiltonian is H0 = (γμB)(Hz σ_z)/2. The perturbing part is V(t) = (γμB)f(t)σ_x /2, where f(t) = (Hx/T) t for t ∈ [0,T].\n\nWe'll work in units where ħ=1 for simplicity.\n\nSince the initial state is |↑⟩_z, which is an eigenstate of H0 with energy E_up = (γμB Hz)/2.\n\nTo calculate the amplitude c_down(t), we can use first-order time-dependent perturbation theory. The general expression is:\n\nc_n^(1)(t) = (-i/ħ) ∫_{0}^{t} ⟨n|V(t')|i⟩ e^{iω_{ni} t'} dt'\n\nWhere ω_{ni}=(E_n^0 - E_i^0)/ħ.\n\nHere, n corresponds to |↓⟩_z, i is |↑⟩_z.\n\nCompute ⟨down|V(t)|up⟩:\n\n⟨↓| (γμB f(t) σ_x /2 ) |↑⟩ = (γμB f(t))/2 * ⟨↓|σ_x|↑⟩.\n\nRecall that σ_x |↑⟩ = |↓⟩, so ⟨↓|σ_x|↑⟩ = 1.\n\nThus,\n\nc_down(t) ≈ (-i) ∫₀ᵗ [(γμB f(t'))/2] e^{i(E_down - E_up) t' } dt'.\n\nNote that E_down - E_up = -(γμB Hz)/2 - (γμB Hz)/2 = -γμB Hz.\n\nTherefore, exponent factor is exp(-i γμB Hz t'). Plugging back in:\n\nc_down(t) = (-i γμB / 2) ∫₀ᵗ f(t') e^{-i γμB Hz t'} dt'.\n\nGiven f(t') = (Hx / T) t', substitute:\n\nc_down(t) = (-i γμB Hx)/(2T) ∫₀ᵗ t' e^{-i γμB Hz t'} dt'.\n\nIntegrate t' e^{-i k t'}, where k = γμB Hz.\n\nIntegration by parts:\n\n∫ t e^{-ik t} dt = (-i/k) t e^{-ik t} + (i/k²) e^{-ik t}\n\nEvaluated from 0 to t:\n\n[ (-i/k) t e^{-ik t} + (i/k²) e^{-ik t} ] - [0 + (i/k²)]\n\nWhich gives:\n\n(-i/k) t e^{-ik t} + (i/k²)(e^{-ik t} - 1)\n\nPutting it together:\n\nc_down(t) = (-i γμB Hx)/(2T) [ (-i/(k)) t e^{-ik t} + (i/(k²))(e^{-ik t} - 1) ]\n\nSubstitute k = γμB Hz:\n\nFactor constants:\n\nMultiply each term inside brackets by (-i γμB Hx)/(2T):\n\nTerm 1: (-i γμB Hx)/(2T) * (-i/(γ"}
{"id": "quantum/3-3007", "questions": "A particle in a central potential has an orbital angular momentum $l = 2 \\hbar$ and a spin $s = 1 \\hbar$. Find the energy levels and degeneracies associated with a spin-orbit interaction term of the form $H_{s0} = A \\mathbf{L} \\cdot \\mathbf{S}$, where $A$ is a constant.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about a particle in a central potential with given orbital angular momentum and spin. The task is to find the energy levels and their degeneracies due to a spin-orbit interaction Hamiltonian H_so = A L·S.\n\nFirst off, let me recall what I know about angular momentum coupling. Orbital angular momentum (L) and spin angular momentum (S) can combine to give total angular momentum J. So maybe that's relevant here since the Hamiltonian involves the dot product of L and S.\n\nThe operator L·S appears in the expression for J² because:\n\nJ = L + S\n\nSo,\n\nJ² = (L + S) · (L + S) = L² + S² + 2 L·S\n\nTherefore, we can solve for L·S as:\n\nL·S = [J² - L² - S²] / 2\n\nThat seems useful because if I express H_so in terms of J², then perhaps it becomes diagonal in some basis related to quantum numbers j, l, s.\n\nGiven that, substituting into H_so gives:\n\nH_so = A * [J² - L² - S²]/(2)\n\nNow, each of these operators should be proportional to identity matrices when acting on states with definite quantum numbers. That means eigenvalues will just depend on the corresponding quantum numbers squared times ħ² or something like that.\n\nWait, but actually, more precisely, the expectation value would involve the quantum numbers multiplied appropriately.\n\nLet me think: For any state |j, m_j>, which is an eigenstate of J² and J_z, etc., the expectation values of L², S², and J² are known.\n\nSpecifically,\n\n< L² > = l(l+1)ħ²,\n< S² > = s(s+1)ħ²,\nand < J² > = j(j+1)ħ².\n\nBut wait, no—actually, those are the eigenvalues themselves for the respective operators. Since our Hamiltonian now depends only on these quadratic terms, each term in H_so will contribute based on these eigenvalues.\n\nThus, plugging back into H_so:\n\nE_so = A/2 [ j(j+1)ħ² - l(l+1)ħ² - s(s+1)ħ² ] \n\nBecause all three terms are scalars; they don't mix different states once you're in a specific j, l, s manifold.\n\nHmm, okay, so E_so is determined entirely by the quantum number j, along with fixed l and s from the problem statement.\n\nIn the problem, l=2ħ and s=1ħ? Wait, hold on—that notation might need clarification. Usually, l is an integer such that L² = l(l+1)ħ². Similarly, s is another integer (or half-integer). But sometimes people write l=2 meaning l=2 units of ħ, i.e., l=2 implies L² = 6ħ²?\n\nYes, exactly. In standard terminology, l is an integer, say l=0,1,2,... Then L² = l(l+1)ħ². Same for s—if s=1, then S²=s(s+1)ħ²=2ħ².\n\nSimilarly, j takes values |l-s| ≤ j ≤ l+s in integer steps.\n\nSince l=2 and s=1, possible j values are 3, 2, 1. Because starting from |2−1|=1 up to 2+1=3.\n\nEach distinct j corresponds to a different energy level under H_so because E_so depends linearly on j(j+1).\n\nMoreover, each j multiplet has its own degeneracy, which is (2j + 1), right? Each j level splits into (2j +1) states with magnetic quantum numbers m_j ranging from -j to +j.\n\nHowever, before jumping to conclusions, let me make sure whether the initial system without considering spin had certain symmetries or not. If the original potential was central, then yes, the z-component of angular momentum Lz commutes with the Hamiltonian, hence good quantum numbers include m_l. However, introducing spin-orbit coupling mixes different m_l states unless combined with spin projections.\n\nBut regardless, after including spin-orbit coupling, the total angular momentum J becomes a better quantum number than individual L and S components. Therefore, the energy levels split according to j, each having a degeneracy of (2j +1)(something else?) Hmm, wait—the overall Hilbert space dimensionality needs consideration.\n\nOriginal orbital angular momentum l=2 has (2l +1)=5 states. Spin s=1 has (2s +1)=3 states. Without spin-orbit coupling, the total Hilbert space is 5×3=15-dimensional. When spin-orbit coupling is introduced via H_so ~ L·S, the states become coupled across different m_l and m_s combinations, leading to new eigenstates labeled by j and m_j.\n\nFor each allowed j (which are 1,2,3 in this case), there are (2j +1) states per j. Let's compute how many dimensions each j contributes:\n\n- j=3: 7 states\n- j=2: 5 states\n- j=1: 3 states\n\nAdding them together: 7 + 5 + 3 = 15, which matches the original 15-dimensional space. Good, consistency check passed.\n\nTherefore, the energies correspond to each j value, with multiplicities equal to (2j +1):\n\nEnergy for j=3:\nE_3 = A/(2ħ²)[3*4 - 2*3 -1*2]*ħ² = A[12 -6 -2]/2 = A*(4)/2 = 2A\n\nWait, hold on—I may have messed up substitution earlier. Let me re-express E_so correctly.\n\nFrom above:\n\nE_so = (A)/(2) [j(j+1) - l(l+1) - s(s+1)] * ħ² ?\n\nNo, wait, looking again:\n\nWe had H_so = A(L·S) = A/[2](J² - L² - S² )\n\nAnd each operator’s eigenvalue is:\n\n<L²> = l(l+1)ħ²,\n\n<S²> = s(s+1)ħ²,\n\n<J²> = j(j+1)ħ².\n\nThen,\n\n<H_so> = A/(2) [j(j+1) - l(l+1) - s(s+1)] * ħ²\n\nIs that correct? Yes, because each term is divided by 2.\n\nAlternatively, factor out ħ²:\n\nE_so = (A ħ² )/(2) [j(j+1) - l(l+1) - s(s+1)]\n\nPlugging in l=2, s=1:\n\nCompute inside brackets:\n\n[j(j+1) - 2*3 -1*2]\n\nWhich simplifies to:\n\n[j(j+1) -6 -2]= [j(j+1)-8]\n\nMultiply by (A ħ²)/2:\n\nE_so = (A ħ² /2 )(j^2 +j -8 )\n\nSo depending on j=1,2,3:\n\nCalculate for each j:\n\nCase j=1:\n\nE_1=(A ħ² /2)(1 +1 -8)=(A ħ² /2)(-6)= (-3 A ħ² )\n\nCase j=2:\n\nE_2=(A ħ² /2)(4 +2 -8)=(A ħ² /2)(-2)=(-A ħ² )\n\nCase j=3:\n\nE_3=(A ħ² /2)(9 +3 -8)=(A ħ² /2)(4)=2 A ħ²\n\nSo the energy levels are at -3Aħ², -Aħ², and 2Aħ².\n\nEach of these levels has a degeneracy of (2j +1): 3,5,7 respectively.\n\nTo recapitulate:\n\n- Energy Level 1: E=-3Aħ², Degeneracy=3\n- Energy Level 2: E=-Aħ², Degeneracy=5\n- Energy Level 3: E=2Aħ², Degeneracy=7\n\nI believe that makes sense. To double-check, summing the degeneracies: 3+5+7=15, matching the original 15-dimensional space. Also, the ordering goes from lower j giving lower energy (if A positive?), assuming A is positive. Alternatively, if A were negative, higher j could mean lower energy—but the question doesn’t specify sign of A, so probably leave it general.\n\nAnother point to consider: does the order matter? Well, typically, larger j leads to stronger spin-orbit coupling effect, but direction depends on the coefficient A. Anyway, the calculation remains consistent irrespective of A's sign.\n\nJust making sure my algebra didn't slip anywhere. Rechecking calculations:\n\nFor j=1:\n\nj(j+1)=2, subtracting l(l+1)=6 and s(s"}
{"id": "quantum/5075", "questions": "A particle is initially in its ground state in a box with infinite walls at 0 and $L$. The wall of the box at $x = L$ is suddenly moved to $x = 2L$.\n\n(a) Calculate the probability that the particle will be found in the ground state of the expanded box.\n(b) Find the state of the expanded box most likely to be occupied by the particle.\n(c) Suppose the walls of the original box $[0, L]$ are suddenly dissolved and that the particle was in the ground state. Construct the probability distribution for the momentum of the freed particle.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about quantum mechanics involving particles in boxes. It's part (a), (b), and (c). Let me try to tackle each one step by step. \n\nStarting with part (a): A particle is initially in its ground state in a box from x=0 to x=L. Then the right wall moves to x=2L. We need to find the probability it stays in the new ground state.\n\nHmm, okay. So when the box expands, the wavefunction doesn't instantly change; instead, it remains as it was before but now exists within a larger potential well. To find the probability of being in the new ground state, we'll probably use some sort of overlap integral between the old and new states.\n\nThe initial wavefunction ψ₁(x) for the ground state in [0,L] should be sqrt(2/L) sin(πx/L). After expansion, the new ground state φ₁(y) would be sqrt(1/(2L)) sin(πy/(2L)). Wait, because the length doubled, so n=1 here too?\n\nSo the probability P is |⟨φ₁|ψ₁⟩|². That means integrating over all space where both functions exist, which is from 0 to L since beyond that, ψ₁ is zero. But wait, actually after expansion, does the wavefunction extend? Or do we consider only up to L or up to 2L?\n\nWait no, the wavefunctions are defined on their respective intervals. Before expansion, ψ₁ is non-zero only from 0 to L. After expansion, φ₁ is non-zero from 0 to 2L. Since the system abruptly changes, the wavefunction immediately becomes Ψ(x)=ψ₁(x) multiplied by something... Hmm, maybe not exactly. Actually, upon sudden expansion, the wavefunction doesn’t collapse instantaneously—it just continues evolving under the new Hamiltonian. Therefore, the coefficients c_n can be calculated via inner products ⟨φₙ|Ψ_initial⟩.\n\nRight, so more precisely: When you expand the box, the new eigenstates are φₙ(x) = sqrt(1/(2L)) sin(n π x / (2L)), for n=1,2,... And our initial state is ψ₁(x) = sqrt(2/L) sin(π x / L).\n\nTo express ψ₁ in terms of the new basis {φₙ}, compute cₙ = ∫₀^{2L} φₙ*(x) ψ₁(x) dx. However, note that ψ₁(x) is zero outside [0, L], so effectively, the integral reduces to [0, L].\n\nTherefore, c₁ = ∫₀^L [sqrt(1/(2L))] sin(π x / (2L)) * [sqrt(2/L)] sin(π x / L) dx\n\nSimplify constants first:\n\nsqrt(1/(2L)) * sqrt(2/L) = sqrt((1)(2)/(2L*L)) ) = sqrt(1/L²) = 1/L\n\nThen inside the integral: sin(π x / (2L)) * sin(π x / L)\n\nUse trig identity: sin(A)sin(B) = [cos(A-B)-cos(A+B)]/2\n\nLet’s set A = πx/(2L), B = πx/L = 2πx/(2L). Thus,\n\nsin(A)sin(B) = [ cos(-πx/(2L)) - cos(3πx/(2L)) ] / 2\n\nWhich simplifies to [cos(πx/(2L)) - cos(3πx/(2L))]/2\n\nThus, the integral becomes:\n\n∫₀^L [cos(πx/(2L)) - cos(3πx/(2L))]/2 dx\n\nMultiply by 1/L:\n\nc₁ = (1/L)*(1/2)[ ∫₀^L cos(πx/(2L)) dx - ∫₀^L cos(3πx/(2L)) dx ]\n\nCompute integrals separately.\n\nFirst integral: let u = πx/(2L); du = π/(2L)dx => dx = (2L)/π du\n\nLimits: x=0 → u=0; x=L→u=π/2\n\nIntegral becomes ∫₀^{π/2} cos(u) * (2L)/π du = (2L)/π [sin(u)]₀^{π/2}= (2L)/π (1 - 0) = 2L/π\n\nSecond integral: similarly, v = 3πx/(2L); dv=(3π)/(2L)dx ⇒ dx=(2L)/(3π)dv\n\nLimits same: x=0→v=0;x=L→v=3π/2\n\nIntegral becomes ∫₀^{3π/2} cos(v) * (2L)/(3π) dv = (2L)/(3π)[sin(v)]₀^{3π/2}= (2L)/(3π)[-1 - 0]= (-2L)/(3π)\n\nPutting back into expression:\n\nc₁ = (1/L)*(1/2)[ (2L/π) - (-2L/(3π)) ] = (1/(2L))[ (2L/π + 2L/(3π)) ] ?\n\nWait hold on, perhaps miscalculating signs.\n\nWait second integral result was negative: (2L)/(3π)*[-1]\n\nBut then subtracting that gives minus times negative, positive.\n\nWait, let me re-express:\n\nInside brackets: First integral gave 2L/pi, Second integral gave -2L/(3 pi). Subtract them: 2L/pi - (-2L/(3pi)) = 2L/pi + 2L/(3 pi) = (6L + 2L)/(3 pi) = 8L/(3 pi)\n\nHence, overall:\n\nc₁ = (1/L)*(1/2)*(8L/(3 pi)) ) = (1/L)*(4L/(3 pi)) ) = 4/(3 pi )\n\nSo modulus squared is |c₁|^2 = (16)/(9 pi² ). Approximately, but exact value needed.\n\nAlternatively, did I make any mistake in calculation steps?\n\nWait, double-check the integration limits and substitutions.\n\nFor the first integral, substitution correct: yes, got 2L/pi.\n\nSecond integral: Integral of cos(3πx/(2L)) from 0 to L.\n\nAt upper limit x=L, argument is 3π/2, whose cosine is 0, sine is -1.\n\nYes, so sin(3π/2)=-1, hence integral evaluates correctly.\n\nSubtracting these two results: 2L/pi - (-2L/(3 pi )) = indeed 8L/(3 pi ).\n\nMultiplying by 1/(2L):\n\n(8L/(3 pi )) * (1/(2L)) )= 4/(3 pi ), yeah.\n\nSo c₁=4/(3 pi ). Hence, probability is (4/(3 pi ))^2 ≈ 16/(9 pi² ). Which is approximately 16/(9*9.87)≈16/88.8≈0.18, around 18%.\n\nIs there another way to think about this? Maybe using orthogonality conditions or Fourier series expansions.\n\nI don't see immediate mistakes, seems consistent.\n\nMoving on to part (b): Find the state of the expanded box most likely to be occupied by the particle.\n\nThis suggests finding the maximum |cₙ|. From part (a), we saw how to calculate c₁. Similarly, other cₙ involve similar integrals.\n\nGiven symmetry, perhaps even n might contribute differently than odd n? Not sure yet.\n\nIn general, cₙ = √(1/(2L))√(2/L) ∫₀^L sin(n π y / (2L)) sin(π y / L) dy\n\nAgain, product of sines, apply identity:\n\nsin(a)y sin(b)y = [cos(a-b)y - cos(a+b)y]/2\n\nHere, a=n pi/(2L), b= pi/L=2 pi/(2L). So a= n pi/(2L), b=2 pi/(2L)= pi/L.\n\nThus, sin(n pi y/(2L)) sin(pi y/L) = [cos( (n-2) pi y/(2L) ) - cos( (n+2) pi y/(2L) ) ] / 2\n\nIntegrate term-wise from 0 to L.\n\nEach integral becomes:\n\n∫₀^L cos(k pi y/(2L)) dy = [2L/(k pi)] sin(k pi y/(2L)) evaluated from 0 to L.\n\nAt y=L: k pi/(2L)*L=k pi/2.\n\nIf k is integer multiple, say k=m,"}
{"id": "quantum/6052", "questions": "An $^{27}\\text{Al}$ nucleus is bound in a one-dimensional harmonic oscillator potential with natural frequency $w$. Label the states as $\\psi_{m \\alpha} = \\psi_m(x)\\phi_{\\alpha}$, where $\\psi_m(x)$, $m = 0, 1, 2, \\ldots$, an eigenstate of the harmonic oscillator potential, describes center-of-mass motion and $\\phi_{\\alpha}(x)$, $\\alpha = 0, 1, 2, \\ldots$, is the wave function specifying the intrinsic nuclear state. Suppose the nucleus is initially in the state $\\psi_0(x)\\phi_1$ and then decays to the ground state $\\phi_0$ by emitting a photon in the $-x$ direction. Assume that the nuclear excitation energy is much greater than the harmonic oscillator excitation energy: $ E^* = (E_{\\alpha = 1} - E_{\\alpha = 0}) \\gg \\hbar \\omega $.\n\n(a) What is the wave function for the nucleus after the photon has been emitted?\n\n(b) Write an expression for the relative probability $ P_1/P_0 $, where $ P_n $ is the probability for the nucleus to be left in the state $\\psi_{n0} = \\psi_n(x)\\phi_0$.\n\n(c) Estimate numerically $ P_1/P_0 $ with $ E^* = 840 \\, \\text{keV} $ and $ \\hbar \\omega = 1 \\, \\text{keV} $.\n\n", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about an aluminum-27 nucleus in a one-dimensional harmonic oscillator potential. The setup involves some quantum mechanics concepts like wave functions, decay processes, and probabilities. Let me try to unpack each part step by step.\n\nStarting with part (a): It asks for the wave function of the nucleus after it emits a photon and transitions from ψ₀φ₁ to φ₀. Hmm, okay. So initially, the nucleus is in the state ψ₀(x)φ₁. When it decays, it goes to the ground state φ₀ but also emits a photon. Since the emission is in the -x direction, does that affect the momentum or something? \n\nI remember that when a system emits a photon, there's conservation of momentum involved. But wait, since we're dealing with a nucleus in a harmonic oscillator, maybe the recoil affects its position or momentum. Also, because the nuclear excitation energy E* is much larger than ℏω, which suggests that the change due to the photon might not significantly alter the overall state beyond causing a small shift in the oscillator state.\n\nSo, if the nucleus was originally in ψ₀, which is the lowest energy state of the oscillator, upon emitting a photon, perhaps it gets excited into another oscillator state. Wait, no—emitting a photon usually means losing energy, right? Or does it depend on whether the nucleus gains or loses energy during the transition?\n\nWait, actually, the nucleus itself isn't changing; rather, it's transitioning between internal states while possibly moving within the oscillator. Maybe the process causes a displacement in the oscillator state. Alternatively, considering that the photon carries away some momentum, leading to a shift in the center-of-mass motion described by ψₘ.\n\nBut how exactly would that translate into the new wavefunction? If the initial state is ψ₀φ₁, and it decays to φ₀, what happens to the spatial part ψ₀? Does it stay the same, or does it get modified?\n\nHmm, thinking more carefully: In such decay processes, especially spontaneous emission, the atom (or here, the nucleus) can emit a photon carrying away some momentum, resulting in a slight shift in the center-of-mass motion. However, given that E* >> ℏω, the energy imparted to the oscillator mode should be relatively small compared to the total energy difference. Therefore, the main effect is probably just a small displacement in the oscillator state.\n\nAlternatively, could it cause a transition from m=0 to m=1? Because emitting a photon typically changes the angular momentum, but in a linear case, maybe it increases the vibrational level. Wait, but photons carry spin angular momentum, whereas the oscillator levels involve orbital angular momentum... Not sure if they directly correspond.\n\nWait, let's think differently. The nucleus starts in ψ₀φ₁. After emitting a photon, it must go to ψ_nφ₀, where n depends on the energy released. Given that E* ≫ ℏω, most of the energy comes from the nuclear transition, but the remaining energy will determine the possible n.\n\nThe energy before emission is E_initial = E_oscillator(ψ₀) + E_phi1. Similarly, after emission, it's E_final = E_photon + E_oscillator(ψ_n) + E_phi0.\n\nSince E_phi1 - E_phi0 = E*, and assuming the photon takes away approximately E*, leaving behind any excess in the oscillator. But since E* >> ℏω, the leftover energy would still allow multiple quanta excitations, but likely only a single quantum jump because higher jumps require more precise matching of energies.\n\nTherefore, the nucleus may transition to either ψ₁φ₀ or remain mostly in ψ₀φ₀, depending on selection rules. For a harmonic oscillator, the allowed transitions are Δm = ±1, corresponding to absorption/emission of a phonon. Here, the nucleus emits a photon, which effectively acts similarly—it should lower the oscillator state by one, but since it started at m=0, going down isn't possible without negative indices, which don't exist. Hence, maybe it stays in m=0 or moves up to m=1.\n\nWait, hold on—if you start at m=0 and emit a photon, which requires lowering the oscillator state, but m cannot go below zero. That seems contradictory. Unless instead, the emission doesn't necessarily decrease m but allows for a different kind of transition.\n\nPerhaps my confusion arises from conflating two types of emissions: one involving the internal state and the other the external oscillator. Actually, the photon emission corresponds to the nuclear transition from α=1 to α=0, releasing energy E*. This energy is carried away by the photon, whose wavelength relates to E*. Meanwhile, the center-of-mass motion remains in the oscillator, potentially being displaced slightly.\n\nIn terms of wavefunctions, the initial state is |ψ₀⟩|φ₁⟩. Upon emitting a photon, the nuclear state becomes |φ₀⟩, and the photon carries away some momentum p. By conservation of momentum, the nucleus should acquire a recoil momentum opposite to the photon's. Thus, the center-of-mass wavefunction would undergo a translation.\n\nHowever, translating a Gaussian wave packet (which ψ₀ is) results in a phase factor multiplied by e^{-ikx}, but in discrete basis states, this translates into a superposition over all m states weighted by their overlap with the translated wavefunction.\n\nGiven that the recoil imparts a certain momentum kick, the resulting state would be a sum over various m states. Specifically, using the expansion:\n\ne^{i k x} ψ₀(x) = Σ c_m ψ_m(x)\n\nwhere k is related to the photon's momentum. Then, the coefficients c_m give the amplitude for each m state.\n\nThus, the post-emission state would be a combination of ψ_m(x)|φ₀⟩ summed over m, with coefficients determined by the Fourier transform of ψ₀ under the translation induced by the photon emission.\n\nHence, the wave function after emission is a superposition across different m values times φ₀. To write it explicitly, it'd look like Σ c_m ψ_m(x) φ₀.\n\nBut the question says \"the wave function\", implying perhaps the dominant term(s). Considering that E* >> ℏω, the recoil momentum imparted is significant enough to cause a noticeable shift in the oscillator state. But since the initial state is m=0, the first-order approximation would suggest mainly m=±1 components, though m=-1 doesn't make sense, so primarily m=1.\n\nWait, but again, starting from m=0, you can’t go to m=-1, so the next available is m=1. Is that correct? Or do you need to consider both positive and negative shifts?\n\nActually, the photon is emitted in the -x direction, meaning the nucleus recoils in the +x direction. Translating the wavepacket in +x would lead to a coherent state shifted towards positive positions, which in Fock space would result in a mixture of even and odd m states, but predominantly around m≈k/(√(ℏ/mω)) ), but quantized.\n\nBut perhaps, given that the energy E* is large, the recoil velocity v_recoil ~ sqrt(E*/M), M being the mass of Al-27. Which would relate to the momentum transfer p = hν /c * ... hmm, getting confused now.\n\nMaybe stepping back: the key idea is that the emission leads to a displacement in the oscillator state, creating a superposition of neighboring m states. Due to the high energy E*, the displacement is substantial, making several m states contribute. But since E*>>ℏω, the number of accessible m states is roughly O((E*)/ℏω)^½, but exact calculation needed.\n\nBut regardless, the general form is a sum over m of c_m ψ_m(x) φ₀. Without knowing the specifics of the matrix elements, we can say that the wave function is a superposition of"}
{"id": "quantum/1-1063", "questions": "Give a brief discussion of why there are energy bands in a crystalline solid. Use the ideas of quantum mechanics but do not attempt to carry out any complicated calculations. You should assume that anyone reading your discussion understands quantum mechanics but does not understand anything about the theory of solids.", "graphs": null, "llm_answers": "<think>\nOkay, so I need to explain why there are energy bands in a crystalline solid using quantum mechanics concepts without getting into complex math or prior knowledge of solid-state theory. Hmm.\n\nFirst, let me recall what I know about crystals. A crystal has atoms arranged periodically, right? So each atom is spaced regularly in all directions. Now, when it comes to electrons around these atoms, they're influenced by multiple nuclei because of this periodic structure.\n\nIn isolated atoms, electrons have discrete energy levels due to quantization—like how n=1, 2, etc., work. But in a crystal, since the atoms are close together, their atomic orbitals must interact with neighboring ones. This interaction probably changes those individual energy levels somehow.\n\nWait, from quantum mechanics, particles like electrons can't be treated as independent if they're near other potentials. The idea here might be similar to particle-in-a-box problems where boundary conditions affect allowed energies. Maybe the periodic potential leads to some kind of standing wave solutions for electrons?\n\nOh yeah! Bloch's theorem says that in a periodic potential, electron waves take on a specific form: ψ_k(x) = e^{ikx} u_k(x), where u_k is periodic with the same period as the lattice. That means the electron doesn't just have free-particle plane waves; instead, its behavior is modulated by the crystal's periodicity.\n\nSo, considering many such states (different k values), we get ranges of possible energies rather than single points. These continuous ranges become the energy bands. When you plot E vs. k, you see regions where electrons can exist (bands) separated by gaps where no states are allowed.\n\nBut wait, why exactly do these interactions lead to bands instead of remaining separate lines? Because when you bring atoms together, their atomic orbitals overlap. Each orbital splits into several states with slightly different energies. For example, two p-orbitals overlapping would split into bonding and antibonding combinations. In a crystal, which has many more overlaps, this splitting becomes extensive enough to create broadened energy ranges—the bands.\n\nAlso, thinking about perturbation theory, even small interactions between adjacent atoms cause shifts in energy levels. Over millions of atoms, these tiny shifts accumulate, leading to extended bands instead of isolated lines.\n\nAnother point is symmetry. Crystals have translational symmetry, meaning repeating units every lattice constant 'a'. Quantum mechanically, symmetries impose constraints on wavefunctions. Here, the translational symmetry enforces the formation of these Bloch functions, whose properties directly relate to the band structures observed.\n\nPutting it all together: the regular arrangement of atoms creates a periodic potential experienced by conduction electrons. According to Bloch’s theorem, electrons adopt wave-like states within this potential, resulting in energy being a function of both momentum (k) and position through the periodic part. As the number of interacting atoms increases, the originally discrete energy levels broaden into contiguous bands.\n</think>\n\nEnergy bands arise in crystalline solids due to the interplay of quantum mechanical principles and the periodic structure of the material. \n\n1. **Periodic Potential**: Atoms in a crystal are arranged periodically, creating a periodic potential field that influences the motion of electrons. Unlike isolated atoms, where electrons occupy distinct energy levels, the periodic nature causes the atomic orbitals to interact across the entire crystal lattice.\n\n2. **Bloch's Theorem**: Electrons in a periodic potential exhibit wave-like behavior described by Bloch's theorem. Their wavefunctions take the form \\( \\psi_k(x) = e^{ikx}u_k(x) \\), where \\( u_k(x) \\) repeats the crystal's periodicity. This modulation introduces dependencies on both momentum (\\( k \\)) and spatial variations, leading to energy becoming a function of \\( k \\).\n\n3. **Splitting and Band Formation**: Interactions between nearby atomic orbitals result in the splitting of energy levels. With numerous atoms contributing, these splittings extend into continuous ranges called energy bands. Originally discrete levels broaden into bands due to collective interactions over vast numbers of atoms.\n\n4. **Symmetry Considerations**: The translational symmetry inherent in crystals imposes constraints on electronic wavefunctions, enforcing the formation of Bloch states. This symmetry underpins the emergence of band structures, linking them fundamentally to the crystal's geometry.\n\nThus, the combination of quantum mechanical effects and the periodic arrangement of atoms results in the characteristic energy bands seen in crystalline materials."}
{"id": "quantum/3-3009", "questions": "Suppose an electron is in a state described by the wave function\n\n$$\n\\psi = \\frac{1}{\\sqrt{4\\pi}} \\left( e^{i\\phi} \\sin \\theta + \\cos \\theta \\right) g(r) ,\n$$\n\nwhere\n\n$$\n\\int_{0}^{\\infty} |g(r)|^2 r^2 \\, dr = 1 ,\n$$\n\nand $\\phi, \\theta$ are the azimuth and polar angles respectively.\n\n(a) What are the possible results of a measurement of the z-component $L_z$ of the angular momentum of the electron in this state?  \n\n(b) What is the probability of obtaining each of the possible results in part (a)?  \n\n(c) What is the expectation value of $L_z$?", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about quantum mechanics involving an electron's wavefunction. Let me try to figure it out step by step. \n\nThe wave function given is:\n\nψ = [1/√(4π)] * (e^(iφ) sinθ + cosθ) * g(r)\n\nAnd we know that ∫₀^∞ |g(r)|² r² dr = 1, which means g(r) is normalized with respect to radial coordinates. The question has three parts: finding possible Lz measurements, their probabilities, and the expectation value of Lz.\n\nStarting with part (a): Possible results for measuring Lz.\n\nI remember that the operator for the z-component of angular momentum is ħ times the derivative with respect to φ. So,\n\nLz = -iħ (∂/∂φ)\n\nIn spherical coordinates, eigenstates of Lz correspond to states where ψ depends on φ as e^(imφ), where m is an integer or half-integer depending on whether the particle is bosonic or fermionic. Since electrons are fermions, but here maybe since they're talking about orbital angular momentum, not spin, perhaps m can be any integer?\n\nWait no, actually, even for orbitals, l must be non-negative integers, and m ranges from -l to +l in integer steps. But wait, our wavefunction isn't necessarily just one such state; it might be a combination.\n\nLooking at the given ψ, let's see its dependence on φ. It seems like there's only a term multiplied by e^(iφ). Wait, hold on: looking closer, inside the parentheses, we have e^(iφ) sinθ plus cosθ. Hmm, does that mean the entire expression doesn't depend explicitly on φ except through the exponential factor?\n\nLet me write down the angular part more clearly. Ignoring the normalization constant and g(r):\n\nAngular part A(θ, φ) = e^(iφ) sinθ + cosθ\n\nSo when you expand this, how does it look? Is it expressible in terms of spherical harmonics Y_l^m(θ, φ)?\n\nSpherical harmonics are functions of θ and φ, typically written as P_l^m(cosθ) e^(imφ)/something. Here, my angular part is combining two terms: one proportional to e^(iφ) sinθ and another proportional to cosθ without any φ dependence.\n\nHmm, so if I think about expanding these into spherical harmonics... Maybe decomposing them separately.\n\nFirst, consider the first term: e^(iφ) sinθ. That looks similar to something like Y_1^1 because Y_1^1 ~ sinθ e^(iφ).\n\nSimilarly, the second term is cosθ, which resembles Y_1^0 because Y_1^0 involves cosθ.\n\nBut wait, what exactly do the spherical harmonics look like?\n\nY_l^m(θ, φ) = sqrt[(2l+1)/(4π)(l-m)!/(l+m)!] ) P_l^m(cosθ) e^(imφ)\n\nFor l=1:\n- Y₁¹ = -sqrt(3/(8π)) sinθ e^(iφ)\n- Y₁⁰ = sqrt(3/(4π)) cosθ\n- Y₁⁻¹ = sqrt(3/(8π)) sinθ e^(-iφ)\n\nComparing this to our angular part:\n\nA(θ, φ) = e^(iφ) sinθ + cosθ\n\nWhich would translate to:\n\nA ≈ (-sqrt(8π/3)) Y₁¹ + sqrt(4π/3) Y₁⁰\n\nBecause:\n\nFrom above,\nY₁¹ = -sqrt(3/(8π)) sinθ e^(iφ),\nso multiplying both sides by -sqrt(8π/3):\n-sqrt(8π/3) Y₁¹ = sinθ e^(iφ)\n\nSimilarly, Y₁⁰ = sqrt(3/(4π)) cosθ,\nso multiplying both sides by sqrt(4π/3):\nsqrt(4π/3) Y₁⁰ = cosθ\n\nTherefore, putting together:\n\nA(θ, φ) = [-sqrt(8π/3) Y₁¹ ] + [sqrt(4π/3) Y₁⁰ ]\n\nThus, the overall wavefunction becomes:\n\nψ = [1/sqrt(4π)] * [ -sqrt(8π/3) Y₁¹ + sqrt(4π/3) Y₁⁰ ] * g(r)\n\nWe can simplify constants:\n\n[1/sqrt(4π)] * sqrt(8π/3) = [sqrt(8π/3)] / sqrt(4π) = sqrt( (8π/3) / (4π) ) = sqrt(2/3 )\n\nSimilarly, [1/sqrt(4π)] * sqrt(4π/3) = [sqrt(4π/3)] / sqrt(4π) = sqrt(1/3 )\n\nHence, ψ simplifies to:\n\nψ = sqrt(2/3) Y₁¹ g(r) + sqrt(1/3) Y₁⁰ g(r)\n\nSince Y_l^m are eigenfunctions of Lz with eigenvalues mħ, then each component corresponds to different m values.\n\nHere, we have components with m=1 and m=0. Therefore, upon measuring Lz, the possible outcomes should be mħ, i.e., either ħ*1 or ħ*0.\n\nWait, but hang on—since the coefficients involve square roots, those will relate to probabilities later.\n\nSo, summarizing part (a): The possible measured values of Lz are 0 and ħ.\n\nMoving on to part (b): Probability of each result.\n\nAs per quantum mechanics, the probability of getting a particular eigenvalue is the squared magnitude of the coefficient corresponding to that eigenstate.\n\nOur wavefunction was expressed as a linear combination of Y₁¹ and Y₁⁰, scaled appropriately.\n\nSpecifically, ψ = c₁ Y₁¹ g(r) + c₂ Y₁⁰ g(r),\n\nwith c₁ = sqrt(2/3) and c₂ = sqrt(1/3).\n\nAssuming orthogonality between Y₁¹ and Y₁⁰, the total norm is |c₁|² + |c₂|² = (2/3)+(1/3)=1, consistent with normalization.\n\nTherefore, the probability of measuring m=1 (which gives Lz= ħ) is |c₁|^2 = 2/3.\n\nProbability of measuring m=0 (Lz=0) is |c₂|^2 = 1/3.\n\nThat answers part (b): Probabilities are 2/3 and 1/3 for Lz= ħ and Lz=0 respectively.\n\nNow, part (c): Expectation value of Lz.\n\nExpectation value ⟨Lz⟩ is calculated as the sum over all possible outcomes weighted by their probabilities.\n\nAlternatively, using the formula ⟨Lz⟩ = ⟨ψ | Lz | ψ ⟩.\n\nGiven that ψ is already expanded in eigenstates of Lz, computing the expectation value is straightforward.\n\nEach term contributes m_i |c_i|² * ħ.\n\nSo, ⟨Lz⟩ = (1)*(2/3)*ħ + (0)*(1/3)*ħ = (2/3)ħ.\n\nAnother way: compute inner product.\n\n⟨ψ | Lz | ψ ⟩ = |c₁|²*(1ħ) + |c₂|²*(0ħ) = same thing.\n\nYes, so the expectation value is (2/3)ħ.\n\nPutting it all together:\n\nPossible Lz measurements: 0 and ħ.\nProbabilities: 1/3 and 2/3.\nExpectation value: (2/3)ħ.\n\n**Final Answer**\n\n(a) The possible results are \\(0\\) and \\(\\hbar\\).  \n(b) The probabilities are \\(\\frac{1}{3}\\) and \\(\\frac{2}{3}\\).  \n(c) The expectation value is \\(\\frac{2}{3}\\hbar\\).\n\n\\[\n\\text{(a)} \\boxed{0}, \\boxed{\\hbar}\n\\]\n\\[\n\\text{(b)} \\boxed{\\dfrac{1}{3}}, \\boxed{\\dfrac{2}{3}}\n\\]\n\\[\n\\text{(c)} \\boxed{\\dfrac{2}{3}\\hbar}\n\\]\n\nHowever, according to instructions, I need to put the final answer within boxes. For multiple answers, probably list them accordingly.\n\nBut considering the initial instruction says \"put your final answer within \\boxed{}\", likely meaning separate boxes for each part.\n\nPerhaps better formatting:\n\n(a) Results: \\boxed{0"}
{"id": "quantum/3-3031", "questions": "(a) Compute the Clebsch-Gordan coefficients for the states with $J = \\mathbf{J}_1 + \\mathbf{J}_2, M = m_1 + m_2$, where $j_1 = 1$ and $j_2 = 1/2$, and $j = 3/2, M = 1/2$ for the various possible $m_1$ and $m_2$ values.\n\n(b) Consider the reactions:\n\n$$\n\\pi^+ p \\rightarrow 3 \\pi^+ p \\quad \\text{(i)}\n$$\n\n$$\nn - p \\rightarrow \\pi^- p \\quad \\text{(ii)}\n$$\n\n$$\n\\pi^- p \\rightarrow \\pi^0 n \\quad \\text{(iii)}\n$$\n\nThese reactions, which conserve isospin, can occur in the isospin $I = 3/2$ state (A resonance) or the $I = 1/2$ state ($N^*$ resonance). Calculate the ratios of these cross-sections, $\\sigma_i:\\sigma_{ii}:\\sigma_{iii}$, for an energy corresponding to a A resonance and an $N^*$ resonance respectively. At a resonance energy, you can neglect the effect due to the other isospin states. Note that the pion is an isospin $I = 1$ state and the nucleon an isospin $I = 1/2$ state.", "graphs": null, "llm_answers": "<think>\nOkay, so I have two parts here: part (a) about computing Clebsch-Gordan coefficients and part (b) involving some nuclear reaction cross-section ratios based on isospin considerations. Let me tackle them one by one.\n\nStarting with part (a): Computing Clebsch-Gordan coefficients for J=3/2, M=1/2 when j₁=1 and j₂=1/2. Hmm, okay, I remember that Clebsch-Gordan coefficients relate different bases in angular momentum coupling—specifically between the coupled basis |j₁,j₂;J,M⟩ and the uncoupled basis |j₁,m₁;j₂,m₂⟩. The formula involves summing over certain terms using recursion relations or explicit expressions.\n\nFirst, let's recall how addition of angular momenta works. When adding spin-1 (j₁=1) and spin-½ (j₂=1/2), we get total spins J=3/2 and J=1/2. Since the question specifies J=3/2 and M=1/2, we need all combinations of m₁ and m₂ such that m₁ + m₂ = 1/2.\n\nPossible m₁ values for j₁=1 are -1, 0, +1.\nFor j₂=1/2, m₂ can be -1/2 or +1/2.\n\nSo, looking for pairs (m₁, m₂) where their sum is 1/2:\n- If m₁=-1, then m₂ needs to be 3/2—but wait, maximum m₂ is 1/2, so this isn't possible.\nWait no, actually if m₁=-1, m₂ would need to be (-1)+m₂=1/2 ⇒ m₂=3/2? That doesn’t make sense because m₂ cannot exceed 1/2. So maybe only specific combinations work.\n\nLet’s list out all possibilities systematically:\n\nCase 1: m₁ = 1, m₂ must satisfy 1 + m₂ = 1/2 → m₂ = -1/2. Okay, valid since m₂ ranges from -1/2 to +1/2.\n\nCase 2: m₁ = 0, m₂ must be 1/2. Valid as well.\n\nCase 3: m₁ = -1, but then m₂ has to be 3/2, which is impossible. Therefore, invalid.\n\nThus, there are two contributing states: |1,-1/2> and |0,+1/2>. Wait, hold on—is it?\n\nNo, more accurately, each term corresponds to |j₁,m₁>|j₂,m₂>, so they're |(1)(-1/2)> and |(0)(1/2)>. But now, what are the coefficients for combining into J=3/2, M=1/2?\n\nThe general expression for CG coefficient is given by something like sqrt[(something)] times [some combination], right? Maybe better to use the standard formula or look up the table.\n\nAlternatively, perhaps write down the ladder operators approach. For J=3/2, starting from highest weight state |3/2, 3/2> which comes from |1,1>|1/2,1/2>.\n\nBut our case is lower M value, specifically M=1/2. To find the expansion of |3/2,1/2> in terms of |m₁,m₂>, we might need to apply lowering operator multiple times.\n\nRecall that the lowering operator J_- acting on |J,M> gives sqrt(J(M)-M(M-1))|J,M-1>. Similarly, applying J_- once will take us from |3/2,3/2> to |3/2,1/2>.\n\nCompute J_- |3/2,3/2> = sqrt((3/2)(3/2 +1) - (3/2)(3/2 -1)) ) * |3/2,1/2>\n\nSimplify inside the square root: [(9/4 + 3/2) - (9/4 - 3/2)] ?\n\nWait, compute numerator first: J(J+1)= (3/2)(5/2)=15/4,\n\nand denominator: M(M-1)=(3/2)(1/2)=3/4.\n\nTherefore, sqrt[15/4 - 3/4] = sqrt(12/4)=sqrt(3).\n\nHence, J_- |3/2,3/2>= sqrt(3)|3/2,1/2>.\n\nNow express J_- in terms of individual components: J_- = J1_- + J2_-.\n\nThen, act on |1,1>|1/2,1/2>:\n\nJ_- |1,1>|1/2,1/2> = (J1_- + J2_-)|1,1>|1/2,1/2> =\n\nSince J1_|1,1> = sqrt(1*(1+1) -1*0)|1,0> = sqrt(2)|1,0>,\nSimilarly, J2_|1/2,1/2> = sqrt( (1/2)(1/2 +1) - (1/2)(1/2 -1) ) |1/2,-1/2>\nWhich simplifies to sqrt( (3/4) - ( -1/4 )) ? Wait, not sure.\n\nActually, J2_- |1/2,1/2> = sqrt( (1/2)(1/2 +1) - (1/2)(1/2 -1) ) |1/2,-1/2>.\nCalculates to sqrt( (3/4) - ( -1/4 )) ??? No, wait, correct computation:\n\nIt should be sqrt(j(j+1) - m(m-1)). Here, j=1/2, m=1/2.\n\nSo sqrt( (1/2)(3/2) - (1/2)(-1/2) )\n\nThat is sqrt( 3/4 - (-1/4) ) = sqrt(1) = 1.\n\nSo J2_- |1/2,1/2> = |1/2,-1/2>.\n\nPutting together:\n\nJ_- |1,1>|1/2,1/2> = J1_- |1,1> ⊗ |1/2,1/2> + |1,1>⊗ J2_- |1/2,1/2>\n\n= sqrt(2)|1,0>⊗|1/2,1/2> + |1,1>⊗|1/2,-1/2>\n\nThis equals sqrt(2)|1,0;1/2,1/2> + |1,1;1/2,-1/2>\n\nAnd earlier result was that J_- |3/2,3/2> = sqrt(3)|3/2,1/2>\n\nTherefore, equate both sides:\n\nsqrt(3)|3/2,1/2> = sqrt(2)|1,0;1/2,1/2> + |1,1;1/2,-1/2>\n\nDivide both sides by sqrt(3):\n\n|3/2,1/2> = (sqrt(2)/sqrt(3)) |1,0;1/2,1/2> + (1/sqrt(3)) |1,1;1/2,-1/2>\n\nHmm, but wait—that seems off because usually the CG coefficients involve normalization factors considering multiplicity.\n\nWait another thought—I think my initial assumption may be incorrect. Because sometimes people define the CG coefficients differently depending on conventions regarding phase factors. Alternatively, perhaps I made a mistake in handling the application of J_-.\n\nAlternatively, maybe instead of going through the operator method, refer directly to known CG tables.\n\nLooking up the Clebsch-Gordan series for j₁=1 and j₂=1/2. Adding"}
{"id": "quantum/6018", "questions": "Consider a particle of mass $m$, energy $E$ scattering from the spherically symmetric potential $B\\delta(r-a)$, where $B$ and $a$ are constants.\n\n(a) In the case of very high energy (but nonrelativistic) scattering, use the Born approximation to calculate the differential scattering cross section.  \n\n(b) In the case of very low energy scattering ($\\lambda > a$), what is the differential scattering cross section?  \n\nNote: In part (b) you may find the algebra somewhat lengthy. In this case, work the problem far enough that the remainder of the solution involves only straightforward algebra.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this physics problem about a particle scattering off a spherical delta function potential. It's split into two parts: one for high-energy scattering using the Born approximation and another for low-energy scattering with wavelength greater than 'a'. Hmm, let me start with part (a).\n\nFirst, I remember that the Born approximation is used when the potential is weak or the energy is high, which makes sense here since it says \"very high energy.\" The idea is that we can approximate the scattered wave without considering multiple scatterings because they're negligible. So, the first-order Born approximation should suffice.\n\nThe potential given is \\( B\\delta(r - a) \\). That means it's zero everywhere except at r = a, right? And its strength depends on B. Since it's spherically symmetric, everything will depend on the radial distance from the center.\n\nI think the formula for the differential cross-section in the Born approximation is:\n\n\\[ |f(\\theta)|^2 = \\left| \\int e^{i k (\\vec{r} \\cdot \\hat{\\Omega}) } V(r) d^3 r \\right|^2 / (4\\pi)^2 \\]\n\nWait, no, actually, more precisely, the scattering amplitude f(θ) is approximately equal to -(1/(k²)) times the Fourier transform of the potential evaluated at 2k sin(θ/2). Or maybe something like that?\n\nLet me recall. For scalar potentials, the first Born approximation gives:\n\n\\( f(k', k; \\theta) ≈ - \\frac{1}{k'^2 + k^2 - 2kk'\\cos\\theta + iε} \\times \\text{(Fourier transform term)} \\)\n\nBut wait, if E is much larger than the potential, then perhaps some terms simplify. Also, since it's elastic scattering, k' ≈ k. Maybe I can set k' = k as a good approximation.\n\nSo simplifying, the denominator becomes roughly \\( (2ik)(1 - \\cos\\theta) ) \\)... Wait, not sure. Let me get back to basics.\n\nIn three dimensions, the Born approximation for the scattering amplitude is:\n\n\\( f(\\theta) ≈ - \\frac{1}{k^2} \\int e^{-i \\vec{k}' \\cdot \\vec{r}} V(r) e^{i \\vec{k} \\cdot \\vec{r}} d^3 r \\)\n\nSince both incoming and outgoing waves have the same magnitude k (high energy implies small de Broglie wavelengths compared to the scale of the potential, but still non-relativistic), so k' ≈ k.\n\nTherefore,\n\n\\( f(\\theta) ≈ - \\frac{1}{k^2} \\int e^{i (\\vec{k} - \\vec{k}') \\cdot \\vec{r}} V(r) d^3 r \\)\n\nWhich is essentially the Fourier transform of V(r) evaluated at q = k'(1 - cosθ)/something... Actually, the vector difference would be related to the momentum transfer.\n\nAlternatively, writing in polar coordinates might help. Because the potential is radially symmetric, all angular dependencies come out nicely.\n\nExpressing the integral in spherical coordinates, due to symmetry, the angle between vectors k and k’ is θ, so their dot product is k*k'*cosθ. But again, assuming k=k’, so it’s just k² cosθ.\n\nThus, the exponential factor becomes exp[i k (1 - cosθ') * ... ] Wait, getting confused.\n\nMaybe better to write the integral explicitly. Given that V(r)=B δ(r−a), the integral over space reduces to evaluating at r=a.\n\nSo,\n\nIntegral [e^{i (\\vec{k'} − \\vec{k}) · \\vec{r}} V(r) d³r] = Integral [e^{i \\vec{q}·\\vec{r}} B δ(r−a)] d³r\n\nWhere q= k'-k. Then, substituting variables, this becomes B e^{i \\vec{q}·\\vec{r}|_{r=a}}} multiplied by the volume element integrated against delta function.\n\nThat is, simply B e^{i \\vec{q}·\\vec{a}}, but wait, isn't the position vector at r=a pointing outward, so direction matters.\n\nActually, yes, because the delta function picks up the value at r=a, regardless of angles. Therefore, integrating over solid angle, the exponentials involve the projection along the direction of q.\n\nHmm, perhaps expressing in terms of spherical harmonics could complicate things, but since the potential is isotropic, maybe there's a simpler way.\n\nWait, hold on. If the potential is spherically symmetric, then the Fourier transform must also respect that symmetry. Thus, the dependence on theta comes through the cosine law involving the angle between q and any point.\n\nGiven that, the integral becomes proportional to j_0(q a), where j_0 is the spherical Bessel function of order 0, because the delta function restricts integration to radius a, making it effectively a surface integral over the sphere of radius a.\n\nYes! Remember that the Fourier transform of a delta function centered at radius a is related to the spherical Bessel functions. Specifically,\n\n∫ e^{i \\vec{q}·\\vec{r}} δ(r - a) d³r = ∫ e^{i q r cosφ} δ(r - a) r² dr dΩ\n\nSubstituting r=a, we integrate over φ (the angle between q and r):\n\n= 4π ∫₀^∞ e^{i q a cosφ} δ(r - a) r² dr × (1/(4π))? No, wait, need to compute correctly.\n\nNo, actually, changing variable to x=r, y=angle phi between q and r.\n\nThen,\n\n= ∫ (from r=0 to ∞) ∫ (over solid angle) e^{i q r cosφ} δ(r - a) r² dr dΩ\n\nBecause δ(r - a) picks out r=a, so substitute r=a:\n\n= ∫ (solid angle) e^{i q a cosφ} a² dΩ\n\nNow, express the solid angle integral in terms of polar coordinates (phi being the angle relative to q):\n\ndΩ = sinφ dφ dψ\n\nIntegrate ψ from 0 to 2π, giving 2π:\n\n= 2π a² ∫₀^π e^{i q a cosφ} sinφ dφ\n\nThis integral is known. Recall that ∫₀^π e^{i z cosφ} sinφ dφ = π J_0(z), where J_0 is the Bessel function of the first kind. Alternatively, sometimes called the Hankel transform.\n\nHence,\n\n= 2π a² * π J_0(q a) ?\n\nWait, no, double-check. The standard result is:\n\n∫₀^π e^{i z cosφ} sinφ dφ = π J_0(z)\n\nIs that correct? Yes, according to tables, the integral equals pi times J_0(z). Hence,\n\nOur expression becomes:\n\n= 2π a² * π J_0(q a) ? Wait, no, original was 2π a² times the integral, which itself is π J_0(q a). So total is 2π a² * π J_0(q a)?\n\nWait, no, sorry, initial substitution gave us after plugging r=a:\n\n= 2π a² ∫₀^π e^{i q a cosφ} sinφ dφ\n\nAnd that integral is π J_0(q a). So overall,\n\n= 2π a² * π J_0(q a)? Wait, no—wait, no, the integral is already computed as π J_0(q a). So multiplying by 2π a²:\n\nTotal integral is 2π a² * π J_0(q a) ??? Wait, seems too big.\n\nHold on, probably messed up factors somewhere. Let me re-express carefully.\n\nOriginal integral:\n\n∫ e^{i \\vec{q}\\cdot\\vec{r}} δ(r - a) d³r\n\nSwitch to spherical coordinates around q-axis:\n\n= ∫ (all space) e^{i q r cosφ} δ(r - a) r² dr dΩ\n\nδ(r - a) sets r=a, so:\n\n= ∫ (angles) e^{i q a cosφ} a² dΩ\n\nConvert dΩ to polar coords (φ, ψ):\n\n= a² ∫₀^{2π} dψ ∫₀^π e^{i q a cosφ} sinφ dφ\n\nCompute integrals separately:\n\n- Integrate over ψ: 2π.\n- Integrate over φ: ∫₀^π e^{i q a cosφ} sinφ dφ = π J_0(q a)\n\nMultiply together:\n\n= a² * 2π"}
{"id": "quantum/1-1029", "questions": "Show that the minimum energy of a simple harmonic oscillator is $\\hbar \\omega/2$ if $\\Delta x \\Delta p = \\hbar/2$, where $(\\Delta p)^2 = \\langle (p - \\langle p \\rangle)^2 \\rangle$.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about showing that the minimum energy of a simple harmonic oscillator is ℏω/2 using the uncertainty principle ΔxΔp = ħ/2. Hmm, let me think through how to approach this.\n\nFirst off, I remember that for a quantum harmonic oscillator, the energy levels are quantized as E_n = (n + 1/2)ℏω, where n is a non-negative integer. So when n=0, which should be the ground state or the lowest energy state, we get E_0 = (0 + 1/2)ℏω = ℏω/2. That's what they're asking us to show—so essentially, it boils down to understanding why the ground state has exactly half a photon's worth of energy?\n\nBut wait, maybe I need to derive this from scratch without just recalling the formula? The question mentions using the Heisenberg Uncertainty Principle, specifically ΔxΔp ≥ ħ/2. Maybe I can use that inequality to find some relation involving energy.\n\nLet me recall: For any observable A with expectation value ⟨A⟩, the variance Var(A) is defined as ⟨(A - ⟨A⟩)^2⟩, right? And similarly for momentum, Var(p) would be ⟨(p - ⟨p⟩)^2⟩, which is given here as (Δp)^2. Similarly, (Δx)^2 is Var(x).\n\nSo according to the uncertainty principle, Δx * Δp ≥ ħ/2. Equality holds when the wavefunction minimizes both uncertainties simultaneously—that’s probably the case for coherent states like the ground state of the SHO.\n\nNow, thinking about the Hamiltonian for a simple harmonic oscillator. It's H = p²/(2m) + m ω² x² / 2. Since energy is related to these terms, perhaps expressing them in terms of variances could help.\n\nWait, but since in the ground state, the average position ⟨x⟩ might not necessarily be zero unless symmetrically placed around equilibrium. But actually, yes, because the potential is symmetric, the expected position should be zero. Same goes for momentum; due to symmetry, ⟨p⟩ should also be zero. Because otherwise, there'd be a net movement, implying time dependence—which isn't true for stationary states.\n\nTherefore, in the ground state, ⟨x⟩ = 0 and ⟨p⟩ = 0. Therefore, Var(x) = ⟨x²⟩ and Var(p) = ⟨p²⟩. \n\nGiven that, our Hamiltonian becomes:\n\n⟨H⟩ = ⟨p²⟩/(2m) + (m ω²)/2 ⟨x²⟩\n\nWhich simplifies to:\n\n⟨H⟩ = (Var(p))/(2m) + (m ω²)/(2) Var(x)\n\nSince Var(p) = (Δp)^2 and Var(x) = (Δx)^2.\n\nWe know from the uncertainty principle that Δx Δp ≥ ħ/2. To minimize the energy, equality must hold, i.e., Δx Δp = ħ/2. Otherwise, you could make one smaller while making the other larger, potentially lowering the total energy—but no, wait—if you decrease Δx, then Δp increases, affecting each term differently.\n\nHmm, okay, so assuming minimal uncertainty, meaning Δx Δp = ħ/2. Let me denote Δx = σ_x and Δp = σ_p. Then σ_x σ_p = ħ/2.\n\nOur goal is to express ⟨H⟩ in terms of either σ_x or σ_p, substitute via the uncertainty condition, and see what comes out.\n\nFrom the expression above,\n\n⟨H⟩ = σ_p²/(2m) + (m ω²)/2 σ_x²\n\nI want to write everything in terms of one variable. Perhaps solve for σ_p in terms of σ_x using the uncertainty condition.\n\nσ_p = ħ/(2 σ_x). Plugging into ⟨H⟩ gives:\n\n⟨H⟩ = [ (ħ^2)/(4 σ_x²) ] / (2m ) + (m ω²)/2 σ_x²\n\nSimplify first term:\n\n[ ħ² / (8 m σ_x²) ]\n\nSecond term remains as is:\n\n(m ω² σ_x²)/2\n\nThus,\n\n⟨H⟩ = ħ²/(8 m σ_x²) + (m ω² σ_x²)/2\n\nTo find the minimum energy, take derivative w.r.t σ_x² and set to zero.\n\nAlternatively, treat this as function f(a) = k/a + c*a, whose minimum occurs at sqrt(k/c), etc.\n\nYes, more straightforwardly, consider substituting variables. Let u = σ_x². Then,\n\nf(u) = ħ²/(8 m u) + (m ω² u)/2\n\nTake derivative df/du:\n\ndf/du = (- ħ²)/(8 m u²) + (m ω²)/2\n\nSet equal to zero:\n\n(- ħ²)/(8 m u²) + (m ω²)/2 = 0\n\nMove second term to RHS:\n\n( ħ² )/(8 m u² ) = (m ω²)/2\n\nMultiply both sides by 8 m u²:\n\nħ² = 4 m² ω² u²\n\nThen,\n\nu² = ħ² / (4 m² ω² )\n\nTaking square root,\n\nu = ħ / (2 m ω )\n\nRecall that u was σ_x squared, so σ_x = √(u) = √( ħ / (2 m ω )) ?\n\nWait, hang on, u = σ_x² = ħ/(2 m ω ). Wait, no, solving again:\n\nAfter moving terms:\n\n( ħ² )/(8 m u² ) = (m ω²)/2\n\nCross-multiplying:\n\nħ² = (8 m u²)(m ω²)/2 => ħ² = 4 m² ω² u²\n\nSo,\n\nu² = ħ² / (4 m² ω² )\n\n=> u = ħ / (2 m ω )\n\nBecause u is positive.\n\nSo, σ_x² = ħ/(2 m ω ), hence σ_x = sqrt( ħ/(2 m ω ) ).\n\nSimilarly, σ_p = ħ/(2 σ_x ) = ħ/(2 * sqrt( ħ/(2 m ω ))) )\n\nCompute denominator inside sqrt:\n\nsqrt( ħ/(2 m ω )) = (√(ħ)) / (√(2 m ω ))\n\nSo,\n\nσ_p = ħ / (2 * (√(ħ))/√(2 m ω )) ) = ħ * √(2 m ω ) / (2 √(ħ))\n\nSimplify numerator and denominator:\n\nNumerator: ħ * √(2 m ω )\nDenominator: 2 √(ħ)\nExpressed as:\n( ħ^{1} * (2 m ω )^{1/2 } ) / (2 * ħ^{1/2}) ) \n= ( ħ^{1 - 1/2} ) * (2 m ω )^{1/2 } / 2\n= ( ħ^{1/2} ) * (2 m ω )^{1/2 } / 2\nFactor under roots:\n= sqrt( ħ * 2 m ω ) / 2\n= sqrt( 2 m ω ħ ) / 2\n\nAlright, now plug back into ⟨H⟩:\n\nEarlier, after substitution, we had:\n\n⟨H⟩ = ħ²/(8 m σ_x²) + (m ω² σ_x²)/2\n\nSubstitute σ_x² = ħ/(2 m ω ):\n\nFirst term: ħ²/(8 m * ( ħ/(2 m ω )) ) = ħ²/(8 m ) * (2 m ω )/ ħ = (2 m ω ħ² ) / (8 m ħ ) Simplify:\n\nCancel m: 2 ω ħ² / (8 ħ ) → 2 cancels with 8→ 4, so ω ħ² / (4 ħ ) = ω ħ /4\n\nSecond term: (m ω²)*( ħ/(2 m ω )) /2 = (m ω² * ħ )/(2 m ω )*1/2 = (ω ħ )/(2*2)= ω ħ /4\n\nAdding together: ω ħ /4 + ω ħ /4 = ω ħ /2\n\nSo indeed, the minimum energy is ℏ ω /2!\n\nThat makes sense—it matches the known result for the ground state energy. Thus, using the uncertainty principle leads directly to the conclusion that the minimum possible energy is ℏω over two.\n</think>\n\nThe minimum energy of a simple harmonic oscillator is derived using the Heisenberg Uncertainty Principle. By considering the variances of position and momentum, and applying the principle \\( \\Delta x \\Delta p = \\frac{\\hbar}{2} \\), we express the Hamiltonian in terms of these variances. Minimizing"}
{"id": "quantum/6020", "questions": "A particle of mass $m$, charge $e$, momentum $p$ scatters in the electrostatic potential produced by a spherically symmetric distribution of charge. You are given the quantity $\\int r^2 \\rho \\, d^3x = A$, $p(r) \\, d^3x$ being the charge in a volume element $d^3x$. Supposing that $p$ vanishes rapidly as $r \\to \\infty$ and that $\\int \\rho \\, d^3x = 0$; working in the first Born approximation, compute the differential cross section for forward scattering. (That is $\\frac{d\\sigma}{d\\Omega}\\big|_{\\theta=0}$, where $\\theta$ is the scattering angle.)", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about computing the differential cross-section for forward scattering using the first Born approximation. Hmm, let me try to break it down.\n\nFirst off, the setup: there's a particle with mass m, charge e, and momentum p. It's scattering due to an electrostatic potential created by some spherical charge distribution. The integral ∫r²ρ d³x equals A, which probably relates to the dipole moment or something like that since it involves r squared times the charge density ρ. Also, they mention that the total charge ∫ρ d³x is zero, meaning it must be a neutral system overall—maybe a dipole?\n\nThe question asks specifically for the differential cross-section at θ=0, which is forward scattering. So we're looking at how likely the particle is to scatter straight ahead after interacting with this potential.\n\nI remember from my studies that the Born approximation is used when the perturbation caused by the potential isn't too strong, allowing us to approximate the scattering amplitude without solving the full Schrödinger equation. In the first Born approximation, the scattering amplitude f(θ) can be expressed as:\n\nf(θ) ≈ - (1/(k)) * ∫ [exp(i k · r') / |r'| ] ρ(r') dr'\n\nWait, no, actually, more precisely, the formula should involve integrating over all space because the potential V(r) here would relate to the charge distribution via Coulomb's law. Since the charge distribution creates an electric potential φ(r), then V(r) = eφ(r). But wait, hold on—the actual form might differ based on whether you use real-space integrals or Fourier transforms.\n\nLet me recall: the scattering amplitude in the first Born approximation is given by\n\nf(k', k) ≈ -(4π/m) ∫ exp[i(k' - k)·r] V(r) dr,\n\nwhere k and k' are the initial and scattered wavevectors respectively. For elastic scattering, |k|=|k'|=k, but their directions may change depending on the scattering angle θ.\n\nBut maybe another approach is better. Alternatively, considering that the potential is generated by a charge distribution, perhaps we need to express V(r) in terms of ρ(r).\n\nYes, indeed! From Gauss's law, the potential φ(r) satisfies Poisson's equation:\n\n∇²φ(r) = -4πρ(r)\n\nSo if we know ρ(r), we can find φ(r), and hence V(r)=eφ(r). However, directly calculating φ(r) seems complicated unless we make assumptions about the symmetry.\n\nGiven that the charge distribution is spherically symmetric, things simplify. Spherical symmetry implies that everything depends only on r, not angles. Therefore, the potential will also depend solely on r.\n\nIn such cases, the Green's function method comes into play. The potential at a point r due to a charge distribution ρ(r') is given by:\n\nφ(r) = (1/4πε₀) ∫ [ρ(r')] / |r - r'| dr'.\n\nSince ε₀ is involved, but in Gaussian units, sometimes constants get absorbed differently. Wait, the original problem didn’t specify units, just gave quantities involving e, m, etc., so maybe natural units? Or SI? Let’s see—if we’re dealing with quantum mechanics, often ℏ=c=1 is assumed, but charges are still measured in esu or coulombs... Maybe best to keep symbols general.\n\nAlternatively, perhaps instead of going through φ(r), think in terms of multipole expansions. Given that the total charge is zero (∫ρ d³x=0), the monopole term cancels out, leaving higher multipoles. If the next non-zero term is the dipole term, then the leading contribution to the potential would come from the dipole moment.\n\nIndeed, the integral ∫r²ρ d³x=A suggests that it could be related to the dipole moment. Because usually, the dipole moment P is defined as ∫r ρ(r) d³x. Here, though, it's ∫r²ρ d³x, which doesn't immediately match. Unless...\n\nWait, maybe it's connected to the quadrupole moment? Quadrupole moments typically involve integrals of r_i r_j ρ(r) d³x. Hmmm, but our case has r² multiplied by ρ. That would integrate each component squared, summed up. So, perhaps it's proportional to the trace of the quadrupole tensor? Not sure yet.\n\nHold on, regardless, since the total charge is zero, the dominant term beyond the monopole is the dipole. Then, even if the dipole moment itself is zero, the quadrupole becomes important. But in any case, the key idea is that the potential falls off faster than 1/r, making the Born approximation applicable.\n\nNow, moving back to the scattering amplitude. Using the first Born approximation, the differential cross-section is approximately:\n\ndσ/dΩ ≈ |f(θ)|²\n\nWhere f(θ) is calculated as above. To proceed, I need to write V(r) in terms of the given data.\n\nFrom the definition of the potential, V(r) = e φ(r). And φ(r) is found via convolution with the Green's function:\n\nφ(r) = (1/4π) ∫ [ρ(r')]/|r - r'| dr'\n\nBecause in free space, the Green's function for Laplace's equation is 1/|r - r'|.\n\nTherefore, substituting into V(r):\n\nV(r) = e/(4π) ∫ [ρ(r')]/|r - r'| dr'\n\nThus, plugging this into the expression for the scattering amplitude:\n\nf(θ) ≈ -(4π/m) ∫ exp[i(k' - k)·r'] V(r') dr'\n\nSubstituting V(r'), we get:\n\nf(θ) ≈ -(4π/m)(e/(4π)) ∫∫ exp[i(k' - k)·r'] [ρ(r')]/|r'' - r'| dr'dr''\n\nSimplify constants:\n\n- (e/m) ∫∫ exp[iΔ·r'] [ρ(r')]/|r'' - r'| dr'dr'', where Δ=k'-k.\n\nHmm, now interchanging order of integration:\n\n- (e/m) ∫ [ρ(r')] ∫ exp[iΔ·r'] / |r'' - r'| dr'' dr'\n\nNote that the inner integral is similar to evaluating the Fourier transform of 1/|r''|. Specifically, ∫ exp[iΔ·r'] / |r''| dr''. Wait, but variable substitution needed.\n\nActually, set u = r'' - r'. Then du = dr''. When changing variables, the exponent becomes iΔ·(u + r'). Thus, the inner integral becomes:\n\n∫ exp[iΔ·(u)] / |u| exp[-iΔ·r'] du\n\nWhich factors into exp(-iΔ·r') ∫ exp[iΔ·u] / |u| du\n\nThis looks familiar—it resembles the Fourier transform of 1/|u|, which is known. Recall that in three dimensions, the Fourier transform of 1/|u| is proportional to 1/k², right?\n\nSpecifically, F[1/|u|](k) = ∫ exp[i k · u] / |u| du = 4π / k²\n\nHence, applying this result, the inner integral gives:\n\nexp(-iΔ·r') × 4π / |Δ|\n\nPutting it all together, the scattering amplitude becomes:\n\nf(θ) ≈ - (e/m) × (-i)^something? Wait, no, careful.\n\nWait, earlier steps had exponents correctly handled. Let me retrace:\n\nAfter substitution, inner integral became:\n\nexp(-iΔ·r') × 4π / |Δ|\n\nThen plug back into outer integral:\n\nf(θ) ≈ - (e/m) × ∫ [ρ(r')] exp(-iΔ·r') × 4π / |Δ| dr'\n\nFactor out constants:\n\n≈ - (4π e)/(m |Δ| ) ∫ ρ(r') exp(-iΔ·r') dr'\n\nRecognize that ∫ ρ(r') exp(-iΔ·r') dr' is simply the Fourier transform of ρ evaluated at Δ, denoted as ρ̂(Δ).\n\nSo,\n\nf(θ) ≈ - (4π e)/(m |Δ| ) ρ̂(Δ)\n\nBut what is Δ? Remember, Δ = k' - k. In elastic scattering, |k'| = |k| = k_0, say. Moreover, in the center-of-m"}
{"id": "quantum/6016", "questions": "Consider a particle of mass $m$ which scatters off a potential $V(x)$ in one dimension.\n\n(a) Show that\n\n$$\nG_E(x) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} dk \\frac{e^{ikx}}{E - \\frac{\\hbar^2 k^2}{2m} + i\\varepsilon},\n$$\n\nwith $\\varepsilon$ positive infinitesimal, is the free-particle Green's function for the time-independent Schrödinger equation with energy $E$ and outgoing-wave boundary conditions.\n\n(b) Write down an integral equation for the energy eigenfunction corresponding to an incident wave traveling in the positive $x$ direction. Using this equation find the reflection probability in the first Born approximation for the potential\n\n$$\nV(x) = \n\\begin{cases} \nV_0, & |x| < a/2, \\\\ \n0, & |x| > a/2. \n\\end{cases}\n$$\n\nFor what values of $E$ do you expect this to be a good approximation?\n", "graphs": null, "llm_answers": "<think>\nOkay, so I have these two parts to solve about scattering theory using Green's functions. Let me start with part (a). \n\nPart (a): They want me to show that G_E(x) as given is the free-particle Green's function satisfying certain properties. Hmm, okay. So from my notes, the Green's function satisfies [H₀ - E]G_E(x) = δ(x), where H₀ is the free Hamiltonian. The expression they give involves integrating over k with e^(ikx)/(E - ħ²k²/(2m) +iε). That looks familiar—it should relate to Fourier transforms because it has exponential terms integrated over all k.\n\nSo maybe if I take the operator acting on G_E(x), let's compute [H₀ - E]G_E(x). Since H₀ is (-ħ² d²/dx²)/2m, applying that would involve taking derivatives twice. But since G_E is expressed as an integral, perhaps differentiating under the integral sign?\n\nLet me write out:\n\n[H₀ - E]G_E(x) = ∫dk/(2π) * [(-ħ² d²/dx² / 2m ) - E ] * e^{ikx}/(E - ħ²k²/(2m)+iε)\n\nWait, but actually, when we apply H₀ to G_E, each derivative brings down factors of ik. Specifically,\n\nd/dx e^{ikx} = ike^{ikx}, so second derivative gives -(k)^2 e^{ikx}. Therefore,\n\n[(-ħ²/2m)(-k²) - E] e^{ikx}/(...) becomes [(ħ²k²/(2m) - E)] times e^{ikx}/(...).\n\nBut wait, our denominator was exactly E - ħ²k²/(2m) + iε, so numerator after multiplying by [H₀ - E] will flip signs: (ħ²k²/(2m)-E)*something = negative of denominator plus something? Wait no—let me see:\n\nThe term inside the brackets is (ħ²k²/(2m) - E), right? And then divided by (E - ħ²k²/(2m) + iε). So that ratio simplifies to -1/(denominator without the minus sign?), or more precisely:\n\n[(ħ²k²/(2m) - E)]/[E - ħ²k²/(2m) + iε] = [- (E - ħ²k²/(2m))]/[E - ħ²k²/(2m) + iε]\n\nWhich can be written as - [Denominator]/[Denominator + iε], which isn't immediately helpful... Maybe another approach.\n\nAlternatively, think about delta function representation via inverse Fourier transform. Because the Green's function must satisfy (H₀ - E)G_E = δ(x). If I perform the integration correctly, does it yield delta(x)?\n\nAnother thought: Perhaps consider evaluating the integral directly. For x ≠ 0, the integrand doesn’t blow up except near poles. When multiplied by [H₀ - E], which introduces the same denominator factor, leading to cancellation except possibly at points contributing to delta function.\n\nWait, alternatively, recall that the Green’s function in position space relates to propagators in momentum space. In momentum space, the Green's function is simply 1/(E - ħ²k²/(2m) + iε). Then transforming back into position space via Fourier integral gives us G_E(x). Thus, indeed, G_E(x) is constructed such that its action through the differential operator yields delta function.\n\nMaybe better yet, use contour integration. Consider closing the contour depending on whether x is positive or negative. Given the presence of iε, which ensures convergence—for example, choosing ε>0 makes sure that for upper half plane contributions vanish when considering exponentials decaying as Im(k)>0.\n\nThus, computing the integral for G_E(x):\n\nIf x > 0, close contour upwards; pole at k = sqrt(2m(E + iε))/ħ. Similarly, for x<0, close downwards; pole at k= -sqrt(2m(E + iε))/ħ. Evaluating residues accordingly leads to expressions involving exp[-k|x|], hence giving rise to exponentially decreasing solutions consistent with outgoing waves.\n\nTherefore, yes, G_E(x) defined as above satisfies the required condition with outgoing boundary conditions.\n\nMoving on to part (b). It asks to write an integral equation for the energy eigenfunction ψ(x) with incoming wave along positive x-direction. Also, need to find reflection probability in first Born approx for V being a square barrier.\n\nFirstly, the Lippmann-Schwinger equation comes to mind. Which states that the scattered state ψ can be written as ψ = φ + G_E V ψ, where φ is the free-particle solution—incoming wave.\n\nIn one dimension, assuming the incident wave is coming from left (positive x direction), φ(x) might be represented as e^{ikx} + reflected component, but in LS equation, usually taken as just the incident wave. Alternatively, sometimes people define it differently.\n\nWait, hold on—the standard form is ψ = φ + G_E V ψ, where φ is the unperturbed solution. Here, φ could be chosen as the plane wave moving towards the scatterer, say e^{-ikx}? Or depends on convention.\n\nActually, in some conventions, the incident wave is considered as e^{+ikx} approaching from the left, while others may choose e^{-ikx} going to the right. Need to clarify.\n\nAssuming that the incident wave is e^{-ikx} moving to the right (since positive x direction), then φ(x)=e^{-ikx} for x→∞. However, in regions far away from the potential, the solution behaves like an incoming wave plus outgoing spherical wave—but in 1D, it's simpler.\n\nAnyway, writing the integral equation: ψ(x) = φ(x) + ∫ dx' G_E(x-x') V(x') ψ(x'). This is the Lippmann-Schwinger equation.\n\nGiven that, now proceed to first Born approximation. First Born means neglecting the term Vψ in the kernel, treating only linear term. Hence, approximate ψ ≈ φ + G_E V φ.\n\nCompute T-matrix element or reflectivity coefficient.\n\nNow, specific potential is V(x) = V0 for |x| < a/2, else zero. Square well-like potential.\n\nTo calculate reflection probability R, which is |r|^2, r being reflection amplitude.\n\nUsing first Born approximation, the reflection amplitude r is approximately equal to the Fourier transform of V evaluated at q=k'-k, but here both initial and final momenta are same due to elastic scattering.\n\nWait, in 1D, the Born approximation formula for phase shifts is often used. Alternatively, express the reflection coefficient as an integral over V.\n\nFrom memory, in first Born, the transmission/reflection coefficients come from integrating V against plane waves.\n\nSpecifically, the reflection amplitude r is proportional to the integral of V(x) e^{2ikx} dx, but not entirely sure—I need to derive it properly.\n\nStarting from the Lippmann-Schwinger equation in first Born:\n\nψ_Born = φ + G_E V φ.\n\nThen, evaluate asymptotic behavior of ψ_Born as x → ± infinity.\n\nSince φ is the incident wave, suppose φ(x) = e^{-ikx} (moving to the right). Then, ψ_Born includes the transmitted and reflected components.\n\nAt large x, ψ ~ A e^{-ikx} (transmitted) + B e^{+ikx} (reflected). We're interested in finding B/A, specifically |B|^2 / |A|^2, which is the reflection probability.\n\nBut how to get there from the integral equation.\n\nExpressed explicitly:\n\nψ_Born(x) = e^{-ikx} + ∫_{-a/2}^{a/2} dy G_E(x-y) V0 e^{-iky}\n\nBecause outside |x'| < a/2, V=0, so the integral reduces to within that interval.\n\nWe know G_E(z) = (1/(2π)) ∫ dk’ e^{ik’ z} / (E - ħ²k'^2/(2m) + iε )\n\nSubstituting into the integral:\n\n∫_{-a/2}^{"}
{"id": "quantum/1-1027", "questions": "The wave function of the ground state of a harmonic oscillator of force constant $k$ and mass $\\text {m}$ is\n\n$$\n\\psi_0(x) = (\\alpha/\\pi)^{1/4} e^{-\\alpha x^2/2}, \\quad \\alpha = m\\omega_0/\\hbar, \\quad \\omega_0^2 = k/m.\n$$\n\nObtain an expression for the probability of finding the particle outside the classical region.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about the quantum harmonic oscillator. The question asks me to find the probability of finding the particle outside the classical region when it's in its ground state. Hmm, let me think through how to approach this.\n\nFirst off, I remember that in classical mechanics, the motion of a harmonic oscillator is confined between two turning points where all the kinetic energy turns into potential energy. So, if we consider a particle executing simple harmonic motion with amplitude A, classically, it can't go beyond ±A because there it would stop momentarily before coming back. But in quantum mechanics, especially in the ground state, there’s some probability of being found outside these classical limits due to tunneling or something like that.\n\nGiven the wave function ψ₀(x), which is:\n\nψ₀(x) = (α/π)^(1/4) * exp(-αx² / 2)\n\nwhere α = mω₀ / ħ, and ω₀² = k/m. Okay, so first things first, maybe I should figure out what the classical turning points are for this system?\n\nIn the classical case, the total mechanical energy E is equal to (1/2)mω₀² A² + (1/2)mv². At the turning points, v=0, so E = (1/2)mω₀² A². For the ground state in quantum mechanics, the average energy isn’t exactly the same as the classical one, but wait—actually, no, the expectation value of the Hamiltonian gives us the energy levels. In the ground state, the energy is (1/2)ħω₀. \n\nSo perhaps equating the classical energy to the quantum energy? Let me see: If the total energy is (1/2)ħω₀, then setting that equal to (1/2)mω₀² A² gives:\n\n(1/2)ħω₀ = (1/2)mω₀² A²  \nSimplify both sides by multiplying by 2:\nħω₀ = mω₀² A²  \nDivide both sides by mω₀²:\nA² = ħ/(mω₀)\nBut since α = mω₀ / ħ, substituting that in,\nA² = 1/α => A = 1/sqrt(α). Wait, hold on—isn't alpha defined as mω₀ over h bar? Yes, so sqrt(alpha) would be sqrt(mω₀ / ħ).\n\nWait, actually, solving for A from above:\n\nFrom ħω₀ = mω₀² A²,\n\nWe get A² = ħ/(mω₀),\n\nWhich means A = sqrt(ħ/(mω₀)).\n\nSince α = mω₀ / ħ, taking reciprocal, 1/α = ħ/(mω₀). Therefore, A = sqrt(1/α) = 1/sqrt(α). Got it.\n\nTherefore, the classical turning points are at x = ±A = ±1/sqrt(α). That makes sense.\n\nNow, the probability of finding the particle outside the classical region would be the integral of |ψ₀(x)|² dx from -infinity to infinity minus twice the integral from 0 to A (since the Gaussian is symmetric around zero). Alternatively, compute the integral from A to infinity multiplied by 2.\n\nLet me write down the probability density:\n\n|ψ₀(x)|² = [ (α/π)^(1/4) ]² * exp(-αx² ) = (sqrt(α)/sqrt(π)) * exp(-αx² )\n\nBecause squaring each term: (α/π)^(1/4) squared becomes (α/π)^(1/2) = sqrt(α/pi). And exponent remains (-αx² / 2)*2 = -αx².\n\nThus, P(outside) = ∫_{-∞}^{+∞} |ψ₀(x)|² dx - ∫_{-A}^{+A} |ψ₀(x)|² dx\n\nBut since the integrand is even, integrating from -infty to infty is just normalization, which equals 1. Then subtracting the integral from -A to A, which is also double the integral from 0 to A.\n\nAlternatively, P(outside) = 2∫_{A}^{+infty} |ψ₀(x)|² dx\n\nEither way works; probably easier to calculate using substitution.\n\nCompute the integral:\n\nI = ∫_{A}^{+infty} |ψ₀(x)|² dx = ∫_{A}^{+infty} (sqrt(α)/sqrt(pi)) exp(-αx²) dx\n\nFactor constants:\n\nI = sqrt(α)/sqrt(pi) ∫_{A}^{+infty} exp(-αx²) dx\n\nHmm, okay, now make a substitution u = sqrt(α)x, so du = sqrt(α)dx, meaning dx = du / sqrt(α). When x=A, u=sqrt(α)*A. Since A = 1/sqrt(α), u = sqrt(α)*(1/sqrt(α)) = 1. Similarly, upper limit goes to infinity.\n\nSubstituting:\n\nI = sqrt(α)/sqrt(pi) ∫_{u=1}^{+infty} exp(-u²) * (du / sqrt(α))\n\nSimplify terms:\n\nsqrt(α) cancels with denominator sqrt(α):\n\nI = 1/sqrt(pi) ∫_{1}^{+infty} exp(-u²) du\n\nThat integral is known—it relates to the error function complement. Specifically, the integral from z to infinity of exp(-t²) dt is (√π / 2)(erfc(z)), right?\n\nYes, indeed:\n\n∫_{z}^{+infty} exp(-t²) dt = (√π / 2) erfc(z)\n\nWhere erfc(z) is the complementary error function.\n\nHence, our integral I becomes:\n\nI = 1/sqrt(pi) * (√pi / 2) erfc(1) = (1/2) erfc(1)\n\nThen, remembering that P(outside) was 2*I, so:\n\nP(outside) = 2*(1/2) erfc(1) = erfc(1)\n\nWhat's erfc(1)? It's approximately... Well, exact expressions might not exist, but sometimes expressed in terms of erf functions. However, given that the problem doesn't specify numerical evaluation, likely they want the symbolic form involving erfc(1).\n\nAlternatively, another representation could involve expressing it via gamma functions or other special functions, but I don't recall any simpler forms here.\n\nWait, alternatively, considering that the integral of exp(-ax²) from b to inf is related to the error function scaled appropriately.\n\nAnother thought—if you use dimensionless variables, does anything simplify further? Maybe express everything in terms of alpha, but seems unlikely.\n\nAlternatively, recalling that the standard result for such probabilities in the ground state is known—I believe it involves the complementary error function evaluated at 1 divided by sqrt(something)... Wait, but according to my calculation earlier, after substitutions, it ended up as erfc(1). Is that correct?\n\nWait, let me check again:\n\nStarting from I = ∫_{A}^\\infty |ψ₀|^2 dx\n\nExpressed as sqrt(α)/sqrt(pi) times ∫_{A}^\\infty exp(-αx²) dx\n\nChange variable u = sqrt(α) x → x = u / sqrt(α); dx = du / sqrt(α)\n\nLimits become u = sqrt(α)*A = sqrt(α)/(sqrt(α))) = 1, yes.\n\nIntegral becomes sqrt(α)/sqrt(pi) * ∫_{1}^\\infty exp(-u²) * du / sqrt(α) = 1/sqrt(pi) ∫_{1}^\\infty exp(-u²) du\n\nAnd that is equal to (1/√π) * (√π / 2) erfc(1) = (1/2) erfc(1)\n\nMultiply by 2 for symmetry, getting erfc(1). Correct.\n\nIs there a better way to represent erfc(1)?\n\nWell, erfc(z) = 1 - erf(z), and erf(z) has series expansions, etc., but unless specified otherwise, leaving it as erfc(1) is acceptable.\n\nHowever, looking back, do I know whether erfc(1) simplifies more? Or perhaps relate it to the original parameters?\n\nWait, erfc(1) is simply a number, approximately 0.1573, but symbolically, it's fine as erfc(1). Unless the problem expects a different form...\n\nWait, going back to definitions, erfc(z) = (2 / sqrt(pi)) ∫_{z}^\\infty exp(-t²) dt. Which matches our previous steps.\n\nAlternatively, writing it in terms of the Gamma function? Because Γ(n,z) is the incomplete Gamma function, but for n=1/"}
{"id": "quantum/5031", "questions": "Muonic atoms consist of mu mesons (mass $m_{\\mu} = 206m_e$) bound to atomic nuclei in hydrogenic orbits. The energies of the mu mesic levels are shifted relative to their values for a point nucleus because the nuclear charge is distributed over a region with radius $\\mathbf{R}$. The effective Coulomb potential can be approximated as\n\n$$\nV(r) = \\begin{cases} \n-\\frac{Ze^2}{r}, & r \\geq R, \\\\\n-\\frac{Ze^2}{R} \\left( \\frac{3}{2} - \\frac{1}{2} \\frac{r^2}{R^2} \\right), & r \\leq R. \n\\end{cases}\n$$\n\n(a) State qualitatively how the energies of the $1s, 2s, 2p, 3s, 3p, 3d$ muonic levels will be shifted absolutely and relative to each other, and explain physically any differences in the shifts. Sketch the unperturbed and perturbed energy level diagrams for these states.\n\n(b) Give an expression for the first order change in energy of the $1s$ state associated with the fact that the nucleus is not point-like.\n\n(c) Estimate the $2s-2p$ energy shift under the assumption that $R/a_{\\mu} \\ll 1$, where $a_{\\mu}$ is the Bohr radius for the muon and show that this shift gives a measure of $\\mathbf{R}$.\n\n(d) When is the method of part (b) likely to fail? Does this method underestimate or overestimate the energy shift? Explain your answer in physical terms. Useful information:\n\n$$\n\\psi_{1s} = 2N_0 e^{-r/a_{\\mu}} Y_{00}(\\theta, \\phi),\n$$\n\n$$\n\\psi_{2s} = \\frac{1}{\\sqrt{8}} N_0 \\left( 2 - \\frac{r}{a_{\\mu}} \\right) e^{-r/2a_{\\mu}} Y_{00}(\\theta, \\phi),\n$$\n\n$$\n\\psi_{2p} = \\frac{1}{\\sqrt{24}} N_0 \\frac{r}{a_{\\mu}} e^{-r/2a_{\\mu}} Y_{1m}(\\theta, \\phi),\n$$\n\nwhere\n\n$$\nN_0 = \\left( \\frac{1}{a_\\mu} \\right)^{3/2}.\n$$", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about muonic atoms, which involves some quantum mechanics concepts. Let me try to break it down piece by piece. \n\nStarting with part (a): It asks about the qualitative changes in the energies of various muonic levels when considering the finite size of the nucleus compared to a point nucleus. Hmm, okay. So normally, without worrying about the nuclear size, we model the atom using the Coulomb potential, leading to specific energy levels based on n and l quantum numbers—like the usual Hydrogen-like spectrum. But here, since the nucleus has a finite radius R, the potential inside the nucleus isn't just flat; instead, there's a different form given by V(r). For r less than R, the potential becomes more complicated—it’s like a quadratic function multiplied by Ze²/R. And outside, it behaves as the regular Coulomb potential.\n\nSo, what does this mean for the energy levels? Well, if the nucleus were a point particle, all s-states would have lower energies due to being closer to the nucleus on average, right? Because they penetrate deeper into regions near the origin. However, now that the potential within R is modified, maybe those s-states experience a slightly different potential. Since the inner potential is flatter but still negative, perhaps the binding energy decreases?\n\nWait, let me think again: If you're inside the nucleus, the potential doesn’t drop off as quickly as before. In the original case, the potential goes as -Ze²/r, getting stronger as you approach zero. Now, inside R, it's approximately linear up to a certain term—I need to compute whether this makes the overall potential \"softer\" or harder.\"\n\nLooking at the formula:\nFor r ≤ R,\nV(r) ≈ -(Ze²)/(R)*(3/2 - (1/2)(r²/R²)).\n\nLet me plug in r=0: Then V(0)=-(Ze²)/R*(3/2). At r=R, V(R)=-(Ze²)/R*(3/2 - 1/2)= -Ze²/(R)*1. Which matches the outer potential at r=R, good continuity.\n\nComparing to the pure Coulomb potential at small r: Previously, V ~ -Ze²/r, which tends to minus infinity as r approaches zero. Here, the potential only reaches around -1.5*Ze²/R at r=0, then increases quadratically towards r=R until it meets the standard Coulomb value. Therefore, the potential well is actually shallower inside the nucleus. That means electrons/muons don't feel such a strong attraction close to the center anymore—they’re effectively experiencing a weaker potential in the interior.\n\nHow does this affect the energy levels? Remember that the expectation value of the Hamiltonian determines the energy. A softer potential should lead to higher energies because the particles aren't pulled as strongly inward. Wait—isn't a weaker attractive force making them less tightly bound, hence higher (less negative) energies? Yes, exactly. So the energy levels become less deeply bound, meaning their absolute values decrease—their energies increase.\n\nBut wait, no—if the potential is less deep, the kinetic energy might compensate... Or do I consider the total effect? Maybe better to recall that in the Coulomb case, the ground state has the lowest energy. With a softened core, the wavefunction spreads out a bit more, reducing the depth of confinement, thus increasing the energy (making it less negative).\n\nTherefore, every orbital whose probability density extends significantly into the region r < R will see its energy increased—that is, made less negative. How much depends on how much time the electron spends inside R.\n\nNow, looking at orbitals: s-orbitals have non-zero probability at r=0, while p, d etc., have nodes there. Hence, s-orbitals spend more time near the nucleus, especially in smaller radii. Thus, their energies are affected more by the modification of the potential inside R.\n\nIn contrast, higher angular momentum states (l > 0) have nodes at the origin, so their wavefunctions start from zero at r=0 and rise outward. Their penetration into the central region is less significant, so their energies won't be changed as much.\n\nThus, comparing the same principal quantum number n, the s-state will have a larger shift upwards (towards zero) than the p, d states. Similarly, between different ns and np states across different n, say 1s vs 2s, both are s-states but with different radial extents.\n\nThe 1s state has most of its probability concentrated near the nucleus, whereas 2s has a node somewhere further out. Depending on the ratio of R to the typical radius of the orbital, the impact varies. If R is comparable to the Bohr radius scaled appropriately, then even 2s may extend into the region where the potential differs.\n\nGiven that the question mentions R << a_mu, which is probably going to come into play later, but for part (a), assuming general behavior.\n\nPutting it together: All the listed states' energies go up (become less negative) because the potential is less confining. Among them, the s-states (1s, 2s, 3s) get lifted more than the p and d states (which include 2p, 3p, 3d). Also, among the same n, the s is raised more than p, which is more than d.\n\nHence, the ordering of energy shifts is largest for s, next p, then d. Moreover, for the same type (e.g., s), higher n states have their centers farther away, so their dependence on the inner potential diminishes. So 1s experiences a bigger shift than 2s, which is bigger than 3s, similarly for p and d.\n\nAs for sketching the energy diagram: Normally, without the finite nucleus, all states with the same n have the same energy regardless of l (degenerate). After including the finite nucleus effect, the degeneracy breaks—for example, 2s and 2p split, with 2s having higher energy (since it was originally lower). Same applies to 3s, 3p, 3d splitting—with 3s moving up more than 3p, which moves up more than 3d.\n\nSo the unperturbed diagram shows horizontal lines for each n, with multiple states (different l) overlapping. Perturbed diagram shows each line breaking into separate points ordered by decreasing energy per l: highest for s, followed by p, then d.\n\nMoving on to part (b): First-order change in energy for the 1s state. This sounds like a perturbation theory calculation. We treat the difference between the actual potential and the Coulomb potential as a perturbing Hamiltonian H'.\n\nSince the exact potential is known, the correction ΔE is ⟨ψ₁ₛ|ΔV|ψ₁ₛ⟩, integrated over all space. Where ΔV is the difference between the true potential and the Coulomb one.\n\nExpress"}
{"id": "quantum/6011", "questions": "A slow electron of wave number $k$ is scattered by a neutral atom of effective (maximum) radius $R$, such that $kR \\ll 1$.\n\n(a) Assuming that the electron-atom potential is known, explain how the relevant phase shift $\\delta$ is related to the solution of a Schrödinger equation.\n\n(b) Give a formula for the differential scattering cross section in terms of $\\delta$ and $k$. (If you do not remember the formula, try to guess it using dimensional reasoning.)\n\n(c) Explain, with a diagram of the Schrödinger-equation solution, how a non-vanishing purely attractive potential might, at a particular $k$, give no scattering.\n\n(d) Explain, again with a diagram, how a potential that is attractive at short distances but repulsive at large distances might give resonance scattering near a particular $k$.\n\n(e) What is the maximum value of the total cross section at the center of the resonance?", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about electron scattering off a neutral atom. It's divided into five parts: from part (a) to (e). Let me take them one by one because each seems connected logically.\n\nStarting with part (a): \"Assuming that the electron-atom potential is known, explain how the relevant phase shift δ is related to the solution of a Schrödinger equation.\"\n\nHmm, okay. So when dealing with quantum mechanical scattering problems, especially like electrons scattering off atoms or nuclei, we often use partial waves analysis. The idea is that the incoming particle can be represented as a plane wave, which then scatters due to the potential created by the target. \n\nThe time-independent Schrödinger equation comes into play here. For a central potential V(r), the solutions can be expressed in spherical coordinates. We usually separate variables into radial and angular parts. Each angular momentum state l contributes its own phase shift δ_l. These phase shifts account for the changes in amplitude and phase caused by the scattering process.\n\nSo if k is the wave number of the incident electron, and R is the effective radius of the atom, given that kR << 1 means that the wavelength λ = 2π/k is much larger than R. This suggests that the electron doesn't resolve the detailed structure of the atom; it sees the atom more like a point-like scatterer rather than something extended over space.\n\nBut wait, even though kR is small, does that mean we're necessarily in the Born approximation regime where linear terms dominate? Maybe, since higher-order terms would involve products of potentials evaluated at different points, scaled down by powers of kR. But perhaps that's getting ahead of myself.\n\nBack to phase shifts. In scattering theory, after solving the Schrödinger equation, the asymptotic form of the wavefunction includes both the incoming plane wave and the outgoing spherical wave components. The relative phases between these determine the reflection and transmission coefficients, encapsulated in the phase shifts δ.\n\nSpecifically, for each partial wave l, there's a corresponding phase shift δ_l. When all possible l contribute, they interfere constructively or destructively depending on their respective δs. However, in some cases, particularly s-wave scattering (l=0), only δ_0 matters significantly, especially when other contributions are negligible—like maybe when the de Broglie wavelength is longer than typical interatomic distances?\n\nWait, actually, considering that kR << 1 implies low energy, long wavelengths. At low energies, lower orbital angular momenta tend to dominate because higher l require more kinetic energy—or is it the opposite? Hmm, let me think. Higher angular momentum states correspond to particles moving faster transversely, right? Since E ~ p²/(2m), higher p leads to higher E. If our energy is low, high-l states may not be excited. Therefore, probably only the s-wave (l=0) contributes meaningfully here.\n\nTherefore, the phase shift δ in question is likely just δ_0, the s-wave phase shift. To find δ, we solve the radial Schrödinger equation:\n\n[-(ħ²)/(2μ)(d²/dr²) + V(r)]ψ(r) = [E]ψ(r)\n\nWhere μ is the reduced mass of the system (electron plus nucleus?), but since the atom is neutral, maybe treating it as fixed? Wait, the atom has charge zero, so the interaction must be via van der Waals forces or something else—but regardless, the exact nature isn't specified beyond being a potential V(r).\n\nIn any case, the boundary conditions matter. As r approaches infinity, ψ should behave like e^{ikr} (incoming) plus an outgoing wave term involving e^{-ikr}. Or wait, standard scattering setup uses an incoming plane wave and outgoing spherical wave. Actually, the general solution for the wave function far away from the potential is:\n\nψ(r) ≈ e^{i(kz)} + f(θ)e^{ikr}/r\n\nwhere z is along the direction of incidence, and f(θ) is the scattering amplitude.\n\nFor each partial wave l, the scattering amplitude involves the Legendre polynomials P_l(cos θ):\n\nf(θ) = (1/ik) ∑_{l=0}^∞ (2l+1)[e^{2iδ_l} - 1]P_l(cos θ)\n\nSince in our case, kR << 1, implying low-energy scattering, and thus mainly s-waves (l=0) contribute. Hence, the dominant term will come from l=0, giving us:\n\nf(θ) ≈ (1/ik)[e^{2iδ_0} - 1]\n\nWhich simplifies things quite a bit.\n\nThus, relating back to the original question: How is δ related to the solution of the Schrödinger equation? Well, δ arises naturally from matching the regular solution inside the potential region to the outgoing wave solution outside. Solving the radial Schrödinger equation gives us the phase shift δ through the requirement of smoothness across the entire domain.\n\nMoving on to part (b): \"Give a formula for the differential scattering cross section in terms of δ and k.\" And hint says if don't remember, use dimensional reasoning.\n\nI recall that the differential cross-section dσ/dΩ is proportional to |f(θ)|². From above, f(θ) was approximated as (1/ik)[e^{2iδ} - 1], assuming only s-wave contribution.\n\nCalculating |f|²:\n\n|f|^2 = (1/k²)|(e^{2iδ} - 1)|² = (4 sin² δ)/k²\n\nBecause |e^{iφ} - 1|² = (cos φ - 1)^2 + sin² φ = 2(1 - cos φ) = 4 sin²(φ/2). Here, φ = 2δ, so becomes 4 sin² δ / k².\n\nHence,\n\ndσ/dΩ = |f|² = (4 sin² δ)/k²\n\nAlternatively, sometimes written as (sin² δ)/(k²/4), but same thing.\n\nLet me check dimensions. Cross-section σ has units of area, m². Differential cross-section per solid angle also needs to be m²/sr.\n\nGiven that k has units of inverse length (since ħk/p → dimensionless), so k² has units 1/m². Thus, 1/k² has units m², correct. Sin squared delta is dimensionless, hence overall unit matches.\n\nYes, makes sense.\n\nPart (c): \"Explain...how a non-vanishing purely attractive potential might, at a particular k, give no scattering.\"\n\nInteresting. A purely attractive potential could lead to bound states—if the binding energy is sufficient. But in scattering scenarios, unless the energy corresponds exactly to a pole in the S-matrix (resonance condition), you still get scattering.\n\nHowever, suppose the potential supports a bound state precisely at the energy corresponding to wavenumber k. Then, what happens?\n\nAt threshold, say, when the energy equals the binding energy, the Gamow factor causes enhanced cross sections. Alternatively, consider that when the energy coincides with a virtual level, the phase shift goes to π, leading to destructive interference.\n\nWait, another thought: if the potential is strongly attractive, it can create a situation where the phase shift δ reaches ±π/2. Because in the limit of very strong attraction, the wavefunction gets pushed outward, causing significant phase change.\n\nBut specifically, if the potential allows for a half-bound state—a state whose energy is exactly at the continuum—the scattering amplitude can vanish. That occurs when the denominator in certain expressions hits a node.\n\nWait, going back to the expression for the scattering amplitude f(θ)= (1/ik)(e^{2iδ}-1). Suppose δ = nπ, integer multiple of pi. Then e^{2iδ}= (-1)^{2n}=1. So f(theta)=(1/ik)(1 -1 )=0. Hence, no scattering!\n\nAh! Exactly. So if the phase shift δ is equal to an integer times π, i.e., δ = nπ, then the numerator becomes zero, making the scattering amplitude zero. Consequently, the differential cross-section is zero everywhere—it’s as if there were no scattering.\n\nThis phenomenon is called “zero scattering” or “no scattering.” Such situations occur when the potential creates a standing wave that perfectly cancels out the outgoing component, effectively reflecting nothing.\n\nGraphically, imagine the solution to the Schrödinger equation within the potential well. Without any outgoing wave, the probability current remains entirely ingoing except for ev"}
{"id": "quantum/3-3043", "questions": "Consider a neutral particle with intrinsic angular momentum $\\sqrt{s(s+1)}$, where $s = \\hbar/2$, i.e., a spin-1/2 particle.\n\nAssume the particle has a magnetic moment $M = \\gamma s$, where $\\gamma$ is a constant. The quantum-mechanical state of the particle can be described in a spin space spanned by the eigenvectors $|+\\rangle$ and $|-\\rangle$ representing alignments parallel and antiparallel to the z-axis:\n\n$$\n\\hat{s}_z |+\\rangle = \\frac{\\hbar}{2} |+\\rangle, \\quad \\hat{s}_z |-\\rangle = -\\frac{\\hbar}{2} |- \\rangle.\n$$\n\nAt time $t = 0$ the state of the system is $|\\psi(t) = 0) \\rangle = |+\\rangle$. The particle moves along the y-axis through a uniform magnetic field $\\mathbf{B} = B\\hat{y}$ oriented along the y-axis.\n\n(a) As expressed in the $|+\\rangle$, $\\lvert - \\rangle$ basis, what is $\\lvert \\psi(t) \\rangle$?  \n\n(b) What would be the expectation values for measurements of the observables $s_x, s_y, s_z$ as functions of time?", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about a spin-1/2 particle moving through a magnetic field. Let me try to break it down part by part. \n\nFirst, let's understand the setup. We're dealing with a neutral particle that has spin-1/2. Its magnetic moment M is given by γ times its spin s. So, M = γs. Since s is sqrt[s(s+1)] ħ, but wait, actually, no—wait, hold on. Spin s here refers to the magnitude squared being s(s+1), right? But since we know it's spin-1/2, then s=1/2, which makes sense because the eigenvalues of sz are ±ħ/2.\n\nThe initial state at t=0 is |ψ(0)⟩ = |+⟩, meaning the spin is aligned along the positive z-direction. Now, the particle is moving along the y-axis through a uniform magnetic field pointing along the y-axis: B = Bŷ. Hmm, okay.\n\nSo, part (a) asks for the state vector |ψ(t)⟩ expressed in the |+⟩, |−⟩ basis. To find this, I think I need to figure out how the state evolves over time under the influence of the magnetic field. That means finding the Hamiltonian first, probably using the interaction between the magnetic moment and the external field.\n\nRight, the energy due to a magnetic dipole in a magnetic field is H = -μ·B. Here, μ is the magnetic moment, which is given as M = γs. Wait, does that mean μ = γs or μ = γS, where S is the actual operator? Probably, yes—it should involve operators because we’re talking about quantum mechanics. So, the Hamiltonian should be proportional to the dot product of the spin operator and the magnetic field direction.\n\nSince the magnetic field is along y, the relevant component will be sy. Therefore, H = -γ s · B ŷ. Which simplifies to H = -γ B sy. Because only the y-component matters when you take the dot product with B in the y-direction.\n\nWait, hang on—is there a negative sign involved? In some conventions, the Zeeman term is written as H = -μ·B, while others might write it differently depending on whether they include signs elsewhere. Given that M = γs, perhaps H = -M·B becomes H = -γ s · B. Yeah, that seems correct.\n\nBut also, sometimes people define the gyromagnetic ratio such that the Hamiltonian could just be ω_s σ · n, etc.—but maybe not necessary now. Anyway, proceeding with H = -γ B sy.\n\nNow, knowing that, we can express H in terms of Pauli matrices if needed, but since our states are already in the sz basis, maybe expressing H in matrix form would help us solve the Schrödinger equation.\n\nLet’s recall that the spin components sx, sy, sz correspond to (ħ/2)σx, (ħ/2)σy, (ħ/2)σz respectively. So, sy = (ħ/2)σy. Thus,\n\nH = -γ B * (ħ/2) σy.\n\nTherefore, H = (-γ B ħ / 2 ) σy.\n\nExpressed in the standard basis {|+⟩, |−⟩}, σy looks like [[0, -i], [i, 0]]. Hence, multiplying by constants gives:\n\nH = (-γ B ħ / 2 ) * [[0, -i], [i, 0]] \n   = (γ B ħ / 2 ) * [[0, i], [-i, 0]]\n\nHmm, interesting. Alternatively, factoring out the imaginary unit, but regardless, the important thing is that H is diagonalized by its own eigenstates, which aren't necessarily the same as the sz eigenstates unless the field was along z.\n\nGiven that, solving the time evolution requires either diagonalizing H or working directly in the current basis. Maybe another approach is better—if we consider the Heisenberg picture or use the time-evolution operator.\n\nIn any case, the general method is to compute U(t)=exp(-i H t / ħ). Then apply this to the initial state |+⟩.\n\nAlternatively, note that H commutes with itself at different times, so the exponential can be computed straightforwardly.\n\nLet me proceed step-by-step.\n\nCompute H:\nAs above, H = -(γ B)(ħ/2) σy.\n\nThus,\nH = -(γ B ħ / 2) σy.\n\nWe can factor out the scalar coefficient:\n\nH = Ω σy, where Ω = -(γ B ħ)/2.\n\nThen, the time evolution operator is U(t) = exp(-i H t / ħ).\n\nSubstituting H into U(t):\n\nU(t) = exp[ (-i/(ħ)) * (Ω σy) * t ]\n\nWhich is equal to exp[ (-i Ω t / ħ) σy ].\n\nNote that Ω includes factors involving ħ, so substituting back:\n\nΩ = -(γ B ħ)/2 ⇒ Ω t / ħ = -(γ B t)/2.\n\nThus,\n\nU(t) = exp[ (i γ B t / 2 ) σy ].\n\nBecause the exponent became (-i)*(-(γ B t)/(2)) σy = (i gamma B t / 2) sigma y.\n\nAlright, so U(t) is e^{i theta sigma_y }, where theta = (gamma B t)/2.\n\nRecall from Euler's formula for Pauli matrices: For any real number α, exp(i alpha sigma_j) equals cos(alpha) I + i sin(alpha) sigma_j.\n\nYes! This identity holds because sigma_j^2 = I, similar to complex numbers' properties.\n\nApplying this identity, we get:\n\nU(t) = cos(theta) I + i sin(theta) sigma_y.\n\nWhere theta = (gamma B t)/2.\n\nSo expanding this, in matrix form, considering sigma_y is [[0,-i],[i,0]], multiplied by i sin(theta):\n\nI is [[1,0],[0,1]]\nsigma_y is [[0,-i],[i,0]]\nMultiply by i sin(theta): i*[[0,-i],[i,0]]= [[0,1],[-1,0]]\n\nSimilarly, cos(theta) I remains [[cos(theta),0],[0,cos(theta)]]\n\nAdding them together:\n\nU(t) = [[cos(theta), sin(theta)], [-sin(theta), cos(theta)]].\n\nThat's nice; it's a rotation matrix around the y-axis by angle phi = 2 theta?\n\nWait, more precisely, the overall effect is a rotation generated by sigma_y, hence rotating around y-axis.\n\nIndeed, the time evolution caused by H ~ sigma_y leads to precession around the y-axis.\n\nBut anyway, having found U(t), we can act upon the initial state |+⟩.\n\nWhat is |+⟩ in matrix terms? It's represented as column vector [1; 0].\n\nSo applying U(t) to |+⟩:\n\nU(t)|+> = [[cos(theta), sin(theta)], [-sin(theta), cos(theta)]] * [1; 0] = [cos(theta); -sin(theta)]\n\nSo, writing this as a linear combination of |+⟩ and |->:\n\n|ψ(t)> = cos(theta) |+> - sin(theta) |->.\n\nWith theta = (gamma B t)/2.\n\nIs that all? Well, except for possible phase factors—I wonder if there's a global phase difference.\n\nWait, looking again at the expression for U(t):\n\nIt's e^{i theta sigma_y } acting on |+>.\n\nAnother way to see this is via expansion:\n\ne^{i theta sigma_y} |+>\n\nUsing the series expansion, but alternatively, recognizing that |+> and |-> are eigenkets of sigma_z, whereas sigma_y acts on these states mixing them.\n\nAlternatively, one can represent |+> and |-> in terms of eigenstates of sigma_y, but that may complicate things further.\n\nHowever, my earlier calculation suggests that after application of U(t), the state becomes cos(theta)|+> - sin(theta)|->.\n\nTo check"}
{"id": "quantum/2-2017", "questions": "Prove that in any single-particle bound energy eigenstate the following relation is satisfied in nonrelativistic quantum mechanics for a central field-of-force potential $ V(r) $,\n\n$$\n| \\psi(0) |^2 = \\frac{m}{2\\pi} \\left\\langle \\frac{dV(r)}{dr} \\right\\rangle - \\frac{1}{2\\pi} \\left\\langle \\frac{L^2}{r^3} \\right\\rangle,\n$$\n\nwhere $ \\psi(0) $ is the wave function at the origin, $ m $ the particle mass, and $ L^2 $ the square of the orbital angular momentum operator (let $ \\hbar = 1 $). Give a classical interpretation of this equation for the case of a state with angular momentum $ \\neq 0 $.", "graphs": null, "llm_answers": "<think>\nOkay, so I have to prove this quantum mechanical identity involving the probability density at the origin, some expectation values related to the derivative of the potential, and the angular momentum squared over r cubed. Hmm.\n\nFirst off, let me write down what's given:\n\nWe need to show that for a single-particle bound state in a central potential \\( V(r) \\), the modulus squared of the wavefunction at zero satisfies:\n\\[ |\\psi(0)|^2 = \\frac{m}{2\\pi} \\left\\langle \\frac{dV(r)}{dr} \\right\\rangle - \\frac{1}{2\\pi} \\left\\langle \\frac{L^2}{r^3} \\right\\rangle. \\]\n\nAlright, since it's a central potential problem, we can use spherical coordinates and separate variables into radial and angular parts. The wavefunction should be expressible as \\( \\psi(\\mathbf{r}) = R(r) Y_{lm}(\\theta, \\phi) \\).\n\nGiven that, maybe I can start from the Schrödinger equation because all these terms probably come out of manipulating it somehow.\n\nThe time-independent Schrödinger equation for a central potential is:\n\\[ \\left[ -\\frac{\\nabla^2}{2m} + V(r) \\right] \\psi = E \\psi. \\]\nSince \\( \\psi = R(r) Y_{lm} \\), substituting gives us:\n\\[ -\\frac{R''}{2m} - \\frac{(l(l+1))}{2mr^2} R + V'(r) R' + [E - V(r)] R = 0? Wait, no, perhaps better to compute each term properly.\n\nWait, actually, when you substitute \\( \\psi = R(r)Y \\) into the Laplacian, you get:\n\\[ \\nabla^2 \\psi = \\frac{1}{r^2}\\frac{\\partial}{\\partial r}(r^2 \\frac{\\partial R}{\\partial r}) Y + \\frac{1}{r^2} \\nabla_Y^2 Y R. \\]\nBut \\( \\nabla_Y^2 Y = -l(l+1) Y \\), right?\n\nSo putting it together:\n\\[ \\nabla^2 \\psi = \\left[ \\frac{1}{r^2} \\frac{d}{dr} (r^2 \\frac{dR}{dr}) ) - \\frac{l(l+1)}{r^2} R \\right] Y. \\]\n\nTherefore, plugging back into the Schrödinger equation:\n\\[ -\\frac{1}{2m} \\left[ \\frac{1}{r^2} \\frac{d}{dr} (r^2 \\frac{dR}{dr}) - \\frac{l(l+1)}{r^2} R \\right] + V(r) R Y = E R Y. \\]\n\nMultiplying both sides by \\( 2m \\):\n\\[ - \\frac{1}{r^2} \\frac{d}{dr} (r^2 \\frac{dR}{dr}) + \\frac{l(l+1)}{r^2} R + 2m(V(r)-E) R = 0. \\]\n\nLet me rearrange this:\n\\[ \\frac{1}{r^2} \\frac{d}{dr} (r^2 \\frac{dR}{dr}) = \\frac{l(l+1)}{r^2} R + 2m(V(r)-E) R. \\]\n\nHmm, not sure if directly helpful yet. Maybe integrating something or taking inner products would help here.\n\nAlternatively, think about operators: Since \\( L^2 \\) acts on the angular part only, its action on \\( Y_{lm} \\) is known—it just gives \\( l(l+1) Y_{lm} \\). So perhaps expressing things in terms of expectation values could lead somewhere.\n\nAlso, note that \\( dV/dr \\) appears in one of the terms—maybe differentiating the potential comes up naturally in force considerations.\n\nAnother thought: In three dimensions, sometimes identities like integration by parts or using delta functions might pop up, especially considering evaluating quantities at the origin.\n\nWait, but how do we relate \\(|\\psi(0)|^2\\) which involves the value at a point, while other terms involve integrals over space via expectation values.\n\nPerhaps consider multiplying both sides of the Schrödinger equation by something and then integrate.\n\nSuppose I take the original Schrödinger equation:\n\\[ H \\psi = E \\psi, \\]\nwhich implies:\n\\[ (-\\frac{\\nabla^2}{2m} + V(r)) \\psi = E \\psi. \\]\nThen moving everything except kinetic energy to the other side:\n\\[ -\\frac{\\nabla^2}{2m} \\psi = (E - V(r)) \\psi. \\]\nTaking the inner product with another function... But wait, our target expression has \\(|\\psi(0)|^2\\), which suggests looking at matrix elements where position is involved.\n\nMaybe instead, look at specific components—for example, evaluate the equation at the origin.\n\nHowever, the issue is that derivatives at the origin may require careful handling due to singularities or boundary conditions.\n\nIn particular, near the origin, the behavior of the wavefunction depends on whether there's angular momentum or not. For s-states (\\( l=0 \\)), the wavefunction remains finite; otherwise, higher angular momenta cause nodes at the origin.\n\nBut regardless, perhaps expanding around \\( r=0 \\)?\n\nWait, alternatively, recall that in 3D, the Fourier transform relates position and momentum representations, but I'm not sure if that helps immediately.\n\nHold on, thinking again about the form of the desired equation. It resembles certain virial theorem expressions or Ehrenfest relations, but more localized.\n\nWait, the left-hand side is \\(|\\psi(0)|^2\\), which is proportional to the probability density at the origin. On the RHS, we have two expectation values multiplied by constants.\n\nI wonder if dimensional analysis holds—if units make sense. Let’s check quickly:\n\n- Left side: Probability density, unit of inverse volume.\n  \nRight side first term: \\( m/(2π) * ⟨dV/dr⟩ \\).\nAssuming \\( V \\) has units of energy, \\( dV/dr \\) has units of energy per length. Multiply by mass (kg): kg*(energy/length)= kg*(J/m)= kg*(N*m)/m= N*kg*s²/m /m ? Wait, J=kg·m²/s², so dV/dr is kg·m/s², i.e., Newtons per charge? Not quite, but anyway...\n\nMultiply by m/(2π): Mass times (force)/(mass)? Because dV/dr is F (if V is potential energy, then gradient is force). So Force is kg·m/s². Then m/(2π)*⟨F⟩ → kg*(kg·m/s²)/something? Wait, getting confused.\n\nSimilarly, second term: 1/(2π) * ⟨L²/r³⟩. Angular momentum squared is (kg·m²/s)^2, divided by m³, giving kg²·m^{-4}/s². That seems complicated.\n\nNot sure if dimensionally consistent without knowing exact factors—but assuming they match, proceed.\n\nBack to approach: Perhaps multiply the Schrodinger equation by something and integrate.\n\nConsider starting from the Schrödinger equation:\n\\[ -\\frac{\\nabla^2}{2m} \\psi + V(r)\\psi = E \\psi. \\]\nBring the Hamiltonian to the other side:\n\\[ -\\frac{\\nabla^2}{2m} \\psi = (E - V(r)) \\psi. \\]\nNow, take the inner product with ψ itself:\n\\[ \\int \\psi^*(-\\frac{\\nabla^2}{2m}) \\psi dr^3 = \\int \\psi^*(E - V(r)) \\psi dr^3. \\]\nWhich simplifies to:\n\\[ \\frac{1}{2m} \\int |\\nabla \\psi|^2 dr^3 = \\int (E - V(r)) |\\psi|^2 dr^3. \\]\nThis is similar to the standard result relating kinetic and potential energies, but doesn't seem directly applicable.\n\nAlternative idea: Use vector calculus identities. Consider the divergence of some quantity.\n\nRecall that ∇ · (∇ψ*) = △ψ*, etc. Alternatively, perhaps manipulate the equation to find an expression involving δ(r), since ψ(0) is evaluated at the"}
{"id": "quantum/3-3028", "questions": "A negatively charged $\\pi^-$ meson (a pseudoscalar particle: zero spin, odd parity) is initially bound in the lowest-energy Coulomb wave function around a deuteron. It is captured by the deuteron (a proton and a neutron in a $^{3}S_1$ state), which is converted into a pair of neutrons:\n\n$$\n\\pi^- + d \\rightarrow n + n.\n$$\n\n(a) What is the orbital angular momentum of the neutron pair?  \n\n(b) What is their total spin angular momentum?  \n\n(c) What is the probability for finding both neutron spins directed opposite to the spin of the deuteron?  \n\n(d) If the deuteron spin is initially 100% polarized in the $\\hat{R}$ direction, what is the angular distribution of the neutron emission probability (per unit solid angle) for a neutron whose spin is opposite to that of the initial deuteron?\n\nYou may find some of the first few (not normalized) spherical harmonics useful:\n\n$$\nY_0^0 = 1, \\quad Y_1^{\\pm1} = \\mp \\sin \\theta \\ e^{\\pm i \\phi},\n$$\n\n$$\nY_1^0 = \\cos \\theta, \\quad Y_2^{\\pm1} = \\mp \\sin 2 \\theta \\ e^{\\pm i \\phi}.\n$$", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about pion capture on deuterium leading to two neutrons being produced. The question has four parts, but let me start with part (a): determining the orbital angular momentum of the neutron pair.\n\nFirst, I need to recall what's given. A π⁻ meson collides with a deuteron (d). Deuteron is made up of a proton and a neutron in a specific quantum state—given as a ^3S₁ state. So, it's symmetric under exchange because protons and neutrons are different particles, right? Wait, no—the symmetry depends on whether they're identical or not. Since proton and neutron aren't identical fermions, maybe the overall wavefunction doesn't necessarily require antisymmetry... Hmm, actually, wait, the deuteron itself must obey Fermi-Dirac statistics since it consists of a proton and neutron, which are distinguishable. But regardless, its internal structure is important here.\n\nThe reaction is π⁻ + d → n + n. So, we go from one deuteron plus a pion to two neutrons. Now, considering conservation laws—I think angular momentum should be conserved here. Let me break down each component's contribution before and after the collision.\n\nBefore the collision:\n- The deuteron is in a ^3S₁ state. That means its orbital angular momentum l_d is 0 (since S states have l=0?), wait no—it’s a triplet S-state, meaning the spatial part is s-wave (l=0), but the spin part is symmetric. For a deuteron, the total angular momentum J is equal to the sum of the individual spins because there's no orbital angular momentum between them. Each nucleon has spin ½, so when combined, you can get either singlet (J=0) or triplet (J=1). In this case, it's a ^3S₁ state, so J=1. Therefore, the deuteron has total angular momentum J=1.\n\nWait, hold on—if the deuteron is in a ^3S₁ state, then yes, the orbital angular momentum l is 0, and the spin multiplicity gives J=1. Got it.\n\nNow, the incoming pion is described as having zero spin (pseudoscalar particle) and negative charge. Its orbital angular momentum relative to the deuteron would depend on how it approaches. However, if it's initially bound in the \"lowest-energy Coulomb wave function,\" perhaps that implies it's in an s-orbital, so l_pion = 0. Because lower energy corresponds to lower angular momentum; higher angular momentum leads to more kinetic energy due to centrifugal barrier.\n\nSo, assuming the pion is in an s-state (l=0) orbiting the deuteron. Then, the total angular momentum before the collision comes only from the deuteron's own angular momentum, which is J=1.\n\nAfter the collision, we produce two neutrons. We need to figure out their orbital angular momentum. Again, using conservation of angular momentum, the total angular momentum before equals the total after.\n\nBut also, note that the pion is a pseudoscalar particle, which affects parity considerations. Parity might play a role in allowed transitions, especially regarding angular momentum changes.\n\nLet me outline all components contributing to angular momentum before and after.\n\nInitial State:\n- Pion: Spin 0, Orbital Angular Momentum L_initial. Assuming it's in the ground state, probably L_initial = 0.\n- Deuteron: Total Angular Momentum J_d = 1 (from ^3S₁).\n\nTherefore, total initial angular momentum J_total_initial = J_d + L_initial = 1 + 0 = 1.\n\nFinal State:\nWe have two neutrons. Their total angular momentum will come from three sources:\n1. Orbital angular momentum between the two neutrons, say L_final.\n2. Spin angular momenta of each neutron, S1 and S2.\n3. Any possible coupling between these.\n\nHowever, since the process involves strong interaction (as pion captures typically do via nuclear forces), angular momentum is definitely conserved, but parity could impose selection rules.\n\nGiven that the pion is a pseudoscalar, its intrinsic parity is -1. The deuteron, being composed of a proton and neutron, has positive parity (+1). When combining with the pion, the system's parity becomes (-1)*(+1)*(-1)^L_initial, where L_initial is the orbital angular momentum of the pion. Since L_initial was assumed to be 0, the parity remains (-1)*(+1)= -1.\n\nIn the final state, the two neutrons form a nucleus-like configuration. Neutron-neutron systems have certain parity properties too. The parity of the entire system after decay needs to match the initial parity times any change due to interactions—but since the interaction is strong, which conserves parity, the final parity should match the initial.\n\nEach neutron is a fermion with half-integer spin, but together, their combination can lead to integer or half-integer total spin depending on alignment. Also, their orbital angular momentum contributes to the parity.\n\nParity transformation reverses the sign based on the number of spatial dimensions changed, hence for orbital angular momentum, parity is (-1)^L. Similarly, for spin, since spinors transform differently, but for bosons vs fermions, but in our case, the two neutrons' spin state isn’t directly affecting parity unless coupled with something else.\n\nWait, perhaps better to consider the whole thing. Initial parity is -1. Final parity is determined by the product of the parities of the two neutrons and the parity associated with their orbital motion.\n\nSince each neutron is a fermion, their intrinsic parity is +1 each (for Dirac particles, fermions have parity +1). Thus, the product is (+1)(+1)=+1. Multiply by the parity from orbital angular momentum, which is (-1)^L_final. Hence, total parity is (+1)*(-1)^L_final.\n\nThis must equal the initial parity, which was -1. So,\n\n(+1)*(-1)^L_final = -1 ⇒ (-1)^L_final = -1 ⇒ L_final must be odd.\n\nThus, L_final is an odd integer. Possible values are 1, 3,... etc., but likely minimal value, so L_final=1.\n\nHence, the orbital angular momentum of the neutron pair is 1.\n\nAlternatively, another approach: Considering the initial state had total angular momentum J=1. After the reaction, the two neutrons carry away angular momentum. To conserve angular momentum, the total J_after must still be 1.\n\nAssuming that the two neutrons don't have much net orbital angular momentum beyond what's necessary, but given that the initial state had J=1, the final state must accommodate that.\n\nMoreover, the two neutrons can combine their spins and orbital angular momentum such that the total adds up vectorially to 1.\n\nIf the orbital angular momentum is 1, and their spins add up appropriately, that seems plausible.\n\nAnother point: The pion is spin-zero, so it cannot contribute anything except through its orbital angular momentum, which we considered as zero. So, the deuteron's spin (which is 1) must transfer entirely into the resulting system.\n\nWhen the deuteron breaks apart into two neutrons, those neutrons inherit the angular momentum. Given that the deuteron was spinning with J=1, the two neutrons must collectively carry away J=1.\n\nPossible ways to achieve this include:\n\nCase 1: Both neutrons have parallel spins adding to S=1, and their orbital angular momentum L=0. This way, total J=S⊕L=1⊕0=1.\n\nCase 2: One neutron spin-up, other spin-down, giving S=0, requiring L=1 to give total J=1.\n\nWhich scenario occurs?\n\nWell, looking back at parity consideration earlier, we found that L_final must be odd, specifically 1. So Case 2 applies: S=0, L=1.\n\nBecause if L were even, like 0, the parity wouldn't work out. As per above, L must be 1.\n\nAdditionally, the two neutrons are identical fermions, so their total wavefunction must be anti"}
{"id": "quantum/6042", "questions": "A hydrogen atom in its first excited $(2p)$ state is placed in a cavity. At what temperature of the cavity are the transition probabilities for spontaneous and induced emissions equal?", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about a hydrogen atom in its first excited 2p state being placed inside a cavity. The question asks: \"At what temperature of the cavity are the transition probabilities for spontaneous and induced emissions equal?\" Hmm, interesting.\n\nAlright, let me start breaking it down. First off, when we talk about transitions between energy states in atoms, especially in quantum mechanics contexts like this, we're dealing with processes such as spontaneous emission and stimulated (or induced) emission. These concepts were made famous by Einstein's work on radiation theory, right?\n\nSo, spontaneous emission occurs when an electron drops from a higher energy level to a lower one without any external influence, emitting a photon in the process. On the other hand, induced or stimulated emission happens when a passing photon causes the electron to drop to a lower energy level, releasing another photon identical to the incoming one. This is actually how lasers work—amplifying light through stimulated emission.\n\nNow, the problem mentions that these two types of emissions should have equal transition probabilities. So, mathematically, their rates must be equal. Let me recall the formulas involved here.\n\nFor spontaneous emission, the rate constant is given by Einstein's A coefficient multiplied by some factors involving Planck's constant and frequency. Specifically, the probability per unit time for spontaneous emission is:\n\n\\[ W_{spont} = A_{21} \\]\n\nWhere \\( A_{21} \\) is the Einstein coefficient for spontaneous emission from state 2 to state 1.\n\nOn the flip side, the rate for induced emission depends on the number density of photons present in the cavity. It’s proportional to the intensity of the incident radiation, which relates to the Bose-Einstein distribution because photons obey Bose statistics. The formula for the induced emission rate is:\n\n\\[ W_{induced} = B_{21} u(\\nu) \\]\n\nHere, \\( B_{21} \\) is the Einstein coefficient for induced emission, and \\( u(\\nu) \\) represents the spectral radiance or the photon occupation number at the relevant frequency ν.\n\nWait, but hold on—the exact expression might involve more terms depending on whether we’re considering blackbody radiation or something else. Since the cavity has thermal radiation, it would follow the Bose-Einstein distribution. For high temperatures where the occupancy isn't too large, maybe we can approximate using classical expressions, but perhaps not necessary yet.\n\nI remember there's a relation connecting the coefficients A and B via detailed balance. In equilibrium conditions, the ratio of populations of two energy levels follows Boltzmann distribution, leading to relations among the Einstein coefficients. But since our case involves comparing spontaneous and induced emission rates, maybe those ratios come into play.\n\nBut wait, no—if we set the two transition probabilities equal, then:\n\n\\[ A_{21} = B_{21} u(\\nu) \\]\n\nThat seems straightforward. Now, solving for u(ν), we get:\n\n\\[ u(\\nu) = \\frac{A_{21}}{B_{21}} \\]\n\nHmm, okay. What do I know about the relationship between A and B coefficients? From Einstein's original treatment, he found that:\n\n\\[ \\frac{A_{21}}{B_{21}} = \\frac{\\hbar \\omega}{4\\pi c^3 / V} } \\times ... Wait, maybe better expressed differently.\n\nActually, the key relation comes from the fact that in thermodynamic equilibrium, the population inversion cannot occur unless you pump energy into the system. That aside, the specific connection between A and B coefficients is given by:\n\n\\[ \\frac{A_{21}}{B_{21}} = \\frac{\\hbar \\omega}{k_B T} \\cdot g_1 - g_2 \\right]^{-1} \\]\n\nNo, wait—that doesn’t seem quite accurate. Maybe I need to think again.\n\nEinstein derived that the ratio of the spontaneous emission coefficient to the absorption coefficient is related to the Bose factor. More precisely,\n\n\\[ \\frac{A_{21}}{B_{12}} = \\frac{\\hbar \\omega}{k_B T} \\]\n\nIs that correct? Or was it different... Actually, now I'm getting confused. Let me check my notes mentally.\n\nAh! Yes, Einstein showed that under thermal equilibrium, the ratio of the spontaneous emission coefficient to the absorption coefficient is inversely proportional to the exponential of the energy difference over kT. Alternatively, if I consider the detailed balance condition, the ratio of upward and downward transitions must match the population ratio.\n\nAlternatively, recalling that the Einstein coefficients satisfy certain relationships. One important equation is:\n\n\\[ \\frac{A_{21}}{B_{21}} = \\frac{\\hbar \\omega}{8\\pi v_p^3} \\left( e^{\\hbar \\omega/(k_B T)} - 1 \\right)^{-1} \\]\n\nWait, that looks familiar. Here, \\( v_p \\) is the phase velocity of light in the medium, but assuming vacuum, it'd just be c. Also, note that sometimes people write \\( u(\\nu) \\) as part of the expression.\n\nHold on, perhaps instead of trying to directly relate A and B, I should express both transition probabilities in terms of known quantities and equate them.\n\nGiven that, let's define each term properly.\n\nThe spontaneous emission rate is simply \\( W_s = A_{21} \\).\n\nInduced emission rate is \\( W_i = B_{21} n_\\gamma \\), where \\( n_\\gamma \\) is the number of photons in the mode corresponding to the transition.\n\nIn a cavity, the photon number density can be approximated based on the cavity volume and modes available. However, often times, particularly in problems like this, they use the concept of the average photon number in a particular mode due to thermal fluctuations.\n\nAssuming the cavity supports electromagnetic waves, the average number of photons in a mode at temperature T is given by the Bose-Einstein distribution:\n\n\\[ \\langle n \\rangle = \\frac{1}{e^{\\hbar \\omega / (k_B T)} - 1} \\]\n\nTherefore, substituting back into the induced emission rate:\n\n\\[ W_i = B_{21} \\cdot \\frac{1}{e^{\\hbar \\omega / (k_B T)} - 1} \\]\n\nSetting \\( W_s = W_i \\):\n\n\\[ A_{21} = B_{21} \\cdot \\frac{1}{e^{\\hbar \\omega / (k_B T)} - 1} \\]\n\nSolving for T:\n\nFirstly, rearrange the equation:\n\n\\[ e^{\\hbar \\omega / (k_B T)} - 1 = \\frac{B_{21}}{A_{21}} \\]\n\nThen add 1 to both sides:\n\n\\[ e^{\\hbar \\omega / (k_B T)} = 1 + \\frac{B_{21}}{A_{21}} \\]\n\nTake natural logarithm on both sides:\n\n\\[ \\frac{\\hbar \\omega}{k_B T} = \\ln\\left(1 + \\frac{B_{21}}{A_{21}}\\right) \\]\n\nThus,\n\n\\[ T = \\frac{\\hbar \\omega}{k_B \\ln\\left(1 + \\frac{B_{21}}{A_{21}}\\right)} \\]\n\nNow, I need to find the value of \\( \\frac{B_{21}}{A_{21}} \\). Is there a standard relation between these coefficients?\n\nYes, indeed. From Einstein's derivation, the ratio of the induced emission coefficient to the spontaneous emission coefficient is connected to the properties of the radiation field. Specifically, in free space, the relation is:\n\n\\[ \\frac{B_{21}}{A_{21}} = \\frac{k_B T}{\\hbar \\omega} \\cdot \\frac{g_1}{g_2} \\]\n\nWait, hmm—is that correct? Because earlier, I thought the ratio \\( A/B \\) had a dependence on ω/T.\n\nLet me double-check. Another approach: Einstein's coefficients satisfy\n\n\\( \\frac{A_{21}}{B_{21}} = \\frac{\\hbar \\omega}{k_B T} \\)\n\nWait, does that make sense dimensionally? Let's see: [A] is s⁻¹; [B] is cm³·s⁻¹; hbar omega has units J*s * Hz → J=kg m²/s², so hbar omega is kg m²/s. Divided by k_b*T, which is also in Joules (since k_b has units J/K, T is K): so overall, [hbarω/kBT] is dimensionless. Therefore, yes, the left-hand side is s⁻¹ divided by (cm³ s⁻¹), giving cm⁻³, while the RHS is dimensionless. Hmmm, that suggests inconsistency.\n\nWait, probably missing a geometric factor somewhere. Perhaps including the density of states or something similar.\n\nAnother way: Considering that the Einstein coefficients are defined such that:\n\n- Spontaneous emission rate: \\( W_{spont} = A_{21} \\)\n- Induced emission rate: \\( W_{induced} = B_{21} N_\\gamma \\)\n  \nAnd similarly, the absorption rate is \\( W_{absorption} = B_{12} N_\\gamma \\)\n\n"}
{"id": "quantum/5053", "questions": "A particle of mass $\\text m$ is moving in the three-dimensional harmonic oscillator potential $V(x, y, z) = \\frac{1}{2} m \\omega^2 (x^2 + y^2 + z^2)$. A weak perturbation is applied in the form of the function $\\Delta V = kxyz + \\frac{k^2}{\\hbar \\omega} x^2 y^2 z^2$, where $k$ is a small constant. Note the same constant $k$ appears in both terms.\n\n(a) Calculate the shift in the ground state energy to second order in $k$.\n(b) Using an argument that does not depend on perturbation theory, what is the expectation value of $x$ in the ground state of this system?\n\nNote:You may wish to know the first few wave functions of the one-dimensional harmonic oscillator:\n\nground state\n\n$$\n\\psi_0(x) = \\left(\\frac{m\\omega}{\\pi \\hbar}\\right)^{1/4} \\exp\\left(-\\frac{m\\omega}{2\\hbar} x^2\\right),\n$$\n\nfirst excited state\n\n$$\n\\psi_1(x) = \\left(\\frac{m\\omega}{\\pi \\hbar}\\right)^{1/4} \\sqrt{\\frac{2m\\omega}{\\hbar}} \\, x \\exp\\left(-\\frac{m\\omega}{2\\hbar} x^2\\right),\n$$\n\nsecond excited state\n\n$$\n\\psi_2(x) = \\left(\\frac{m\\omega}{\\pi \\hbar}\\right)^{1/4} \\frac{1}{\\sqrt{2}}\\left(\\frac{2m\\omega}{\\hbar} x^2 - 1\\right) \\exp\\left(-\\frac{m\\omega}{2\\hbar} x^2\\right).\n$$\n", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about a particle in a three-dimensional harmonic oscillator potential with a weak perturbation added. The question has two parts: part (a) asks for the shift in the ground state energy up to second order in \\( k \\), and part (b) wants me to find the expectation value of \\( x \\) without using perturbation theory. Let's tackle them one by one.\n\nStarting with part (a). First, let me recall some basics about perturbation theory. In non-degenerate perturbation theory, the first-order energy shift is given by the expectation value of the perturbing Hamiltonian in the unperturbed state. For higher orders, like second order here, we need to consider transitions through intermediate states via matrix elements squared divided by energy differences.\n\nThe original potential is \\( V(x,y,z) = \\frac{1}{2} m \\omega^2 (x^2 + y^2 + z^2) \\). So it's symmetric in all three coordinates. The perturbation is \\( \\Delta V = k xyz + \\frac{k^2}{\\hbar \\omega} x^2 y^2 z^2 \\).\n\nSince the perturbation includes two terms, each involving products of different powers of \\( x, y, z \\), maybe they can be treated separately? Or perhaps together since they're multiplied by the same parameter \\( k \\)?\n\nWait, but actually, when considering perturbations, if you have multiple terms, sometimes cross-terms appear in higher orders. But since the problem says \"to second order,\" which would include contributions from both linear (\\( k \\)) and quadratic (\\( k^2 \\)) terms as well as any possible mixing between them.\n\nBut wait, no—actually, in standard Rayleigh-Schrödinger perturbation theory, the first-order term comes only from the first-order perturbation, i.e., proportional to \\( k \\), while the second-order term involves either squares of the first-order perturbation or direct inclusion of the second-order term. Hmm, now I'm getting confused because our perturbation itself already contains a \\( k^2 \\) term. Maybe I should think carefully about how to structure this.\n\nLet me denote the full perturbation as:\n\\( H' = k xyz + \\lambda x^2 y^2 z^2 \\)\nwhere \\( \\lambda = \\frac{k^2}{\\hbar \\omega} \\). Then, treating \\( H' \\) as having two separate components might complicate things, especially since \\( \\lambda \\) depends on \\( k^2 \\). Alternatively, treat everything as being scaled by \\( k \\); then, expanding to second order in \\( k \\) will involve terms up to \\( k^2 \\).\n\nSo, more precisely, the total perturbation is \\( H' = k xyz + \\frac{k^2}{\\hbar \\omega} x^2 y^2 z^2 \\). Therefore, when calculating the energy shifts, the first-order correction is due to \\( k xyz \\), and the second-order correction could come from two sources: (i) squaring the first term, leading to something proportional to \\( k^2 \\), and (ii) directly taking into account the second term \\( \\frac{k^2}{\\hbar \\omega} x^2 y^2 z^2 \\).\n\nTherefore, the overall approach is:\n\nFirst-order energy shift: \\( E^{(1)} = \\langle \\psi_{0} | H'_1 | \\psi_{0} \\rangle \\), where \\( H'_1 = k xyz \\)\n\nSecond-order energy shift: \\( E^{(2)} = \\sum_{n \\neq 0} \\frac{|\\langle n | H'_1 | 0 \\rangle|^2}{E_0 - E_n} + \\langle \\psi_{0}| H'_2 | \\psi_{0} \\rangle \\), where \\( H'_2 = \\frac{k^2}{\\hbar \\omega} x^2 y^2 z^2 \\)\n\nHence, combining these gives the total shift up to second order.\n\nNow, starting with the first-order term. Since the ground state of the 3D harmonic oscillator is separable into product states along each axis, specifically,\n\n\\( \\psi_{0}(x,y,z) = \\psi_0(x)\\psi_0(y)\\psi_0(z) \\),\n\nand each \\( \\psi_0 \\) is even under reflection (since Gaussian-like exponential times even power). Now, looking at \\( H'_1 = k xyz \\): multiplying odd functions (each coordinate is odd) leads to an odd function. However, the integral over all space of an odd function is zero. Hence, \\( E^{(1)} = 0 \\).\n\nThat was straightforward. Moving on to the second-order term. This consists of two parts: the sum over intermediate states and the expectation value of \\( H'_2 \\).\n\nCalculating the expectation value of \\( H'_2 \\):\n\n\\( \\langle 0 | H'_2 | 0 \\rangle = \\frac{k^2}{\\hbar \\omega} \\langle 0 | x^2 y^2 z^2 | 0 \\rangle \\)\n\nGiven the separability, this becomes:\n\n\\( \\frac{k^2}{\\hbar \\omega} [\\langle x^2 \\rangle]^2 [y] similar... Wait, hold on. Actually, \\( x^2 y^2 z^2 \\) factors into \\( x^2 \\times y^2 \\times z^2 \\), so the expectation value is just the product of individual expectations:\n\n\\( \\langle x^2 \\rangle \\cdot \\langle y^2 \\rangle \\cdot \\langle z^2 \\rangle \\)\n\nFor each direction, the expectation value of \\( x^2 \\) in the ground state of a 1D HO is known. From quantum mechanics, \\( \\langle x^2 \\rangle = \\frac{\\hbar}{2 m \\omega} \\). Similarly for \\( y^2 \\) and \\( z^2 \\). Thus,\n\n\\( \\langle x^2 y^2 z^2 \\rangle = \\left( \\frac{\\hbar}{2 m \\omega} \\right)^3 \\)\n\nThus, plugging back into \\( E^{(2)} \\):\n\n\\( E^{(2)}_{direct} = \\frac{k^2}{\\hbar \\omega} \\times \\left( \\frac{\\hbar}{2 m \\omega} \\right)^3 = \\frac{k^2}{\\hbar \\omega} \\times \\frac{\\hbar^3}{8 m^3 \\omega^3} }= \\frac{k^2 \\hbar^2}{8 m^3 \\omega^4} \\)\n\nSimplify numerator and denominator:\n\nHmm, simplifying exponents:\n\nNumerator: \\( k^2 \\hbar^2 \\)\n\nDenominator: \\( 8 m^3 \\omega^4 \\ )\n\nAlternatively, factor out constants:\n\nIt remains as \\( \\frac{k^2 \\hbar^2}{8 m^3 \\omega^4} \\ ). Okay, note that down as one contribution to \\( E^{(2)} \\).\n\nNext, compute the other part of \\( E^{(2)} \\), which is the summation over intermediate states:\n\n\\( \\sum_{n \\neq 0} \\frac{| \\langle n | H'_1 | 0 \\rangle |^2}{E_0 - E_n} \\)\n\nHere, \\( H'_1 = k xyz \\), so the matrix element is \\( k \\langle n |"}
{"id": "quantum/3-3021", "questions": "A system of two particles each with spin $1/2$ is described by an effective Hamiltonian\n\n$$\nH = A(s_{1z} + s_{2z}) + B \\mathbf{s_1} \\cdot \\mathbf{s_2} \\, ,\n$$\n\nwhere $\\mathbf{s_1}$ and $\\mathbf{s_2}$ are the two spins, $s_{1z}$ and $s_{2z}$ are their z-components, and $A$ and $B$ are constants. Find all the energy levels of this Hamiltonian.  ", "graphs": null, "llm_answers": "<think>\nOkay, so I have to find the energy levels for a system of two spin-½ particles using the given Hamiltonian:\n\n\\[ H = A(s_{1z} + s_{2z}) + B \\mathbf{s_1} \\cdot \\mathbf{s_2}. \\]\n\nHmm, let me think about how to approach this.\n\nFirst off, since we're dealing with two spin-½ particles, it might be helpful to consider their combined states. Each particle has spin up or down along the z-axis, right? So together, they can form four possible basis states: |↑↑⟩, |↑↓⟩, |↓↑⟩, and |↓↓⟩. These should span our Hilbert space here.\n\nNow, looking at the Hamiltonian, there's a term involving \\( s_{1z} + s_{2z} \\) multiplied by constant A, and another term which is the dot product of the two spin vectors times B. Let me break these terms apart.\n\nStarting with the first part, \\( A(s_{1z} + s_{2z}) \\). For each particle, \\( s_z \\) can take values ±ħ/2 because they're spin-½ particles. But wait, actually, when working in units where ħ=1 (which seems common in such problems), then \\( s_z \\) would just be ±1/2. Hmm, but sometimes people use different conventions... Wait, no, more accurately, for spin ½, \\( s_z \\) eigenvalues are indeed ±½. So if that's the case, maybe I need to adjust my thinking accordingly.\n\nBut hold on, perhaps instead of getting bogged down in factors like ħ, I'll proceed symbolically. The key point is that \\( s_{1z} \\) acts as σ_z / 2 on the first qubit, similarly for \\( s_{2z} \\).\n\nWait, yes! Because for Pauli matrices, σ_z has eigenvalues +1 and -1, corresponding to spin-up and spin-down. Therefore, \\( s_z = \\frac{\\hbar}{2} \\sigma_z \\), but again, depending on whether we include ℏ or not. Since the problem doesn't specify, maybe I can assume natural units where ℏ=1, making \\( s_z = \\frac{1}{2}\\sigma_z \\). That way, the eigenvalues become ±1/2.\n\nSo, moving forward under that assumption, \\( s_{1z} \\) acting on state |↑⟩ gives (+1/2)|↑⟩, same for |↓⟩ giving (-1/2)|↓⟩, etc., for both particles.\n\nTherefore, the operator \\( s_{1z} + s_{2z} \\) will act diagonally on the computational basis states |↑↑⟩, |↑↓⟩, |↓↑⟩, |↓↓⟩. Specifically, its matrix representation in this basis would be diagonal with entries:\n\nFor |↑↑⟩: 1/2 + 1/2 = 1,\n\n|↑↓⟩: 1/2 - 1/2 = 0,\n\n|↓↑⟩: -1/2 + 1/2 = 0,\n\nand |↓↓⟩: -1/2 -1/2 = -1.\n\nThus, multiplying by A, the contribution from the first term becomes A*(1, 0, 0, -1)^T across those states.\n\nNext, considering the second term, \\( B\\mathbf{s}_1 \\cdot \\mathbf{s}_2 \\). This is a bit trickier. Remembering that the dot product of two vector operators is given by:\n\n\\( \\mathbf{s}_1 \\cdot \\mathbf{s}_2 = s_{1x}s_{2x} + s_{1y}s_{2y} + s_{1z}s_{2z} \\).\n\nAlternatively, there's a formula relating this to total spin squared:\n\n\\( \\mathbf{s}_1 \\cdot \\mathbf{s}_2 = \\frac{1}{2}(S^2 - s_1^2 - s_2^2) \\),\n\nwhere S = s₁ + s₂ is the total spin. Maybe that could help simplify things?\n\nSince each s_i² is fixed for spin-½ particles; specifically, \\( s_i^2 = (\\hbar/2)(\\hbar/2 + 1)\\ ), but again assuming ℏ=1, simplifies calculations.\n\nIn any case, regardless of the exact value, knowing that \\( s_i^2 = 3/4 \\times \\text{(some unit)} \\)... Actually, without worrying too much about scaling, perhaps expressing everything in terms of S² makes sense.\n\nLet me try that substitution:\n\n\\( \\mathbf{s}_1 \\cdot \\mathbf{s}_2 = \\frac{1}{2}(S^2 - s_1^2 - s_2^2) \\).\n\nGiven that each s_i² is known—since each is spin-½—their squares add up to something specific. If s_i² = (1/2)(1/2 + 1) * some factor—but perhaps better yet, express them numerically.\n\nAssuming s_i² = (s(s+1)) where s=1/2, so s_i² = (1/2)(3/2)= 3/4. Thus, substituting back into the equation:\n\n\\( \\mathbf{s}_1 \\cdot \\mathbf{s}_2 = \\frac{1}{2}(S^2 - 3/4 - 3/4 ) = \\frac{1}{2}(S^2 - 3/2 ). \\)\n\nThat simplifies nicely. Hence, the second term in the Hamiltonian becomes:\n\n\\( B \\cdot \\frac{1}{2}(S^2 - 3/2 ) = \\frac{B}{2} S^2 - \\frac{3B}{4} \\).\n\nSo now, rewriting the entire Hamiltonian:\n\n\\( H = A(s_{1z} + s_{2z}) + \\frac{B}{2} S^2 - \\frac{3B}{4} \\).\n\nThis looks promising because S² commutes with itself obviously, and also with s_z_total, which is s_{1z} + s_{2z}, correct? Or does it commute?\n\nActually, S² commutes with S_z, which is exactly what s_{1z} + s_{2z} represents. Yes, because [S², S_z] = 0. Therefore, the two parts of the Hamiltonian share good quantum numbers related to S² and S_z.\n\nWhich means that simultaneous eigenstates exist for both S² and S_z, allowing us to block-diagonalize the Hamiltonian based on the total spin multiplets.\n\nSpecifically, for two spin-½ particles, the total spin S can be either 1 (triplet states) or 0 (singlet state). So, the Hilbert space splits into a triplet subspace (dimension 3) and a singlet subspace (dimension 1). However, wait—that’s only true when you combine two spin-½ systems. In reality, combining two spin-½ particles leads to one singlet (S=0) and one triplet (S=1). But in our current setup, do we have three states for S=1 and one for S=0?\n\nYes, precisely. So, within the triplet manifold, S_z can be +1, 0, -1, while in the singlet, S_z must be zero.\n\nHowever, note that the original basis was built upon individual spins' z-directions, i.e., |↑↑⟩, |↑↓⟩, |↓↑⟩, |↓↓⟩. To make progress, perhaps changing the basis to the coupled (total spin) basis would be beneficial.\n\nThe coupled basis consists of the following states:\n\nTriplet:\n- |1,1⟩ = |↑↑⟩,\n- |1,0⟩ = (|↑↓⟩ + |↓↑⟩)/√2,\n- |1,-1⟩ = |↓↓⟩,\n\nSinglet:\n- |0,0⟩ = (|↑↓⟩ - |↓↑⟩)/√2.\n\nThese states are eigenstates of S² and S_z. Knowing this allows us to write the Hamiltonian in blocks corresponding to S=1 and S=0.\n\nLooking back at the rewritten Hamiltonian:\n\n\\( H = A(S_z^{(total)}) + \\frac{B}{2} S^2 - \\frac{3B}{4} \\).\n\nBecause \\( s_{1z} + s_{2z} = S_z \\), yeah, that's straightforward.\n\nMoreover, S² is already accounted for in the expression above.\n\nSo, evaluating H on each of these coupled states:\n\nFor the triplet states (S=1):\n\nEach state has definite S² and S_z. So, plugging into H:\n\nE_triplet = A"}
{"id": "quantum/5054", "questions": "A spin-$\\frac{1}{2}$ particle of mass $\\text m$ moves in spherical harmonic oscillator potential $V = \\frac{1}{2}m\\omega^2r^2$ and is subject to an interaction $\\lambda \\mathbf{\\sigma} \\cdot \\mathbf{r}$ (spin orbit forces are to be ignored). The net Hamiltonian is therefore\n\n$$\nH = H_0 + H',\n$$\n\nwhere \n\n$$\nH_0 = \\frac{P^2}{2m} + \\frac{1}{2}m\\omega^2r^2, \\quad H' = \\lambda \\mathbf{\\sigma} \\cdot \\mathbf{r}.\n$$\n\nCompute the shift of the ground state energy through second order in the perturbation $H$", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem where a spin-½ particle with mass m is moving in a spherical harmonic oscillator potential V(r) = ½ m ω² r². Additionally, it's interacting via a term λ σ · r, which they call H'. So the total Hamiltonian is H = H₀ + H', where H₀ is just the usual harmonic oscillator part.\n\nThe question asks for the shift in the ground state energy up to second-order perturbation theory due to the perturbation H'.\n\nHmm, okay. Let me recall how perturbation theory works. In non-degenerate perturbation theory, the first-order correction is given by ⟨ψ|H'|ψ⟩, and the second-order correction involves summing over all intermediate states |n⟩ not equal to the original state: Σ [⟨ψ|H'|n⟩⟨n|H'|ψ⟩ / (E₀ - E_n)].\n\nBut wait, before jumping into that, let me think about whether the unperturbed system has any degeneracy because if there's degeneracy, then we might need to use degenerate perturbation theory instead. For a three-dimensional isotropic harmonic oscillator, each energy level is actually triply degenerate since you can have different angular momentum quantum numbers l=0,1,... but same n? Wait no, hold on—the energy levels depend only on the principal quantum number N = 2n_r + l, right?\n\nWait, maybe more accurately, the energy levels for a 3D isotropic HO are equally spaced regardless of l or m. Each energy level E_N = ħω(N + 3/2), where N = 2n_r + l, and n_r is radial node count. But within each N, multiple states exist depending on l and m. However, when considering spin, does that affect the degeneracy?\n\nIn our case, the particle is spin-½, so without considering spin-orbit coupling, the spin doesn't mix with orbital angular momentum. Therefore, the eigenstates would factorize as spatial wavefunctions times spinors. Since H₀ commutes with L² and S_z perhaps? Or rather, H₀ includes kinetic and potential terms dependent only on position and momentum, so yes, it should commute with L² and L_z, hence good quantum numbers l, m_l, etc., while the spin part remains separate unless acted upon by something like H'.\n\nSo, the ground state—what is its structure? Ground state means lowest possible energy, so minimal N. For a 3D HO, the ground state corresponds to N=0, meaning n_r=0 and l=0. Because N = 2n_r + l; setting both to zero gives N=0. Then, the next excited states would be either n_r=1, l=0 giving N=2, or n_r=0, l=1 also giving N=1—but wait, hang on, isn't N supposed to start from some value... Maybe my indexing is off.\n\nActually, correct formula is E_N = ħω( N + 3/2 ), where N = 2n_r + l. So for the ground state, n_r=0 and l=0 ⇒ N=0, so E_ground = (3/2)ħω. Next comes N=1, which could be achieved by either n_r=0, l=1 or n_r=1, l=0. Hmm, but those correspond to higher energies than the ground state.\n\nTherefore, the ground state is unique except for spin. Oh! Right, because even though the spatial part is spherically symmetric (l=0), the spinor still allows two possibilities: spin-up and spin-down along whatever axis. Hence, the ground state is doubly degenerate due to spin.\n\nAh, important point here. If the ground state is twofold degenerate, then standard non-degenerate PT won't apply directly—we'd have to consider degenerate perturbation theory. That complicates things a bit.\n\nLet me confirm: Without including spin, the ground state is non-degenerate. With spin included, it becomes twice degenerate because the spin adds another degree of freedom. Thus, the Hilbert space splits into product spaces of spatial and spin parts. So, the unperturbed Hamiltonian acts separately on these subspaces.\n\nGiven that, what happens when we add the perturbation H'? It couples the spin degrees of freedom with the spatial ones. Specifically, H' = λ σ·r. Now, σ is the Pauli matrices vector, acting on spin space, and r is the position operator, acting on spatial coordinates.\n\nThus, H' connects different spin states based on their positions. To compute the energy shifts, especially using perturbation theory, we'll probably have to diagonalize the matrix representation of H' in the basis of the degenerate ground states.\n\nSince the ground state is two-fold degenerate, consisting of |s=½, m_s=+½> and |s=½, m_s=-½>, multiplied by the spatial ground state |n_r=0,l=0>. So, the full ground state is:\n\n|ψ₁> = φ_{ground}(r) ⊗ |↑>\n|ψ₂> = φ_{ground}(r) ⊗ |↓>\n\nWhere φ_{ground}(r) is the spatial ground state wavefunction.\n\nNow, the perturbing Hamiltonian H' will act on these states. Let's write down the matrix elements of H' between these states.\n\nFirstly, note that H' = λ σ · r. Expanding this out,\n\nσ · r = x σ_x + y σ_y + z σ_z.\n\nEach component σ_i acts on spin, while the coordinate components x,y,z act on spatial variables.\n\nHowever, the expectation values involving products of spatial operators and spin operators may simplify because the spatial wavefunction is spherically symmetric (since l=0).\n\nSpecifically, for the ground state, which is spherically symmetric, certain integrals vanish. For example, the expectation value <x> must be zero because the distribution is symmetric around origin. Similarly, <y>=<z>=0.\n\nMoreover, cross terms such as <xy>, <xz>, <yz> also vanish because integrating an odd function over symmetric limits yields zero.\n\nWhat about <x²>, <y²>, <z²>? These don't necessarily vanish—they contribute to the variance of position.\n\nIndeed, for a Gaussian wavepacket (which the ground state of HO is), the expectation value of r² is known. For a 3D isotropic HO, the average radius squared is related to the uncertainty principle.\n\nRecall that for the ground state of a 3D HO, the expectation value of r² is (3/(4)) * (ħ/(mω))^2. Alternatively, sometimes written as (a)^2*(something), where a is the oscillator length.\n\nAlternatively, recalling that for one dimension, the expectation value of x² is ħ/(2mω). So in 3D, it's thrice that, i.e., 3*ħ/(2mω). Is that accurate?\n\nYes, indeed. For each direction, the expectation value of x² is ħ/(2mω), so overall, <r²> = 3*(ħ/(2mω)). Got it.\n\nSimilarly, the expectation value of x³ would be zero again because of symmetry—it’s an odd function integrated against a symmetric probability density.\n\nAlright, now back to computing the matrix elements of H' in the spin-basis.\n\nBecause H' = λ (x σ_x + y σ_y + z σ_z), when calculating matrix elements between the spin-up and spin-down states, say, we get contributions proportional to the expectation values of x, y, z in the spatial ground state.\n\nBut earlier, we saw that <x> = <y> = <z> = 0. Moreover, the expectation values of mixed terms like <x σ_x> involve <φ|x|φ><σ_x>.\n\nWait, but σ_x flips spins—if you take the inner product of |↑> with σ_x |↑>, you get <↑|σ_x|↑> = 0, similarly for other components.\n\nHold on, perhaps better approach is needed.\n\nWait, let's clarify: When evaluating the matrix element ⟨ψ_a| H’ |ψ_b⟩, where |ψ_a> and |ψ_b> are the two degenerate ground states differing only in spin.\n\nExpressed explicitly, suppose |ψ_a> = φ(r)|↑> and |ψ_b> = φ(r)|↓>.\n\nThen, H’ is λ(x σ_x + y σ_y + z σ_z). So, applying H’ to |ψ_a>:\n\nH’ |ψ_a> = λ [x φ(r) σ_x |↑> + y φ(r"}
{"id": "quantum/1-1042", "questions": "In one dimension, a particle of mass $\\text m$ is in the ground state of a potential which confines the particle to a small region of space. At time $t=0$, the potential suddenly disappears, so that the particle is free for time $t>0$. Give a formula for the probability per unit time that the particle arrives at time $t$ at an observer who is a distance $L$ away.", "graphs": null, "llm_answers": "<think>\nOkay, let me try to figure out this problem. So we have a particle in one dimension confined in some potential well, right? And it's in its ground state initially. Then, at t=0, the potential just disappears, making the particle free from then on. The question asks for the probability per unit time (which I think means the probability density) that the particle will be found at position L at time t.\n\nHmm, okay. Let me break this down. When the potential is present, the particle is in the ground state. That should mean it has the lowest energy possible, with a certain wavefunction ψ₀(x). Once the potential vanishes, the particle becomes free, meaning it can move without any confinement anymore. But how does the wavefunction evolve after the potential disappears?\n\nI remember something about sudden approximation in quantum mechanics. If the Hamiltonian changes abruptly, like here when the potential goes away instantly, the wavefunction doesn't change instantaneously; instead, it continues as if under the old conditions but evolves according to the new Hamiltonian. So, the initial condition at t=0+ would still be ψ₀(x), but now governed by the free-particle Schrödinger equation.\n\nSo, the wavefunction immediately after t=0 is ψ₀(x), and then it starts spreading because there’s no confining force anymore. To find where the particle ends up later, we need to calculate the time evolution of this wavepacket.\n\nWait, actually, since the particle was localized before, once released, it'll start moving outward. Since it's a free particle, maybe we can model this using plane waves or perhaps express the evolved wavefunction as a Fourier transform of the initial wavefunction.\n\nLet me recall: For a free particle, the time-evolved wavefunction can be written as:\n\nψ(x,t) = ∫ [dk/(2π)] e^{ikx} φ(k) e^{-iω_k t}\n\nwhere ω_k = k²/(2m) is the dispersion relation, and φ(k) is the momentum-space representation of the initial wavefunction.\n\nBut wait, our initial wavefunction is ψ₀(x), so φ(k) must be the Fourier transform of ψ₀(x):\n\nφ(k) = ∫ dx' e^{-ikx'} ψ₀(x')\n\nTherefore, substituting back into ψ(x,t):\n\nψ(x,t) = ∫ dk/(2π) e^{ikx - i(k²/(2m))t} φ(k)\n\nNow, the probability current density J(x,t) gives us the flow of probability per unit area per unit time. In 1D, it simplifies to:\n\nJ(x,t) = (ħ/m) Im[ψ*(x,t) (∂/∂x) ψ(x,t)]\n\nAlternatively, another expression might involve integrating over all momenta contributing to movement towards x=L.\n\nWait, hold on. Maybe thinking in terms of flux makes more sense. Because the question is asking for the probability per unit time arriving at point L, which sounds like the rate at which particles pass through L due to their motion.\n\nFor a free particle, each component of the wavefunction with momentum p contributes a velocity v=p/m. Therefore, the group velocity associated with each mode determines how quickly they reach L.\n\nIf we consider the initial wavefunction ψ₀(x), upon release, different parts of the wave packet propagate at different velocities depending on their momentum components. Thus, the arrival time depends on these velocities.\n\nThe idea is probably similar to calculating the transmission coefficient across a barrier, except here the \"barrier\" isn’t really a barrier—it's just the absence of confinement leading to propagation.\n\nAnother approach could be considering the propagator for a free particle. The Green's function G(x,t|x',0) describes the amplitude for going from x’ at t=0 to x at time t. Then, the total amplitude at x=L is given by integrating G(L,t|x’,0) multiplied by the initial wavefunction ψ₀(x’) over all x’.\n\nMathematically,\n\nψ(L,t) = ∫ dx’ G(L,t|x’,0) ψ₀(x’)\n\nThen, |ψ(L,t)|² dt would give the probability of finding the particle between L and L + dL at time t. Wait, not exactly—actually, in 1D, the probability density is |ψ|², so the probability per unit length is |ψ(L,t)|². However, the question says \"probability per unit time,\" which suggests we're looking for the number of times the particle crosses L per second, rather than being located within an interval around L.\n\nAh! Right, so it's the transition rate—the number of crossings per unit time. This relates to the concept of tunneling currents or reflection/transmission coefficients, but again, in this case, it's a free expansion.\n\nTo compute the crossing rate, you'd look at the expectation value of the velocity operator at position L. Alternatively, use the probability current J evaluated at x=L.\n\nYes, that seems correct. Probability current J(x,t) represents the amount of probability flowing past a point x per unit time. Hence, evaluating J(L,t) would directly give the desired quantity.\n\nGiven that, let's write expressions accordingly.\n\nFirstly, the initial wavefunction is ψ₀(x). After t=0, it evolves freely. As such, the wavefunction at time t is:\n\nψ(x,t) = (1/sqrt(2πħ)) ∫ dp e^{ip(x)/ħ - ip² t/(2mħ)} ψ₀(p)/(sqrt(m))\n\nWait, correction—I think my earlier substitution had ħ factors missing. Actually, the proper way is:\n\nSince the free particle propagator is\n\nG(x,t;x') = (m/(2πiħ t))^(1/2) exp(i m (x - x')² / (2ħ t))\n\nThus, the wavefunction at time t is:\n\nψ(x,t) = ∫ dx' G(x,t;x') ψ₀(x')\n\nWhich leads to:\n\nψ(x,t) = sqrt(m/(2πiħ t)) ∫ dx' exp[i m (x - x')² / (2ħ t)] ψ₀(x')\n\nThis integral may not be straightforward unless ψ₀(x) is known explicitly. Hmm, but the problem states that the particle is in the ground state of a confining potential. Without knowing what kind of potential, though... Is it harmonic oscillator? Infinite square well? Or something else?\n\nWait, the problem mentions \"confines the particle to a small region.\" Ground state implies minimal spatial extent. Perhaps assuming it's a Gaussian wavefunction, typical of the harmonic oscillator's ground state. Even if it's another potential, often the ground state tends to resemble a Gaussian.\n\nAssuming ψ₀(x) ≈ N exp(-α x²), where α is related to the confinement strength.\n\nPlugging this into the integral above, we get:\n\nψ(x,t) ~ sqrt(m/(2πiħ t)) ∫ dx' exp[-α x'^2] exp[i m (x^2 - 2x x' + x'^2)/(2ħ t)] \n\nSimplify exponent:\n\n-α x'^2 + i m (x^2 - 2x x' + x'^2)/(2ħ t)\n= (-α + im/(2ħ t)) x'^2 - i m x x'/ħ t + i m x²/(2ħ t)\n\nCompleting the square in x':\n\nCoefficient A = (-α + im/(2ħ t))\nLinear term B = -i m x / ħ t\n\nComplete the square:\nA y² + By = A(y² + (B/A)y ) \n= A[(y + B/(2A))^2 - (B²)/(4A²)]\nHence,\nexp(A y² + B y) = exp(-(B²)/(4A)) * exp[A(y + B/(2A))²]\n\nApplying this to our exponent:\n\nExponent = A x'^2 + B x'\nWhere A = (-α + im/(2ħ t)), B = -i m x / ħ t\n\nCompute completing the square:\n\n= A[x'^2 + (B/A)x']\n= A[(x' + B/(2A))² - (B²)/(4A²)]\n= A(x' + B/(2A))² - B²/(4A)\n\nSubstituting back:\n\nIntegral becomes proportional to exp(-B²/(4A)) * exp(A(...)^2 )\n\nHowever, note that A is complex, so the exponential involving (...)^2 decays only if Re(A) < 0. Given that alpha >0 and m/(2ħ t) positive imaginary part, so overall, real part negative, hence convergence.\n\nCalculating constants:\n\nAfter integration, the result is:\n\nN * sqrt( π / (alpha - i m/(2ħ t))) ) * exp[ ... ]\n\nWait, getting bogged down. Instead, recalling that convolution of Gaussians results in another Gaussian. Specifically, multiplying two Gaussians in x' domain corresponds to adding exponents quadratically.\n\nPerhaps better yet, recognizing that the product of two Gaussians integrates"}
{"id": "quantum/6010", "questions": "Consider the quantum-mechanical scattering problem in the presence of inelastic scattering. Suppose one can write the partial wave expansion of the scattering amplitude for the elastic channel in the form \n\n$$\nf(k, \\theta) = \\sum_{\\ell=0}^{\\infty} (2\\ell + 1) \\frac{\\eta_\\ell e^{2i\\delta_\\ell} -1}{2ik} P_\\ell (\\cos \\theta),\n$$\n\nwhere $\\delta_\\ell (k)$ and $\\eta_\\ell (k)$ are real quantities with $0 \\leq \\eta_\\ell \\leq 1$, the wave number is denoted by $\\mathbf{k}$, and $\\theta$ is the scattering angle. For a given partial wave, obtain the lower and upper bounds for the elastic cross section $\\sigma_\\text{elastic}^{(\\ell)}$ in terms of $\\sigma_\\text{inelastic}^{(\\ell)}$.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about quantum mechanical scattering, specifically dealing with inelastic scattering effects on the elastic cross-section. Hmm, let me try to unpack what's being asked here.\n\nThe question provides a partial wave expansion for the elastic scattering amplitude:\n\n\\[ f(k, \\theta) = \\sum_{\\ell=0}^{\\infty} (2\\ell + 1) \\frac{\\eta_\\ell e^{2i\\delta_\\ell} -1}{2ik} P_\\ell (\\cos \\theta). \\]\n\nI remember that in scattering theory, each term in the sum corresponds to a different angular momentum state ℓ. The coefficients involve phase shifts δ_ℓ and some parameter η_ℓ which ranges between 0 and 1 because it’s mentioned as such: \\(0 \\leq \\eta_\\ell \\leq 1\\).\n\nThey want me to find the lower and upper bounds for the elastic cross-section σ_elastic^(ℓ) in terms of the inelastic cross-section σ_inelastic^(ℓ). So, essentially, relate these two cross-sections using their expressions from partial waves.\n\nFirst, maybe I should recall how cross-sections are calculated from the scattering amplitudes. In general, the differential cross-section is proportional to |f(θ)|², but since we're looking at individual partial waves, perhaps there's a way to express the cross-section per partial wave.\n\nWait, actually, when considering just a single partial wave ℓ, both the elastic and inelastic processes contribute? Or does each process correspond to its own set of partial waves?\n\nHmm, no, wait. Elastic scattering refers to the case where the particle scatters without changing its internal state—so same energy, same type after collision. Whereas inelastic would mean something changes, like excitation or reaction, leading to different outgoing states.\n\nBut in the context of partial waves, if you consider only the elastic part, then the total cross-section might be split into elastic and inelastic parts based on whether they change the state or not.\n\nSo, probably, the total cross-section for a particular ℓ is the sum of the elastic and inelastic contributions. But I need to think more carefully.\n\nLet me start by writing down the expression for the elastic cross-section. From the formula provided, the scattering amplitude f has a specific structure involving η_ℓ and δ_ℓ. Let me compute |f|² for a single partial wave.\n\nFor a fixed ℓ, ignoring other partial waves, the amplitude contribution is:\n\n\\[ f_\\ell = (2\\ell + 1)\\frac{\\eta_\\ell e^{2i\\delta_\\ell} -1}{2ik}. \\]\n\nThen, the modulus squared is |f_ℓ|^2 multiplied by |P_ℓ(cosθ)|² integrated over all angles divided by sinθ dθ... Wait, hold on—the standard approach is that the differential cross-section is |f(θ)|² times constants, but integrating over solid angle gives the total cross-section.\n\nAlternatively, for each partial wave, the contribution to the cross-section involves |a_ℓ|², where a_ℓ is the scattering coefficient. Because usually,\n\n\\[ f(\\theta) = \\sum_{\\ell} (2\\ell+1)a_\\ell P_\\ell(\\cos\\theta), \\]\nand the total cross-section is ∫|f(θ)|² dΩ / k², right?\n\nYes, so the total cross-section is (4π/k²) * sum_{ℓ}|a_ℓ|²*(2ℓ+1)/something...\n\nWait, let me get precise formulas.\n\nIn non-relativistic scattering, the differential cross-section is given by:\n\ndσ/dΩ = |f(θ)|².\n\nTherefore, the total cross-section is:\n\nσ_total = ∫ (dσ/dΩ) dΩ = ∫ |f(θ)|² dΩ.\n\nGiven that f(θ) is expressed via partial waves, expanding it out, squaring it, etc., leads us to use orthogonality relations of Legendre polynomials.\n\nSpecifically, the integral becomes:\n\n∫ [∑_{ℓ}(2ℓ+1)(η_ℓ e^{2iδ_ℓ}-1)/(2ik) P_ℓ] × [∑_{ℓ'}(2ℓ'+1)(η_{ℓ'} e^{-2iδ_{ℓ'}}-1)/(2ik) P_{ℓ'} ] dΩ.\n\nDue to orthogonality, most terms vanish except when ℓ=ℓ', giving:\n\nSum_{ℓ} [(2ℓ+1)^2/(4k²)] |(η_ℓ e^{2iδ_ℓ} -1)|² ∫ P_ℓ² dΩ.\n\nAnd the integral of P_ℓ² over the sphere is known—it equals 4π/(2ℓ+1). Therefore, putting together:\n\nEach term contributes [(2ℓ+1)^2/(4k²)] * |(η_ℓ e^{2iδ_ℓ} -1)|² * (4π)/(2ℓ+1)\n\nSimplify:\n\n[(2ℓ+1)* π]/(k²) * |(η_ℓ e^{2iδ_ℓ} -1)|².\n\nThus, the total cross-section is Sum_{ℓ} [ (2ℓ+1) π / k² ) * |(η_ℓ e^{2iδ_ℓ} -1)|² ]\n\nSimilarly, the elastic cross-section must come from certain terms—if the entire amplitude includes both elastic and inelastic channels, but in our case, the given f seems to represent the elastic channel alone.\n\nWait, now I'm confused—is the given f purely elastic, meaning that any inelasticity isn't included here? That complicates things because if f already accounts for inelasticity through η_ℓ, then perhaps the inelastic cross-section comes from another component.\n\nHold on, going back to the original setup. It says \"the partial wave expansion of the scattering amplitude for the elastic channel.\" So, f represents the elastic scattering amplitude. Then, presumably, the inelastic scattering amplitude is separate.\n\nHence, the total scattering amplitude F is the sum of the elastic and inelastic components:\n\nF = f_elastic + f_inelastic.\n\nHowever, in many treatments, especially in cases where multiple channels exist, the S-matrix formalism is used, where each channel has its own transmission and reflection coefficients.\n\nBut regardless, focusing on the given f_elastic, whose partial-wave decomposition is given. We’re supposed to bound σ_elastic^(ℓ) in terms of σ_inelastic^(ℓ).\n\nPerhaps instead, within each partial wave, the elastic and inelastic cross sections add up to give the total cross-section for that wave.\n\nWait, but in reality, the total cross-section across all possible outcomes (elastic plus inelastic) is determined by the unitarity condition.\n\nUnitarity implies that the probability of all possible outcomes sums to 1. Translating this into cross-sections, the total cross-section is related to the imaginary part of the forward-scattering amplitude.\n\nBut perhaps for each partial wave, similar conditions apply.\n\nLooking again at the given f_elastic, written as:\n\nf(k,θ)=Σ[...] Each term is labeled by ℓ, so for each ℓ, the corresponding amplitude is (2ℓ+1)[(η_ℓ e^{2iδ_ℓ}−1)/(2ik)] P_ℓ.\n\nIf I denote a_ℓ=(η_ℓ e^{2iδ_ℓ}−1)/(2ik), then f=Σ(2ℓ+1)a_ℓ P_ℓ.\n\nNow, the modulus square |a_ℓ|² relates to the cross-section for that partial wave.\n\nFrom earlier steps, the total cross-section due to this partial wave is (2ℓ+1)π/k² * |(η_ℓ e^{2iδ_ℓ} −1)|².\n\nWhich simplifies to (2ℓ+1)π/k² * |e^{2iδ_ℓ}(η_ℓ - e^{-2iδ_ℓ})|².\n\nSince |ab|=|a||b|, and |e^{iφ}|=1, so |e^{2iδ_ℓ}|=1, thus:\n\n|(η_ℓ e^{2iδ_ℓ} −1)|² = |η_ℓ e^{2iδ_ℓ} −1|² = |η_ℓ - e^{-2iδ_ℓ}|².\n\nExpanding this, it's equal to (η_ℓ - cos(2δ_ℓ))² + (sin(2δ_ℓ))^2.\n\nBecause |z - w"}
{"id": "quantum/1-1021", "questions": "Consider a particle of mass $\\text{m}$ in the one-dimensional $\\delta$ function potential\n\n$$\n V(z) = V_0 \\delta(x) . \n$$\n\nShow that if $V_0$ is negative there exists a bound state, and that the binding energy is $mV_0^2/2\\hbar^2$.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about a particle in a delta function potential. The potential is given as \\( V(x) = V_0 \\delta(x) \\), where \\( V_0 \\) is negative. I need to show two things: first, that when \\( V_0 \\) is negative, there's a bound state, and second, find the binding energy which should be \\( E = \\frac{m V_0^2}{2 \\hbar^2} \\).\n\nHmm, let me start by recalling what a bound state means here. In quantum mechanics, a bound state occurs when the total energy of the particle is less than the potential at infinity (which for a 1D delta potential is zero). So, since \\( V_0 \\) is negative, it acts like an attractive force, pulling the particle towards the origin.\n\nI remember solving similar problems before with the delta potential. Usually, you set up the Schrödinger equation and solve for the wavefunction outside and inside the potential region. But wait, the delta function is only non-zero at x=0, so maybe we can model the wavefunction symmetrically on both sides?\n\nLet me write down the time-independent Schrödinger equation:\n\n\\[ -\\frac{\\hbar^2}{2m} \\psi''(x) + V_0 \\delta(x) \\psi(x) = E \\psi(x) \\]\n\nSince \\( V_0 < 0 \\), this represents an attractive potential well. For bound states, the energy \\( E \\) must be negative because the particle doesn't escape to infinity; its kinetic energy isn't enough to overcome the potential barrier.\n\nSo, assuming \\( E = -|E| \\), but actually, more precisely, \\( E \\) will just be negative. Let me denote \\( |E| = k^2 \\hbar^2 / (2m) \\), or something along those lines? Wait, no, perhaps better to express everything in terms of \\( k \\).\n\nWait, another approach: Since the potential is symmetric around x=0, the solutions can be even or odd functions. However, due to the nature of the delta function being even, probably the ground state is symmetric (even parity). Maybe higher excited states would involve nodes, but for simplicity, considering the lowest energy state, which is likely symmetric.\n\nBut hold on, in 1D, any bound state has a single node except the ground state, right? Or does it not apply here? Hmm, maybe overcomplicating. Perhaps focusing on finding the wavefunctions away from x=0.\n\nFor regions far from x=0, the potential is effectively zero, so the Schrödinger equation becomes free particle equations:\n\nIn regions \\( x > 0 \\): \n\n\\( -\\frac{\\hbar^2}{2m} \\psi''(x) = E \\psi(x) \\)\n\nSimilarly, for \\( x < 0 \\):\n\nSame equation applies.\n\nGiven that \\( E \\) is negative, say \\( E = -|\\epsilon|^2 \\hbar^2/(2m) \\), then substituting into the equation gives us exponential decay on either side.\n\nTherefore, the general solution for each region is exponentially decaying:\n\nFor \\( x > 0 \\):\n\\( \\psi(x) = A e^{-k x} \\)\nwhere \\( k = |\\epsilon| = \\sqrt{-2 m E}/\\hbar \\)\n\nAnd similarly, for \\( x < 0 \\):\nDue to symmetry, maybe \\( \\psi(-x) = \\psi(x) \\)? If yes, then:\n\\( \\psi(x) = B e^{k x} \\) for \\( x < 0 \\)\n\nAlternatively, could also consider different coefficients, but since the potential is symmetric, the amplitudes might match. Alternatively, using continuity conditions.\n\nAt x=0, the wavefunction needs to be continuous, so matching left and right parts.\n\nAlso, importantly, the derivative may have a discontinuity due to the delta function potential. That comes from integrating the Schrödinger equation across x=0.\n\nYes, integration method! To handle the delta function, integrate both sides from \\( -a \\) to \\( a \\) and take limit as \\( a \\to 0+ \\).\n\nIntegrate the SE:\n\nLeft-hand side integral:\n\n\\( \\int_{-a}^{a} [ -\\frac{\\hbar^2}{2m} \\psi''(x) ] dx + \\int_{-a}^{a} V_0 \\delta(x) \\psi(x) dx = \\int_{-a}^{a} E \\psi(x) dx \\)\n\nAs \\( a \\to 0 \\), the last term goes to zero because \\( \\psi(x) \\) approaches \\( \\psi(0) \\times 2a \\), which tends to zero. Similarly, the middle term simplifies because the delta function picks out \\( \\psi(0) \\).\n\nCompute LHS integrals:\n\nFirst term:\n\n\\( -\\frac{\\hbar^2}{2m} [\\psi'(a) - \\psi'(-a)] \\)\n\nSecond term:\n\n\\( V_0 \\psi(0) \\)\n\nRHS integral tends to zero.\n\nThus,\n\n\\( -\\frac{\\hbar^2}{2m} [\\psi'(a) - \\psi'(-a)] + V_0 \\psi(0) = 0 \\)\n\nTaking \\( a \\to 0 \\),\n\nThe expression becomes:\n\n\\( -\\frac{\\hbar^2}{2m} [\\lim_{a \\to 0} (\\psi'(a) - \\psi'(-a))] + V_0 \\psi(0) = 0 \\)\n\nWhich implies:\n\n\\( \\lim_{a \\to 0} (\\psi'(a) - \\psi'(-a)) ) = \\frac{2 m V_0 }{\\hbar^2} \\psi(0) \\)\n\nThis tells us how the derivatives jump at x=0.\n\nNow, going back to our assumed form of the wavefunction.\n\nAssuming for \\( x>0 \\), \\( \\psi(x) = A e^{-k x} \\); thus, \\( \\psi'(x) = -A k e^{-k x} \\). At x approaching 0+, \\( \\psi'(0+) = -A k \\).\n\nSimilarly, for \\( x<0 \\), suppose \\( \\psi(x) = B e^{k x} \\); hence, \\( \\psi'(x) = B k e^{k x} \\). As x approaches 0-, \\( \\psi'(0-) = B k \\).\n\nFrom the condition above, the difference between these derivatives is related to \\( V_0 \\psi(0) \\).\n\nSo compute \\( \\psi'(0+) - \\psi'(0-) = (-A k) - (B k) = -(A + B)k \\)\n\nSet equal to \\( \\frac{2 m V_0 }{\\hbar^2} \\psi(0) \\)\n\nNote that \\( \\psi(0) = A e^{0} = A \\) from the positive side, same as \\( B e^{0}=B \\) from the negative side. Therefore, \\( A = B \\) for continuity.\n\nHence, \\( \\psi(0) = A = B \\)\n\nSubstituting back, the derivative condition becomes:\n\n\\( -(A + A)k = \\frac{2 m V_0 }{\\hbar^2} A \\implies -2 A k = \\frac{2 m V_0 }{\\hbar^2} A \\)\n\nWe can cancel A (assuming A ≠ 0, otherwise trivial solution):\n\n\\( -2 k = \\frac{2 m V_0 }{\\hbar^2} \\implies k = - \\frac{m V_0 }{\\hbar^2} \\)\n\nBut earlier, I defined \\( k = \\sqrt{-2 m E}/\\hbar \\). Wait, let me check my substitution again.\n\nActually, starting from the TISE without approximation:\n\nIf \\( E = -|\\epsilon|^2 \\hbar^2/(2m) \\), then plugging into the equation:\n\n\\( -\\frac{\\hbar^2}{2m} \\psi''(x) = E \\psi(x) \\implies \\psi''(x) = \\frac{2m}{\\hbar^2} E \\psi(x) \\)\n\nBut since E is negative, let’s define \\( E = - \\alpha^2 \\hbar^2/(2m) \\), so that:\n\n\\( \\psi''(x) = - \\alpha^2 \\psi(x) \\)\n\nThen, the general solution is exponentials instead of oscillatory.\n\nIndeed, for \\( x > 0 \\), \\( \\psi(x) = A e^{- \\alpha x} \\), and for \\( x < 0 \\), \\( \\psi(x)= A e^{\\alpha x} \\) (since it's symmetric, so same amplitude on both sides.)\n\nThus, the wavenumber \\( \\alpha = \\sqrt{-2 m E}/\\hbar \\), meaning \\( E = - \\hbar^2 \\alpha^2/("}
{"id": "quantum/5037", "questions": "Explain why excited states of atomic hydrogen can show a linear Stark effect in an electric field, but the excited states of atomic sodium show only a quadratic one.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this question about the Stark effect in different atoms—hydrogen versus sodium—and how their excited states behave differently when placed in an electric field. Hmm, let me try to break it down.\n\nFirst off, what's the Stark effect? From my notes, I remember that it's the shift or splitting of spectral lines due to an external electric field. There are two types: linear and quadratic. The linear Stark effect occurs when there’s a first-order change in energy levels with respect to the electric field strength E, while the quadratic is second order because the energy depends on E squared.\n\nWait, right! Linear Stark effect happens mainly for systems without spherical symmetry, especially those where certain quantum numbers allow for dipole transitions. But wait, isn't hydrogen spherically symmetric? Oh yeah, hydrogen has high symmetry because its potential is radial (1/r). So maybe even though it's symmetrical, some states might still exhibit linear effects under specific conditions?\n\nHydrogen being a single-electron atom... Let me think about its electron configuration. In ground state, it's 1s orbital which is spherically symmetric. When you excite it, say to 2p orbitals, does that affect the symmetry? Well, p orbitals aren’t spherically symmetric—they're directional. So if we apply an electric field, perhaps these asymmetric orbitals cause a linear response.\n\nBut then why doesn't sodium do the same thing? Sodium has more electrons; it's [Ne]3s¹. Wait, no—the neutral sodium atom actually has an electronic structure similar to neon plus one valence electron. Its ground state is 3s, just like hydrogen's n=1 level. If we consider an excited state, say from 3s to higher orbitals like 4p or something else...\n\nHold on, both hydrogen and sodium have possible p-orbital excitations. Why would hydrogen's excited states show linear Stark effect but not sodium's? Maybe it's related to whether they have nodes or other factors affecting the dipole moment.\n\nI recall that for the linear Stark effect to occur, the perturbation must be non-zero. That means the expectation value of the position operator (dipole) should be non-zero. For hydrogen, in the case of degenerate states, such as the 2p triplet, applying an electric field mixes these states leading to shifts proportional to E. This mixing allows for a linear dependence since the matrix elements between different m_p states are significant.\n\nIn contrast, sodium has multiple electrons. Even in an excited state, the presence of inner shells may screen the outer electron less effectively than in hydrogen. Or maybe the selection rules differ because of angular momentum considerations. Alternatively, could it be that sodium's excited states don't have the necessary parity properties required for a linear Stark effect?\n\nAnother thought: Hydrogen is a one-electron system, so all the complexities come purely from the motion of that single electron. Sodium, having multiple electrons, introduces additional interactions. Perhaps the wavefunctions of sodium's excited states are such that the average dipole moment remains zero even in an electric field, making the linear term vanish and leaving only the quadratic term.\n\nOh, yes! Selection rules play a role here too. For the linear Stark effect, the transition involves changing the magnetic quantum number m by ±1, allowing for a first-order shift. However, in multi-electron atoms like sodium, the overall wavefunction includes spin and spatial parts, and the total angular momentum might impose restrictions. Additionally, the presence of closed shells in sodium (like Ne core) affects the polarizability and the way the electric field interacts with the valence electron.\n\nMoreover, considering the fine structure constants, hydrogen's simplicity leads to clearer manifestations of various quantum phenomena compared to heavier atoms like sodium. Since sodium has a larger nuclear charge, relativistic effects become more pronounced, possibly altering the nature of the Stark effect observed.\n\nSo putting it together: Hydrogen's excited states, particularly those with l >0 (like p orbitals), lack perfect spherical symmetry, enabling a linear Stark effect through dipole interaction. On the flip side, sodium's excited states either maintain enough symmetry or have constraints due to many-body interactions that prevent a net dipole moment, resulting solely in a quadratic Stark effect.\n</think>\n\nThe difference arises due to the distinct electronic structures and symmetries of hydrogen and sodium:\n\n- **Atomic Hydrogen**: A single-electron system with highly symmetrical orbitals. Excited states (e.g., 2p) lose spherical symmetry, allowing a non-vanishing dipole moment in an electric field. This enables a linear Stark effect via first-order energy shifts caused by dipole interactions.\n\n- **Atomic Sodium**: Possesses multiple electrons, including a filled Neon core. While excited states exist, the complex interplay of electron configurations results in screening effects and maintains sufficient symmetry to suppress the linear Stark effect. Consequently, only a quadratic Stark effect is observable, involving second-order energy changes.\n\nThus, the stark behavior stems from hydrogen's simpler, asymmetrical excited states versus sodium's more complex, relatively symmetrical ones.\n\n\\[\n\\text{Linear Stark Effect in Hydrogen vs Quadratic in Sodium}\n\\]\n\n**Final Answer**\nThe excited states of hydrogen display a linear Stark effect due to broken spherical symmetry, whereas sodium exhibits only a quadratic effect because of maintained symmetry and many-body interactions. Thus,\n\n\\[\n\\boxed{\\text{Hydrogen shows a linear Stark effect, while sodium shows a quadratic one}}\n\\]"}
{"id": "quantum/3-3042", "questions": "A hydrogen atom is in a $^2P_{1/2}$ state with total angular momentum up along the t-axis. In all parts of this problem show your computations and reasoning carefully.\n\n(a) With what probability will the electron be found with spin down?  \n\n(b) Compute the probability per unit solid angle $P(\\theta, \\varphi)$ that the electron will be found at spherical angles $\\theta, \\varphi$ (independent of radial distance and spin).  \n\n(c) An experimenter applies a weak magnetic field along the positive z-axis. What is the effective magnetic moment of the atom in this field?  \n\n(d) Starting from the original state, the experimenter slowly raises the magnetic field until it overpowers the fine structure. What are the orbital and spin quantum numbers of the final state?  \n\n[Assume the Hamiltonian is linear in the magnetic field.]  \n\n(e) What is the effective magnetic moment of this final state?  ", "graphs": null, "llm_answers": "<think>\nOkay, so I have to solve these five parts about a hydrogen atom in a specific state: ^2P_{1/2} with total angular momentum J=3/2 aligned along some axis called the t-axis. Hmm, let me try to break each part down one by one.\n\nStarting with part (a): \"With what probability will the electron be found with spin down?\" \n\nAlright, first, understanding the given state. The notation ^2P_{1/2} refers to the principal quantum number n=2, orbital angular momentum l=1 (since P corresponds to l=1), and the total angular momentum j can take values |l - s| to l + s where s for electron is 1/2. So possible j's here would be 1/2 or 3/2 since l=1 and s=1/2. But wait, the question says it's specifically a ^2P_{1/2} state, which means j=1/2?\n\nWait no, hold on. Wait, actually, when you write ^nL_j, L stands for the letter corresponding to l: S(0), P(1), D(2)... So ^2P_{1/2} implies n=2, l=1, j=1/2. Because if j were 3/2, it'd still be written as ^2P_{3/2}. Got it.\n\nSo we're dealing with j=1/2. Now, the state has total angular momentum up along the t-axis. Let me think—so the eigenstate is |j, m_j> = |1/2, 1/2>, but along the t-axis instead of the usual z-axis. Since the spin-up is usually considered along z, maybe they've rotated their coordinate system such that the quantization axis is now the t-axis. However, regardless of the direction, the probabilities should remain similar because spins don't depend on axes; only relative orientations matter.\n\nBut perhaps more importantly, the question asks about finding the electron with spin down. Spin-down typically refers to the spin component opposite to the quantization axis. If our initial state is spin-up along t-axis, then measuring spin along another axis might give different results. Or does it refer to just the intrinsic spin being down irrespective of orientation?\n\nWait, probably not. When someone mentions spin up/down without specifying, sometimes they mean along the same axis as the measurement. Alternatively, in many cases, especially in problems like this, unless specified otherwise, spin up/down could refer to the standard z-direction. But in this case, the state is prepared with total angular momentum up along the t-axis, which may complicate things.\n\nAlternatively, considering that the total angular momentum includes both orbital and spin contributions, but the question seems to ask purely about the spin part. Maybe I need to consider how much of the total angular momentum comes from spin versus orbit.\n\nGiven that the state is |j=1/2, m_j=1/2>. For a hydrogen atom, the total angular momentum j combines orbital angular momentum l=1 and spin s=1/2. Therefore, the Clebsch-Gordan coefficients come into play here. Specifically, the state |j,m> can be expressed as a combination of |l,s> states.\n\nThe general expression is:\n\n|j, m⟩ = Σ [C^{j}_{m}(l, s)] * |l, m_l⟩ ⊗ |s, m_s⟩\n\nWhere C^{j}_{m}(l, s) are the Clebsch-Gordan coefficients.\n\nIn our case, j=1/2, l=1, s=1/2. We want to find the expansion of |1/2, 1/2⟩ in terms of |l, m_l⟩⊗|s, m_s⟩.\n\nI remember that for adding l=1 and s=1/2, there are two possible j's: j=3/2 and j=1/2. Our state is j=1/2, so its decomposition must involve combinations of |l=1, m_l⟩ and |s=1/2, m_s⟩ such that m_l + m_s = m_j = 1/2.\n\nPossible pairs:\n- m_l = 1, m_s = -1/2 → sum is 1 - 1/2 = 1/2.\n- m_l = 0, m_s = 1/2 → sum is 0 + 1/2 = 1/2.\n- m_l = -1, m_s = 3/2 → Not possible because maximum m_s is 1/2.\n\nTherefore, the state |1/2, 1/2⟩ is a superposition of |1,1⟩⊗|-1/2⟩ and |1,0⟩⊗|+1/2⟩.\n\nLooking up the exact CG coefficients for coupling l=1 and s=1/2 to get j=1/2:\n\nFrom tables or formulas, the coefficient for |1,1⟩⊗|-1/2⟩ is sqrt(1/(3)) and for |1,0⟩⊗|+1/2⟩ is sqrt(2/3).\n\nHence,\n\n|1/2, 1/2⟩ = √(1/3)|1,1⟩⊗|↓⟩ + √(2/3)|1,0⟩⊗|↑⟩\n\nHere, ↑ represents spin-up (+1/2) and ↓ spin-down (-1/2).\n\nNow, the question is asking for the probability of finding the electron with spin down. That occurs when the second term in the product is |-1/2⟩, i.e., the first term above.\n\nThus, the amplitude for spin down is √(1/3), hence the probability is (√(1/3))^2 = 1/3 ≈ 0.333...\n\nSo the probability is 1/3.\n\nMoving on to part (b): \"Compute the probability per unit solid angle P(θ, φ) that the electron will be found at spherical angles θ, φ (independent of radial distance and spin).\" \n\nHmm, okay. Probability density independent of radius and spin suggests integrating out the radial variable and tracing out the spin degrees of freedom. So essentially, compute the spatial distribution averaged over spin.\n\nSince the wavefunction is a combination of different spatial and spin components, after taking modulus squared and averaging over spin, we'll get something depending on θ and φ.\n\nFirstly, recall that the overall state is |ψ⟩ = |r⟩ ⊗ |j=1/2, m_j=1/2⟩_t, meaning the position part is entangled with the spin part. To find the probability per unit solid angle, ignoring r, we integrate over dr and average over spin.\n\nExpressed mathematically, the joint probability is |⟨r, Ω, σ | ψ ⟩|^2, integrated over r gives us the angular dependence times the spin contribution.\n\nHowever, since the spin part is already accounted for in the state |j=1/2, m_j=1/2⟩, which itself is a mixture of different spin states, we need to project onto the spatial coordinates while considering the spin.\n\nLet me formalize this.\n\nWe can express the state as:\n\n|ψ⟩ = R(r) Y_{lm}(Ω_t) ⊗ |spin⟩\n\nwhere Ω_t denotes the angles around the t-axis. But since the observer uses θ,φ measured around the lab frame (say, z-axis), we need to relate them via rotation matrices.\n\nThis complicates matters because the spherical harmonics are defined with respect to the quantization axis. Here, the state is aligned along t-axis, whereas measurements are made along z-axis.\n\nTo handle this, perhaps using Wigner d-functions or Euler rotations. Alternatively, assuming that the t-axis is arbitrary, but the probability depends only on the alignment between t and the observation direction.\n\nWait, but the question specifies computing P(θ, φ) independent of spin. So perhaps we trace out the spin variables.\n\nThat is, the reduced density matrix for space is obtained by partial trace over spin. Then, the probability per unit solid angle is Tr[ρ_space |Y_{LM}⟩⟨Y_{LM}| ] summed appropriately.\n\nBut maybe simpler approach: Given that the state is |j=1/2, m_j=1/2⟩, which is a combination of |l=1, m_l=1⟩⊗|s=-1/2⟩ and |l=1, m_l=0⟩⊗|s=+1/2⟩, as"}
{"id": "quantum/5008", "questions": "A one-dimensional simple harmonic oscillator is subjected to a small perturbing potential $W(x)$, producing a impulse at the center of the motion. Thus\n\n$$\nH = \\frac{P^2}{2m} + \\frac{1}{2} m\\omega^2 x^2 + \\frac{\\lambda}{x^2 + a^2} = \\frac{P^2}{2m} + V + \\delta V.\n$$\n\nCalculate the correction to the ground state energy of the oscillator to first order in $\\lambda$ in the event that  \n\n(a) $ a \\ll \\sqrt{\\hbar/m\\omega} $,  \n\n(b) $ a \\gg \\sqrt{\\hbar/m\\omega} $.\n\nHints: the normalized ground state wave function of a simple harmonic oscillator is\n\n$$\n\\psi_0(x) = \\left(\\frac{m\\omega}{\\pi \\hbar} \\right)^{1/4} \\exp(-m\\omega x^2 / 2\\hbar),\n$$\n\nand\n\n$$\n\\int_{-\\infty}^{+\\infty} \\frac{dx}{x^2 + a^2} = \\frac{\\pi}{a},\n$$", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about a one-dimensional simple harmonic oscillator with a perturbation. The Hamiltonian is given as H equals P squared over 2m plus (1/2)mω²x² plus λ divided by (x² + a²). They want me to find the first-order correction to the ground state energy for two cases: when a is much smaller than sqrt(ħ/(mω)) and when a is much larger than that same quantity.\n\nFirst off, since it's a perturbation theory question, I remember that the first-order energy shift is just the expectation value of the perturbing potential δV with respect to the unperturbed ground state. So E₀' should be equal to ⟨ψ₀|δV|ψ₀⟩ where ψ₀ is the ground state wavefunction provided.\n\nThe perturbing term here is W(x) which is λ/(x² + a²), right? So actually, δV is λ/(x² + a²).\n\nSo my task reduces to computing the integral:\n\nE₀^(1) = ∫ from -∞ to ∞ [ψ₀*(x)] * [λ/(x² + a²)] * ψ₀(x) dx\n\nWhich simplifies because everything except maybe constants can go into the integrals. Let me write out what ψ₀ looks like again. It’s [(mω)/(π ħ)]^(1/4) times exp[-mω x²/(2ħ)]. Therefore, |ψ₀|^2 would be [(mω)/(π ħ)]^(1/2) exp[-mω x²/ħ].\n\nTherefore, plugging back into the integral:\n\nE₀^(1) = λ * [(mω)/(π ħ)]^(1/2) ∫_{-∞}^{+∞} [exp(-mω x²/ħ)] / (x² + a²) dx\n\nHmm, okay. That seems manageable but integrating something like e^{-k x²}/(x² + a²) might not be straightforward. Wait, they gave us a hint! There was an integral mentioned: ∫_{-∞}^{+∞} dx/(x² + a²) = π/a. But our case has an exponential multiplied by 1/(x² + a²). Maybe we need another approach or perhaps use some substitution?\n\nAlternatively, could there be a way to express 1/(x² + a²) using Fourier transforms or other methods? Hmm... Alternatively, think if we can relate this integral to known results involving Gaussian functions.\n\nWait, let me recall that sometimes such integrals can be evaluated via contour integration or differentiation under the integral sign. Or perhaps even recognizing them as standard forms.\n\nLet me consider writing down the integral more formally:\n\nI = ∫_{-∞}^{+∞} [e^{-α x²}] / (x² + b²) dx,\n\nwhere α = mω/ħ and b = a. Then, how do you compute this integral?\n\nOne method I know involves expressing 1/(x² + b²) as an integral itself. Specifically, note that 1/(x² + b²) = ∫_{0}^{∞} e^{-(x² + b²)t} dt. Is that correct? Because Laplace transform kind of thing—yes, indeed, ∫_{0}^\\infty e^{-(s)t} ds = 1/s, so s being x² + b² gives exactly that expression.\n\nThus, substituting this representation into the original integral:\n\nI = ∫_{-∞}^{+∞} e^{-α x²} [∫_{0}^{∞} e^{-(x² + b²)t} dt] dx\n\nWe can interchange the order of integration assuming Fubini's theorem applies:\n\nI = ∫_{0}^{∞} e^{-b² t} [∫_{-∞}^{+∞} e^{-α x² - t x²} dx ] dt\n\nSimplify inside the innermost integral:\n\n= ∫_{0}^{∞} e^{-b² t} [∫_{-∞}^{+∞} e^{-(α + t)x²} dx ] dt\n\nNow, the integral over x is a Gaussian integral. We know that ∫_{-∞}^{+∞} e^{-k x²} dx = sqrt(π/k). Here k = α + t, so:\n\nInner integral becomes sqrt(π/(α + t))\n\nThus, now I is:\n\nI = sqrt(π) ∫_{0}^{∞} e^{-b² t} / sqrt(α + t) dt\n\nThis still needs evaluation. Perhaps make a substitution u = alpha + t => du = dt; then when t=0, u=alpha, t=infinity, u=infinity.\n\nBut wait, alternatively, set z = sqrt(alpha + t); no, maybe better yet, substitute y = sqrt(t + alpha)\n\nWait, hold on—if I change variable to u = t + alpha, then t = u - alpha, dt = du, limits become u from alpha to infinity.\n\nThen,\n\nI = sqrt(pi) ∫_{u=alpha}^{infinity} e^{-b²(u - alpha)} / sqrt(u) du\n\nBecause t = u - alpha, so exponent becomes -b²(u - alpha):\n\n= sqrt(pi) e^{b² alpha} ∫_{alpha}^{inf} e^{-b² u} / sqrt(u) du\n\nThat integral resembles Gamma function or error function terms. Recall that Γ(n,z) is the incomplete gamma function defined as ∫_{z}^{inf} t^{n-1} e^{-t} dt.\n\nIn our case, n = 1/2, and t = b² u. Let me see:\n\nSet v = b² u ⇒ dv = b² du ⇒ du = dv/b²\n\nWhen u = alpha, v = b² alpha; u → inf, v→ inf.\n\nSubstituting:\n\nIntegral becomes ∫_{v=b² alpha}^{inf} (v/b²)^{-1/2} e^{-v} (dv)/b² )\n\nSimplify exponents:\n\n(v)^{-1/2} / b^{-1} ) * (1/b²) dv ?\n\nWait, let's calculate each part carefully.\n\nOriginal integral after substitution:\n\n∫_{alpha}^{inf} e^{-b² u} / sqrt(u) du = ∫_{b² alpha}^{inf} e^{-v} / sqrt(v / b²) * (dv / b²)\n\nSince u = v / b², so sqrt(u)=sqrt(v)/b.\n\nAnd du = dv / b².\n\nPutting together:\n\n= ∫_{b² alpha}^{inf} e^{-v} / (sqrt(v)/b) * (dv / b²)\n= ∫ ... e^{-v} * b / sqrt(v) * dv / b²\n= (1/b) ∫_{b² alpha}^{inf} e^{-v} / sqrt(v) dv\n\nNote that ∫ e^{-v} / sqrt(v) dv is related to the Gamma function. Indeed, Γ(1/2, z) = ∫_{z}^{inf} e^{-v} v^{-1/2} dv.\n\nGamma(1/2) is sqrt(pi), but here it starts at z instead of zero.\n\nSo putting all together:\n\nOur initial integral I is:\n\nI = sqrt(pi) e^{b² alpha} * (1/b) Γ(1/2, b² alpha)\n\nRecall that Γ(s, z) is called the upper incomplete gamma function.\n\nHowever, unless we can simplify further, this may not help directly. However, considering specific cases when either a << sqrt(ħ/(m omega)) or a >> sqrt(ħ/(m omega)), i.e., whether b is small or large compared to certain scales.\n\nGiven that alpha = m omega / hbar, so sqrt(1/alpha) is proportional to sqrt(hbar/(m omega)). Since a is either less than or greater than that scale, meaning b is either small or large relative to 1/sqrt(alpha).\n\nCase (a): a << sqrt(ħ/(m omega)) implies that b is very small compared to 1/sqrt(alpha). In this limit, b² alpha ~ (small number"}
{"id": "quantum/1-1026", "questions": "An approximate model for the problem of an atom near a wall is to consider a particle moving under the influence of the one-dimensional potential given by\n\n$$\nV(x) = -V_0 \\delta(x), \\quad x > -d, \\\\\nV(x) = \\infty, \\quad x < -d,\n$$\n\nwhere $\\delta(x)$ is the so-called delta function\n\n(a) Find the modification of the bound-state energy caused by the wall when it is far away. Explain also how far is \"far away\".\n(b) What is the exact condition on $V_0$ and $d$ for the existence of at least one bound state?", "graphs": null, "llm_answers": "<think>\nOkay, I have this physics problem about a particle near a wall modeled with a specific potential. Let me try to understand what's being asked here.\n\nSo part (a) says: Find the modification of the bound-state energy caused by the wall when it is far away. Also explain how far is \"far away.\" Hmm. The setup has a 1D potential where V(x) is negative infinity times the Dirac delta function δ(x) for x greater than -d, and infinite otherwise. So that means beyond x=-d, the potential becomes infinitely high—like a hard wall there. For x>-d, we have a single attractive delta potential centered at x=0 because δ(x)=δ(−x). \n\nWait, actually, no—the way it's written, V(x) is −V₀ δ(x) only for x > -d, but since δ(x) is zero except at x=0, maybe the position doesn't matter much as long as d isn’t too close. But if the wall is very far away, like d approaches infinity, then effectively, the system reduces to just the free space with a delta potential. In that case, without any walls, the bound state would be known from standard quantum mechanics problems.\n\nI remember that for a simple delta potential well, the bound state energy E is given by something involving V₀. Specifically, the ground state energy is usually proportional to -mV₀²/(2ħ²), or similar—I need to recall exactly. Wait, let me think more carefully.\n\nThe general approach for solving such potentials involves setting up the Schrödinger equation:\n\nFor regions outside the delta potential, which is everywhere except x=0, the wavefunction behaves exponentially because the potential is zero. Since the wall is at x=-d, but if d is large, say approaching infinity, the boundary conditions simplify.\n\nIn the absence of the wall (i.e., when d→∞), the bound state exists only once, right? Because a single delta potential supports only one bound state. Its energy can be found using the method of integrating across the delta function.\n\nBut now, introducing a finite distance d might perturb this energy slightly. When d is very large compared to some characteristic length scale related to the bound state, perhaps the effect is small, allowing us to treat it as a perturbation?\n\nAlternatively, maybe instead of treating it as a perturbation, we should solve the full problem considering both the delta potential and the presence of the wall.\n\nLet’s outline the steps needed:\n\nFirstly, set up the time-independent Schrödinger equation for this piecewise potential.\n\nSince the potential is symmetric around x=0? No, wait—it's not symmetric unless d is placed symmetrically, but in our case, the wall is at x=-d while the delta is at x=0. So the region between x=-d and x=+infinity has the delta potential at x=0, and everything else is either flat (zero?) or infinite.\n\nActually, hold on—if x < -d, V(x)=infinite; for x >=-d, V(x)= -V₀ δ(x).\n\nTherefore, the domain accessible to the particle is x >= -d, and within that, the potential is mostly zero except at x=0, where it's a delta function.\n\nTo find the bound states, we look for solutions ψ(x) that satisfy the Schrödinger equation in each region and match appropriately at boundaries.\n\nGiven that, let's split into two regions:\n\nRegion I: x < 0,\n\nRegion II: x > 0,\n\nbut note that Region I starts at x=-d, goes to x=0, and Region II extends from x=0 onwards.\n\nHowever, due to symmetry considerations, even though the wall breaks translational symmetry, the potential itself is still symmetric about x=0 because the delta function is symmetric. Therefore, the eigenstates will likely retain parity symmetry—they'll be either even or odd functions.\n\nBut wait, does having a wall at x=-d affect the parity? If you reflect over x=0, points left of 0 go further left towards the wall, whereas reflection takes them past the origin. Maybe the wavefunctions won't necessarily maintain strict parity unless certain conditions are met.\n\nHmm, perhaps better to proceed without assuming symmetry.\n\nSo, first, write down the TISE in each region.\n\nIn Regions I and II (excluding x=0):\n\n(-ħ²/2m)(ψ''(x)) + V(x)ψ(x) = Eψ(x)\n\nBut V(x) is zero except at x=0, so in these regions, the equation simplifies to:\n\nψ''(x) = k² ψ(x),\n\nwhere k² = -2mE / ħ².\n\nNote that E must be negative for bound states, hence k is real.\n\nThus, in Region I (left side): x ∈ (-d, 0)\n\nSolution form: ψ_I(x) = A e^{k x} + B e^{-k x}\n\nSimilarly, in Region II (right side): x ≥ 0\n\nSolution form: ψ_{II}(x) = C e^{-k x} + D e^{k x}\n\nNow apply boundary conditions.\n\nAt x → ∞, the wavefunction must decay, so D=0 because e^{k x} blows up.\n\nAlso, at x=-d, the potential jumps to infinity, meaning the probability current vanishes there. Thus, the derivative of ψ at x=-d must ensure continuity—but actually, since V(x) is infinite below x=-d, the wavefunction must vanish abruptly at x=-d. Hence, ψ(-d) = 0.\n\nAdditionally, at x=0, the delta function imposes discontinuity in the derivative. Recall that for a delta potential, the jump condition comes from integrating the SE through x=0.\n\nSpecifically, integrate the SE from -ε to ε as ε→0:\n\n∫[SE] dx => [ψ'(0^+) - ψ'(-0)] = -(2m/ħ²)V₀ ψ(0)\n\nBecause V(x) is -V₀ δ(x), so integral gives -V₀ * 2m/ħ² multiplied by ψ(0).\n\nHence, the discontinuity in the derivative is Δψ’ = (2m/V₀) ψ(0). Or wait, signs may vary depending on conventions.\n\nWait, let me rederive quickly.\n\nFrom the SE:\n\n-(ħ²/2m) ψ''(x) - V₀ δ(x) ψ(x) = E ψ(x)\n\nMultiply both sides by 2m/ħ²:\n\n-ψ''(x) - (2m V₀)/ħ² δ(x) ψ(x) = (2m E)/ħ² ψ(x)\n\nIntegrate from -ε to ε:\n\n[-ψ'(ε) + ψ'(-ε)] - (2m V₀)/ħ² ψ(0) = (2m E)/ħ² ∫_{-ε}^{ε} ψ(x) dx ≈ (2m E)/ħ² * 2ε ψ(0)\n\nAs ε→0, RHS tends to zero, so LHS equals approximately:\n\nΔψ' = ψ'(0^+) - ψ'(-0) = (2m V₀)/ħ² ψ(0)\n\nYes, correct.\n\nSo, the matching condition at x=0 is:\n\nψ'_R - ψ'_L = (2m V₀)/ħ² ψ(0)\n\nWhere ψ_R is the derivative from the right, ψ_L from the left.\n\nAlright, putting all together.\n\nWe have four constants: A, B, C, D. From above, D=0 because exponential blowup at infinity is forbidden.\n\nThen, applying ψ(-d)=0:\n\nψ_I(-d) = A e^{-k d} + B e^{k d}=0\n\nWhich implies A = -B e^{2k d}, since e^{-kd}/e^{kd}=e^{-2kd}. So A = -B e^{2k d}\n\nNext, express ψ_I and ψ_{II}:\n\nψ_I(x) = A e^{k x} + B e^{-k x}; substituting A:\n\nψ_I(x) = -B e^{2k d} e^{k x} + B e^{-k x} = B [ e^{-k x} - e^{k(x + 2d)} ]\n\nAnd ψ_{II}(x) = C e^{-k x}\n\nContinuity at x=0 requires ψ_I(0) = ψ_{II}(0):\n\nLeft limit: ψ_I(0^-) = B [ e^{0} - e^{2k d} ] = B (1 - e^{2k d})\n\nRight limit: ψ_{II}(0^+) = C e^{0} = C\n\nSet equal: C = B"}
{"id": "quantum/6017", "questions": "Calculate the Born approximation to the differential and total cross sections for scattering a particle of mass $m$ off the δ-function potential $V(r) = g \\delta^3(r)$.  ", "graphs": null, "llm_answers": "<think>\nOkay, so I need to calculate the Born approximation for both the differential and total cross sections when a particle scatters off a delta function potential. Hmm, let me start by recalling what the Born approximation is.\n\nFrom my notes, the Born approximation is used in quantum mechanics as a perturbative method to solve the Schrödinger equation for scattering problems. It's particularly useful when the potential is weak or short-ranged. The idea is that we can approximate the scattered wave without considering multiple scatterings beyond the first order. So it should be applicable here since the delta function potential is very localized.\n\nThe given potential is V(r) = g δ³(r), which means it's zero everywhere except at r=0, where it has an infinite spike but with finite strength proportional to g. This kind of potential models something like a point interaction.\n\nI remember that in the Born approximation, especially the first-order (or single-scattering) case, the differential cross section is calculated using the Fourier transform of the potential. Let me write down the formula:\n\nIn three dimensions, the differential cross-section dσ/dΩ in the Born approximation is approximately |f(θ)|² multiplied by some constants. Specifically,\n\n\\[ f(\\theta) = -\\frac{1}{k} \\int e^{i \\mathbf{k}' \\cdot \\mathbf{r}} V(\\mathbf{r}) e^{-i \\mathbf{k} \\cdot \\mathbf{r}} d^3 r \\]\n\nwhere k' and k are the outgoing and incoming momenta respectively, related through conservation of momentum: **k**' = **k** + q, where q is the transfer momentum vector. But wait, actually, more accurately, in elastic scattering, energy is conserved, so |k'| = |k| = k because E = ħ²k²/(2m). Therefore, θ is the angle between **k** and **q**, right?\n\nBut maybe another way to think about this integral is taking into account the plane waves involved. Since our potential is spherically symmetric (it depends only on r, not direction), perhaps there's a simpler expression involving spherical harmonics? Or maybe just integrating over all space due to symmetry.\n\nWait, no, hold on. For a central potential, the scattering amplitude f(θ) will depend only on cosθ, making things easier. Also, the delta function simplifies integrals significantly.\n\nSo plugging V(r)=gδ³(r) into the expression for f(θ):\n\nFirst, note that the exponential terms evaluated at r=0 become 1, because exp(i(**k'**-**k**)·0) = 1. Thus, the integral becomes simply g times the volume element at r=0, which is just g*(something).\n\nLet me compute the integral explicitly:\n\n∫ e^{i (**k'** − **k**) · **r** } δ³(r) d³r\n\nSince the delta function picks out the value at r=0, this integral equals 1. Hence, the entire expression inside the absolute square becomes:\n\nf(θ) ≈ -(1/k) * [g] * ∫ ... Wait, hang on, isn't the integral equal to 4π/g? No, wait, no—actually, substituting back, the integral itself gives us 1, so then f(θ) would be (-g)/(ik)?\n\nNo, wait, confusion arises from different conventions. Maybe better to recall the exact form.\n\nAlternatively, I might have mixed up factors; perhaps the correct expression involves 4π somewhere else.\n\nAnother approach: In the Born approximation, the scattering amplitude is given by:\n\nf(θ) = (1/√(8 m π ℏ)) ) × (some factor)\n\nWait, perhaps going back to basic principles. The time-independent Schrodinger equation for scattering states is:\n\n[−(ħ²/(2m))∇² + V(r)] ψ(r) = Eψ(r),\n\nand the asymptotic behavior of the wavefunction is:\n\nψ(r) → e^{ikz} + f(θ)(e^{ikr}/r)}.\n\nWhere z is along the initial direction of motion, and f(θ) is the scattering amplitude.\n\nUsing partial-wave expansion, each angular momentum component contributes, but if the potential is weak enough, higher l contributions may be neglected. However, for the delta function potential, which is non-zero only at origin, perhaps only certain terms survive.\n\nActually, thinking again, the delta function is spherically symmetric, so its Fourier transform must also respect that symmetry. That suggests that the resulting f(θ) could be isotropic, meaning independent of θ. Is that possible?\n\nWait, but how does that make sense? If you have a delta function potential, wouldn't the scattering be isotropic? Because the potential doesn't prefer any particular direction, hence the scattering shouldn't either. So yes, f(θ) should indeed be constant, i.e., same in every direction.\n\nTherefore, the differential cross-section σ(θ) would be |f|^2, which is a constant, implying uniform scattering across all angles. Interesting!\n\nNow, computing f(θ): As per the Born approximation, f(k',k) is given by:\n\nf(k',k) = (1/(2m)) ∫ e^{i(**k'** − **k**) ⋅ **r**} V(r) d³r / (E_k' − E_k + iε )\n\nBut since it's elastic scattering, E_k' = E_k, so denominator becomes iε, leading to principal value considerations... Wait, but in the Born approximation, sometimes they neglect such subtleties assuming ε→0+.\n\nHowever, in many treatments, including Landau-Lifshitz, the Born approximation ignores these poles and directly computes the integral.\n\nGiven that, let's proceed formally.\n\nCompute the integral:\n\nIntegral = ∫ e^{i(**k'** − **k**)⋅**r**} V(r) d³r\n\nWith V(r) = g δ³(r), this reduces to evaluating the exponent at r=0, giving 1. So Integral = g*1 = g.\n\nThus, f(k',k) = (1/(2m)) * g / (iε ). Hmm, division by epsilon seems problematic—it goes to infinity unless regularized.\n\nWait, perhaps instead, the standard treatment uses a slightly different formulation. Alternatively, perhaps the proper expression includes a term with 4π.\n\nHold on, looking back, I think the actual formula for the scattering amplitude in the Born approximation is:\n\nf(θ) = (1/(k)) ∫ e^{i(**k'** − **k**) ⋅ **r**} V(r) d³r\n\nBecause earlier expressions had 1/k scaling.\n\nYes, according to Messiah's Quantum Mechanics book, the first Born approximation is:\n\nf^(1)(k,k') = (1/(2m)) ∫ e^{i(**k'** − **k**) ⋅ **r**} V(r) d³r\n\nAnd the differential cross-section is |f^(1)|².\n\nBut in other sources, I've seen it written differently, possibly scaled by 1/k.\n\nHmm, conflicting memories. Perhaps best to derive it properly.\n\nStarting fresh: The Lippmann-Schwinger equation provides the basis for the Born series. To first order, the T-matrix is approximated as T ≈ V, ignoring higher-order terms. Then, the scattering amplitude is given by:\n\nf(θ) = (1/(2m)) ∫ e^{i(**k'** − **k**) ⋅ **r**} V(r) d³r\n\nWhich makes sense dimensionally—if V has units of energy×length³, then multiplying by e^{i...}, which is unitless, and integrating over d³r (units length³) gives Energy×Length⁶. Dividing by 2m (mass) gives Length⁴×Energy/mass. Hmmm, probably getting confused.\n\nWait, alternatively, dimensional analysis aside, proceeding with calculation.\n\nFor V(r) = g δ³(r), plug into the integral:\n\nIntegral = ∫ e^{i(**k'** − **k**) ⋅ **r**} g δ³(r) d³r = g\n\nHence, f(θ) = (1/(2m)) * g\n\nIs that correct? Seems too simple. Moreover, why do we get a result independent of θ? Which aligns with previous thought—the scattering is isotropic.\n\nThen, the differential cross-section is |f(θ)|² = (g²)/(4m²)\n\nBut wait, usually, differential cross-section has units of area per solid angle. What are the units here?\n\nAssuming g has units of energy×length³, because V(r) has units of energy density (since it's a 3D delta function). So g needs to compensate for that, thus having units of energy×length³.\n\nDivided by m squared, which has units of mass squared. And Planck’s constant is missing—I wonder if I missed a factor of ħ somewhere.\n\nAh! Probably forgot the dependence on ħ. Let me re-examine the derivation.\n\nIn the Born approximation, the general formula is often expressed as:\n\nf(θ) = (1/(k)) ∫ e^{i(**k'** − **k**)"}
{"id": "quantum/2-2018", "questions": "A spinless particle of mass $ m $ is subject (in 3 dimensions) to a spherically symmetric attractive square-well potential of radius $ r_0 $.\n\n(a) What is the minimum depth of the potential needed to achieve two bound states of zero angular momentum?  \n\n(b) With a potential of this depth, what are the eigenvalues of the Hamiltonian that belong to zero total angular momentum? (If necessary you may express part of your answer through the solution of a transcendental equation.)  \n\n(c) If the particle is in the ground state, sketch the wave function in the coordinate basis and the corresponding coordinate probability distribution. Explain carefully the physical significance of the latter.  \n\n(d) Predict the result of a (single) measurement of the particle kinetic energy in terms of this wave function. You may express your prediction through one-dimensional definite integrals.  \n\n(e) On the basis of the uncertainty principle, give a qualitative connection between parts (c) and (d) above.", "graphs": null, "llm_answers": "<think>\nAlright, so I have this problem about a spinless particle in a spherically symmetric square well potential. It's divided into five parts: from finding the minimum depth for two bound states with zero angular momentum, determining their eigenvalues, sketching the wave functions, predicting the kinetic energy measurement, and connecting it all via the uncertainty principle. Let me tackle each part step by step.\n\nStarting with part (a): The question asks for the minimum depth required to have two bound states with l=0 (zero angular momentum). Since we're dealing with a spherical square well, which means V(r) = -V₀ for r ≤ r₀ and V(r)=0 otherwise. For such potentials, solving the Schrödinger equation involves considering regions inside and outside the well separately.\n\nFor l=0, the effective potential doesn't contribute because the centrifugal term is absent. So, the radial equation simplifies significantly. Inside the well (r < r₀), the particle experiences a negative potential, leading to oscillatory solutions, while outside, since E > 0 there but less than |V₀| for bound states, the solutions decay exponentially.\n\nTo find the number of bound states, we can use the condition on the logarithmic derivative or match boundary conditions at r=r₀. But specifically, when looking for the minimum depth where exactly two bound states exist, we need to consider how many times the effective potential allows for nodes in the wavefunction before they become unbound.\n\nWait, actually, another approach is using phase shifts or quantization conditions. In three dimensions, the number of bound states depends on the strength of the potential. A deeper potential supports more bound states. To get two bound states, the potential must be deep enough so that after certain matching conditions, both energies correspond to valid solutions within the well.\n\nI recall that for a square well, the number of bound states increases as the depth V₀ increases. Each time the argument of the Bessel function crosses multiples of π/2, a new bound state appears. Specifically, for l=0, the quantization condition comes down to something like:\n\ntan(k r₀) = sqrt((k² r₀²)/(1 + ...)) Hmm, maybe not exact. Wait, let's think again.\n\nThe general method is to solve the Schrödinger equation inside and outside the well. Inside, the solution is sinusoidal, say sin(k r)/r, and outside, exponential decaying e^{-κ r}/r, where k^2 = 2m(E + V₀)/ħ² and κ^2 = -2mE/ħ².\n\nMatching these solutions at r=r₀ requires continuity of ψ and its first derivative. This leads to equations involving sine, cosine, and exponentials. Taking ratios gives us tan(k r₀) = [something], perhaps?\n\nYes, if I set up the ratio correctly, the condition becomes:\n\nsin(k r₀) / cos(k r₀) = [some expression involving κ]\n\nBut wait, no—actually, upon differentiating the inner and outer solutions, the derivatives involve factors related to k and κ respectively. After some algebraic manipulation, the resulting equation should relate k and κ, but given that E is positive inside the well and negative outside, we might substitute κ in terms of k.\n\nAlternatively, substituting κ = i k', then expressing everything in terms of real variables could help simplify things. However, regardless of substitution, ultimately, the key point is setting up the transcendental equation whose roots determine possible values of k, hence E.\n\nSince we want two bound states, we'll look for the value of V₀ where the second root just disappears—that would mean increasing V₀ beyond that will allow more states, decreasing below won’t support them anymore. Therefore, the minimal V₀ corresponds to the case where the second bound state touches the continuum; meaning, the binding energy approaches zero.\n\nSo, mathematically, the critical points occur when the tangent function equals specific asymptotic expressions. Perhaps plotting the left-hand side (LHS) vs right-hand side (RHS) of our transcendental equation against k*r₀ would show intersections—the number of crossings determines the number of bound states.\n\nTherefore, to have two bound states, the LHS curve needs to cross RHS twice. As V₀ decreases, eventually, only one crossing remains, indicating only one bound state exists. Hence, the threshold occurs when the curves touch tangentially, giving precisely two bound states—one being the last possible before becoming unbound.\n\nThus, the minimal V₀ is found when the second bound state has E approaching zero. At this point, the wavefunctions extend far outwards, making the system barely hold onto the second state.\n\nMoving on to part (b): Once we've determined the appropriate V₀, now we need to compute the actual eigenenergies for l=0. These come from solving the transcendental equation derived earlier.\n\nGiven that, without knowing the explicit form, generally, the eigenvalues satisfy:\n\ntan(k r₀) = sqrt( (k² r₀²) )/(sqrt(...) )\n\nWait, probably better expressed as:\n\nLet’s denote x = kr₀, y = κ r₀\n\nFrom the boundary conditions, equating the log-derivative inside and outside:\n\nInside:\nψ'(r)/ψ(r) = cot(x) * (1/r)\n\nOutside:\nψ'(r)/ψ(r) = -y / sinh(y) * (1/r)\n\nAt r = r₀,\n\ncot(x) = (-y)/tanh(y)\n\nBecause sinh(y) ≈ e^{y} for large y, but here, near the threshold, y isn't necessarily very small or big—it depends on whether E is close to zero.\n\nHowever, for the lowest lying states, especially the ground state, y tends to be larger because E is lower (more tightly bound).\n\nIn any case, combining these relations along with the definitions of x and y in terms of E and V₀, we'd arrive at a transcendental equation purely in terms of x or y.\n\nSpecifically, defining z = x, then:\n\nz = sqrt( (2m(V₀))/ ħ² ) * r₀ ?\n\nNo, wait, let's clarify.\n\nDefine k = sqrt(-2mE)/ħ, but wait—if E is negative, then k is imaginary. Maybe better to write:\n\nInside the well, E' = E + V₀ is positive, so define k = sqrt(2m(E + V₀)} / ħ\n\nOutside, E is still negative, so define κ = sqrt(-2mE)/ħ\n\nThen, the relation from the boundary condition is:\n\ntan(k r₀) = (κ/k) * coth(κ r₀)\n\nThis seems familiar—I remember similar forms in scattering theory.\n\nHence, writing:\n\ntan(z) = (sqrt{(z')}) / z * coth(sqrt{z'^2})\n\nWhere z = k r₀, z’ = κ r₀.\n\nBut note that z’ relates to z via the energies:\n\nWe know that E = -(ħ² z²)/(2m r₀²)\nAnd also,\nE = -(ħ² {z'}²)/(2m r₀²}\n\nWait, no, that's incorrect because E is same everywhere except inside/outside.\n\nActually, correct relationship is:\n\nE = - (ħ² z²)/(2m r₀²) \n\nand\n\nAlso, E = - (ħ² {z'}²)/(2m r₀²)\n\nWhich implies that z = z'\n\nHmm, confusing notation. Alternatively, perhaps better to keep track properly.\n\nLet me re-express:\n\nInside the well, E + V₀ = (ħ² k²)/(2m)\n\nOutside, E = - (ħ² κ²)/(2m)\n\nSet equal:\n\nE = - (ħ² κ²)/(2m) => E + V₀ = V₀ - (ħ² κ²)/(2m) = (ħ² k²)/(2m)\n\nSolving for V₀:\n\nV₀ = (ħ² (k² + κ²))/(2m)\n\nNow, going back to the boundary condition:\n\n[ dψ/dr ]_{inside} / ψ_inside = [ dψ/dr ]_{outside} / ψ_outside\n\nCompute derivatives:\n\nInside: ψ_in ~ sin(k r); derivative is k cos(k r)\n\nDivided by ψ_in: k cot(k r)\n\nSimilarly, Outside: ψ_out ~ e^{-κ r}; derivative is -κ e^{-κ r}\n\nDivided by ψ_out: -κ\n\nSetting equality at r = r₀:\n\nk cot(k r₀) = -κ\n\nSubstitute κ from previous equation:\n\nRecall that V₀ = (ħ² (k² + κ²))/("}
{"id": "quantum/1-1010", "questions": "$\\psi(x, t)$ is a solution of the Schrödinger equation for a free particle of mass m in one dimension, and \n\n$$\n\\psi(x, 0) = A \\exp\\left(-x^2 / a^2 \\right).\n$$\n\n(a) At time $t = 0$ find the probability amplitude in momentum space.\n\n(b) Find $\\psi(x, t)$.\n\n", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about quantum mechanics where ψ(x, 0) is given as A exp(-x²/a²). It's part (a) asking for the probability amplitude in momentum space at time t=0, which means finding φ(p), right? And then part (b) wants me to find ψ(x, t).\n\nStarting with part (a): The wave function in position space is given, but we need it in momentum space. From what I remember, the relation between them involves Fourier transforms because they're conjugate variables. So, the formula should be something like:\n\nφ(p) = ∫_{-∞}^{∞} e^{-ipx/ħ} ψ(x, 0) dx\n\nBut wait, sometimes people use different conventions for the exponential sign or factors involving ħ. Let me double-check that. Yeah, usually it's either positive or negative depending on convention, but since p is related to -iħ d/dx, maybe the exponent here has a negative i times px over ħ. Hmm, actually, let me recall: In some textbooks, the Fourier transform from x-space to p-space uses e^{-ikx}, while others might use e^{+ikx}. But considering the standard definition, if you take the inverse Fourier transform, it would involve e^{+ikx}. Wait no, perhaps more accurately, when going from position to momentum representation, the kernel is typically e^{-ipx/ħ}/√(2πħ)} or similar normalization.\n\nWait, hold on, another thought: Maybe the exact expression depends on how the Fourier transform is defined. Since the question doesn't specify, I think using the standard integral without worrying too much about constants except ensuring proper units will work. Alternatively, recalling that the Gaussian functions under Fourier transforms remain Gaussians, just scaled appropriately.\n\nSo first off, ψ(x, 0) is a Gaussian centered at zero with width parameter a. Therefore, its Fourier transform should also be a Gaussian in momentum space. That makes sense because minimum uncertainty states maintain their shape upon transformation.\n\nLet me write down the specific form again:\n\nψ(x, 0) = A exp(-x²/a²)\n\nI know that the Fourier transform of a Gaussian is another Gaussian. Specifically,\n\n∫_{-infty}^\\infty e^{-k x²} e^{-i p x} dx = sqrt(pi/k) * e^{-p²/(4 k)}\n\nOr something along those lines. Comparing my case, our integrand is e^{-x²/a²} multiplied by e^{-i p x / ħ}? Or do I not include ħ yet?\n\nActually, in many cases, especially in non-dimensionalized forms, the Fourier transform can ignore ħ, treating it implicitly through scaling. However, since we’re dealing with physical quantities, including ħ properly is essential.\n\nAlternatively, perhaps better to express everything in terms of natural units where ħ=1 temporarily, compute the result, and then reintroduce ħ correctly.\n\nBut stepping back, let's get precise definitions straight. For a general function f(x), its Fourier transform F(k) is often written as:\n\nF(k) = (1/sqrt{2 pi}) ∫_{-infty}^\\infty e^{-i k x} f(x) dx\n\nSimilarly, the inverse transform brings us back. Now, in quantum mechanics, the relationship between position and momentum eigenstates requires careful consideration of these factors.\n\nIn particular, the correct formula connecting ψ(x) and φ(p) is:\n\nφ(p) = (1/sqrt{2 π ħ}) ∫_{-infty}^\\infty e^{-i p x / ħ} ψ(x) dx\n\nYes, that seems familiar—because the commutator [x, p] = i ħ implies such relationships. Hence, the factor includes both 1/sqrt{2 pi ħ} and the exponential term with p x divided by ħ.\n\nTherefore, plugging into this formula, φ(p) becomes:\n\nφ(p) = (A / sqrt{2 π ħ}) ∫_{-infty}^\\infty e^{-i p x / ħ} e^{-x² / a²} dx\n\nNow, combining exponents:\n\ne^{-x² / a² - i p x / ħ}\n\nThis looks like a Gaussian integral. To evaluate it, complete the square in the exponent.\n\nThe exponent is (-x²)/(a²) + (-i p x)/ħ. Let’s rewrite this quadratic in x:\n\n-(x²)/(a²) - (i p x)/ħ\n\nFactor out -(1/a²):\n\n= -[x² + (i p a² x)/ħ ] / a²\n\nTo complete the square inside the brackets:\n\nx² + b x = (x + b/2)^2 - (b²)/4\n\nHere, b = (i p a²)/ħ\n\nThus,\n\n[x + (i p a²)/(2 ħ)]² - [(i p a²)/(2 ħ)]²\n\nSubstituting back:\n\nExponent becomes:\n-[ (x + (i p a²)/(2 ħ))² - (i² p² a⁴)/(4 ħ²) ) ] / a²\n\nSimplify each term:\n\nFirst term: -[(x + ... )²]/a² remains as is.\nSecond term: -[- (p² a⁴)/(4 ħ²) ] / a² = (p² a⁴)/(4 ħ² a²) = (p² a²)/(4 ħ² )\n\nHence, overall exponent simplifies to:\n\n- (x + c)^2 / a² + (p² a²)/(4 ħ² ), where c=(i p a²)/(2 ħ)\n\nPutting all together, the integral becomes:\n\nIntegral = e^{(p² a²)/(4 ħ²)} ∫_{-infty}^\\infty e^{- (x + c)^2 / a² } dx\n\nSince integrating over x shifts the variable; shifting z = x + c, dz = dx, limits still go from -infty to infinity.\n\nThen Integral = e^{(p² a²)/(4 ħ²)} ∫_{-infty}^\\infty e^{-z² / a²} dz\n\nWe know that ∫_{-infty}^\\infty e^{-z² / a²} dz = a sqrt(pi)\n\nBecause generally, ∫ e^{-k z²} dz = sqrt(pi/k); here, k=1/a², so sqrt(pi*a²)=a sqrt(pi)\n\nTherefore, putting it all together:\n\nIntegral = a sqrt(pi) e^{(p² a²)/(4 ħ² )}\n\nGoing back to φ(p):\n\nφ(p) = (A / sqrt{2 pi ħ}) * a sqrt(pi) e^{(p² a²)/(4 ħ² )}\n\nSimplify constants:\n\n(A / sqrt{2 pi ħ}) * a sqrt(pi) = A a / sqrt{2 ħ }\n\nAnd the exponential term is e^{(p² a²)/(4 ħ² )} ?\n\nWait, hang on—isn’t there a minus sign missing somewhere? Because originally, after completing the square, the exponent was split into two parts—one being negative squared term, leading to a positive contribution outside.\n\nLooking back, yes—the original exponent had a negative sign factored out, resulting in adding a positive term when moving it outside. Thus, the exponent indeed ends up as positive (p² a²)/(4 ħ² ). But wait—that leads to an exponentially increasing function, which isn't possible for a physically meaningful wavefunction in momentum space—it must decay.\n\nHmm, did I make a mistake in signs during completion of squares?\n\nLet me re-examine the steps carefully.\n\nOriginal exponent: (-x²)/(a²) - (i p x)/ħ\n\nFactoring out -1/a² gives:\n\n- [x² + (i p a² x)/ħ ]\n\nCompleting the square inside the bracket:\n\nx² + (i p a² x)/ħ = [x + (i p a²)/(2 ħ)]² - (i² p² a⁴)/(4 ħ² )\n\nWhich is equal to [x + (i p a²)/(2 ħ)]² + (p² a⁴)/(4 ħ² )\n\nBecause i squared is -1, hence subtracting a negative becomes addition.\n\nSo substituting back into the exponent:\n\n- [ [x + (i p a²)/(2 ħ)]² + (p² a⁴)/(4 ħ² ) ] / a² =\n\n- [x + (i p a²)/(2 ħ)]² / a² - (p² a⁴)/(4 ħ² a² )\n\nSimplifying second term:\n\n-p² a²/(4 ħ² )\n\nAh! There we go—I missed the negative sign earlier. So the entire exponent is now:\n\n- [x + c]^2 / a² - p² a²/(4 ħ² )\n\nWhere c = (i p a²)/(2 ħ)\n\nTherefore, the integral becomes:\n\n"}
{"id": "quantum/6021", "questions": "Use Born approximation to find, up to a multiplicative constant, the differential scattering cross section for a particle of mass $m$ moving in a repulsive potential\n\n$$\nV = A e^{-r^2/a^2}.\n$$", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem where I need to use the Born approximation to find the differential scattering cross section for a particle of mass m moving in a repulsive potential V(r) = A e^{-r²/a²}. Hmm, let me think about how to approach this.\n\nFirst off, what's the Born approximation? From my notes, it's an approximation method used in quantum mechanics for solving scattering problems when the potential is weak or the energy is high enough that the perturbation theory applies. It gives us an approximate expression for the scattering amplitude, which we can then square to get the differential cross-section.\n\nThe formula for the first-order Born approximation for the scattering amplitude f(θ) is given by:\n\nf(θ) ≈ (1/(k)) ∫ [e^{i k r cos θ} / 4π] * (-2m/ħ²) V(r') dr'\n\nWait, actually, more precisely, the integral should be over all space with some exponential terms involving vectors... Let me recall: The exact expression involves integrating the product of plane waves and the Fourier transform of the potential. So maybe another way to write it is using the Fourier transform of the potential?\n\nYes! In three dimensions, the scattering amplitude in the Born approximation is proportional to the Fourier transform of the potential evaluated at wavevector q related to the angle θ. Specifically,\n\nf(θ) ≈ -(2m)/( ħ² ) ∫ d³r' e^{i q · r'} V(r'),\n\nwhere q is the momentum transfer vector, |q| = 2k sin(θ/2), but since we're working in units where ħ=1 and c=1 sometimes, perhaps here too they set ħ=1? Wait no, probably not necessarily—maybe better to keep constants explicit.\n\nBut wait, hold on—the standard form might involve factors depending on whether you’re considering non-relativistic particles. Since the question mentions a particle of mass m, likely dealing with non-relativistic QM.\n\nSo, recalling from textbooks like Griffiths or Gasiorowicz, the first Born approximation for the scattering amplitude is:\n\nf(k', k; V) ≈ - (2m)/(ħ²) ∫ e^{i (k' - k)·r} V(r) d³r,\n\nwhere k and k’ are initial and scattered momenta respectively. But because in elastic scattering, |k'| = |k| = k, so their difference relates to the scattering angle via q = k' - k = 2k sin(theta/2) pointing perpendicular direction—I think theta is the angle between k and k'.\n\nAlternatively, if we express everything in spherical coordinates, the integral becomes radial times angular parts. Given our potential depends only on r, i.e., spherically symmetric, the Fourier transform simplifies due to symmetry.\n\nTherefore, instead of computing the full 3D integral, we can switch to polar coordinates aligned along the z-axis (say). Then, the exponentials become functions of cosine angles, leading to integrals involving Bessel functions or something similar.\n\nLet me try writing out the integral explicitly. First, note that the potential is radially symmetric, so V(r) = A exp(-r²/a²).\n\nExpressing the integral as:\n\n∫ d³r e^{i q · r } V(r)\n\nIn spherical coordinates, this becomes:\n\n∫₀^∞ r² dr ∫_{-1}^1 du ∫_0^{2π} dφ e^{i q r u} V(r),\n\nsince the azimuthal integration around φ will just give 2π, and the remaining integral reduces to a function of the dot product, hence dependent on the cosine term 'u'. Therefore, after doing the φ-integration, the integral becomes:\n\n2π ∫₀^∞ r² dr V(r) ∫_{-1}^1 du e^{i q r u}\n\nNow, compute the innermost integral over u:\n\n∫_{-1}^1 e^{i q r u} du = [ (e^{i q r} - e^{-i q r}) ] / (i q r )\n\nWhich simplifies to:\n\n[2 sin(q r)] / (q r )\n\nThus, putting back into the main integral:\n\n2π ∫₀^∞ r² dr V(r) [2 sin(q r) / (q r)]\n\nSimplify this:\n\n(4 π)/q ∫₀^∞ r sin(q r) V(r) dr\n\nSince V(r)=A e^{-r²/a²}, substitute that in:\n\n(4 π A)/q ∫₀^∞ r sin(q r) e^{-r²/a²} dr\n\nHmm, okay, now I need to evaluate this integral:\n\nI = ∫₀^∞ r sin(b r) e^{-c r²} dr,\n\nwith b=q and c=1/a².\n\nIs there a known result for such an integral? Yes, I remember that integrals of the form ∫ x^n sin(p x) e^{-q x²} dx from 0 to infinity can often be expressed in terms of error functions or complementary error functions, especially when n is small.\n\nLooking specifically, for n=1, p=b, q=c>0.\n\nFrom tables or calculus references, the integral ∫₀^∞ x sin(bx) e^{-c x²} dx equals (√π / (8 c^(3/2))) ) [b erf(d) + ...], hmm, maybe I'm misremembering.\n\nWait, alternatively, consider expressing sine in terms of complex exponentials:\n\nsin(br) = (e^{ibr} - e^{-ibr}) / (2i)\n\nThen, plug into the integral:\n\nI = ∫₀^∞ r e^{-cr²} [e^{ibr} - e^{-ibr}] / (2i) dr\n\nFactor out 1/(2i):\n\nI = 1/(2i) [ ∫₀^∞ r e^{-(c - ib)r²} dr - ∫₀^∞ r e^{-(c + ib)r²} dr ]\n\nEach integral is of the form ∫₀^∞ r e^{-α r²} dr, which is straightforward:\n\n∫₀^∞ r e^{-α r²} dr = 1/(2 α ), provided Re(alpha)>0.\n\nHere, alpha would be (c ± ib). However, these alphas may not lie entirely in the right half-plane unless Im(alpha) doesn't cause issues—but since we're taking real part later, perhaps manageable.\n\nCompute each integral:\n\nFirst integral: ∫₀^∞ r e^{-(c - ib) r²} dr = 1/[2(c - ib)]\nSecond integral: ∫₀^∞ r e^{-(c + ib) r²} dr = 1/[2(c + ib)]\n\nSubtract them:\n\n[1/(2(c - ib))] - [1/(2(c + ib))] = [ (c + ib) - (c - ib) ] / [2(c - ib)(c + ib) ]\n= [2ib]/[2(c² + b²)]\n= ib / (c² + b²)\n\nMultiply by 1/(2i):\n\nI = 1/(2i) * [ ib / (c² + b²) ] = (1/(2i))(ib)/(c² + b²)\n= (b)/(2(c² + b²))\n\nBecause (1/i)*i = 1.\n\nHence, the integral evaluates to I = b / [2(c² + b²)]\n\nPlugging back b=q and c=1/a²:\n\nI = q / [2( (1/a²)^2 + q² ) ] ?\n\nWait, hold on—that seems incorrect dimensionally. Because c was defined as 1/a², so c squared is 1/a⁴, while q has dimensions of inverse length. Thus, adding c² and q² isn’t directly compatible—they don’t have same dimensions. That suggests I made a mistake somewhere.\n\nAh, yes, indeed, earlier substitution had c=1/a², correct. So plugging back correctly:\n\nI = q / [2( (1/a²)^2 + q² ) ] → No, wait, original variables were c=1/a², so denominator is c² + b² = (1/a²)^2 + q².\n\nBut that leads to having different powers of a. Maybe I messed up variable definitions.\n\nWait, going back, initially, I said c=1/a², so substituting into I:\n\nI = q / [2( (1/a²)^2 + q² ) ] ? Or did I make a wrong substitution.\n\nHold on, let's re-examine:\n\nWe started with I = ∫₀^∞ r sin(q r) e^{-r²/a²} dr\n\nSet c=1/a², so e^{-c r²}=e^{-r²/a²}\n\nAnd found I = q / [2(c² + q²)]. Is that correct?\n\nNo, wait, according to above steps, I ended up with I = b / [2(c² + b²)], where b=q, c=1/a².\n\nSo yeah, I = q / [2( (1/a²)^2 + q² ) ]\n\nThat’s equal to q / [2(1/a⁴ + q²"}
{"id": "quantum/1-1028", "questions": "Consider a linear harmonic oscillator and let, $\\psi_0$ and $\\psi_1$ be its real, normalized ground and first excited state energy eigenfunctions respectively. Let $A\\psi_0 + B\\psi_1$ with $A$ and $B$ real numbers be the wave function of the oscillator at some instant of time. Show that the average value of $x$ is in general different from zero. What values of $A$ and $B$ maximize $(x)$ and what values minimize it?", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about a quantum harmonic oscillator. The question involves finding when the expectation value of position x is non-zero for a given superposition of states. Hmm.\n\nAlright, let me start by recalling what's given. We're dealing with a linear harmonic oscillator, which has well-known energy eigenstates ψ₀ (ground state) and ψ₁ (first excited state). Both are real and normalized functions. So, any linear combination like Aψ₀ + Bψ₁ should also satisfy certain conditions because they form a basis.\n\nThe wavefunction given is Ψ = Aψ₀ + Bψ₁ where A and B are real numbers. They ask to show that the average value ⟨x⟩ isn't necessarily zero. Wait, but wait—usually, symmetric potentials lead to even or odd parity states. For the harmonic oscillator, ψ₀ is symmetric (even), right? Because the ground state is Gaussian-like, symmetric around x=0. Then ψ₁ must be antisymmetric (odd), since each higher state alternates parity. That makes sense: n=0 is even, n=1 is odd, etc.\n\nSo if we take a linear combination of these two, will their sum preserve symmetry? Well, not unless one coefficient cancels out the other's contribution. But here both coefficients can vary independently as long as normalization holds. However, maybe without considering normalization yet, just looking at the expectation value.\n\nWait, actually, before getting into that, perhaps compute ⟨Ψ|x|Ψ⟩ directly?\n\nBut hold on—the operator x itself is an odd operator. In such cases, matrix elements between even and odd states might come into play. Specifically, the integral ∫ψₘ* x ψₙ dx would only be non-zero when m and n differ by 1, due to the properties of Hermite polynomials involved in the solutions.\n\nIn our case, Ψ is a combination of ψ₀ and ψ₁. Therefore, computing ⟨x⟩:\n\n⟨x⟩ = ⟨Ψ | x | Ψ⟩\n\nExpanding Ψ gives us four terms:\n- A² ⟨ψ₀ | x | ψ₀ ⟩,\n- AB ⟨ψ₀ | x | ψ₁ ⟩,\n- BA ⟨ψ₁ | x | ψ₀ ⟩,\n- B² ⟨ψ₁ | x | ψ₁ ⟩.\n\nNow, evaluating each term individually.\n\nFirstly, ⟨ψ₀ | x | ψ₀ ⟩. Since ψ₀ is even, multiplying by x (which is odd) results in an integrand that's odd. Integrating over all space (-∞ to ∞) would give zero. Similarly, ⟨ψ₁ | x | ψ₁ ⟩: ψ₁ is odd; multiply by x (also odd) becomes even. Integrate over all space—it won’t necessarily be zero, but wait no—if you square an odd function times x... Actually, wait, more carefully: ψ₁(x) is proportional to x e^{-ax²}, so integrating x * [x e^{-2a x²}] dx from -infty to infinity. That’s x squared times exponential, which is positive definite, hence the integral doesn't vanish. Wait, but hang on—isn't x^3 multiplied by e^{-something} integrated over symmetric limits? No, wait, ψ₁ is ~x e^{-ax²}; then x ψ₁ is x*(x e^{-ax²}) )= x² e^{-ax²}. Which is even, yes, but does that mean the integral is non-zero? Yes! It'll result in something finite, specifically related to sqrt(ħ/(mω)) or similar constants depending on how the HO is defined.\n\nHowever, going back, in my initial expansion, the cross terms involve ⟨ψ₀ | x | ψ₁⟩ and vice versa. Now, ψ₀ is even, ψ₁ is odd, so x acting on ψ₁ brings down another factor of x, making it even again. Thus, ψ₀ being even, x ψ₁ is even, so their product is even. Hence, the integral may not be zero. Alternatively, using orthogonality relations—I think there's a standard formula for ⟨ψ_m | x | ψ_n⟩.\n\nI remember that for the harmonic oscillator,\n\n⟨ψ_m | x | ψ_n⟩ = δ_{m,n±1} } * something involving sqrt(n+1)} or sqrt{n}}.\n\nSpecifically, I believe it relates to ladder operators. Recall that x is expressed in terms of creation and annihilation operators: x = sqrt{ ħ / (2mω) }(a + a†).\n\nTherefore, applying x to ψ_n gives sqrt{ ħ / (2mω) }(sqrt{n} ψ_{n−1} + sqrt{n+1} ψ_{n+1} ). \n\nThus, when taking inner products, ⟨ψ_m | x | ψ_n⟩ equals sqrt{ ħ / (2mω) } times (√n δ_{m, n−1} + √(n+1) δ_{m, n+1} ).\n\nApplying this to our specific case where m=0 and n=1:\n\n⟨ψ₀ | x | ψ₁⟩ = sqrt{ ħ / (2mω) } * sqrt{1} δ_{0,0} + ... ?\n\nWait, plugging m=0 and n=1 into the expression above:\n\nIt's sqrt{ ħ / (2mω) } times (sqrt{1} delta_{0,0} ), because m=n−1 implies 0=1−1=0, so delta_{0,0}=1. And the second term would require m = n+1 => 0=1+1=2, which is false. So overall, ⟨ψ₀ | x | ψ₁⟩ = sqrt{ ħ / (2mω) } * sqrt{1} = sqrt{ ħ / (2mω) }.\n\nSimilarly, ⟨ψ₁ | x | ψ₀⟩ would be equal to the same thing, because swapping m and n, but since m=1 and n=0, now m = n+1, so delta_{1,1} comes up, giving sqrt{0}? Wait, no, wait.\n\nHold on, let me re-examine. If m=1 and n=0, then according to the earlier relation, ⟨ψ₁ | x | ψ₀⟩ = sqrt{ ħ / (2mω) } times (sqrt{0} delta_{1,-1} + sqrt{1} delta_{1,1}). But delta_{1,-1}=0, and delta_{1,1}=1, so it's sqrt{1}*delta_{1,1}=1. So indeed, ⟨ψ₁ | x | ψ₀⟩ = sqrt{ ħ / (2mω) }, same as the previous.\n\nHence, putting it together, the expectation value ⟨x⟩ is:\n\nA² * 0 + AB * sqrt{ ħ / (2mω) } + BA * sqrt{ ħ / (2mω) } + B² * C,\n\nwhere C was supposed to be ⟨ψ₁ | x | ψ₁⟩. Earlier thought process said that wasn't zero, but upon reflection, wait—that seems conflicting.\n\nWait, no, actually, ψ₁ is an odd function, so x ψ₁ is even, thus ⟨ψ₁ | x | ψ₁⟩ is the integral of an even function, which is twice the integral from 0 to infinity. But regardless, whether it's zero or not depends on specifics—but wait, actually, for stationary states, the expectation value of x is always zero because the potential is symmetric. Is that correct?\n\nYes, exactly—for any single energy eigenstate, the expectation value of x is zero because those states are either even or odd, leading to symmetric probability distributions. So despite ψ₁ being odd, ⟨x⟩ for ψ₁ alone is still zero. How does that reconcile with the fact that x ψ₁ is even?\n\nAh, okay, because while x ψ₁ is even, the integral of x ψ₁² is zero because ψ₁ is odd. Wait, no, sorry, confusion arises.\n\nLet me clarify: When calculating ⟨ψ₁ | x | ψ₁⟩, it's equivalent to ∫_{-infinity}^{infinity} ψ₁(x)* x ψ₁(x) dx. Since ψ₁ is odd, ψ₁(x)^2 is even, and x is odd. Multiplying them together, x ψ₁(x)^2 is odd. Integral of an odd function over symmetric limits is zero. Ah, got it!\n\nThat means ⟨ψ₁ | x | ψ₁⟩ = 0 too. So correction: All diagonal terms (same state) yield zero. Only off-diagonal terms contribute.\n\nTherefore, returning to the calculation:\n\n⟨x⟩ = A² * 0 + AB * sqrt{"}
{"id": "quantum/6019", "questions": "A nucleon is scattered elastically from a heavy nucleus. The effect of the heavy nucleus can be represented by a fixed potential\n$$\nV(r) = \\begin{cases} \n-V_0, & r < R, \\\\\n0, & r > R, \n\\end{cases}\n$$\n\nwhere $V_0$ is a positive constant. Calculate the deferential cross section to the lowest order in $V_0$.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem where a nucleon is scattering elastically off a heavy nucleus. The potential is given as V(r), which is -V₀ inside radius R and zero outside. They want me to calculate the differential cross-section to the lowest order in V₀.\n\nHmm, let's start by recalling what elastic scattering means. It means that kinetic energy is conserved; no energy loss during the collision. So we're dealing with quantum mechanical scattering here because it involves potentials and wave functions.\n\nThe setup seems like a standard scattering problem. Since the nucleus is heavy, its recoil isn't considered—so we treat it as a fixed target. That makes sense since heavy nuclei don’t move much when light particles like protons or neutrons hit them.\n\nNow, the potential is piecewise: negative inside radius R, zero outside. This looks similar to the square well potential but only attractive (since V₀ is positive). In such cases, for low energies compared to the depth of the potential, you get bound states, but if the particle has enough energy, they just scatter off without being captured.\n\nBut wait, our case is about scattering, not capture. So maybe even though there’s a potential barrier, the incident particle doesn’t get stuck—it scatters away. But how does this affect the cross-section?\n\nI remember that in scattering theory, especially non-relativistic, the Born approximation is often used. To first-order perturbation, the scattering amplitude f(θ) can be approximated using the Fourier transform of the potential. Maybe that applies here? Let me think.\n\nYes, the Born approximation gives us:\n\nf(θ) ≈ -(1/2m)(k') ∫ e^{i(k' - k)·r'} V(r') dr'\n\nWhere m is mass, k and k’ are initial and final momenta vectors, respectively. For elastic scattering, |k| = |k'|, right? Because energy is conserved.\n\nSo simplifying, since E = ħ²k²/(2m) remains same before and after, hence k'=k magnitude-wise. Therefore, k' · something would involve angles between incoming and outgoing waves.\n\nWait, actually, more precisely, in three dimensions, the integral becomes over all space, involving the exponential factor depending on the difference vector (k' - k).\n\nGiven that, perhaps I should express everything in terms of spherical coordinates or use some symmetry arguments.\n\nSince the potential is spherically symmetric (depends only on r), then angular dependence might simplify things. Also, due to rotational symmetry around the direction of incidence, the scattering amplitude depends only on θ—the angle between initial momentum and final momentum.\n\nTherefore, probably switching into partial-wave analysis could help. Each l contributes to different angular dependencies via Legendre polynomials P_l(cosθ).\n\nIn the Born approximation, each partial wave contribution is calculated separately. Specifically, the phase shift δ_l is approximately equal to the term involving the integral of the potential multiplied by certain factors.\n\nFor the first-order approximation, the phase shifts δ_l^(1st order) are proportional to the matrix elements ⟨l|V|l⟩, integrated appropriately.\n\nLet me recall the formula for the phase shift in the first Born approximation:\n\nδ_l ≈ (-1/4π) * ∫_{all space} [j_l(q r)]^* V(r) j_l(q r) r² sinθ dθ dr,\n\nbut q is related to the momentum transfer... Wait, hold on, maybe another approach.\n\nAlternatively, considering plane wave expansion in terms of spherical Bessel functions times Y_l^m. Then, the T-matrix element in the Born approximation is simply the Fourier transform of the potential evaluated at specific points.\n\nActually, yes, the first Born approximation says that the scattering amplitude is:\n\nf(Ω) = (e^{ikz})^{-1} * (-1/(2m)) * ∫ e^{-iqr} V(r) dq,\n\nor something along those lines—I need to make sure.\n\nWait, better yet, the general expression for the scattering amplitude in the first Born approximation is:\n\nf(θ) = (1/k) ∫ e^{i q · r } V(r) dr / (2m),\n\nwith q = 2k sin(θ/2), which comes from the change in momentum.\n\nHold on, let me clarify variables. If the initial wavevector is k, and the final is also k', but in a different direction making an angle θ with respect to the original direction. Thus, the momentum transfer is Δq = k' - k, whose magnitude squared is |Δq|^2 = |k'|² + |k|² - 2kk' cosθ. But since |k|=|k'|=k, this reduces to |Δq|² = 2k²(1 - cosθ) = 4k² sin²(θ/2). Hence, |Δq| = 2k sin(θ/2). \n\nThus, the integrand in the Born approximation will depend on exp(i (Δq) · r ). However, since the potential is radially symmetric, the integral may reduce to radial components.\n\nMoreover, integrating over all directions, but due to the delta function nature of the exponentials, perhaps we can switch to polar coordinates aligned with the z-axis (the direction of the initial momentum). So, choosing coordinate system such that the initial momentum is along z, and the scattering occurs at angle θ relative to z.\n\nHence, the integral becomes:\n\n∫ e^{i (Δq) · r } V(r) d³r = ∫ e^{i qr cosφ} V(r) r² sinφ dφ dr dψ,\n\nwhere φ is the angle between r and the z-axis, ψ is azimuthal angle. Due to circular symmetry, the integration over ψ yields 2π.\n\nThen, the remaining integral is:\n\n2π ∫_{0}^\\infty r² dr ∫_{-1}^1 dz e^{i q r z} V(r) ,\n\nbecause substituting u = cosφ, du = -sinφ dφ, limits become from 1 to -1, but flipping the limits removes the minus sign.\n\nSimplify further:\n\n2π ∫_{0}^\\infty r² V(r) [ ∫_{-1}^1 e^{i q r z} dz ] dr\n\nCompute inner integral:\n\n∫_{-1}^1 e^{i q r z} dz = [ e^{i q r z}/(i q r ) ] from -1 to 1 =\n\n(e^{i q r} - e^{- i q r}) / (i q r )\n\nWhich equals 2i sin(q r)/(i q r ) = 2 sin(q r)/ (q r)\n\nSo overall, the integral becomes:\n\n2π ∫_{0}^\\infty r² V(r) [2 sin(q r)/(q r)] dr\n\nFactor out constants:\n\n4π/q ∫_{0}^\\infty r V(r) sin(q r) dr\n\nPutting back together, the scattering amplitude f(θ):\n\nf(θ) = (1/(2m)) * (1/k) * [4π/(2k)] ∫ ... ?\n\nWait, going back to the exact expression.\n\nFrom earlier steps, the Born approximation for f(θ) is:\n\nf(θ) = (1/(2m)) * (1/k) * Integral[ e^{i (Δq) · r } V(r) d³r ]\n\nWe found that Integral[...] = 4π/(q) ∫_{0}^\\infty r V(r) sin(q r) dr\n\nSubstituting back:\n\nf(θ) = (1/(2m)) * (1/k) * [4π/(q) ∫_{0}^\\infty r V(r) sin(q r) dr]\n\nNote that q = |Δq| = 2k sin(theta/2). So replacing q:\n\nf(theta) = (1/(2m)) * (1/k) * [4 pi / (2k sin(theta/2))] ∫_{0}^\\infty r V(r) sin(2k sin(theta/2) r) dr\n\nSimplify coefficients:\n\nFirst, multiply constants:\n\n(1/(2m))*(1/k)*(4 pi)/(2k sin(theta/2))) = (4 pi)/(8 m k² sin(theta/2))\n\nBecause denominator: 2m*k*(2k sin(...)) => 4 m k² sin(...) in denominator.\n\nNumerator: 4 pi.\n\nSo coefficient is (pi)/(2 m k² sin(theta/2))\n\nAnd the rest is the integral over r of r V(r) sin(2k sin(theta/2) r) dr.\n\nBut now, looking at the potential V(r): it's -V0 within radius R, else zero.\n\nSo split the integral into two parts: from 0 to R, and R to infinity. But beyond R, V(r)=0, so second part vanishes.\n\nThus, integral becomes:\n\nIntegral_{0}^{R} r*(-V0)*sin(2k sin(theta/2) r"}
{"id": "quantum/1-1043", "questions": "A free particle of mass $\\text m$ moves in one dimension. The initial wave function of the particle is $\\psi(x,0)$.\n\n(a) Show that after a sufficiently long time $t$ the wave function of the particle spreads to reach a unique limiting form given by\n\n$$\n\\psi(x,t) = \\sqrt{m/\\hbar t} \\exp \\left( -i\\pi/4 \\right) \\exp \\left( imx^2/2\\hbar t \\right) \\varphi(mx/\\hbar t),\n$$\n\nwhere $\\varphi$ is the Fourier transform of the initial wave function:\n\n$$\n\\varphi(k) = (2\\pi)^{-1/2} \\int \\psi(x, 0) \\exp(-ikx) \\, dx.\n$$\n\n(b) Give a plausible physical interpretation of the limiting value of $|\\psi(x, t)|^2$.\n\nHint: Note that when $\\alpha \\to \\infty$,\n\n$$\n\\exp(-i\\alpha u^2) \\to \\sqrt{\\pi/\\alpha} \\exp(-i\\pi/4) \\delta(u).\n$$", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about quantum mechanics where a free particle's wave function evolves over time. It has two parts, but let me focus on part (a) first because it seems foundational for understanding what happens as time goes on.\n\nThe question says that we need to show that after a very long time \\( t \\), the wave function reaches a specific form involving its Fourier transform. Hmm, okay, I remember from my studies that the evolution of a free particle's wave function can be expressed using the propagator or something related to Gaussian integrals. Maybe I should start with the general expression for the time-evolved wave function.\n\nI recall that the Schrödinger equation for a free particle (\\( V=0 \\)) allows us to write the solution as a superposition of plane waves. So, if the initial wave function is \\( \\psi(x, 0) \\), then the evolved state at time \\( t \\) would involve integrating over all possible momenta or energies? Wait, more precisely, since each momentum eigenstate evolves in time with a phase factor, maybe I can express \\( \\psi(x, t) \\) as an integral over these states.\n\nLet me try writing down the time-dependent solution. Using the Fourier transform approach, which decomposes the initial wave function into plane waves. That makes sense because any square-integrable function can be written as a sum (integral) of exponentials. So, perhaps:\n\n\\( \\psi(x, t) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^\\infty e^{ikx} e^{-i E_k t / \\hbar} \\tilde{\\psi}(k) dk \\)\n\nWhere \\( E_k = k^2/(2m) \\) is the energy corresponding to momentum \\( k \\). And \\( \\tilde{\\psi}(k) \\) is the Fourier transform of \\( \\psi(x, 0) \\).\n\nWait, actually, isn't the Fourier transform defined here as \\( \\varphi(k) = \\frac{1}{\\sqrt{2\\pi}} \\int e^{-ikx} \\psi(x, 0) dx \\)? Yes, exactly. So substituting back, our expression becomes:\n\n\\( \\psi(x, t) = \\int_{-\\infty}^\\infty e^{ikx} e^{-i k^2 t/(2m)} \\varphi(k) dk \\)\n\nHmm, now how do I evaluate this integral? For large times \\( t \\), does some approximation apply?\n\nOh! This looks like a Fresnel integral situation. When dealing with oscillatory integrals, especially those involving quadratic terms in the exponent, stationary phase approximations might come into play. But wait, another thought—maybe instead of directly evaluating the integral, there’s a substitution or scaling that could simplify things.\n\nAlternatively, considering that for large \\( t \\), the spreading of the wave packet occurs such that the dependence on \\( x/t \\) scales appropriately. Let me think about changing variables to make the integral look simpler.\n\nSuppose I set \\( y = kt/m \\cdot t \\)... No, not sure. Alternatively, consider making a substitution inside the exponential. Since the exponent involves both \\( ikx \\) and \\( -i k^2 t/(2m) \\), combining them gives:\n\nExponent: \\( i k x - i k^2 t/(2m) = i [k x - k^2 t/(2m)] \\)\n\nMaybe complete the square in the exponent. Completing the square often helps in Gaussian-like integrals. Let me see:\n\nFactor out \\( i k \\):\n\nBut no, better yet, treat the term inside the brackets as a quadratic in \\( k \\): \n\nQuadratic in \\( k \\): \\( -k^2 t/(2m) + k x \\)\n\nSo, completing the square:\n\n- Coefficient of \\( k^2 \\): \\( -t/(2m) \\)\n- Linear coefficient: \\( x \\)\n\nCompleting the square formula: \\( ak^2 + bk + c = a(k + b/(2a))^2 - b²/(4a) + c \\)\n\nApplying that here:\n\nCoefficient \\( a = -t/(2m) \\), linear term \\( b = x \\). Then,\n\n\\( -k^2 t/(2m) + k x = (-t/(2m))(k^2 - (2mx)/t * k ) \\)\n\nComplete the square inside the parentheses:\n\n\\( k^2 - (2mx)/(t) k = (k - mx/t)^2 - (m^2 x^2)/(t^2) \\)\n\nTherefore,\n\nOriginal expression:\n\\( (-t/(2m))[ (k - mx/t)^2 - m^2 x^2 / t^2 ] \\)\nWhich simplifies to:\n\\( (-t/(2m))(k - mx/t)^2 + (m x^2)/(2t) \\)\n\nThus, putting it back together, the exponent becomes:\n\n\\( i[ (-t/(2m))(k - mx/t)^2 + (m x^2)/(2t) ] \\)\n\nBreaking this apart:\n\n\\( i*(-t/(2m))(k - mx/t)^2 + i*(m x^2)/(2t) \\)\n\nNow, substitute this back into the integral:\n\n\\( \\psi(x, t) = \\int_{-\\infty}^\\infty e^{i*(-t/(2m))(k - mx/t)^2 } e^{i(m x^2)/(2t)} \\varphi(k) dk \\)\n\nNotice that the second exponential doesn’t depend on \\( k \\), so it factors outside the integral:\n\n\\( \\psi(x, t) = e^{i m x^2/(2\\hbar t)} \\int_{-\\infty}^\\infty e^{i*(-t/(2m))(k - mx/t)^2 } \\varphi(k) dk \\)\n\nSimplify constants:\n\nNote that \\( i*(-t/(2m)) = -i t/(2m) \\). Also, note that \\( \\hbar = h/(2\\pi) \\), but in many equations, people use units where \\( \\hbar = 1 \\); however, here they explicitly include \\( \\hbar \\), so probably shouldn't ignore it.\n\nWait, hold on—the original exponent was \\( -i k^2 t/(2m) \\)—so yes, correct, including \\( \\hbar \\)?\n\nNo, wait, earlier steps didn't account for \\( \\hbar \\). Because in the standard expression, the kinetic energy is \\( p^2/(2m) \\), and \\( p = \\hbar k \\). Therefore, \\( E_k = \\hbar^2 k^2/(2m) \\). Thus, the time evolution factor is \\( e^{-i E_k t / \\hbar} = e^{-i \\hbar k^2 t/(2m)} \\).\n\nAh, right—I missed the \\( \\hbar \\) in the previous expressions. So correcting myself:\n\nEarlier, the exponent was supposed to be \\( -i E_k t / \\hbar = -i (\\hbar^2 k^2/(2m))/ \\hbar t = -i \\hbar k^2 t/(2m) \\times t ? Wait, no, sorry, confusion arises.\n\nActually, starting again: Energy \\( E = \\hbar \\omega = \\hbar^2 k^2/(2m) \\). Time evolution operator introduces a phase \\( e^{-i E t / \\hbar} = e^{-i (\\hbar^2 k^2/(2m)) t / \\hbar} = e^{-i \\hbar k^2 t/(2m)} \\). Correct.\n\nHence, going back, the exponent in the integral is:\n\n\\( i k x - i \\hbar k^2 t/(2m) \\)\n\nThen, following similar completion of squares:\n\nExpressed as:\n\n\\( i k x - i \\hbar k^2 t/(2m) = i [k x - \\hbar k^2 t/(2m)] \\)\n\nAgain, treating this as a quadratic in \\( k \\):\n\nCoefficients: Quadratic term \\( -\\hbar t/(2m) \\), linear term \\( x \\).\n\nCompleting the square:\n\n\\( -\\hbar t/(2m)(k^2 - (2"}
{"id": "quantum/5052", "questions": "A quantum mechanical system is described by the Hamiltonian $H = H_0 + H$ where $H = i\\lambda [A, H_0]$ is a perturbation on the unperturbed Hamiltonian $H_0$, $A$ is a Hermitian operator and $\\lambda$ is a real number. Let $B$ be a second Hermitian operator and let $C = i[B, A]$.\n\n(a) You are given the expectation values of the operators $A$, $B$ and $C$ in the unperturbed (and nondegenerate) ground states; call these $\\langle A \\rangle_0, \\langle B \\rangle_0$ and $\\langle C \\rangle_0$. With perturbation switched on, evaluate the expectation value of $B$ in the perturbed ground state to first order in $\\lambda$.\n\n(b) Test this result on the following three-dimensional problem.\n\n$$\nH_0 = \\sum_{i=1}^{3} \\left( \\frac{p_i^2}{2m} + \\frac{1}{2} m\\omega^2 x_i^2 \\right), \\quad H = \\lambda x_3,\n$$\n\nby computing the ground state expectation value $\\langle x_i \\rangle, (i = 1, 2, 3)$ to lowest order in $\\lambda$. Compare this result with an exact evaluation of $\\langle x_i \\rangle$.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this quantum mechanics problem here about perturbations. It's part (a) and then testing it in part (b). Hmm, okay, starting with part (a).\n\nThe setup: The Hamiltonian is H = H₀ + H', but wait, actually they wrote H as H = H₀ + H, which seems confusing because usually we denote the full Hamiltonian as H and the perturbation as V or something else. Wait no, looking again, maybe that was a typo? Because if you look closely, \"where H = iλ[A,H₀]\" – oh! So perhaps there was a mistake in notation. Maybe the total Hamiltonian should be written differently?\n\nWait, hold on. They say H = H₀ + H, but since both terms are called H, that can't be right. Probably, it's supposed to be H_total = H₀ + λV or similar. But according to what's written, H is defined as iλ times the commutator of A and H₀. That must mean that the perturbing term is H' = iλ[A, H₀], making the total Hamiltonian H_total = H₀ + H'. Yeah, that makes more sense.\n\nSo, H_total = H₀ + H', where H' = iλ[A, H₀].\n\nGiven that, we need to find the expectation value of another operator B in the perturbed ground state up to first order in λ.\n\nWe know ⟨A⟩₀, ⟨B⟩₀, and ⟨C⟩₀, where C = i[B,A]. Since all these operators are Hermitian, their expectation values will be real numbers.\n\nIn time-independent perturbation theory, when dealing with degenerate or non-degenerate cases, for non-degenerate systems, the first-order correction to energy levels comes from the expectation value of the perturbing Hamiltonian. Similarly, the expectation value of any observable also gets corrected due to the perturbation.\n\nBut how does one compute the change in ⟨B⟩ under the perturbation?\n\nI remember that in first-order perturbation theory, the expectation value of an operator O in the perturbed state |ψ_n> is approximately:\n\n⟨O⟩ ≈ ⟨ψ_n⁰| O |ψ_n⁰⟩ + ⟨ψ_n⁰| O' |ψ_n⁰⟩,\n\nbut only if the perturbation doesn't cause transitions between different eigenstates. However, sometimes when the perturbation causes mixing, higher-order corrections come into play even in the first order.\n\nAlternatively, using Dyson series or adiabatic perturbation might help, but probably not necessary here.\n\nAnother approach could involve calculating the shift in the expectation value directly via the Hellmann-Feynman theorem or through expansion of the wavefunction.\n\nLet me think. In standard Rayleigh-Schrödinger perturbation theory, the first-order correction to the expectation value of an operator is given by:\n\n⟨O⟩^(1) = ⟨ψ_n⁰| O' |ψ_n⁰⟩ / (E_n⁰ - E_m⁰),\n\nwait, no—that’s for matrix elements contributing to the energy shifts. Actually, the general formula for the expectation value of O after perturbation is:\n\n⟨O⟩ = ⟨ψ_n⁰| O |ψ_n⁰⟩ + Σₘ≠n [⟨ψ_n⁰| O' |ψ_m⁰⟩⟨ψ_m⁰| ψ_n⁰⟩] * (⟨ψ_m⁰| H' |ψ_n⁰⟩)/(E_n⁰ - E_m⁰)) ) ?\n\nHmm, maybe my memory isn’t precise here. Alternatively, considering that the perturbation affects the state itself, leading to a linear response in λ.\n\nPerhaps expanding the state |ψ⟩ as |ψ⟩ = |ψ₀⟩ + λ |ψ₁⟩ + ..., similarly for the energy E = E₀ + λ E₁ + ... Then, plugging into Schrödinger equation:\n\n(H₀ + H')(|ψ₀⟩ + λ |ψ₁⟩) = (E₀ + λ E₁)(|ψ₀⟩ + λ |ψ₁⟩)\n\nExpanding to first order gives two equations:\n\nFirst order:\nH₀ |ψ₀⟩ = E₀ |ψ₀⟩ → already satisfied.\nSecond order:\nH₀ |ψ₁⟩ + H' |ψ₀⟩ = E₀ |ψ₁⟩ + E₁ |ψ₀⟩\n\nTaking inner product with ⟨ψ₀| on both sides:\n\n⟨ψ₀| H₀ |ψ₁⟩ + ⟨ψ₀| H' |ψ₀⟩ = E₀ ⟨ψ₀| ψ₁⟩ + E₁ ⟨ψ₀| ψ₀⟩\n\nSince ⟨ψ₀| ψ₁⟩ is zero (orthonormality?), assuming non-degeneracy, yes. And ⟨ψ₀| ψ₀⟩ is 1. Therefore:\n\n⟨ψ₀| H' |ψ₀⟩ = E₁\n\nWhich tells us that the first-order energy shift is just the expectation value of the perturbation.\n\nNow, going back to our case, we want ⟨B⟩ in the perturbed state. To first order, it would be:\n\n⟨B⟩ = ⟨ψ₀| B |ψ₀⟩ + ⟨ψ₀| B |ψ₁⟩ + ⟨ψ₁| B |ψ₀⟩ + ...\n\nBut since |ψ₁⟩ is small (order λ), cross terms may contribute. Specifically, the first-order correction is:\n\n⟨B⟩^(1) = ⟨ψ₀| B |ψ₁⟩ + ⟨ψ₁| B |ψ₀⟩\n\nBecause the diagonal term ⟨ψ₀| B |ψ₀⟩ is the zeroth order, and the next term involves |ψ₁⟩.\n\nFrom earlier, we had:\n\nH₀ |ψ₁⟩ + H' |ψ₀⟩ = E₀ |ψ₁⟩ + E₁ |ψ₀⟩\n\nRearranged:\n\n(H₀ - E₀)|ψ₁⟩ = -(H' |ψ₀⟩ - E₁ |ψ₀⟩ )\n\nMultiply both sides by ⟨ψ₀|:\n\n⟨ψ₀|(H₀ - E₀)|ψ₁⟩ = -⟨ψ₀| H' |ψ₀⟩ + E₁ ⟨ψ₀| ψ₀⟩\n\nLeft side becomes ⟨ψ₀| H₀ |ψ₁⟩ - E₀ ⟨ψ₀| ψ₁⟩. As before, ⟨ψ₀| H₀ |ψ₁⟩ = E₀ ⟨ψ₀| ψ₁⟩, hence left side is zero minus E₀ times zero, still zero. Right side is -⟨H’⟩₀ + E₁*1. Thus, confirming E₁ = ⟨H’⟩₀.\n\nTherefore, moving forward, to get ⟨B⟩^(1):\n\n⟨B⟩^(1) = ⟨ψ₀| B |ψ₁⟩ + ⟨ψ₁| B |ψ₀⟩\n\nExpressed as twice Re[⟨ψ₀| B |ψ₁⟩ ].\n\nTo express |ψ₁⟩, use the expression above:\n\n(H₀ - E₀)|ψ₁⟩ = -H'|ψ₀⟩ + E₁ |ψ₀⟩\n\nThus,\n\n|ψ₁⟩ = (-1/(H₀ - E₀)) (H'|ψ₀⟩ - E₁ |ψ₀⟩ )\n\nAssuming H₀ has discrete spectrum, and (H₀ - E₀)^{-1} exists except at poles.\n\nThen,\n\n⟨ψ₀| B |ψ₁⟩ = ⟨ψ₀| B [ (-1/(H₀ - E₀))(H' |ψ₀⟩ - E₁ |ψ₀⟩ ) ]\n\nSimplify inside the brackets:\n\n(-1/(H₀ - E₀)) H' |ψ₀⟩ + (E₁/(H₀ - E₀)) |ψ₀⟩\n\nWhen taking ⟨ψ₀| multiplied by this, note that ⟨ψ₀| (something involving (H₀ - E₀)^{-1}) acting on |ψ₀⟩.\n\nHowever, (H₀ - E₀)^{-1} applied to |ψ₀⟩ diverges unless projected onto other states. Hence, likely, we'll expand in terms of basis vectors orthogonal to |ψ₀⟩.\n\nSpecific"}
{"id": "quantum/6043", "questions": "A hydrogen atom (with spinless electron and proton) in its ground state is placed between the plates of a condenser and subjected to a uniform weak electric field\n\n$$\nE = E_0 e^{-\\Gamma t} \\theta(t),\n$$\n\nwhere $\\theta(t)$ is the step function: $\\theta(t) = 0$ for $t < 0$ and $\\theta(t) = 1$ for $t > 0$. Find the first order probability for the atom to be in any of the $n = 2$ states after a long time. Some hydrogenic wave functions in spherical coordinates are\n\n$$\n\\psi_{100} = \\frac{1}{\\sqrt{\\pi a_0^3}} e^{-r/a_0},\n$$\n\n$$\n\\psi_{210} = \\frac{1}{\\sqrt{32 \\pi a_0^3}} e^{-r/2a_0} \\frac{r}{a_0} \\cos \\theta,\n$$\n\n$$\n\\psi_{200} = \\frac{1}{\\sqrt{8 \\pi a_0^3}} \\left(1 - \\frac{r}{2a_0}\\right) e^{-r/2a_0},\n$$\n\n$$\n\\psi_{21\\pm 1} = \\mp \\frac{1}{\\sqrt{64 \\pi a_0^3}} \\frac{r}{a_0} e^{-r/2a_0} \\sin \\theta \\, e^{\\pm i\\phi}.\n$$\n\nA useful integral is $\\int_0^\\infty x^n e^{-ax} dx = \\frac{n!}{a^{n+1}}.$\n", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about a hydrogen atom in its ground state being exposed to a weak electric field that's decaying over time. The goal is to find the first-order probability that the atom transitions into one of the n=2 states after a long time.\n\nFirst off, let me recall what I know about perturbation theory in quantum mechanics because this seems like it involves some sort of transition due to an external perturbing potential. Since the electric field is applied as a perturbation, we can model this using time-dependent perturbation theory.\n\nThe initial state of the hydrogen atom is the ground state, which has quantum numbers n=1, l=0, m=0. So the wavefunction given is ψ₁₀₀. We need to consider all possible transitions from this state to the various n=2 states under the influence of the electric field.\n\nWait, but how does the electric field affect the system? Oh right, the interaction Hamiltonian H' should represent the dipole interaction with the electric field. For a charge q in an electric field E(r,t), the potential energy is V = -q E · r. But since electrons are negatively charged, maybe there’s a negative sign involved here. Let me think... Yes, actually, the perturbing Hamiltonian would typically be something like H’ = -e E(t) · r, where e is the elementary charge.\n\nGiven that the electric field is uniform and along a particular direction—probably taken as z-axis without loss of generality—the vector nature might simplify things if we express everything in terms of spherical harmonics or use selection rules based on angular momentum changes.\n\nSo, more formally, the perturbing Hamiltonian is:\n\nH'(t) = -e E(t) z,\n\nsince the electric field is assumed to point in the z-direction. Here, z is expressed in spherical coordinates as r cosθ. Therefore, H'(t) becomes proportional to r cosθ multiplied by the electric field strength E(t).\n\nNow, looking back at the expression for E(t):\n\nE(t) = E₀ e^{-Γ t} θ(t),\n\nwhich means the electric field starts abruptly at t=0 and then exponentially decays with rate Γ. This makes sense; it's a sudden application followed by a gradual decrease.\n\nIn time-dependent perturbation theory, the transition amplitude from an initial state |i> to a final state |f> up to first order is given by:\n\nc_f^(1)(t) = (-i/ħ) ∫₀ᵗ ⟨f|H’(t')|i⟩ e^{iω_{fi} t'} dt',\n\nwhere ω_{fi} = (E_f - E_i)/ħ is the Bohr frequency corresponding to the energy difference between the two states.\n\nSince we're dealing with probabilities, once we compute c_f^(1)(∞), squaring its magnitude will give us P_f, the transition probability.\n\nBut wait, our initial state is ψ₁₀₀, and the final states are all the n=2 states: ψ₂₀₀, ψ₂₁₀, ψ₂₁±1. However, not all these transitions may contribute equally—or even at all—due to selection rules related to angular momentum.\n\nLet me remember the selection rules for electric dipole transitions. In such cases, the matrix element ⟨ψ_f|r·ε|ψ_i⟩ must be non-zero only when certain conditions hold. Specifically, for each component of the position operator (like r_x, r_y, r_z), the integrals involving spherical harmonics Y_l'^m' and Y_l''^m'' require that Δl = ±1 and Δm = 0, ±1. \n\nLooking at our case, the initial state is l=0, m=0. When considering the perturbation H', which includes r cosθ, which corresponds to the z-component of the position vector. Cosine theta relates to the polar angle, so perhaps this connects to the Lz component?\n\nActually, no—it's better to note that r cosθ is essentially the z-coordinate, which is part of the position vector. Thus, the relevant term in the perturbation is proportional to r cosθ, meaning that the matrix elements involve integrating products of radial parts times angular parts including cosθ.\n\nTherefore, the matrix element ⟨ψ_fnlm| r cosθ |ψ_i100⟩ needs to be evaluated. Given that the initial state is spherically symmetric (s-state, l=0), multiplying by cosθ introduces dependence on angles, potentially allowing transitions to p-states (l=1). Indeed, the n=2 level consists of both s (l=0) and p (l=1) orbitals. Wait, but according to the given wavefunctions, yes—for example, ψ₂₀₀ is still an s-orbital (l=0), while ψ₂₁₀ and ψ₂₁±1 are p-orbitals (l=1).\n\nHowever, the key thing is whether the matrix element is zero or not. Because the initial state is l=0, m=0, and the perturbation contains cosθ—which transforms under rotation as a rank-1 tensor (so changing l by ±1)—the allowed transitions are those where the final state has l'=1, m'=0, ±1. That suggests that among the n=2 states, only the ones with l=1 (p-orbitals) can be reached via this perturbation. Hence, ψ₂₀₀ cannot be excited because it remains an s-state (l=0); whereas ψ₂₁₀ and ψ₂₁±1 could be accessible.\n\nIs that correct? Hmm, let me double-check. If you start with l=0, applying a perturbation proportional to cosθ (which is associated with l=1, m=0) allows transitions to l=1, m=0. Similarly, other components of the dipole moment (x and y directions) correspond to different m values, hence leading to m=±1. So indeed, the final states must satisfy l'=1 and m'=0, ±1.\n\nThus, out of the four provided n=2 states, three are p-orbitals (ψ₂₁₀, ψ₂₁±1) and one is another s-orbital (ψ₂₀₀). Only the p-orbitals can be accessed through this linear Stark effect.\n\nThat simplifies matters—we don't have to calculate the matrix elements for ψ₂₀₀ because they'll vanish. Now, moving forward, we just focus on calculating the amplitudes for transitioning to ψ₂₁₀ and ψ₂₁±1.\n\nNext, let's write down the general form of the transition amplitude for each final state f:\n\nc_f^(1)(t) = (-ie/ħ) ∫₀ᵗ [⟨ψ_f | r cosθ | ψ_i ⟩] E(t') e^{iω_{fi} t'} dt'\n\nWhere ω_{fi} = (E_f - E_i)/(ħ)\n\nWe also know that E(t') = E₀ e^{-Γ t'}, so substituting that in gives:\n\nc_f^(1)(t) = (-ie E₀ / ħ) ⟨ψ_f | r cosθ | ψ_i ⟩ ∫₀ᵗ e^{-Γ t'} e^{iω_{fi} t'} dt'\n\nSimplify the exponential inside the integral:\n\n∫₀ᵗ e^{(-Γ + iω_{fi}) t'} dt' = [1/( -Γ + iω_{fi}) ) ] (1 - e^{(-Γ + iω_{fi}) t})\n\nAs t approaches infinity, assuming Re[-Γ + iω_{fi}] < 0, which requires that Γ > 0, which it is, so the upper limit goes to zero. Then, the integral evaluates to"}
{"id": "quantum/6044", "questions": "A diatomic molecule with equally massive atoms, each with mass $ M $, separated by $ D $ is electrically polarized, rotating about an axis perpendicular to $ D $ and running through the center of mass of the molecule.\n\n(a) Express the energy of the rotational state of the molecule with angular momentum quantum number $ J $ in terms of its mechanical properties.\n\n(b) What is the selection rule for electric dipole radiation emission from the molecule in one of its rotational states? (DERIVE ANSWER)\n\n(c) Determine the frequency of the electric dipole radiation emitted from the rotating molecule as a function of $ J $. (Express answer as a function of $ J, M, D $ and any universal constants that may enter).", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about a diatomic molecule being electrically polarized and rotating. It's split into three parts: finding the energy of a rotational state, determining the selection rule for electric dipole radiation, and figuring out the frequency of the emitted radiation as a function of J. Hmm, let me take it step by step.\n\nStarting with part (a): They want the energy expressed in terms of mechanical properties like mass M, separation distance D, etc. So, since it's a rotation, I remember that rotational energy levels come from quantizing angular momentum. For a rigid rotor model, which seems applicable here because they mention fixed separation D, the energy should be something involving Planck’s constant squared over moment of inertia times some factor related to J(J+1).\n\nWait, right! The formula for rotational energy is E_J = (ħ² / 2I) * J(J + 1), where ħ is h-bar, and I is the moment of inertia. Since it's a diatomic molecule with two masses M each, the moment of inertia would be I = μ r², but wait, no—wait, actually, for a diatomic, if both masses are equal, then the reduced mass μ isn't necessary; instead, we can think of them as point masses on either side of the center of mass.\n\nSo, each atom has mass M, and their separation is D. Therefore, the moment of inertia I is just MD²/4? Wait, hold on—if you imagine the two masses each located at ±D/2 relative to the center of mass, then yes, each contributes (M)(D/2)^2, so total I is 2*(M)*(D²/4)=MD²/2. No, wait—that doesn’t sound quite right. Let me visualize it again.\n\nIf the two atoms are each at positions -D/2 and +D/2 along the x-axis, say, then the moment of inertia around the y or z axes (perpendicular to the bond) would indeed be sum m_i r_i^2. Each mass is M, radius is D/2, so each term is M(D/2)^2. There are two such terms, so I=2*M*(D²)/4=M*D²/2. Yeah, okay, got that now.\n\nTherefore, plugging back into the energy expression:\n\nE_J = [ħ²/(2I)] * J(J + 1)\nBut substituting I:\nE_J = [ħ²/(2*(M D²/2))] * J(J + 1)\nSimplify denominator: 2 cancels numerator 2, so becomes:\nE_J = (ħ²)/(M D²) * J(J + 1)\n\nAlternatively, sometimes people write ħ as h/(2π), but maybe keeping it as ħ is better unless specified otherwise. But perhaps expressing everything in terms of h might make sense depending on what units they expect. However, given that the question says \"in terms of its mechanical properties,\" probably using standard symbols without necessarily expanding ħ further.\n\nAlright, moving on to part (b): Selection rules for electric dipole radiation. From my notes, when dealing with transitions due to electric dipole radiation, the selection rule involves the change in angular momentum ΔJ. Specifically, for a transition via electric dipole, the allowed changes are ΔJ = ±1. That comes from the fact that the photon carries away one unit of angular momentum, hence changing J by one.\n\nAdditionally, there's also consideration of parity, but for pure rotational spectra, especially in homonuclear molecules, certain symmetries apply. Wait, though—the molecule here is diatomic with equally massive atoms, but does that imply anything else?\n\nWell, regardless, the primary selection rule for electric dipole transitions is ΔJ = ±1. Because the matrix elements involve integrals of spherical harmonics multiplied by Y_{l,m} functions corresponding to the photon's angular momentum, which typically couples l=±1. Hence, only transitions between adjacent J values are possible.\n\nIs there more nuance here? Maybe considering whether the molecule is polar or not. Oh, wait—it's stated that the molecule is electrically polarized, meaning it must have a permanent electric dipole moment. Otherwise, even if it were symmetric, it wouldn't emit electric dipole radiation. So yeah, having a non-zero dipole moment allows these transitions.\n\nHence, the selection rule is simply ΔJ = ±1. So, when emitting a photon, the molecule goes down by one level, i.e., J → J-1.\n\nNow onto part (c): Determining the frequency of the emitted radiation as a function of J. Well, during a transition, the energy difference corresponds to the photon's energy. If the molecule drops from J to J-1, the energy released will correspond to the frequency ν via E = hν.\n\nFrom part (a), the energies are E_J = B J(J + 1), where B is the rotational constant defined as ħ²/(2I). Alternatively, written as B = ℏ²/(2I). Plugging our earlier value of I, B = ħ²/(2*(M D²/2)) ) = ħ²/(M D²). So, E_J = (ħ²/M D²) J(J + 1).\n\nThus, the energy difference between J and J-1 is:\n\nΔE = E_J - E_{J-1}\nCompute that:\n\nE_J = (ħ²/M D²)[J(J + 1)]\nE_{J-1}= (ħ²/M D²)[(J-1)J]\nSubtracting gives:\n\nΔE = (ħ²/M D²)[J(J + 1) - (J - 1)J] \nLet me compute inside the brackets:\n\nExpand J(J + 1) = J² + J\n\nAnd (J - 1)J = J² - J\n\nDifference: (J² + J) - (J² - J) = 2J\n\nSo,\n\nΔE = (ħ²/M D²)*2J\n\nThen, equate this to hν:\n\nhν = (2 ħ² J)/(M D²)\n\nWe know that ħ = h/(2π), so substitute:\n\nhν = (2*(h²/(4 π²))*J)/(M D²)\nSimplify numerator:\n\n2*(h²/(4 π²)) = h²/(2 π²)\n\nThus,\n\nhν = (h² J)/(2 π² M D² )\n\nDivide both sides by h:\n\nν = (h J)/(2 π² M D² )\n\nHmm, alternatively, express in terms of other constants. Remember that another way to represent the rotational constant B is often used in spectroscopy, where B = ħ²/(2I) = ħ c / (k_B T_rotational?), but maybe that complicates things.\n\nAlternatively, note that ħ = h/(2π), so replacing ħ in the original equation:\n\nOriginal ΔE was (2 ħ² J)/(M D² ). Substituting ħ = h/(2π):\n\nΔE = (2*(h²/(4 π² ))*J)/(M D² )\nWhich simplifies to same thing as above.\n\nSo, solving for ν:\n\nν = (2 ħ² J)/(h M D² )\n\nBecause starting from ΔE = hν => ν = ΔE/h.\n\nGiven that ΔE = (2 ħ² J)/(M D² ), divide by h:\n\nν = (2 ħ² J)/(h M D² )\n\nSince ħ = h/(2π),\n\nReplace ħ²:\n\n( h²/(4 π² ) )*2 J / (h M D² )\n\nMultiply numerator:\n\n2 h² J / (4 π² h M D² ) = (2 h J)/(4 π² M D² ) = (h J)/(2 π² M D² )\n\nSame result as before.\n\nAlternatively, writing in terms of fundamental constants:\n\nAnother approach could relate to moments of inertia and known expressions for rotational frequencies.\n\nBut perhaps leaving it in terms of h, M, D, and pi is acceptable.\n\nHowever, looking up typical forms, usually rotational transitions' frequencies are proportional to J, specifically ν ~ J/B, where B is the rotational constant. In our case, B = ħ²/(M D² ).\n\nIndeed, from previous steps, we had:\n\nν = (2B J)/h ?\n\nWait, let me check:\n\nEarlier, after computing ΔE, we found:\n\nΔE = (2 ħ² J)/(M D² )\n\nSet equal to hν:\n\nhν = (2 ħ² J)/(M D² )\n\nBut B = ħ²/(M D² ), so plug that in:\n\nhν = 2 B J\n\nThus,\n\nν = (2 B J)/"}
{"id": "quantum/3-3029", "questions": "An $\\Omega^-$ hyperon (spin 3/2, mass 1672 $\\text{MeV}/c^2$, intrinsic parity $+$) can decay via the weak interaction into a $\\Lambda$ hyperon (spin 1/2, mass 1116 $\\text{MeV}/c^2$, intrinsic parity $+$) and a $K^-$ meson (spin 0, mass 494 $\\text{MeV}/c^2$, intrinsic parity $-$), i.e., $\\Omega^- \\rightarrow \\Lambda + K^-$. \n\n(a) What is the most general form of the angular distribution of the $K^-$ mesons relative to the spin direction of the $\\Omega^-$ for the case when the $\\Omega^-$ has a maximum possible component of angular momentum along the a-axis, i.e., the initial state $|\\Omega^m_j\\rangle = |\\Omega^{+3/2}_{3/2}\\rangle$ (Assume that the $\\Omega^-$ is at rest).\n\n(b) What restrictions, if any, would be imposed on the form of the angular distribution if parity were conserved in the decay process?\n", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about the decay of an Omega minus hyperon into a Lambda hyperon and a Kaon negative. The question is divided into two parts: part (a) asks for the most general form of the angular distribution of the K- mesons relative to the spin direction of the Ω⁻ when it's in its highest spin state. Part (b) then considers how parity conservation might restrict that distribution.\n\nFirst, let me recall what I know about particle decays and angular distributions. Angular distributions often come from considering the quantum numbers involved—like spin, parity—and the selection rules based on these. Since this is a weak interaction decay, which doesn't conserve parity, but maybe we'll get back to that later.\n\nStarting with part (a). So, the Ω⁻ is initially at rest and in the state |Ω⁺³⁄₂_{3/2}⟩. That means its total angular momentum j is 3/2, and m_j is also 3/2—the maximum projection along some axis, say z or here called 'a' as per the problem statement.\n\nThe decay products are Λ and K-. Let's note their spins:\n\n- Ω⁻: Spin J=3/2.\n- Λ: Spin S=1/2.\n- K⁻: Pseudoscalar meson, so spin 0.\n\nSo, since the kaon is spin zero, only the Lambda carries spin after the decay. But wait, no—it’s not just the Lambda; both particles contribute to the overall angular momentum because they're moving apart. However, the kaon being spin-zero implies that all the orbital angular momentum must come from the motion between them.\n\nBut hold on, actually, each daughter particle contributes their own spin plus their orbital angular momentum around the center of mass. Wait, but in the rest frame of the parent, the total angular momentum should still add up correctly.\n\nWait, perhaps another approach. In the decay, the initial state has definite angular momentum components. When the Ω⁻ decays, the resulting system (Λ + K-) will carry away angular momentum through their spins and their relative orbital angular momentum.\n\nSince the Ω⁻ was at rest, the total spatial wavefunction includes the orbital angular momentum L between the Λ and K-. Each particle has their own spin, s_λ and s_k, respectively. For the K-, s_k = 0, so the entire contribution comes from the Λ's spin and the orbital angular momentum.\n\nTherefore, the total angular momentum J_initial of the Ω⁻ is equal to the sum of the orbital angular momentum L and the spin of the Λ, right? Because the K- doesn’t contribute anything beyond its movement.\n\nMathematically,\n\nJ_total = L ⊕ S_Λ\n\nWhere S_Λ is the spin of the Lambda (which is 1/2). Therefore, the possible values of J_total depend on the combination of L and S_Λ.\n\nGiven that the initial J is 3/2, the product of adding L and S_Λ must result in a state where one of the possible J's is 3/2. So, depending on whether L is even or odd, etc.\n\nLet me think more carefully. If the initial state has J=3/2, then the combined state of the decay products must match that. So, the possible ways to combine L and S_Λ need to include J=3/2.\n\nS_Λ is fixed at 1/2. Then, combining L and 1/2 gives possible J values ranging from |L - 1/2| to L + 1/2 in integer steps. To reach J=3/2, either L could be such that L + 1/2 ≥ 3/2 and |L - 1/2| ≤ 3/2.\n\nLooking for minimal cases, suppose L=1. Then, combining L=1 and S=1/2 gives possible J=3/2 and J=1/2. Perfect! So, L=1 allows us to have J=3/2 states. Similarly, higher L may allow other possibilities, but given that the decay involves emission of a pion-like particle (though here it's a strange quark related), probably lower L transitions are favored due to phase space considerations, though I'm not entirely sure.\n\nAlternatively, sometimes in beta decays, you see dipole radiation leading to l=1 transitions. Maybe similar logic applies here?\n\nIn any case, assuming that the dominant contribution is from L=1, which couples with S_Λ=1/2 to give J=3/2 and J=1/2. Thus, our decay amplitude depends on the Clebsch-Gordan coefficients coupling L=1 and S=1/2 to make J=3/2.\n\nNow, the angular distribution typically arises from the square of the spherical harmonics associated with the orbital angular momentum. Specifically, the probability density goes like |Y_LM(θ, φ)|², summed over M appropriately.\n\nHowever, since the initial state has maximal polarization (m_j=3/2), the transition amplitudes will involve matrix elements connecting this state to various combinations of L and S_Λ.\n\nI remember that in such processes, especially involving vector boson emissions (or pseudovector, in this case), the angular dependence tends to follow certain patterns. For example, in Fermi's Golden Rule treatment, the transition rate depends on the square of the matrix element, which itself relates to the Wigner d-functions or spherical harmonics.\n\nMoreover, since the initial state is polarized maximally, the decay products’ angular distribution will reflect this alignment. Hence, the angular distribution function will likely involve terms proportional to cos(theta)^something, possibly squared or raised to powers corresponding to the multipolarity.\n\nIf L=1, then Y₁^m(θ,φ) corresponds to electric dipole type radiation, whose magnitude squared leads to an angular distribution dependent on sin²(theta) times something else. Alternatively, for magnetic dipole moments, there's a different pattern.\n\nWait, but in this context, it's not exactly electromagnetic radiation, but rather a strong or weak interaction decay. Hmm. Well, regardless, the underlying mathematics regarding angular momentum addition remains the same.\n\nAnother thought: the decay amplitude A is determined by the overlap between the initial state and the final state. Given that the initial state is |j,m> = |3/2, 3/2>, and the final state consists of the Lambda and K- with specific angular momenta.\n\nExpressed mathematically, the amplitude is proportional to <f | O | i >, where O represents the operator mediating the decay—in this case, the weak interaction Hamiltonian.\n\nFor the weak interaction, which violates parity, but in this particular decay, does it matter? Not directly yet—we’re focusing first on the structure without worrying about parity constraints.\n\nTo find the angular distribution, we'd compute the modulus squared of the amplitude, integrated over solid angle except theta and phi, giving the differential cross-section or decay rate as a function of angles.\n\nSpecifically, the angular distribution usually takes the form:\n\ndΓ/dΩ ∝ [some polynomial in cosθ]\n\nwhere θ is the angle between the direction of the emitted particle (here, K−) and the quantization axis (the spin direction of the Ω⁻).\n\nWhen dealing with spin-3/2 → spin-1/2 + spin-0, and considering orbital angular momentum L=1, the angular distribution ends up having terms up to cos^3θ, perhaps.\n\nWait, why? Because the Legendre polynomials for L=1 go up to linear in cosine, but when coupled with spin factors, maybe higher terms emerge.\n\nHold on, perhaps using the fact that the decay amplitude is built from the tensor operators transforming under rotation group. The initial state is |3/2, 3/2>. After decay, the final state combines the orbital angular momentum L=1 and the spin of the lambda S=1/2.\n\nThus, the transformation properties require that the amplitude picks out a term that connects |3/2, 3/2> to |LM_S>.\n\nThis connection happens via the Clebsch-Gordan coefficient CG(L,M; S,S_m | J,j_M'), where J=L⊕S.\n\nBut specifically, since the initial state is |3/2, 3/2>, the final state needs to couple L and S_Λ to"}
{"id": "quantum/7010", "questions": "Two identical nonrelativistic fermions of mass $m$, spin $1/2$ are in a one-dimensional square well of length $L$, with $V$ infinitely large and repulsive outside the well. The fermions are subject to a repulsive inter-particle potential $V(x_1 - x_2)$, which may be treated as a perturbation. Classify the three lowest-energy states in terms of the states of the individual particles and state the spin of each. Calculate (in first-order perturbation theory) the energies of second- and third-lowest states; leave your result in the form of an integral. Neglect spin-dependent forces throughout.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about two identical non-relativistic fermions in a 1D infinite square well. They're both spin-½ particles, right? And there's a repulsive interaction between them that we can treat as a perturbation. My task is to classify the three lowest-energy states based on their individual particle states and determine the spin for each. Then, using first-order perturbation theory, calculate the energies of the second and third lowest states.\n\nFirst off, since these are fermions, they must obey the Pauli exclusion principle. That means no two fermions can occupy the same quantum state simultaneously. In a single-particle system without interactions, the energy levels would just depend on their principal quantum numbers n₁ and n₂ where n₁ ≠ n₂ because of the exclusion principle.\n\nBut here, we also have a repulsive interparticle potential V(x₁ - x₂). Since it's repulsive, when the particles get closer together, the potential increases, making the total energy higher. So even though the kinetic part might suggest certain configurations, the potential will add some correction depending on how much overlap there is between the spatial wavefunctions.\n\nIn the unperturbed case (without considering the interaction), the ground state should be the lowest possible configuration allowed by the Pauli principle. For two fermions, the lowest energy occurs when they fill up the lowest available orbitals but with opposite spins. Wait, actually, hold on—since they’re spin-½ particles, each has a spin-up or spin-down state. But wait, in a 1D box, do we consider orbital angular momentum? No, because it's 1D, so all motion is along one axis, hence no orbital angular momentum. Therefore, the only source of degeneracy comes from spin.\n\nSo, if we ignore spin-orbit coupling—which isn't present anyway—the spin doesn’t affect the spatial wavefunction directly except through exchange symmetry. Hmm, maybe not exactly. Let me think again: in a 1D box, each particle’s state is characterized solely by its quantum number n and spin s = ±½. However, since the particles are indistinguishable fermions, the overall wavefunction must be antisymmetric under exchange.\n\nTherefore, the combined wavefunction Ψ(r₁, r₂) must satisfy:\n\nΨ(r₁, r₂) = -Ψ(r₂, r₁)\n\nWhich implies that either the spatial part is symmetric and the spin part is anti-symmetric, or vice versa.\n\nWait, yes! Because swapping coordinates swaps the spatial part and swapping spins swaps the spin part. To make sure the product remains antisymmetric, you need one symmetric and one antisymmetric component.\n\nRight, so let's break down into cases.\n\nCase 1: Spatially symmetric, Spin anti-symmetric\n\nThe singlet spin state is anti-symmetric (S=0, m_s=0): \n\n( |↑↓⟩ - |↓↑⟩ ) / √2\n\nAnd the spatial wavefunction needs to be symmetric under exchange. How does that happen?\n\nFor two particles, the spatial wavefunction ψ_symmetric(n₁,n₂) is given by [ψ_n₁(x₁)ψ_n₂(x₂) + ψ_n₂(x₁)ψ_n₁(x₂)] / √2\n\nSimilarly, Case 2: Spatially anti-symmetric, Spin symmetric\n\nTriplet spin states (S=1):\n\n|↑↑⟩,\n\n(|↑↓⟩ + |↓↑⟩)/√2,\n\nand |↓↓⟩\n\nEach triplet state is symmetric under exchange, so the corresponding spatial wavefunction must be anti-symmetric:\n\n[ψ_n₁(x₁)ψ_n₂(x₂) - ψ_n₂(x₁)ψ_n₁(x₂)] / √2\n\nNow, going back to the original setup: Two fermions in a 1D box. Without any interaction, what are the possible states?\n\nSince the particles don't interact, the Hamiltonian is H₀ = T₁ + T₂, where T_i is the kinetic energy operator for each particle. Each particle occupies a stationary state ψ_n(x).\n\nBecause of the Pauli exclusion principle, the two particles cannot share the same set of quantum numbers including spin. Hence, if they have different spatial states, they could potentially have the same spin—or perhaps not necessarily? Wait, no—if they have the same spatial state, then due to being fermions, their spin state must be anti-symmetric. If they have different spatial states, their spin can be symmetric or anti-symmetric.\n\nWait, more accurately, whether the spatial part is symmetric or anti-symmetric depends on whether the quantum numbers are swapped. Suppose n₁ < n₂. Then, the spatial wavefunction is symmetric if n₁ = n₂, but otherwise, if n₁ ≠ n₂, the spatial wavefunction can be made symmetric or anti-symmetric via symmetrization.\n\nHowever, in our case, since the particles are distinguishable only by their positions and spins, the key point is that the combination of space and spin must lead to an overall anti-symmetric wavefunction.\n\nLet me try listing out the possible combinations.\n\nWithout the interaction, the energy eigenstates are determined purely by the sum of the individual energies E_n = (n²π²ħ²)/(2mL²). So, the total energy is E_total = E_{n₁} + E_{n₂}\n\nTo find the lowest energy states, we look for the smallest sums of E_n while respecting the Pauli exclusion principle.\n\nGiven that, the ground state would consist of the two particles filling the lowest two distinct spatial states with opposite spins. Alternatively, if they can have the same spatial state but opposite spins—that was my initial thought—but wait, in 3D systems, electrons can have the same spatial state but opposite spins, but in 1D, is that still applicable?\n\nHold on—in 1D, the concept applies similarly. Fermions must have an overall anti-symmetric wavefunction. So, if two fermions have the same spatial wavefunction, say ψ_n(x₁)=ψ_m(x₂) with n=m, then the spatial part is symmetric upon exchange, meaning the spin part must be anti-symmetric. Conversely, if the spatial parts are different, i.e., n≠m, then the spatial wavefunction can be constructed to be symmetric or anti-symmetric, allowing the spin part to adjust accordingly.\n\nThus, the minimal energy state would involve having the two particles in the lowest possible spatial states, possibly with parallel spins if the spatial part allows it.\n\nWait, now I'm getting confused. Maybe better approach: list the possible states ordered by increasing energy.\n\nSingle-particle states are labeled by n=1,2,3,... with energies E_n proportional to n².\n\nWhen combining two fermions, the possible states are built from pairs (n₁, n₂) with n₁ ≤ n₂, and accounting for spin.\n\nIf n₁ = n₂, then the spatial wavefunction is symmetric, requiring the spin to be anti-symmetric (singlet state). This gives us a valid state.\n\nAlternatively, if n₁ < n₂, then the spatial wavefunction can be symmetric or anti-symmetric, leading to different possibilities for spin.\n\nBut in reality, for two particles, regardless of whether n₁ equals n₂ or not, the overall wavefunction must be anti-symmetric. Thus, when constructing composite states, we take linear combinations such that the entire thing flips sign upon exchange.\n\nHence, for n₁ ≠ n₂, the spatial wavefunction can be symmetric or anti-symmetric, paired with correspondingly anti-symmetric or symmetric spin states.\n\nThis leads to multiple possible states for each pair (n₁, n₂) with n₁ < n₂.\n\nSpecifically, for each pair (n₁, n₂), n₁ < n₂, we can construct two types of states:\n\n1. Symmetric spatial * Anti-symmetric spin → Total anti-symmetric.\n2. Anti-symmetric spatial * Symmetric spin → Also total anti-symmetric.\n\nThese give rise to two separate states for each (n₁, n₂) pair.\n\nAdditionally, when n₁ = n₂, the spatial wavefunction is forced to be symmetric, thus the spin must be anti-symmetric, giving another type of state.\n\nSo, putting this together, the possible states are:\n\nA. Both particles in the same spatial state (n₁=n₂), with singlet spin (anti-symmetric).\nB. Particles in different spatial states (n₁ < n₂), with either:\n   B1. Symmetric spatial & Singlet spin,\n   B2. Anti-symmetric spatial & Triplet spin.\n\nWait, but hang on—is that correct? Or rather, for each (n₁, n₂) with n₁ < n₂, we have four possible states? No, probably not—it's more like for each (n₁, n₂), n₁ < n₂, we have two possible states—one with symmetric spatial and anti-spin, and another with anti-spatial and"}
{"id": "quantum/7021", "questions": "What is the energy difference in $\\text{eV}$ between the two lowest rotational levels of the $\\text {HD}$ molecule? The  $\\text {HD}$  ( $\\text {D}$  is a deuteron) distance is 0.75 Å.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about finding the energy difference between the two lowest rotational levels of the HD molecule. Hmm, let me think through how to approach this.\n\nFirst off, I remember that molecules can rotate, and their rotational energy levels are quantized. For diatomic molecules like HD, these energy levels depend on the moment of inertia of the molecule. So, maybe I need to calculate the moment of inertia first?\n\nRight, the formula for the moment of inertia \\(I\\) of a diatomic molecule should be something like \\(I = \\mu r^2\\), where \\(\\mu\\) is the reduced mass and \\(r\\) is the bond length. Yeah, that sounds familiar from my studies.\n\nSo, what's the reduced mass here? Since it's HD, one atom is hydrogen (\\(H\\)) and the other is deuterium (\\(D\\)). Deutrium has twice the mass of regular hydrogen because its nucleus contains both a proton and a neutron instead of just a proton. Wait, actually, no—deuterium has approximately double the mass of protium (which is standard hydrogen). Let me confirm: Hydrogen-1 (protium) has a molar mass around 1 g/mol, while deuterium is about 2 g/mol. But when dealing with individual atoms or nuclei, we usually use atomic mass units (u).\n\nThe mass of a proton is roughly 938 MeV/c², but wait—that might not directly help here since we're working with eV later. Maybe better to express everything in terms of kilograms using known constants.\n\nWait, perhaps another way: Using Planck's constant and knowing the masses will allow us to compute the necessary quantities. Alternatively, considering that the reduced mass μ is given by:\n\n\\[\n\\mu = \\frac{m_H \\cdot m_D}{m_H + m_D}\n\\]\n\nWhere \\(m_H\\) is the mass of hydrogen and \\(m_D\\) is the mass of deuterium. Given that deuterium is almost exactly twice as heavy as hydrogen, right? Because deuterium has a mass number A=2, whereas hydrogen typically refers to protium which has A=1.\n\nBut hold on—isn't the actual mass ratio more precise than just doubling? Well, technically, the mass of deuterium includes the binding energy, but compared to the mass of hydrogen, it’s very close to being double. However, if precision matters, maybe I should look up exact values.\n\nAlternatively, perhaps approximate them. Let me recall some basic particle masses. The mass of a proton is about 1.67 × 10^-27 kg, and the mass of a neutron is slightly higher, around 1.675 × 10^-27 kg. Therefore, the mass of deuterium would be approximately the sum of a proton and a neutron, so roughly 3.34 × 10^-27 kg. Whereas the mass of hydrogen (assuming it's mostly protons) is ~1.67 × 10^-27 kg.\n\nTherefore, plugging into the reduced mass equation:\n\nμ = (m_H * m_D)/(m_H + m_D)\n\nPlugging numbers:\n\nm_H ≈ 1.67×10⁻²⁷ kg\n\nm_D ≈ 3.34×10⁻²⁷ kg\n\nThus,\n\nμ = (1.67e-27 * 3.34e-27) / (1.67e-27 + 3.34e-27)\nLet me compute numerator and denominator separately.\n\nNumerator:\n1.67e-27 * 3.34e-27 = approximates to (approx 1.67*3.34)=~5.58; then exponentials add: 10^{-27} * 10^{-27}=10^{-54}, so total numerator≈5.58e-54 kg²\n\nDenominator:\n1.67e-27 + 3.34e-27 = (1.67+3.34)e-27 = 5.01e-27 kg\n\nThen, μ = 5.58e-54 / 5.01e-27 ≈ (5.58/5.01)*1e-27 ≈ 1.114e-27 kg\n\nHmm, okay, so μ ≈ 1.114 × 10^-27 kg.\n\nNow, moving on—the bond length r is given as 0.75 Ångströms. Converting that to meters: 1 Å = 1e-10 m, so 0.75 Å = 0.75e-10 m = 7.5e-11 m.\n\nAlright, now computing the moment of inertia I:\n\nI = μ r²\n\nCompute r squared: (7.5e-11)^2 = 56.25e-22 m² = 5.625e-21 m²\n\nMultiply by μ: 1.114e-27 kg * 5.625e-21 m² =\n\nMultiplying coefficients: 1.114 * 5.625 ≈ 6.266...\n\nExponents: 10^{-27 -21} = 10^{-48}\n\nSo, I ≈ 6.266e-48 kg·m²\n\nThat seems reasonable—I've seen moments of inertia on similar scales before.\n\nNext, the rotational energy levels for a rigid rotor are given by:\n\nE_J = (ħ² J(J+1))/(2I)\n\nwhere ħ is the reduced Planck's constant, J is the rotational quantum number (J=0,1,2,...), and I is the moment of inertia.\n\nWe’re asked for the energy difference between the two lowest rotational levels. That means E_1 - E_0.\n\nCalculating each:\n\nFor J=0: E₀ = (ħ² * 0*(0+1))/(2I) ) = 0\n\nFor J=1: E₁ = (ħ² * 1*(1+1))/(2I) ) = (2ħ²)/(2I) = ħ²/I\n\nSo, ΔE = E₁ - E₀ = ħ²/(I)\n\nGot it. Now, substituting the value of I we found earlier.\n\nBut wait, do I know the numerical value of ħ? Yes, ħ is h-bar, equal to h/(2π). And h is Planck's constant, approximately 6.62607015e-34 Js.\n\nSo, ħ = h/(2π) ≈ 6.626e-34 / 6.283 ≈ 1.0545718e-34 Js\n\nYes, correct. So, ħ ≈ 1.0545718e-34 J·s\n\nSo, calculating ħ²:\n\n(1.0545718e-34)^2 ≈ ?\n\nWell, squaring 1.0545718 gives approximately 1.112, and exponents square too: (1e-34)^2 = 1e-68\n\nSo, ħ² ≈ 1.112e-68 J²·s²\n\nDivide that by I (~6.266e-48 kg·m²):\n\nΔE = ħ² / I ≈ (1.112e-68) / (6.266e-48) ≈ (1.112 / 6.266) * 1e-20\n\nCalculate 1.112 divided by 6.266: Approximately 0.1775\n\nSo, ΔE ≈ 0.1775e-20 Joules per photon transition.\n\nConvert that to electron volts (eV). Remembering that 1 eV = 1.602176634e-19 J.\n\nSo, converting Joules to eV involves dividing by 1.602e-19.\n\nThus,\n\nΔE_eV = (0.1775e-20 J) / (1.602e-19 J/eV) )\n\nSimplify:\n\n0.1775e-20 / 1.602e-19 = (0.1775 / 1.602) * (1e-20 / 1e-19) = (approximately 0.1108) * (0.1) = 0.01108 eV\n\nWait, let me check that calculation again.\n\nHold on, 0.1775e-20 divided by 1.602e-1"}
{"id": "quantum/7026", "questions": "(a) Suppose you have solved the Schrödinger equation for the singly-ionized helium atom and found a set of eigenfunctions $\\phi_N(\\mathbf{r})$.  \n\n(1) How do the $\\phi_N(\\mathbf{r})$ compare with the hydrogen atom wave functions?  \n\n(2) If we include a spin part $\\sigma^+$ (or $\\sigma^-$) for spin up (or spin down), how do you combine the $\\phi$'s and $\\sigma$ to form an eigenfunction of definite spin?\n\n(b) Now consider the helium atom to have two electrons, but ignore the electromagnetic interactions between them.\n\n(1) Write down a typical two-electron wave function, in terms of the $\\phi$'s and $\\sigma$'s, of definite spin. Do not choose the ground state.  \n\n(2) What is the total spin in your example?\n\n(3) Demonstrate that your example is consistent with the Pauli exclusion principle.\n\n(4) Demonstrate that your example is antisymmetric with respect to electron exchange.", "graphs": null, "llm_answers": "<think>\nOkay, so I've got this problem about quantum mechanics involving helium atoms. Let me try to break it down piece by piece because some parts seem tricky, especially since I'm still getting comfortable with these concepts.\n\nStarting with part (a). It says that someone has solved the Schrödinger equation for singly-ionized helium and found eigenfunctions φ_N(r). \n\nPart (a)(1): Comparing these φ's with hydrogen atom wave functions. Hmm, well, ionized helium would be He⁺, which means it only has one electron left after losing another. So effectively, it should behave similarly to a hydrogen atom because both have just one electron orbiting a nucleus. But wait, the charge on the nucleus matters too. Hydrogen has Z=1, while He⁺ has Z=2. Oh right! The nuclear charge affects the potential in the Schrödinger equation. For hydrogen-like atoms, the energy levels depend inversely on the square of the principal quantum number N and directly proportional to Z². So if Z increases from 1 to 2, all the energy levels become more negative—meaning they're lower than those of hydrogen. Also, the spatial extent or size of the orbitals would shrink because higher Z pulls the electron closer. Therefore, the eigenfunctions φ_N(r) for He⁺ will look similar to hydrogen’s ψ_nlm(r) but scaled due to the different Z value. They'll satisfy the same radial equations but adjusted for Z=2 instead of Z=1.\n\nMoving on to part (a)(2): Including a spin part σ+ or σ- for spin-up or spin-down. To get an eigenfunction of definite spin, I think we need to multiply the spatial part by the spin part. Since each electron can either be spin-up or spin-down independently, combining them as tensor products makes sense. Wait, no—if we want a single particle state with definite spin, then yes, it's simply the product of the spatial wave function and the spin state. So Φ = φ_N(r) ⊗ σ±. That way, when you measure the spin, you know exactly what you’ll get—it won't be a mixture.\n\nNow onto part (b). Here, we’re considering neutral helium with two electrons, ignoring their electromagnetic interactions. Interesting, so treating them non-interacting except for the Pauli exclusion principle.\n\nPart (b)(1): Writing a typical two-electron wave function with definite spin. Without interaction, maybe each electron occupies its own orbital. And since spins matter here, let's see... A general approach might involve Slater determinants, but without interactions, perhaps simpler. Each electron could be described separately: say, φ_1σ+ for one and φ_2σ- for the other. Then, combining them into a product state. However, wait—the question specifies \"definite spin.\" So does that mean the overall spin is defined? Or each individual spin is definite? Probably the latter; meaning each electron has a specified spin direction. Alternatively, if combined symmetrically or anti-symmetrically, but given that we don’t specify whether it's singlet or triplet, maybe just writing separate states.\n\nWait, actually, for two particles, having definite spin usually refers to the total spin being quantified. But the question doesn't explicitly ask for total spin yet—that comes later in part (b)(2). Maybe for now, just write a wave function where each electron has a specific spin. Like, first electron in φ_a(r₁) with spin σ+, second in φ_b(r₂) with spin σ-. So something like [φ_a(r₁)⊗σ+] × [φ_b(r₂)⊗σ-], but ensuring proper antisymmetry under exchange.\n\nBut hold on, the user said to not use the ground state. Ground state would probably be both electrons in the lowest orbital with opposite spins. Instead, pick excited states. Perhaps one electron in n=1 level and another in n=2, or even same n but different l or m. But also, including spin considerations.\n\nAlternatively, using a Slater determinant structure automatically ensures antisymmetry. But since we're told to ignore interactions, maybe it simplifies things. Still, regardless, the wave function must account for indistinguishability via antisymmetry.\n\nSo putting together, suppose Electron 1 is in φ_{N}(r₁) with spin up, and Electron 2 is in φ_{M}(r₂) with spin down. Thus, the total wave function would be:\n\nΨ(r₁,r₂) = φ_N(r₁) * σ+(s₁) * φ_M(r₂) * σ-(s₂)\n\nBut wait, isn't that assuming distinguishable particles? No, because electrons are identical fermions, so we need to construct an antisymmetric combination. Hence, better expressed as a Slater determinant:\n\n|φ_N(r₁)σ+   φ_M(r₂)σ-|\n|φ_N(r₁)σ-   φ_M(r₂)σ+|\n\nWhich gives us Ψ = φ_N(r₁)φ_M(r₂)[σ+⊗σ- - σ-⊗σ+]\n\nOr simplified,\n\nΨ = φ_N(r₁)φ_M(r₂)( |↑↓⟩ - |↓↑⟩ )\n\nThis ensures antisymmetry upon swapping r₁ ↔ r₂ and s₁ ↔ s₂.\n\nHowever, the exact expression depends on notation conventions. Another way is to take the antisymmetrized product:\n\nΨ = (1/√2)[φ_N(r₁)σ+(s₁)φ_M(r₂)σ-(s₂) - φ_N(r₂)σ-(s₂)φ_M(r₁)σ+(s₁)]\n\nYes, that seems correct. This combines the spatial parts with the spin parts appropriately.\n\nFor part (b)(2): Total spin in my example. Looking back, our wave function was constructed as |↑↓⟩ - |↓↑⟩ divided by √2, which corresponds to the singlet state. Because the singlet state has S=0, hence total spin zero. Alternatively, had we taken a symmetric combination, it'd be a triplet state with S=1. In our case, though, the antisymmetrization leads to the singlet, so total spin is zero.\n\nThen part (b)(3): Consistency with Pauli exclusion. Well, according to the Pauli principle, no two fermions can occupy the same quantum state simultaneously. Our wave function uses distinct spatial orbitals φ_N and φ_M, unless N=M. Wait, did I assume N≠M earlier? Actually, in constructing the wave function above, I used φ_N and φ_M, possibly allowing N=M. But if N=M, then the spatial parts aren't orthogonal, leading to issues. Wait, no—in reality, if N=M, then the entire wave function becomes zero because exchanging the particles swaps everything, making Ψ=-Ψ ⇒ Ψ=0. Which aligns with Pauli—for identical particles, if all quantum numbers match, the wave function vanishes. So in our construction, if N=M, the wave function indeed enforces orthogonality through antisymmetry, thus respecting Pauli.\n\nLastly, part (b)(4): Antisymmetry under exchange. We already built the wave function as antisymmetric. When swapping r₁↔r₂ and s₁↔s₂, each term flips sign. Specifically, applying P (exchange operator):\n\nPΨ = (1/√2)[φ_N(r₂)σ(s₂)φ_M(r₁)σ'(s₁) - φ_N(r₁)σ(s₁)φ_M(r₂)σ'(s₂)] \n     = -(1/√2)[φ_N(r₁)σ(s₁)φ_M(r₂)σ'(s₂) - φ_N(r₂)σ(s₂)φ_M(r₁)σ'(s₁)]\n     = -Ψ\n\nHence, Ψ → -Ψ under exchange, confirming antisymmetry.\n</think>\n\n**Answer**\n\n**(a)**  \n(1) The eigenfunctions $\\phi_N(\\mathbf{r})$ for the singly-ionized helium atom resemble those of the hydrogen atom but differ due to the increased nuclear charge $Z=2$. Their energies scale as $(Z/\\alpha)^{-2}$ compared to hydrogen ($Z=1$), resulting in smaller radii and deeper binding energies.  \n\n(2) An eigenfunction of definite spin is formed by multiplying the spatial wave function $\\phi_N(\\mathbf{r})$ with a spin component $\\sigma^\\pm$, yielding $\\Phi(\\mathbf{r},\\mathbf{s}) = \\"}
{"id": "quantum/4001", "questions": "We may generalize the semi-classical Bohr-Sommerfeld relation\n\n$$\n\\oint P \\cdot dr = (n + 1/2) \\, h,\n$$\n\n(where the integral is along a closed orbit) to apply to the case where electromagnetic field is present by replacing $P$ with $p - eA/c$, where $e$ is the charge of the particle. Use this and the equation of motion for the linear momentum $p$ to derive a quantization condition on the magnetic flux of a semi-classical electron which is in a magnetic field $B$ in an arbitrary orbit. For electrons in solids this condition can be restated in terms of the size $S$ of the orbit in k-space. Obtain the quantization condition on $S$ in terms of $B$ (Ignore spin effects).", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about generalizing the Bohr-Sommerfeld relation when there's an electromagnetic field involved. Hmm, let me try to break it down.\n\nFirst off, the original Bohr-Sommerfeld relation is given as:\n\n∮ P · dr = (n + 1/2) h,\n\nwhere n is some quantum number, right? And this integral is taken over a closed classical orbit. Now, they want us to modify this because we're dealing with electromagnetism—so instead of just using P, we replace it with p minus something involving the vector potential A. Specifically, they say to use P → p − eA/c. So that makes sense since in electrodynamics, the canonical momentum includes both the mechanical momentum and the electromagnetic contribution from the vector potential.\n\nAlright, so substituting that into the integral gives:\n\n∮ (p − eA/c) · dr = (n + 1/2) h.\n\nNow, what do I know about equations of motion here? The question mentions the equation of motion for linear momentum p. In Hamiltonian mechanics, the time derivative of p would involve forces or perhaps more generally, the force due to electric fields if any. But wait, maybe without external electric fields, assuming only magnetic fields?\n\nWait, but actually, even if E isn't zero, how does that affect things? Let me think. If there's no explicit dependence on velocity except through the Lorentz force, then dp/dt might relate to F = q(E + v × B), but in our substitution above, we've already accounted for the vector potential via minimal coupling. Maybe I don't need to worry too much about the specific form unless told otherwise.\n\nBut hold on, another thought: Since we’re considering orbits under a magnetic field, perhaps the system has certain symmetries or conserved quantities. Also, remember that in such cases, the concept of guiding centers comes up, but not sure if that’s relevant yet.\n\nSo going back, after substitution, my integral becomes ∮ [p · dr] – (e/c) ∮ [A · dr] equals (n + 1/2)h. Wait, integrating around a loop... What is the second term? That looks like the line integral of A around a closed path, which relates to the magnetic flux enclosed by Ampère's Law. Because according to Stokes' theorem, the circulation of A is equal to the integral of B over the area bounded by the contour. More precisely, ∮ A · dl = Φ_B, where Φ_B is the total magnetic flux through the surface whose boundary is the contour.\n\nTherefore, the second term simplifies to (e/c) times the magnetic flux Φ. Putting it all together, the modified Bohr-Sommerfeld condition becomes:\n\n∮ p · dr – (eΦ)/c = (n + 1/2) h.\n\nHmm, now I should figure out what ∮ p · dr represents. From classical mechanics, especially in systems with cyclic coordinates, the action variable often appears as integrals of momenta over their respective displacements. Moreover, in the absence of electromagnetic fields, the first term would give the usual action integral leading to the standard Bohr-Sommerfeld condition.\n\nIn the presence of a magnetic field, though, the situation changes. Perhaps I can rearrange the equation to solve for the flux Φ. Let me write it again:\n\n∮ p · dr = (n + 1/2) h + (eΦ)/c.\n\nI wonder if the left-hand side can also be expressed in terms of other variables. Alternatively, maybe express everything in terms of angular momentum or something else related to the orbit.\n\nAlternatively, thinking about Landau levels—if you consider particles moving in a uniform magnetic field, their orbits become cyclotron orbits, and the allowed energies correspond to discrete Landau levels. However, in this problem, the setup seems more general—it says \"arbitrary orbit,\" so probably looking for a condition applicable beyond just circular paths.\n\nAnother angle: When dealing with magnetic fields, particularly in solid-state contexts, people sometimes talk about the magnetic length and how it affects the density of states. Or, in the context of Bloch electrons, the Berry phase plays a role similar to the Aharonov-Bohm effect, contributing phases proportional to the magnetic flux.\n\nWait, but stepping back—the key point is relating S, the size of the orbit in k-space, to the magnetic flux. How does that come into play?\n\nLet me recall that in crystals, the wavevector k is defined modulo reciprocal lattice vectors, meaning that the Brillouin zone wraps around periodically. Electrons in a crystal move in effective potentials, and when subjected to a magnetic field, their trajectories get influenced by the periodicity of the lattice.\n\nMoreover, in the semiclassical approach, one considers the trajectory in real space and its corresponding trajectory in k-space. These two spaces are connected via the relation between position r(t) and wavevector k(t).\n\nThe idea being that each cycle in real space corresponds to a shift in k-space, and vice versa. Therefore, the product of the areas in these spaces could lead to conditions akin to those found in the integer quantum Hall effect, where the filling factor ν must take integer values based on the ratio of the magnetic flux per unit cell to the flux quantum hc/e.\n\nBut getting back to the problem statement—they mention rephrasing the quantization condition in terms of the size S of the orbit in k-space. So likely, S refers to the area in k-space covered by the electron's orbit during its motion.\n\nGiven that, perhaps the integral ∮ p · dr can be rewritten in terms of the k-space variables. Remembering that p = ħk, so substituting that in, we'd have:\n\n∮ (ħk) · dr.\n\nBut how does that translate into an integral over k-space? Well, typically, in phase space, the integral over position and momentum can be transformed via Fourier transforms or generating functions, but perhaps here, we can parametrize the orbit in terms of k(t) and integrate accordingly.\n\nIf the orbit in k-space encloses an area S, then perhaps the integral ∮ p · dr translates into something involving S multiplied by Planck's constant or similar constants.\n\nWait, let's see. Suppose that the change in position dr is related to dk somehow. Actually, in the Wigner-Weyl formalism, the correspondence between real space and k-space involves Poisson brackets and star products, but maybe that's complicating matters.\n\nAlternatively, recalling that in the adiabatic approximation, the variation of k is slow compared to oscillations in the plane waves, so the dynamics in k-space can be considered separately.\n\nPerhaps a better way is to note that the integral ∮ p · dr is equivalent to the action integral, which in turn is linked to the area in phase space. But in this scenario, combining real space and k-space...\n\nHold on, maybe I'm making this harder than necessary. Let's go back to the expression:\n\n∮ p · dr = (n + 1/2) h + (eΦ)/c.\n\nExpressed differently, this is:\n\n∮ (p · dr) - (eΦ)/c = (n + 1/2) h.\n\nSince p = m v, classically, but in the presence of a magnetic field, the momentum gets shifted by the vector potential. But earlier, we substituted P = p - eA/c, so perhaps the actual physical momentum is different.\n\nWait, confusion arises whether p is the kinetic momentum or the canonical momentum. Here, in the substitution, P was replaced by p - eA/c, implying that P is the canonical momentum, while p is the kinetic momentum. Hence, in the integral, we still have p, which is the kinetic momentum.\n\nThus, the integral ∮ p · dr remains as is, but subtracts the term involving the flux.\n\nTo connect this to the k-space orbit, perhaps expressing p as ħk. Then, ∮ p · dr = ħ ∮ k · dr.\n\nIs there a relationship between ∫ k · dr and the area in k-space? Yes! By Green's theorem, the line integral of k around a closed curve C is equal to twice the area enclosed by C in k-space. Mathematically,\n\n∮_{C} k_x dy - k_y dx = 2 Area(S),\n\nbut depending on orientation, signs vary. Assuming proper orientation, the magnitude would be |∮ k · dr| ≈ 2ħ S, but units-wise, I need to check.\n\nWait, let's make it precise. Using Stokes’ theorem, the line integral of a vector field F around a closed loop is equal to the double integral of curl(F) over the surface bounded by the loop. Applying this to F = k, then:\n\n∮_C k · dr ="}
{"id": "quantum/7019", "questions": "A porphyrin ring is a molecule which is present in chlorophyll, hemoglobin, and other important compounds. Some aspects of the physics of its molecular properties can be described by representing it as a one-dimensional circular path of radius $r = 4 \\text{Å}$, along which 18 electrons are constrained to move.\n\n(a) Write down the normalized one-particle energy eigenfunctions of the system, assuming that the electrons do not interact with each other.\n(b) How many electrons are there in each level in the ground state of the molecule?\n(c) What is the lowest electronic excitation energy of the molecule? What is the corresponding wavelength (give a numerical value) at which the molecule can absorb radiation?", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about a porphyrin ring modeled as a one-dimensional circular path where 18 electrons are moving without interacting. It's divided into three parts: finding the normalized energy eigenfunctions, determining how many electrons occupy each energy level in the ground state, and calculating the lowest electronic excitation energy along with the corresponding wavelength. Hmm, let me try to work through each part step by step.\n\nStarting with part (a): writing down the normalized one-particle energy eigenfunctions. Since the electrons are on a circular path, this sounds like a particle in a ring or a circular box scenario. In quantum mechanics, particles confined to a circle have quantized angular momentum states because their wave functions must satisfy periodic boundary conditions due to the circular geometry.\n\nThe general approach for such problems involves solving the Schrödinger equation in polar coordinates but considering only motion around the circumference since it's effectively one-dimensional. The key here is that the wave function ψ(θ) must be single-valued when θ increases by 2π radians—meaning after going all the way around the circle once, you come back to the same point. So, we require ψ(θ + 2π) = ψ(θ).\n\nFor a free particle on a ring, the solutions are plane waves e^{i m θ}, where m is an integer representing the number of wavelengths fitting around the ring. But these need to be normalized over the interval [0, 2π). Wait, actually, normalization would involve integrating |ψ|^2 over the length of the ring, right?\n\nSo, if the radius r is given as 4 Ångströms, then the circumference L is 2πr. Let me compute that real quick:\n\nL = 2 * π * 4 Å ≈ 8π Å\n\nBut maybe keeping it symbolic will help more generally. Anyway, the position variable isn't x anymore; instead, it's θ measured from some starting angle. Therefore, the coordinate ranges from 0 to 2π, similar to a unit circle.\n\nIn terms of the Hamiltonian, since the electrons don't interact, the potential V(r) is zero everywhere except perhaps confinement effects—but they're already considered via the boundary condition. Thus, the kinetic energy dominates, leading us to use the standard expression for the energy levels of a particle in a ring.\n\nI recall that the energy levels for a particle on a ring are given by E_m = (ħ² m²)/(2m_r), where ħ is the reduced Planck constant, m is the azimuthal quantum number (integer values... wait, no—it should take both positive and negative integers?), and m_r is the mass of the electron. Hold on—isn’t m allowed to be any integer including negatives? Because differentiating between clockwise and counterclockwise motions gives rise to ±m.\n\nWait, but sometimes people index them using n=|m|. Maybe depending on convention. For our purposes, though, whether m starts at -∞ to ∞ or just non-negative depends on context. However, in most cases, especially for identical fermions, we consider m ranging over all integers, positive and negative, but accounting for spin might change things. Hmm, but in this case, since the question says \"one-particle\" eigenfunctions, probably treating each possible m separately regardless of spin.\n\nAnyway, focusing on the wave functions first. Each eigenfunction corresponds to a specific m. They’re typically written as ψ_m(θ) proportional to e^{imθ}. To normalize, we integrate |ψ_m(θ)|^2 dθ over 0 to 2π. That integral equals 1/(2π) times the integral of 1 dθ, which is 1. Hence, the normalized eigenfunctions are (1/√(2π))e^{imθ}.\n\nTherefore, the normalized one-particle energy eigenfunctions are ψ_m(θ) = (1/√(2π)) e^{imθ}, where m is an integer (positive, negative, or zero). Is that correct? Yeah, seems familiar from my studies.\n\nMoving on to part (b): figuring out how many electrons are in each level in the ground state. This requires filling up the available orbitals according to the Pauli exclusion principle, remembering that electrons are fermions with half-integer spins. Each spatial orbital can hold two electrons—one with spin-up and another with spin-down.\n\nFirstly, what are the distinct energy levels? From the energy formula E_m = (ħ² m²)/(2m_e r²)—wait, hang on, earlier I thought E was proportional to m squared, yes. Actually, substituting variables correctly, the energy should depend on m², meaning that both m and –m give the same energy. So, each pair of m and –m has the same energy, forming degenerate levels.\n\nHowever, wait—if m is zero, that’s unique. Then, for m ≠ 0, each positive m pairs with a negative m giving the same energy. So, does that mean each energy level (except m=0) is doubly degenerate? Or rather, each m defines a separate state unless paired with opposite m.\n\nHmm, confusion arises here. If we think of the total Hilbert space, each m provides a basis vector, but energies group them into sets {m, -m} having equal energy. Except for m=0, which stands alone.\n\nThus, the energy levels are characterized by |m|, so each level n = |m| has either one state (for n=0) or two states (for n ≥ 1). Now, considering that each state can accommodate two electrons (spin up and down), the maximum number per energy level becomes 2*(number of states in that level). \n\nLet me clarify:\n\nEach m represents a distinct state. When m≠0, m and –m correspond to different states but share the same energy. So, for each n≥1, there are two states (with m=n and m=-n). And for n=0, only one state exists.\n\nSince each state holds two electrons (due to spin), the capacity per energy level is:\n\n- Level n=0: 2*1 = 2 electrons\n- Levels n>0: 2*2 = 4 electrons per level\n\nIs that accurate? Yes, because each m contributes two electrons (each state being spin-degenerate), and for each n >0, there are two ms contributing four electrons together.\n\nNow, knowing that, we have 18 electrons to fill into these levels. Starting from the lowest energy upwards.\n\nGround state configuration means filling the lowest energy levels completely before moving higher. So, start with n=0: 2 electrons.\n\nThen next, n=±1: 4 electrons.\n\nNext, n=±2: 4 electrons.\n\nContinuing until all 18 are filled.\n\nCompute cumulative electrons:\n\nLevel n=0: 2 → Total: 2\n\nLevels n=±1: 4 → Total: 6\n\nLevels n=±2: 4 → Total: 10\n\nLevels n=±3: 4 → Total: 14\n\nLevels n=±4: 4 → Total: 18\n\nAh! Perfect, exactly fills up to n=±4. So, each level from n=0 to n=4 is fully occupied.\n\nHence, the distribution is:\n\n- n=0: 2 electrons\n- n=1: 4 electrons\n- n=2: 4 electrons\n- n=3: 4 electrons\n- n=4: 4 electrons\n\nTotaling 2+4+4+4+4=18 electrons.\n\nThat makes sense. So, every energy level up to n=4 is filled with 4 electrons each, except n=0 which has 2.\n\nAlternatively, phrasing it differently, each m from -4 to +4 inclusive is filled, with each m holding 2 electrons (since each state allows two electrons with opposite spins). So, counting m from -4 to +4, excluding m=0, each m has 2 electrons, plus m=0 also has 2.\n\nYes, that adds up too: (-4,-3,...,+4) is 9 m-values, each taking 2 electrons, totaling 18. Correct.\n\nProceeding to part (c): find the lowest electronic excitation energy and the corresponding wavelength.\n\nExcitation energy refers to promoting an electron from a lower-energy orbital to a higher one. The minimal excitation occurs when an electron moves from the highest occupied level to the lowest unoccupied level.\n\nFrom part (b), the highest occupied m is m=4 (and m=-4). Next available empty orbitals are m=5 (or m=-5). Alternatively, could promotion occur within the same n-level? No, because those are already filled. So, the smallest ΔE"}
{"id": "quantum/8005", "questions": "Some short questions to warm you up.\n\n(a) What can be said about the Hamiltonian operator if $L_z$ is a constant in time?  \n\n(b) State the optical theorem in scattering theory.  \n\n(c) Why is the optical theorem not satisfied in first Born approximation?  \n\n(d) Explain why the proton cannot have an electric quadrupole moment. \n\n(e) What is the sign of the phase shift when a particle scatters from a weak short range attractive potential? Justify your answer.  ", "graphs": null, "llm_answers": "<think>\nAlright, so I've got these five physics problems here that I need to tackle one by one. Let me start with part (a). The question asks what we can say about the Hamiltonian operator if \\( L_z \\) is a constant in time. Hmm, okay, let's think about this.\n\nI remember that for an observable like \\( L_z \\), which represents angular momentum around the z-axis, its expectation value being constant over time implies something specific about how it commutes with the Hamiltonian. Specifically, if \\( d\\langle L_z\\rangle/dt = 0 \\), then according to Ehrenfest's theorem, the Poisson bracket or the commutator should vanish. In quantum mechanics terms, this means that the commutator of \\( L_z \\) with the Hamiltonian must be zero: [H, L_z] = 0. So, H and L_z commute.\n\nWhat does commuting imply? Well, operators that commute share eigenstates. That suggests that there exists a basis where both H and L_z are diagonal. Therefore, the system has good quantum numbers corresponding to energy and \\( L_z \\).\n\nBut wait, more specifically, since \\( L_z \\) is conserved, the Hamiltonian doesn't depend explicitly on φ, the angle around the z-axis. This would mean that the system possesses rotational symmetry about the z-axis. Hence, the Hamiltonian must respect azimuthal symmetry; otherwise, \\( L_z \\) wouldn't be conserved.\n\nSo putting it all together, if \\( L_z \\) is a constant in time, the Hamiltonian must commute with \\( L_z \\), implying that the system is symmetric under rotations about the z-axis. Thus, the Hamiltonian depends only on variables invariant under such rotations—like r, p_r, p_φ, etc., but not φ itself.\n\nMoving on to part (b): stating the optical theorem in scattering theory. Okay, the optical theorem relates cross-sections in scattering processes. From my notes, I recall that the total cross-section σ_total is related to the imaginary part of the forward-scattering amplitude f(θ=0). More precisely, the theorem states:\n\nσ_total = 4π * Im[f(0)]\n\nThis makes sense because the optical theorem connects unitarity conditions in quantum mechanics to measurable quantities like cross-sections. It ensures probability conservation in scattering experiments.\n\nNow, onto part (c): explaining why the optical theorem isn't satisfied in the first Born approximation. First, the Born approximation linearizes the Schrödinger equation, neglecting higher-order terms involving the potential V. But the exact expression for the S-matrix includes contributions beyond just the first term—it involves multiple scatterings as well.\n\nIn particular, the optical theorem arises from the requirement that probabilities sum to unity, i.e., no loss of particles. However, the first Born approximation truncates the expansion too early, missing out on certain interference effects between different scattering paths. These interferences contribute to the imaginary parts necessary for satisfying the optical theorem. Without including them, especially those arising from second-order terms, the approximate result won’t satisfy the theorem exactly.\n\nTherefore, while higher-order approximations might approach satisfaction of the optical theorem, the first Born approximation lacks sufficient terms to account for the required cancellations or additions needed for the theorem’s validity.\n\nPart (d) asks why protons don't have electric quadrupole moments. Quadrupole moments arise due to non-spherical charge distributions within a nucleus. For a proton, assuming it's composed of quarks arranged spherically symmetrically, any intrinsic quadrupole moment would require deformation—a deviation from perfect spherical symmetry.\n\nHowever, the proton is considered a point-like particle without significant internal structure in many models, particularly in electromagnetism contexts. Moreover, even considering QCD effects, the strong force tends to maintain high degrees of symmetry among quark arrangements unless acted upon externally. Since the electromagnetic interactions are much weaker compared to the strong nuclear forces holding the proton together, they aren't expected to induce enough asymmetry to produce a noticeable quadrupole moment.\n\nAdditionally, experimental measurements haven't detected a significant electric quadrupole moment for the proton, supporting theoretical expectations based on symmetries and binding forces.\n\nFinally, part (e): determining the sign of the phase shift δ_l for a particle scattering off a weak, short-range attractive potential. Phase shifts relate to changes in wavefunction phases after scattering. Attractive potentials tend to cause a delay in the outgoing waves relative to free propagation.\n\nFor low energies or small l values, an attractive potential will lead to a positive real phase shift. Intuitively, the particle spends some extra time near the potential well before escaping, effectively causing a \"retardation\" in the phase. Mathematically, solving the radial Schrödinger equation shows that bound state formation corresponds to negative poles in the S-matrix, leading to positive phase shifts below threshold.\n\nMoreover, using partial-wave analysis, each angular momentum component contributes differently. An attractive potential enhances the probability of finding the particle in regions closer to the origin, thereby affecting the phase accumulation positively.\n\nThus, the phase shift δ_l is positive for an attractive potential.\n</think>\n\n### Summary Answers\n\n**(a)** If \\( L_z \\) is a constant in time, the Hamiltonian operator must commute with \\( L_z \\), indicating that the system exhibits rotational symmetry about the z-axis. Consequently, the Hamiltonian depends solely on variables invariant under rotation around the z-axis.\n\n**Answer:**  \nThe Hamiltonian commutes with \\( L_z \\), meaning the system has rotational symmetry about the z-axis.  \n\\[\n\\text{[Hamiltonian, } L_z\\text{]} = 0\n\\]\n\n---\n\n**(b)** The optical theorem states that the total cross-section (\\(\\sigma_{\\text{total}}\\)) equals four times pi multiplied by the imaginary part of the forward-scattering amplitude (\\(f(0)\\)):  \n\\[\n\\sigma_{\\text{total}} = 4\\pi \\cdot \\text{Im}[f(0)]\n\\]\nThis theorem enforces probability conservation in scattering processes.\n\n**Answer:**  \nThe optical theorem asserts that \\(\\sigma_{\\text{total}} = 4\\pi \\cdot \\text{Im}[f(0)]\\).  \n\\[\n\\boxed{\\sigma_{\\text{total}} = 4\\pi \\cdot \\text{Im}\\left(f(0)\\right)}\n\\]\n\n---\n\n**(c)** The first Born approximation omits higher-order terms essential for fulfilling the optical theorem. By neglecting these terms, it fails to include necessary interference effects responsible for ensuring the theorem holds true.\n\n**Answer:**  \nFirst Born approximation excludes higher-order terms crucial for the optical theorem's validity.  \n\\[\n\\text{(No boxed formula provided)}\n\\]\n\n---\n\n**(d)** Protons lack electric quadrupole moments primarily due to their nearly spherical charge distribution enforced by strong nuclear forces, preventing significant deviations from symmetry.\n\n**Answer:**  \nProton's charge distribution remains sufficiently spherical, precluding a detectable quadrupole moment.  \n\\[\n\\text{(No boxed formula provided)}\n\\]\n\n---\n\n**(e)** A weak, short-range attractive potential induces a **positive** phase shift (\\(+\\delta_l\\)), reflecting additional time spent near the potential well.\n\n**Answer:**  \nThe phase shift is positive.  \n\\[\n\\boxed{+\\delta_l}\n\\]"}
{"id": "quantum/8002", "questions": "(a) Sum the series $ y = 1 + 2x + 3x^2 + 4x^3 + \\cdots $, $ |x| < 1 $.\n\n(b) If $ f(x) = xe^{-x/\\lambda} $ over the interval $ 0 < x < \\infty $, find the mean and most probable values of $ x $. $ f(x) $ is the probability density of $ x $.\n\n(c) Evaluate $ I = \\int_0^{\\infty} \\frac{dx}{4+x^4} $.\n\n(d) Find the eigenvalues and normalized eigenvectors of the matrix\n\n$$\n\\begin{pmatrix} \n1 & 2 & 4 \\\\ \n2 & 3 & 0 \\\\ \n5 & 0 & 3 \n\\end{pmatrix}.\n$$\n\nAre the eigenvectors orthogonal? Comment on this.  ", "graphs": null, "llm_answers": "<think>\nOkay, so I have four problems to solve here: parts (a), (b), (c), and (d). Let me take them one by one.\n\nStarting with part (a): Summing the series \\( y = 1 + 2x + 3x^2 + 4x^3 + \\cdots \\) where \\( |x| < 1 \\).\n\nHmm, that looks like a power series. The general term seems to be \\( (n+1)x^n \\) starting from n=0. So it's similar to the standard geometric series but multiplied by coefficients increasing linearly. \n\nI remember there’s a formula for summing such series. For example, if we consider the basic geometric series:\n\n\\( S = 1 + x + x^2 + x^3 + \\dots = \\frac{1}{1 - x} \\) when \\( |x| < 1 \\).\n\nIf we differentiate both sides with respect to x, maybe something useful comes up because differentiation can bring down powers which might relate to these coefficients.\n\nLet me try differentiating S:\n\n\\( dS/dx = 0 + 1 + 2x + 3x^2 + 4x^3 + \\dots = \\sum_{n=1}^\\infty n x^{n-1} \\)\n\nBut our given series starts as 1 + 2x + 3x² + ..., which is actually equal to \\( \\sum_{n=0}^\\infty (n+1)x^n \\). Notice that this is just \\( \\sum_{k=1}^\\infty k x^{k-1} } \\times x \\)? Wait, let me see.\n\nWait no, more accurately, since my differentiated series gives us \\( \\sum_{n=1}^\\infty n x^{n-1} \\), multiplying both sides by x would give \\( x \\cdot dS/dx = \\sum_{n=1}^\\infty n x^n \\). But our original series has terms \\( (n+1)x^n \\), not \\( nx^n \\). Hmm.\n\nSo perhaps another approach. Maybe integrating or manipulating differently?\n\nAlternatively, think about writing the series as follows:\n\n\\( y = \\sum_{n=0}^\\infty (n + 1) x^n \\)\n\nWhich can also be written as:\n\n\\( y = \\sum_{n=0}^\\infty (n + 1) x^n = \\sum_{m=1}^\\infty m x^{m - 1} \\) substituting m = n + 1.\n\nThen, comparing to the derivative of the geometric series:\n\nWe know that \\( \\sum_{m=1}^\\infty m x^{m - 1} = \\frac{1}{(1 - x)^2} \\). Therefore,\n\nMultiplying both sides by x, we get:\n\n\\( \\sum_{m=1}^\\infty m x^{m} = \\frac{x}{(1 - x)^2} \\)\n\nTherefore, going back to our expression for y:\n\nSince \\( y = \\sum_{n=0}^\\infty (n + 1) x^n = \\sum_{m=1}^\\infty m x^{m - 1} = \\frac{1}{(1 - x)^2} \\)\n\nHence, the sum should be \\( \\frac{1}{(1 - x)^2} \\).\n\nWait, hold on—let me verify again. Because earlier substitution led me astray.\n\nOriginal series: \\( y = 1 + 2x + 3x^2 + 4x³ + ... \\)\n\nThis is indeed \\( \\sum_{n=0}^\\infty (n + 1) x^n \\). As per known formulas, yes, this equals \\( \\frac{1}{(1 - x)^2} \\).\n\nYes, okay, so problem (a)'s answer is \\( \\frac{1}{(1 - x)^2} \\).\n\nMoving on to part (b): Given \\( f(x) = x e^{-x / \\lambda} \\) over \\( 0 < x < \\infty \\), find the mean and most probable values of x. Here, f(x) is the probability density function.\n\nFirst, confirm whether f(x) is properly normalized. Since it's supposed to be a PDF, its integral over all space must be 1.\n\nCompute \\( \\int_0^\\infty x e^{-x / \\lambda} dx \\). That's a gamma function integral. Recall that \\( \\Gamma(n) = \\int_0^\\infty t^{n - 1} e^{-t} dt \\). To match variables, set u = x/λ => du = dx/λ => dx = λ du; then x = λ u.\n\nSubstituting into the integral:\n\nIntegral becomes \\( \\int_0^\\infty (\\lambda u) e^{-u} * \\lambda du = \\lambda^2 \\int_0^\\infty u e^{-u} du = \\lambda^2 Γ(2) \\). And Γ(2)=1! =1. Hence, the integral is \\( \\lambda^2 \\). Thus, unless f(x) was defined without normalization, meaning they probably forgot the coefficient. Wait, wait—the question says \"f(x)\" is the PDF, so likely missing a factor.\n\nBecause otherwise, ∫₀^∞ x e^{-x/λ} dx = λ², hence to make it a proper PDF, you need to multiply by 1/λ². Otherwise, the total area under f(x) isn't 1—it's λ² instead.\n\nIs the given f(x) already normalized? Or do I assume it needs normalizing?\n\nThe problem states \"f(x) is the probability density of x.\" So assuming it's correctly normalized. Then either way, proceed accordingly.\n\nAssuming f(x) is correct, i.e., including any necessary constants—if not, calculations will adjust.\n\nHowever, looking closely, f(x) is given as x e^{-x/λ}, which doesn’t include the normalization constant. In reality, the exponential distribution usually includes a rate parameter, say β, making f(x) = β e^{-β x}. However, sometimes people use scale parameters. Alternatively, Gamma distributions involve shape and scale.\n\nGiven that f(x) = x e^{-x/λ}, it resembles a Gamma distribution with shape parameter α=2 and scale parameter θ=λ. Indeed, the Gamma PDF is typically expressed as:\n\n\\( f(x; \\alpha, \\theta) = \\frac{x^{\\alpha - 1} e^{-x/\\theta}}{\\theta^\\alpha \\Gamma(\\alpha)} \\)\n\nIn our case, f(x) = x e^{-x/λ}; compare to Gamma form:\n\nSet α - 1 = 1 ⇒ α = 2.\n\nThus, Gamma PDF would be \\( \\frac{x e^{-x/\\lambda}}{\\lambda^2 \\Gamma(2)} \\). Since Γ(2)=1!, so denominator is λ²*1=λ². Therefore, the actual PDF should be \\( \\frac{x e^{-x/\\lambda}}{\\lambda^2} \\).\n\nBut the given f(x) lacks the division by λ². So possibly, the user made a typo or mistake—but according to the problem statement, f(x) is given as x e^{-x/λ}, so perhaps treating it as unnormalized? No, the wording says \"is the probability density\", implying it's normalized.\n\nWait, confusion arises. Perhaps better to compute the normalization first.\n\nCalculate Z = ∫₀^∞ x e^{-x/λ} dx = [from before] λ². So to normalize, define f_normalized(x) = (1/Z) x e^{-x/λ} = (1/λ²) x e^{-x/λ}\n\nMean μ = E[x] = ∫₀^∞ x f(x) dx. But if f(x) is already normalized, then E[x] is simply ∫₀^∞ x*(something) dx. Wait, but depending on how f(x) is presented.\n\nHold on, perhaps regardless of normalization, computing expectation directly.\n\nSuppose f(x) is given as x e^{-x/λ}, even though it may not integrate to 1. Compute the expected value anyway.\n\nE[x] = ∫₀^∞ x*f(x)*dx = ∫₀^∞ x*(x e^{-x/λ}) dx = ∫₀^∞ x² e^{-x/λ} dx\n\nSimilarly, Var(X) requires E[X²], etc., but since only asked for mean and mode.\n\n"}
{"id": "quantum/7018", "questions": "Two particles of mass $ m $ are placed in a rectangular box of sides $ a > b > c $ in the lowest energy state of the system compatible with the conditions below. The particles interact with each other according to the potential $ V = A \\delta(\\mathbf{r}_1 - \\mathbf{r}_2) $. Using first-order perturbation theory to calculate the energy of the system under the following conditions:\n\n(a) Particles not identical.  \n\n(b) Identical particles of spin zero.  \n\n(c) Identical particles of spin one-half with spins parallel.\n", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about two particles in a rectangular box interacting via a delta function potential. It's divided into three parts based on whether they're identical or not and their spin states. Hmm, let me try to break it down.\n\nFirst, understanding what is given: Two particles of mass \\( m \\) are in a box with dimensions \\( a > b > c \\). They’re in the lowest energy state possible for each condition (a), (b), (c). Their interaction is through a potential \\( V = A \\delta(\\mathbf{r}_1 - \\mathbf{r}_2) \\).\n\nWe need to use first-order perturbation theory to find the energy shift due to this interaction. So, the unperturbed Hamiltonian would be just the kinetic energies plus any confinement potentials from the box, but since we aren't told there’s another potential besides the particle-particle interaction, maybe that's all? Wait, no—the box itself imposes boundary conditions which define the allowed energy levels without considering interactions.\n\nSo, starting with part (a): particles are not identical. That means they can occupy different quantum states because Pauli exclusion doesn’t apply here. Since they want the lowest energy configuration, both will go into the ground state if possible?\n\nWait, hold on—if they are distinguishable, then yes, they could both be in the same single-particle state. But wait, actually, even non-identical fermions obeying Fermi statistics might still require them to have different wavefunctions... No, wait—no, only identical particles must obey exchange symmetry; otherwise, you don't have such restrictions. Therefore, for non-identical particles, like electrons vs protons, they can share the same spatial wavefunction regardless of spin.\n\nBut in our case, do we know anything else about these particles? Not really—it says \"particles,\" so perhaps treating them as bosons unless specified otherwise isn't correct either. Maybe better to think that when they say 'not identical,' it implies they can be treated independently without worrying about antisymmetry.\n\nTherefore, the total wavefunction is simply a product of individual wavefunctions. For the lowest energy, both should take the ground state of the box. Let me recall the energy levels for a 3D box: Each direction has its own quantum number, n_x, n_y, n_z, leading to energy \\( E_{n_x,n_y,n_z} = \\frac{\\hbar^2}{2m}\\left( \\frac{n_x^2}{a^2} + \\frac{n_y^2}{b^2} + \\frac{n_z^2}{c^2} \\right ) \\).\n\nSince \\( a > b > c \\), the smallest energy occurs when all quantum numbers are minimal, i.e., \\( n_x=n_y=n_z=1 \\). Thus, each particle being in the ground state gives the total energy as twice that value, right? Because each contributes equally.\n\nNow, applying the perturbation \\( V = A \\delta(\\mathbf{r}_1 - \\mathbf{r}_2) \\). In first-order perturbation theory, the correction to the energy is given by the expectation value of the perturbing potential over the unperturbed wavefunction.\n\nThe unperturbed wavefunction for two distinguishable particles is symmetric, meaning \\( \\psi(r_1,r_2) = \\phi_0(r_1)\\phi_0(r_2) \\), where \\( \\phi_0 \\) is the ground state wavefunction.\n\nThus, the first-order energy shift \\( \\Delta E^{(1)} = \\langle \\psi | V | \\psi \\rangle = A \\int d\\mathbf{r}_1 d\\mathbf{r}_2 |\\phi_0(r_1)|^2 |\\phi_0(r_2)|^2 \\delta(\\mathbf{r}_1 - \\mathbf{r}_2) \\).\n\nSimplify this integral: When integrating over r₂, set r₁=r₂, so becomes \\( A \\int d\\mathbf{r} |\\phi_0(r)|^4 \\times volume element? Wait, more precisely,\n\n\\( \\Delta E^{(1)} = A \\int |\\phi_0(r_1)|^2 |\\phi_0(r_2)|^2 \\delta(\\mathbf{r}_1 - \\mathbf{r}_2) dr_1 dr_2 \\)\n\nWhich simplifies to\n\n\\( A \\int |\\phi_0(r)|^4 dr \\)\n\nBecause substituting r₂ = r₁, and the double integral reduces to a single integral multiplied by A.\n\nWhat's \\( |\\phi_0(r)|^2 \\)? Well, for a 3D box, the normalized wavefunction is \\( \\phi_{n_x,n_y,n_z}(x,y,z) = \\sqrt{\\frac{8}{abc}} \\sin(n_x \\pi x/a) \\sin(n_y \\pi y/b) \\sin(n_z \\pi z/c) \\).\n\nFor the ground state, \\( n_x = n_y = n_z = 1 \\):\n\n\\( \\phi_0(x,y,z) = \\sqrt{\\frac{8}{abc}} \\sin(\\pi x/a) \\sin(\\pi y/b) \\sin(\\pi z/c) \\).\n\nHence, \\( |\\phi_0|^2 = \\frac{8}{abc} \\sin^2(\\pi x/a) \\sin^2(\\pi y/b) \\sin^2(\\pi z/c) \\).\n\nThen, \\( |\\phi_0|^4 = \\left( \\frac{8}{abc} \\right)^2 [\\sin^2(\\pi x/a)]^2 [\\sin^2(\\pi y/b)]^2 [\\sin^2(\\pi z/c)]^2 \\).\n\nIntegrating this over all space within the box [from 0 to a in x, 0 to b in y, 0 to c in z]:\n\nCompute \\( \\int_0^a \\sin^4(\\pi x / a) dx \\), similarly for y and z directions.\n\nI remember that \\( \\int_0^\\pi \\sin^n t dt \\) has standard results. Specifically, \\( \\int_0^L \\sin^4(kx) dx \\) can be computed using power-reduction formulas.\n\nRecall that \\( \\sin^4 u = \\frac{3}{8} - \\frac{1}{2} \\cos(2u) + \\frac{1}{8} \\cos(4u) \\). Integrating term-wise:\n\nIntegral over sin⁴(u) du from 0 to π/ something...\n\nLet me compute \\( \\int_0^A \\sin^4(Bx) dx \\). Let substitution: let θ = Bx, so dθ = Bdx => dx = dθ/B.\n\nLimits become 0 to BA.\n\nThus, integral becomes ∫₀^{BA} sin⁴θ * (dθ)/B = (1/B)[∫₀^{BA} sin⁴θ dθ].\n\nUsing identity above:\n\n∫ sin⁴θ dθ = (3/8)θ - (1/(4)) sin(2θ) + (1/(32)) sin(4θ) evaluated between limits.\n\nAssuming BA is multiple of π, e.g., for our case, when computing along x-direction, L=a, k=π/a, so BA=k*a=π.\n\nSimilarly, for y and z, similar substitutions.\n\nIn our specific case, for x-integral:\n\nk = π/a, A = a, so BA = π.\n\nThus, ∫₀^a sin⁴(π x/a) dx = (1/(π/a))[ (3/8)(π) - ... ] ?\n\nWait, plugging back:\n\nAfter substitution, the integral becomes (1/k)*(3/8)*theta| from 0 to pi minus terms involving sine functions.\n\nAt theta = pi, sin(2*theta)=sin(2pi)=0, sin(4*theta)=sin(4pi)=0. Similarly at theta=0, those sines are also zero.\n\nThus, overall, ∫₀^a sin⁴(pi x/a) dx = (1/pi) * (3/8)*pi = 3/8 * a.\n\nSame logic applies for y and z integrals:\n\nEach ∫ sin⁴(...) dx = 3/8 times length.\n\nThus, putting together:\n\n∫ |φ₀|^4 dV = (√(8/(abc)))^4 × [product of integrals]\n\nWait, earlier expression was:\n\n|\\phi_0|^4 = (64)/(a²b²c²) sin^4(...)... etc.\n\nNo, wait, original phi squared is 8/(abc) sin²..., so phi^4 is (8/(abc))^2 sin^4....\n\nYes, exactly. Then, integrate over x, y, z:\n\n[ (8/(abc))^2 ] × [ Integral_x sin"}
{"id": "quantum/4007", "questions": "Determine the energy levels, their degeneracy and the corresponding eigenfunctions of an electron contained in a cube of essentially infinite volume $L^3$. The electron is in an electromagnetic field characterized by the vector potential\n\n$$\n\\mathbf{A} = H_0x\\hat{e}_y \\quad (\\lvert \\hat{e}_y \\rvert = 1).\n$$", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem where I need to determine the energy levels, their degeneracy, and the corresponding eigenfunctions for an electron confined in a cubic box with sides of length L. But there's also an electromagnetic field present described by the vector potential A = H₀ x e_y. Hmm, that seems interesting because it introduces some sort of magnetic effect into the system.\n\nFirst off, let me recall what I know about electrons in boxes without any external fields. In such cases, we usually solve the Schrödinger equation for a particle in a three-dimensional box, which gives us quantized energy levels depending on quantum numbers n_x, n_y, n_z. Each direction contributes its own term proportional to (n_i)^2, leading to energies like E ∝ (n_x² + n_y² + n_z²). And each set of distinct quantum numbers corresponds to different states but sometimes can lead to degenerate energy levels if multiple sets give the same sum.\n\nBut now, introducing a vector potential complicates things. Vector potentials come into play when dealing with electromagnetic fields—specifically, they contribute to the momentum operator through minimal coupling. So instead of just having p = -iħ∇, we'll have p → p - qA/c or something similar? Wait, actually, since we're working in SI units here, maybe it's better expressed as:\n\nThe Hamiltonian becomes:\nH = [ (p - (q/c) A )² ] / (2m) + V(r)\n\nWhere V(r) would be zero inside the box and infinitely large outside, confining the electron within the cube. Since our box has walls at x=0, y=0, z=0 up to x=L, etc., those boundary conditions will still apply.\n\nGiven that A only has a component along y-direction: A = H₀ x ē_y. That means the vector potential varies linearly with x-coordinate. Interesting. Let me write down the components:\n\nA_x = 0,\nA_y = H₀ x,\nA_z = 0.\n\nSo plugging this into the modified momentum operator, the kinetic part of the Hamiltonian should become:\n\n( [ (-iħ∂/∂x - (q H₀ x)/c ) , (-iħ∂/∂y), (-iħ∂/∂z) ] ) squared divided by 2m.\n\nWait, hold on—the charge of the electron is negative, right? Because electrons are negatively charged. So perhaps more accurately,\n\np' = p - (e/c) A,\n\nwhere e is positive elementary charge, so the actual expression might involve subtracting a negative times A?\n\nLet me double-check. Minimal substitution replaces p with p - (e/c) A. For an electron, the charge is -e, so substituting correctly, it'd be p - (-e/c) A = p + (e/c) A. Therefore, yes, the correct replacement is adding (e/c) A to the original momentum.\n\nTherefore, the corrected momentum operators are:\n\np'_x = -iħ (∂/∂x) + (e H₀ x)/(c),\np'_y = -iħ (∂/∂y),\np'_z = -iħ (∂/∂z).\n\nThus, the square of the momentum becomes:\n\n(p')² = [(-iħ ∇ + (e H₀ x/(c)) e_y)] · [(-iħ ∇ + (e H₀ x/(c)) e_y)]\n\nExpanding this out, we get terms from each component. Specifically, expanding the dot product:\n\n[ (-iħ ∂/∂x + (e H₀ x)/(c))^2 +\n(-iħ ∂/∂y)^2 +\n(-iħ ∂/∂z)^2 ]\n\nPlus cross-terms between the first component and others? No, wait—it’s not exactly straightforward because the other components don't mix due to orthogonality. Actually, no, the expansion is additive across each spatial dimension because the gradient acts separately on each coordinate. However, the presence of the vector potential affects only one component—in this case, the x-component.\n\nWait, actually, upon closer inspection, the cross-product terms may vanish because differentiation with respect to different variables commute. Or do they? Maybe not necessarily. Alternatively, perhaps considering how the Laplacian works with variable coefficients... This could complicate matters.\n\nAlternatively, another approach is to shift coordinates to absorb the linear term introduced by the vector potential. Perhaps using gauge transformations? If possible, choosing a gauge where calculations simplify.\n\nGauge transformation allows changing the scalar and vector potentials via:\n\nφ → φ - (1/c)(∂λ/∂t),\n\nA → A + ∇λ.\n\nIf I choose λ appropriately, maybe I can eliminate the position dependence in A. Given that A depends on x, setting λ(x,y,z,t)=something involving x might help.\n\nSuppose I take λ = -(e H₀ x²)/(2 c). Then, ∇λ = [- (e H₀ x)/c] e_x. Thus, under this gauge transformation, the new vector potential A’ becomes:\n\nA’ = A + ∇λ = H₀ x e_y + [ - (e H₀ x)/c e_x ].\n\nHmm, does that make sense? Not sure yet. Alternatively, perhaps trying to find a gauge where A simplifies.\n\nWait, alternatively, consider going into the symmetric gauge or Landau gauge. Here, though, the vector potential isn’t uniform; it increases linearly with x. It resembles a varying magnetic field rather than a constant one.\n\nHold on—if A is given as H₀ x e_y, then taking curl(A):\n\ncurl(A) = ∇ × A = ( ∂A_z/∂y - ∂A_y/∂z ) e_x + ... Similarly, compute all components.\n\nCompute partial derivatives:\n\nSince A has only y-component dependent on x, the non-zero derivative comes from ∂A_y/∂x = H₀.\n\nAll other components of curl(A): \n\nFor example, ∂A_z/∂y - ∂A_y/∂z = 0 - 0 = 0,\n\nSimilarly, ∂A_x/∂z - ∂A_z/∂x = 0 - 0 = 0,\n\nAnd finally, ∂A_y/∂x - ∂A_x/∂y = H₀ - 0 = H₀.\n\nTherefore, curl(A) = H₀ e_z.\n\nThat tells us that the magnetic field B is equal to curl(A), hence B = H₀ e_z. So, the magnetic field is uniform and points in the z-direction with magnitude H₀.\n\nAh! Okay, so despite the vector potential being position-dependent, the resulting magnetic field is uniform. That makes sense because even though A varies with x, the way it's constructed leads to a uniform B-field.\n\nThis changes my perspective. Instead of thinking about a complicated vector potential, I realize that effectively, the electron is placed in a uniform magnetic field pointing along the z-axis. Hence, the problem reduces to finding the spectrum of a particle in a box subjected to a uniform magnetic field perpendicular to two faces of the box.\n\nIn such scenarios, typically, the motion parallel to the magnetic field remains free (or subject to periodic boundary conditions if applicable), while the transverse directions experience quantization due to the Lorentz force.\n\nHowever, in our case, the confinement is hard-wall boundaries—a cubical box—which complicates things compared to the usual infinite cylinder scenario used in Landau problems.\n\nI think the strategy here is to separate the Hamiltonian into parts related to the magnetic field and the confinement. Let me try writing the full Hamiltonian again.\n\nWe had earlier determined that after minimal substitution, the Hamiltonian is:\n\nH = [ (p'_x)^2 + (p'_y)^2 + (p'_z)^2 ] / (2m )\n\nWith p'_x = -iħ ∂/∂x + (e H₀ x)/(c ), and similarly for p'_y and p'_z.\n\nPlugging these back in, the Hamiltonian becomes:\n\nH = [ { (-iħ ∂/∂x + (e H₀ x)/(c )) }^2 + { (-iħ ∂/∂y) }^2 + { (-iħ ∂/∂z) }^2 ] / (2m )\n\nNow, expand the squares:\n\nEach term squared will produce second-order derivatives plus lower order terms.\n\nStarting with the x-component:\n\n[ (-iħ ∂/∂x + (e H₀ x)/(c )) ]^2 =\n\n(-iħ ∂/∂x)^2 + 2*(-iħ ∂/∂x)*(e H₀ x)/(c ) + [(e H₀ x)/(c )]^2\n\nWhich equals:\n\n-ħ² ∂²/∂x² + i*(2 ħ e H₀ / c)*x ∂/∂x + (e² H₀² x²)/(c²)\n\nThen, the y and z-components simply add:\n\n+ (-ħ² ∂²/∂y²) + (-ħ² ∂²/∂z²)\n\nPutting them together over 2m:\n\nH = [ -ħ²"}
{"id": "quantum/7011", "questions": "A one-dimensional box of width $L$ contains two spinless particles each of mass $m$. The interaction between the particles is described by a potential energy $V(x_1, x_2) = a\\delta(x_1 - x_2)$. Calculate the ground state energy to first order in $a$.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about a one-dimensional box with two spinless particles. Each has mass m, and they interact through a delta function potential V(x₁, x₂) = a δ(x₁ − x₂). I need to find the ground state energy to first order in 'a'. Hmm.\n\nFirst off, since it's a 1D box, let me recall that for non-interacting particles (i.e., when a=0), the system would just be two free particles in a box. But here we do have interactions, albeit weak because we're considering small 'a'—so maybe perturbation theory applies?\n\nWait, but wait—the question says \"to first order in a,\" which suggests using perturbative methods where 'a' is treated as a small parameter. So perhaps I can treat the delta potential as a perturbation on top of the non-interacting case.\n\nBut before jumping into perturbation, let me think: what does the unperturbed Hamiltonian look like? For two identical spinless particles in a box, without any interaction, their wavefunctions should satisfy certain symmetry conditions if they were fermions or bosons. Wait—but hold on! Spinless particles... In quantum mechanics, particles with integer spin are bosons and half-integer are fermions. Since these are spinless, meaning spin zero, hence bosons. Therefore, their total wavefunction must be symmetric under exchange.\n\nSo, the unperturbed states will be either both in the same single-particle state (if possible) or some combination respecting Bose-Einstein statistics. However, in a 1D box, even though the spatial part might not necessarily require being symmetric unless there’s another degree of freedom, but since they’re spinless, all the information goes into the spatial wavefunction. Hence, yes, the spatial wavefunction needs to be symmetric.\n\nIn the noninteracting case, the lowest energy state would have both particles in the lowest single-particle orbital. That is, the ground state would be ψ₀(x₁)ψ₀(x₂), right? Because you can't put them lower than that. And due to indistinguishability, the wavefunction becomes [ψ₀(x₁)ψ₀(x₂)] / √2, normalized appropriately.\n\nNow, introducing the interaction term V(x₁,x₂)=aδ(x₁−x₂). This looks like contact interaction; only significant when x₁=x₂. So, how does this affect the energy?\n\nSince we're treating 'a' as a small quantity, we'll use first-order perturbation theory. First-order correction to the energy is given by ⟨Ψ|V|Ψ⟩, where Ψ is the unperturbed ground state.\n\nLet me write down the expression:\n\nE^(1) = ⟨Ψ| a δ(x₁ − x₂) |Ψ⟩\n\nGiven that Ψ is symmetric, E^(1) simplifies to a times the expectation value of δ(x₁ − x₂).\n\nCompute this integral:\n\n⟨Ψ| δ(x₁ − x₂) |Ψ⟩ = ∫ dx₁∫dx₂ [ψ₀*(x₁)ψ₀*(x₂)]² δ(x₁ − x₂)\n\nBecause Ψ is symmetric, it's equal to sqrt(2) ψ₀(x₁)ψ₀(x₂)/sqrt{N}, actually no—I need to get normalization correct.\n\nWait, more carefully: If the particles are distinguishable, then the product ψ₀(x₁)ψ₀(x₂) is fine, but since they are indistinct bosons, the proper way is to symmetrize the wavefunction. Actually, in the noninteracting case, the ground state is already symmetric, so Ψ(x₁, x₂) = [ψ₀(x₁)ψ₀(x₂) + ψ₀(x₂)ψ₀(x₁)]/√2, but since ψ₀ is real, this reduces to ψ₀(x₁)ψ₀(x₂)*√2/(√2)? No, wait—if ψ₀ is real, then ψ₀(x₁)ψ₀(x₂) is symmetric automatically, so the overall factor is 1/sqrt(N!), N=2, so 1/sqrt(2). Thus,\n\nΨ(x₁, x₂) = (1/√2)[ψ₀(x₁)ψ₀(x₂)]\n\nTherefore, the square modulus is (1/2)|ψ₀(x₁)|²|ψ₀(x₂)|².\n\nThus, going back to compute the expectation value:\n\n⟨δ(x₁ − x₂)⟩ = ∫_{box} dx₁ ∫_{box} dx₂ [1/2] |ψ₀(x₁)|² |ψ₀(x₂)|² δ(x₁ − x₂)\n\nThe delta function picks out x₂ = x₁, so integrating over x₂ gives us:\n\n[1/2] ∫ dx |ψ₀(x)|⁴ * L_x ?\n\nNo, wait—it's better written as:\n\nAfter substituting x₂ = x₁, the double integral becomes a single integral multiplied by the length element from x₂ integration, effectively giving:\n\n[1/2] ∫ dx |ψ₀(x)|^4 * something...\n\nActually, let me make sure. When dealing with δ(x₁ - x₂), the inner integral over x₂ becomes 1, evaluated at x₂ = x₁. Then, the remaining integral is over x₁, so:\n\n⟨δ(x₁ − x₂)⟩ = (1/2) ∫ dx |ψ₀(x)|^4\n\nYes, exactly. So now, I need to evaluate ∫ dx |ψ₀(x)|^4.\n\nWhat is ψ₀(x)?\n\nFor a particle in a 1D box of width L, the ground state wavefunction is ψ₀(x) = sqrt(2/L) sin(πx/L). So its magnitude squared is (2/L) sin²(πx/L). Therefore, |ψ₀(x)|^4 is [(2/L)^2] sin^4(πx/L).\n\nHence, the integral becomes:\n\n(1/2)(2/L)^2 ∫₀ᴸ sin^4(πx/L) dx\n\nSimplify constants:\n\n(1/2)*(4/L²) ∫₀ᴸ sin^4(...) dx => (2/L²) ∫₀ᴸ sin^4(kx) dk, where k = π/L.\n\nI remember that the integral of sin^n(ax) can be found via reduction formulas. Specifically, for n=4, the formula is known.\n\nRecall that ∫ sin^4(u) du = (3u/8 - sin(2u)/4 + sin(4u)/32 ) + C\n\nAlternatively, express sin^4 u in terms of multiple angles:\n\nsin^4 u = (3/8) - (1/2) cos(2u) + (1/8) cos(4u)\n\nIntegrate term-wise:\n\nIntegral from 0 to pi (since substitution):\n\nIf our variable substitution is t = πx/L, then dt = π/L dx, so dx = L/pi dt.\n\nWhen x ranges from 0 to L, t ranges from 0 to pi.\n\nThen, the integral becomes:\n\n∫₀ᴸ sin^4(πx/L) dx = (L/π) ∫₀^{pi} sin^4(t) dt\n\nUsing the identity above:\n\nsin^4(t) = 3/8 - (1/2)cos(2t) + (1/8)cos(4t)\n\nIntegrating term-by-term:\n\n∫₀^{pi} sin^4(t) dt = ∫₀^{pi} [3/8 - (1/2)cos(2t) + (1/8)cos(4t)] dt\n\nEach integral separately:\n\n∫ 3/8 dt from 0 to pi = (3/8)(pi)\n∫ -(1/2)cos(2t) dt = -(1/2)*(sin(2t)/2) from 0 to pi → evaluates to 0.\nSimilarly, ∫ (1/8)cos(4t) dt = (1/8)*(sin(4t)/4) from 0 to pi → also 0.\n\nThus, the entire integral is simply (3/8) pi.\n\nPutting it together:\n\n∫₀ᴸ sin^4(pi x/L) dx = (L/pi) * (3 pi /8) ) = (3 L)/8\n\nSo returning to the earlier computation:\n\n⟨δ(x₁ − x₂)⟩ = (2/L²) * (3 L /8 )\n\nMultiply numerator and denominator:\n\n(2*3)/(8*L) ) = (6)/(8 L) = (3)/(4 L)\n\nTherefore, the first-order energy shift is:\n\nE^(1) = a * ⟨δ(x₁ − x₂)⟩ = a * (3)/(4 L)\n\nHmm, okay—that seems straightforward. Let me recapitulate steps to ensure correctness.\n\n1. Identified the system as two bosons in a 1D box with repulsive/attractive delta potential depending on sign of 'a'.\n2. Noted that in absence of interaction, the ground state is both particles in the lowest single-particle state, leading"}
{"id": "quantum/4009", "questions": "(a) Assuming that nonrelativistic quantum mechanics is invariant under time reversal, derive the time reversed form of the Schrodinger wave function.  \n(b) What is the quantum mechanical Hamiltonian for a free electron with magnetic moment $\\mu $ in the external constant magnetic field $H_z$ in the z-direction, in the reference frame of the electron?  \n(c) Suppose that an extra constant magnetic field $H_y$ is imposed in the y-direction. Determine the form of the quantum mechanical operator for the time rate of change of $\\mu $ in this case.  ", "graphs": null, "llm_answers": "<think>\nOkay, so I have these three parts to solve: (a), (b), and (c). Let me start with part (a).\n\n**Part (a): Time Reversed Form of the Schrödinger Wave Function**\n\nAlright, the question says assuming non-relativistic quantum mechanics is invariant under time reversal, we need to find the time-reversed form of the Schrödinger wavefunction.\n\nHmm, okay. So first, what does it mean for something to be invariant under time reversal? Invariance means that if you reverse the direction of time, the laws or equations remain unchanged. For quantum mechanics, specifically the Schrödinger equation, which governs how the wavefunction evolves over time, applying a time reversal should give us another valid solution.\n\nThe standard Schrödinger equation is:\n\n\\[ i\\hbar \\frac{\\partial}{\\partial t} \\psi(\\mathbf{r}, t) = H \\psi(\\mathbf{r}, t) \\]\n\nWhere \\( H \\) is the Hamiltonian operator.\n\nNow, when considering time reversal symmetry, let's think about replacing \\( t \\) with \\( -t \\). But just substituting might not capture everything because operators can also transform under such symmetries.\n\nWait, actually, more precisely, time reversal isn't just flipping the sign of time; it involves complex conjugation as well since probabilities must stay real. Because the wavefunction has both amplitude and phase, reversing time would affect their phases but keep the probability density the same.\n\nSo perhaps the transformed wavefunction under time reversal is related to the original one through some operation involving complex conjugation.\n\nLet me recall from my studies... The time reversal operator usually acts on the state vector as \\( T |\\psi(t)\\rangle = | \\tilde{\\psi}(t) \\rangle \\), where \\( \\tilde{\\psi}(t) \\) satisfies certain conditions.\n\nIn position representation, the effect of time reversal on the wavefunction could involve taking the complex conjugate and possibly changing variables like momentum signs due to velocity reversals.\n\nBut wait, maybe simpler approach: If the system is symmetric under time reversal, then there exists an anti-unitary operator \\( K \\) (since time reversal changes the order of events, hence anti-linear) such that\n\n\\( KT H K^{-1} = H^* \\)\n\nSince the Hamiltonian remains Hermitian after transformation, right?\n\nAlternatively, thinking directly about the wavefunction evolution. Under time reversal, the forward-time evolution becomes backward-time evolution. Since the Schrödinger equation is linear, the time-evolved state will satisfy a similar equation but with negative energy eigenvalues.\n\nSuppose our initial wavefunction is \\( \\psi(\\mathbf{r}, t) \\). After time reversal, say \\( t' = -t \\), the new wavefunction \\( \\phi(\\mathbf{r}, t') \\) should satisfy the same Schrödinger equation but evolved backwards in time.\n\nMathematically,\n\nIf \\( \\psi(\\mathbf{r}, t) \\) solves \\( i\\hbar \\partial_t \\psi = H \\psi \\),\n\nthen \\( \\phi(\\mathbf{r}, t') = \\psi^*(\\mathbf{r}, -t') \\) should solve the time-reversed version.\n\nBecause plugging into the equation:\n\nFirst compute derivatives. Let’s set \\( t'' = -t' \\), so \\( \\phi(\\mathbf{r}, t') = \\psi^*(\\mathbf{r}, t'') \\).\n\nThen,\n\n\\( i\\hbar \\partial_{t'} \\phi = i\\hbar (-1) \\partial_{t''} \\psi^* = -i\\hbar (\\partial_{t''}\\psi)^* \\)\n\nOn the other hand, using the original equation:\n\n\\( \\partial_{t''} \\psi = (i/\\hbar) H \\psi \\Rightarrow (\\partial_{t''}\\psi)^* = (-i/\\hbar) H^* \\psi^* \\)\n\nAssuming \\( H \\) is Hermitian, \\( H^* = H^\\dagger = H \\), so:\n\n\\( (\\partial_{t''}\\psi)^* = (-i/\\hbar) H \\psi^* \\)\n\nThus,\n\n\\( -i\\hbar (\\partial_{t''}\\psi)^* = -i\\hbar * [(-i)/\\hbar] H \\psi^* = -i*(-i) H \\psi^* / 1 = -(i^2) H \\psi^* = H \\psi^* \\)\n\nTherefore,\n\nLeft-hand side: \\( i\\hbar \\partial_{t'} \\phi = H \\phi \\)\n\nWhich matches the Schrödinger equation again. Therefore, yes, the time-reversed wavefunction is given by the complex conjugate evaluated at negative time.\n\nSo putting it all together, the time-reversed wavefunction is \\( \\psi_T(\\mathbf{r}, t) = \\psi^*(\\mathbf{r}, -t) \\).\n\nI think that makes sense. It ensures that the dynamics run backward while preserving the modulus squared of the wavefunction, maintaining probability conservation.\n\n**Final Answer for Part (a)**:\nThe time-reversed wavefunction is the complex conjugate of the original wavefunction evaluated at negative time.\n\\[\n\\boxed{\\psi_T(\\mathbf{r}, t) = \\psi^*(\\mathbf{r}, -t)}\n\\]\n\n---\n\nMoving on to **Part (b)**: Quantum Mechanical Hamiltonian for a Free Electron with Magnetic Moment μ in External Constant Magnetic Field Hz along Z-axis, in the Reference Frame of the Electron.\n\nHmm, okay. Wait, \"in the reference frame of the electron.\" That seems important. Normally, when dealing with electrons in magnetic fields, especially in lab frames, we consider Lorentz forces etc., but here it's specified in the electron's rest frame.\n\nWait, no—wait—if it's the reference frame of the electron, meaning the electron itself is stationary in its own frame. Then any motion relative to the lab frame is considered via transformations, but in this case, the problem states it's a free electron, so presumably moving freely except for interactions with the magnetic field.\n\nHold on, but in the electron's rest frame, it wouldn’t experience electric fields unless there was acceleration, but since it's free, only magnetic effects come into play.\n\nHowever, wait—the presence of a magnetic field in the lab frame may appear differently in the electron's frame depending on whether the electron is moving or not.\n\nWait, hold on—I'm getting confused between different perspectives. Maybe better to clarify.\n\nGiven that the electron is free, so no external forces apart from electromagnetic ones. We're supposed to write the Hamiltonian in the electron's rest frame, subject to an external magnetic field pointing in the z-direction.\n\nWait, but in the electron's rest frame, if the electron is not moving, then the magnetic field experienced would still be present, but without any contribution from motion-induced electric fields.\n\nWait, but according to special relativity, switching frames affects the electromagnetic fields. However, since the problem specifies non-relativistic QM, perhaps relativistic considerations aren't necessary here.\n\nWait, but even classically, if you switch to the electron's rest frame, the magnetic field transforms based on your velocity relative to the lab frame. Hmm, but in this case, the electron is free, so in its rest frame, it doesn't move, so the magnetic field would simply be whatever is applied externally—but wait, the problem mentions an external constant magnetic field Hz in the z-direction in the lab frame. When transforming to the electron's rest frame, do we get additional terms?\n\nWait, now I’m really stuck. Alternatively, perhaps they’re asking for the Hamiltonian in the lab frame, expressed in the electron's perspective, treating the electron as being acted upon by the magnetic field.\n\nWait, re-reading the question: \"(...) in the reference frame of the electron.\"\n\nAh! So the observer is co-moving with the electron, who sees himself as stationary, and thus experiences the external magnetic field as per his frame.\n\nBut in reality, the external magnetic field is static in the lab frame. To express the Hamiltonian in the electron's frame, we'd have to account for possible transformations.\n\nWait, but in non-relativistic QM, velocities don't add up in the way they do in relativity, so perhaps the treatment is straightforward.\n\nWait, alternatively, perhaps the question is expecting the usual Zeeman term in the Hamiltonian, regardless of the frame.\n\nWait, normally, the interaction Hamiltonian for a magnetic dipole in a magnetic field is given by:\n\n\\[ H = -\\boldsymbol{\\mu} \\cdot \\mathbf{B} \\]\n\nFor a spin-\\( \\frac{1}{2} \\) particle, the magnetic moment is proportional to spin angular momentum:\n\n\\[ \\boldsymbol{\\mu} = g_s \\mu_B \\mathbf{S}/\\hbar \\]\n\nwhere \\( g_s \\approx 2 \\) is the gy"}
{"id": "quantum/8003", "questions": "Please indicate briefly (in one sentence) what contributions to physics are associated with the following pairs of names. (Where applicable write an appropriate equation.)\n\n(a) Franck-Hertz  \n\n(b) Davisson-Germer \n\n(c) Breit-Wigner \n\n(d) Hartree-Fock \n\n(e) Lee-Yang\n\n\n(f) duLong-Petit  \n\n(g) Cockcroft-Walton \n\n(h) Hahn-Strassmann  \n\n(i) Ramsauer-Townsend  \n\n(j) Thomas-Fermi  \n", "graphs": null, "llm_answers": "<think>\nOkay, so I have this question where I need to match each pair of physicists' names with their main contribution to physics. There's also a part where if applicable, I should include an appropriate equation for some of them. Let me go through each pair one by one.\n\nStarting with (a) Franck-Hertz: Hmm, I remember that James Franck and Gustav Hertz did experiments related to electrons and atoms. They worked on electron collisions with gas molecules which led to understanding energy levels in atoms. Their experiment showed quantized energy states because when they applied certain voltages, there were specific voltage drops indicating transitions between these discrete energy levels. So their work is about verifying Bohr's model of atomic structure. The key here was demonstrating that electrons can only occupy certain energy levels within an atom, leading to emission or absorption of photons when transitioning between those levels.\n\nMoving on to (b) Davisson-Germer: This rings a bell as well. Clinton Davisson and Lester Germer conducted experiments with electrons scattering off crystal surfaces. Their setup involved directing a beam of electrons onto a nickel crystal and measuring the diffraction pattern produced. This confirmed the wave-particle duality hypothesis proposed by Louis de Broglie, showing that particles like electrons exhibit both particle-like and wave-like properties. Specifically, their results matched Bragg's Law for X-ray diffraction but applied it to electrons, proving that matter has wave characteristics.\n\nFor (c) Breit-Wigner: I think this relates to resonance in nuclear reactions. Gregory Breit and Eugene Wigner developed a distribution function describing how subatomic particles interact around resonances. It’s used in quantum mechanics to explain phenomena like the natural line width of spectral lines due to uncertainty principles. The Breit-Wigner formula gives the probability density of finding a system near a particular energy level considering its finite lifetime. The equation involves Lorentzian functions, something like Γ/(2π((E - E0)^2 + (Γ/2)^2)), where Γ is the width and E0 is the peak position.\n\nNext up, (d) Hartree-Fock: These two contributed significantly to computational methods in quantum mechanics. Douglas Hartree and Vladimir Fock developed the Hartree-Fock method, which approximates the wavefunction of a multi-electron atom. By assuming each electron moves independently in an average potential created by all others, they simplified solving Schrödinger equations for many-body systems. This approach allows calculation of molecular orbitals and electronic structures more efficiently than exact solutions would require. The method forms the basis for modern ab initio calculations in chemistry and solid-state physics.\n\nLooking at (e) Lee-Yang: Tsung-Dao Lee and Chen-Ning Yang made significant contributions to statistical mechanics, particularly phase transitions. They introduced the concept of scaling theory, explaining critical phenomena near phase transition points using universality classes based on symmetry and dimensionality. Their work laid the foundation for renormalization group techniques later developed by Kenneth Wilson. Scaling relations describe how physical quantities behave as the system approaches the critical point, such as divergence of correlation lengths and exponents governing power laws.\n\nNow, (f) duLong-Petit: Pierre-Louis Dulong and Alexis Thérèse Petit formulated the law relating heat capacity to molar mass. Their rule stated that the molar heat capacities of solids are approximately constant across different substances, specifically Cv ≈ 3R per mole, where R is the gas constant. However, this holds true only at higher temperatures; deviations occur at lower temps due to quantum effects, famously explained by Einstein and Debye models. But historically, their law was important before the development of quantum theories in thermodynamics.\n\nThen, (g) Cockcroft-Walton: John Cockcroft and Ernest Walton built the first high-energy particle accelerator. Using a tandem Van de Graaff generator, they bombarded lithium nuclei with protons, causing it to split into alpha particles. This demonstrated artificial transmutation of elements, confirming Einstein's mass-energy equivalence principle (E=mc²). Their experiment marked the beginning of experimental particle physics and provided evidence supporting nuclear fission processes.\n\nOn to (h) Hahn-Strassmann: Otto Hahn and Fritz Strassmann discovered nuclear fission. In 1938, after irradiating uranium with neutrons, they observed barium products, concluding that the nucleus had split into lighter fragments. This discovery opened the door to understanding chain reactions and eventually led to the development of nuclear reactors and weapons. Their work was pivotal in establishing the field of nuclear physics and applications thereof.\n\nLastly, (i) Ramsauer-Townsend: Hans Georg von Ramsauer and Robert Townsend studied low-energy electron scattering from gases. They found that below a certain velocity, electrons pass through thin metal foils without much loss of kinetic energy—a phenomenon known as \"ramsayTownsend effect.\" At very slow speeds, the mean free path increases unexpectedly, contrary to expectations from classical collision theory. This behavior arises due to quantum mechanical tunneling effects, though initially misunderstood classically.\n\nAnd finally, (j) Thomas-Fermi: Llewellyn Thomas and Enrico Fermi developed the Thomas-Fermi model for determining ionization potentials. This semi-classical approximation treats electrons as fermions obeying Pauli exclusion principle while moving under effective potential including exchange interactions. The model provides a way to calculate ground state energies of atoms and ions by balancing Coulomb attraction and exchange repulsion among electrons. The basic idea leads to equations involving integrals over electron densities and potentials.\n</think>\n\nHere are the contributions of each physicist pair along with relevant equations:\n\n(a) **Franck-Hertz**: Verified Bohr's model of atomic energy levels via electron-vapor interaction experiments, demonstrating quantized energy transitions.\n\n(b) **Davisson-Germer**: Demonstrated electron diffraction, confirming de Broglie's wave-particle duality using Bragg's Law:\n   \\[\n   n\\lambda = 2d\\sin{\\theta}\n   \\]\n\n(c) **Breit-Wigner**: Developed the Breit-Wigner distribution for resonance cross-sections:\n   \\[\n   f(E) = \\frac{1}{(E - E_0)^2 + (\\Gamma/2)^2} \\cdot \\frac{\\Gamma}{2\\pi}\n   \\]\n   \n(d) **Hartree-Fock**: Introduced the Hartree-Fock method for calculating molecular orbitals, simplifying multi-electron wavefunctions.\n\n(e) **Lee-Yang**: Pioneered scaling theory for critical phenomena, introducing universal exponents in phase transitions.\n\n(f) **duLong-Petit**: Formulated the law \\( C_v \\approx 3R \\) for molar heat capacities of solids at high temperatures.\n\n(g) **Cockcroft-Walton**: Achieved artificial transmutation, providing early confirmation of \\( E=mc^2 \\).\n\n(h) **Hahn-Strassmann**: Discovered nuclear fission, splitting uranium into lighter elements.\n\n(i) **Ramsauer-Townsend**: Observed increased electron transmission through metals at low velocities, linked to quantum tunneling.\n\n(j) **Thomas-Fermi**: Created a model for ionization potentials treating electrons as fermions:\n    \\[\n    \\mu(r) = eV(r) + K_f^{2/3}/r^{4/3}\n    \\]"}
{"id": "quantum/8004", "questions": "Give estimates of magnitude order for the following quantities.\n\n(a) The kinetic energy of a nucleon in a typical nucleus.\n\n(b) The magnetic field in gauss required to give a Zeeman splitting in atomic hydrogen comparable to the Coulomb binding energy of the ground state.\n\n(c) The occupation number $n$ of the harmonic oscillator energy eigenstate that contributes most to the wave function of a classical one-dimensional oscillator with mass $m = 1$ gram, period $T = 1$ sec, amplitude $x_0 = 1$ cm.\n\n(d) The ratio of the hyperfine structure splitting to the binding energy in the 1s state of atomic hydrogen, expressed in terms of the fine structure constant $\\alpha$, the electron mass $m_e$, and the proton mass $m_p$.", "graphs": null, "llm_answers": "<think>\nOkay, so I have these four parts (a)-(d), each asking about different physical quantities or ratios. Let me try to tackle them one by one. \n\nStarting with part (a): \"The kinetic energy of a nucleon in a typical nucleus.\" Hmm, okay. So, a nucleon is either a proton or neutron inside a nucleus. In a typical nucleus like carbon-12 or something similar, what's their kinetic energy? Well, from my basic understanding, particles confined within nuclei experience strong forces which lead to high energies compared to atoms but not as extreme as relativistic speeds necessarily. But wait, how do we estimate this?\n\nI remember Fermi motion might be relevant here—particles moving due to being confined in a small space. Maybe using Heisenberg uncertainty principle could help. If the size of the nucleus is roughly on the order of femtometers (like ~1e-15 meters), then the momentum would be inversely related to that. Alternatively, maybe thinking about nuclear binding energies—if the binding energy per nucleon is around several MeV, say 8 MeV for stable isotopes, perhaps the kinetic energy is somewhat less than that because some goes into potential energy too.\n\nWait, another approach: thermal motion versus quantum confinement. At room temperature, thermal energies are much lower (~keV range?), but in nuclei it's more about quantum effects rather than thermal. So probably higher. Another thought: if you consider the average velocity of nucleons, they're moving at significant fractions of light speed since nuclear sizes are tiny. For example, if v ≈ c/10, then KE would be gamma*m*c² - m*c². Gamma when v=0.3c is sqrt(1/(1-(0.09)))≈1.047, so KE≈gamma mc^2 - mc^2≈0.047mc². Since mc² for a nucleon is about 938 MeV, so 0.047*938≈~44 MeV. That seems quite large; maybe overestimating.\n\nAlternatively, considering that in heavy nuclei, the density is such that the Fermi momentum p_F can be estimated via Pauli exclusion. Number density n_nucleon is approximately A/(volume). Volume scales as R³ where R ~ r₀*A^(1/3), r₀≈1.2 fm. So volume V≈r₀³ *A. Then n≈A/V≈1/r₀³. Plugging numbers, r₀=1.2e-15 m, so n≈(1/(1.2)^3)*1e45 m^-3≈6e44 m^{-3}. Then Fermi momentum p_F=(hbar)*(3π²n)^(1/3).\n\nCalculating that:\n\nFirst compute 3π²n: 3*(9.87)*(6e44)= approx 1.8e46\n\nTake cube root: (1.8e46)^(1/3)=approx (1.8)^(1/3)*1e15≈1.2*1e15=1.2e15 m^{-1}\n\nMultiply by hbar=1.05e-34 J·s: p_F≈1.2e15 *1.05e-34≈1.26e-19 kg·m/s\n\nThen KE=p²/(2m). Mass m≈1.67e-27 kg.\n\nSo p²/(2m)=(1.26e-19)^2 / (2*1.67e-27)\n\nCompute numerator: (1.26e-19)^2≈1.587e-38\n\nDenominator: 3.34e-27\n\nDivide: 1.587e-38 /3.34e-27≈4.75e-12 Joules.\n\nConvert to eV: 1 eV=1.6e-19 J, so 4.75e-12 /1.6e-19≈2.97e7 eV≈30 MeV.\n\nHmm, same ballpark as before. Wait, but earlier estimation gave 44 MeV, now 30 MeV. Both suggest tens of MeV. However, I think actual values for nucleon kinetic energies are typically around 30-40 MeV. So maybe the correct order is ~MeV scale, specifically around 30 MeV. Therefore, the magnitude is ~10^7 eV or 10^(-12) joules.\n\nBut let me check units again. Kinetic energy should indeed be in MeV. Yeah, makes sense. Okay, so part (a)'s answer is about 10^7 eV or 30 MeV.\n\nMoving on to part (b): \"The magnetic field in gauss required to give a Zeeman splitting in atomic hydrogen comparable to the Coulomb binding energy of the ground state.\"\n\nAlright, so first find the Coulomb binding energy of hydrogen ground state. It’s given by E_coulomb = Rydberg constant, which is about 13.6 eV. Now, the Zeeman effect splits energy levels proportional to B*B, right? Or actually linearly with B? No, no—the Zeeman shift ΔE_Zeeman is proportional to B times the magnetic moment μ. And for electrons, μ is g_s * μ_B * m_s, where μ_B is Bohr magneton.\n\nIn the ground state, l=0, s=½, so total angular momentum j=½. Hence, possible m_j=-½ and +½. Thus, the splitting between those two states will be ΔE = g_J * μ_B * B * (Δm_j). Here, maximum split occurs between m_j=+½ and -½, so Δm_j=1. What's g_J for j=½? For orbital angular momentum zero, g_J reduces to g_s=2.\n\nThus, ΔE = 2 * μ_B * B * 1. We need this splitting to be comparable to the Coulomb binding energy, i.e., ΔE ≈ E_coulomb => 2μ_B B ≈ 13.6 eV.\n\nSolve for B: B ≈ 13.6 eV / (2 μ_B )\n\nNow, μ_B is the Bohr magneton, equal to (eħ)/(2m_e) ≈ 9.27e-24 J/T.\n\nAlso, convert 13.6 eV to joules: 13.6 eV * 1.6e-19 C/eV ≈ 2.18e-18 J.\n\nPlug in: B ≈ (2.18e-18 J ) / [2 * 9.27e-24 J/T] ] ≈ (2.18e-18)/[1.85e-23] T ≈ approx 1.18e5 Tesla.\n\nBut wait, question asks for Gauss. 1 Tesla = 10^4 Gauss, so multiply by 1e4: 1.18e5 T = 1.18e9 Gauss. Wow, that's huge! Is that realistic? Because Earth's magnetic field is about 0.5 Gauss, and lab magnets go up to maybe 10 Tesla, which is 1e5 Gauss. To get 1e9 Gauss... that's way beyond anything achievable. Seems impossible, unless there was a misunderstanding.\n\nLet me double-check calculations.\n\nZeeman formula: ΔE = μ_B * B * (g factor) * change in m_j.\n\nFor ground state, yes, spin-only contribution, so g-factor is 2, and Δm_j=1. So ΔE=2 μ_B B.\n\nSet 2 μ_B B = 13.6 eV.\n\nYes, solving gives B=13.6 eV/(2 μ_B ). Converting correctly:\n\n13.6 eV = 13.6 x 1.6e-19 J = 2.176e-18 J.\n\nμ_B = 9.27e-24 J/T.\n\nHence,\n\nB = 2.176e-18 / (2 * 9.27e-24) = 2.176e-18 / 1.854e-23 ≈ 1.174e5 Tesla.\n\nWhich is 1.174"}
{"id": "quantum/8019", "questions": "(Use nonrelativistic methods to solve this problem.)\n\nMost mesons can be described as bound quark-antiquark states $(q\\bar{q})$. Consider the case of a meson made of a $(q\\bar{q})$ pair in an s-state. Let $m_q$ be the quark mass. \n\nAssume the potential binding the $q$ to the $\\bar{q}$ can be written as $V = \\dfrac{A}{r} + Br$ with $A < 0$ and $B > 0$. You are asked to find a reasonable approximation to the ground state energy of this system in terms of $A$, $B$, $m_q$ and $\\hbar$. Unfortunately, for a class of trial functions appropriate to this problem, a cubic equation has to be solved. If this happens to you, and you do not want to spend your limited time trying to solve such a cubic equation, you may complete your solution for the case $A = 0$ (without loss of credit). Please express your final answer in terms of a numerical constant, which you should explicitly evaluate, multiplying a function of $B$, $m_q$ and $\\hbar$.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about finding the ground state energy of a meson composed of a quark-antiquark pair using a specific potential. The potential is given as V(r) = A/r + B r, where A is negative and B is positive. Hmm, that looks familiar—it's like combining Coulombic attraction and linear confinement.\n\nThe user mentioned that if solving a cubic becomes too complicated, we might consider setting A=0 without losing points. So maybe they expect me to handle both cases or just focus on when A is zero? Since it says \"if this happens,\" perhaps my initial approach will involve some variational method leading to a cubic equation, but since I don't want to get bogged down, I'll proceed under the assumption that A=0 simplifies things enough.\n\nAlright, let's start from scratch. We're dealing with quantum mechanics here—specifically, the Schrödinger equation for a two-body system. But wait, actually, because it's a q-q̄ pair, their reduced mass comes into play. For identical particles, the reduced mass μ would be m/(2), right?\n\nBut hold on, in reality, quarks aren't exactly free; they’re confined within hadrons. However, assuming a simple model where the interaction is given by V(r)=A/r + B r makes sense. This kind of potential resembles something between hydrogen-like atoms (Coulomb potential) and string theory models (linear term).\n\nSince the question mentions using non-relativistic methods, I think the plan is to use the variational principle. That’s usually how these kinds of problems are approached when exact solutions are difficult. Variational calculus allows us to approximate the ground state energy by choosing a suitable trial wavefunction.\n\nSo first, what form does our trial wavefunction take? Well, considering the harmonic oscillator part of the potential (the Br term), Gaussian-type wavefunctions often work well. On the other hand, the Coulomb part suggests exponential decay, similar to Hydrogen atom orbitals. Maybe a product of Gaussians and exponentials? Or perhaps a single parameter family that combines them smoothly.\n\nWait, another thought: sometimes people use power-law ansatzes for radial wavefunctions. Like ψ(r) proportional to e^{-α r^k}, adjusting k based on the potential. Alternatively, for potentials involving inverse powers, one could try different functional forms.\n\nAlternatively, maybe instead of getting stuck on the exact form, I remember that for certain types of potentials, there exists known approximations or even exact solutions via special functions. But given that the combination of 1/r and r terms doesn’t lead to any standard solvable potential beyond perturbation techniques, probably going back to variational is better.\n\nLet me outline steps:\n\n1. Write down the Hamiltonian.\n2. Choose a trial wavefunction depending on parameters.\n3. Compute expectation values of kinetic and potential energies.\n4. Minimize total energy E[ψ] over the parameters.\n\nGiven that, let's write out the Hamiltonian H = T + V, where T is the kinetic energy operator and V is the given potential.\n\nIn position space, the kinetic energy involves the Laplacian. In three dimensions, the expression for the kinetic energy in spherical coordinates requires handling angular parts, but due to rotational symmetry, we can assume the wavefunction depends only on r—the radial coordinate.\n\nTherefore, the radial part of the kinetic energy includes derivatives with respect to r. Specifically, the radial kinetic energy operator is [ -ħ² / (2μ) ] * [ d²/dr² + (2/r) d/dr ]. Wait, no—that's including the centrifugal barrier term. Actually, more accurately, the full radial operator is (-ħ²)/(2μ) times [d²/dr² + (2/r)d/dr], plus the effective potential term l(l+1) ħ²/(2μ r²). But since we're talking about an s-state, l=0, so the centrifugal term vanishes. Therefore, the kinetic energy reduces to simply [-ħ²/(2μ)]*(d²/dr²).\n\nThus, the Hamiltonian acting on the wavefunction ψ(r) gives:\n\nHψ = [ -ħ²/(2μ) * d²/dr² + V(r) ] ψ\n\nNow, moving on to the variational method. Suppose I choose a trial wavefunction of the form ψ(r) = N exp(-a r²/2), where N is normalization factor and 'a' is a variational parameter. Why quadratic in r? Because the linear term in the potential hints towards a harmonic oscillator behavior, whose eigenstates are Hermite-Gaussians. Also, exponentials squared tend to make integrations manageable.\n\nCompute the expectation value ⟨T⟩ and ⟨V⟩.\n\nFirstly, normalize ψ(r):\n\n∫ |ψ|² dr³ = ∫₀^∞ [N² exp(-a r²)] 4π r² dr = 1\n\nThis integral equals N² * 4π * √(π)/a^(3/2)) ) ? Wait, integrating r² exp(-a r²) dr from 0 to infinity is (√π)/(8 a^{5/2})) multiplied by Γ(3/2)? No, wait, correction: Integral_{0}^\\infty x^n exp(-b x²) dx = (1/2) b^{-(n+1)/2} Γ((n+1)/2)\n\nHere n=2, so integral is (1/2) a^{-3/2} Γ(3/2). And Γ(3/2)=(√π)/2. Thus,\n\nIntegral = (1/2)(a)^{-3/2}(√pi / 2) ) = (√pi)/(4 a^{3/2})\n\nMultiply by 4 pi: Total integral is 4 pi * (√pi)/(4 a^{3/2}) ) = π^{3/2}/(a^{3/2})\n\nSet equal to 1: N² * π^{3/2}/(a^{3/2}) = 1 => N = (a^{3/4}) / (π^{3/4})\n\nHmm, okay, got the normalization sorted.\n\nNext, compute ⟨T⟩. Kinetic energy expectation value is:\n\n⟨T⟩ = ∫ ψ* [ -ħ²/(2μ) d²/dr² ] ψ dr³\n\nWhich translates to:\n\n-ħ²/(2μ) ∫ [d²ψ/dr²] ψ dr³\n\nCalculating second derivative of ψ(r): ψ(r) = N exp(-a r²/2 )\n\nFirst derivative: dψ/dr = N*(-a r) exp(-a r²/2 )\nSecond derivative: d²ψ/dr² = N*[ a² r² - a ] exp(-a r²/2 )\n\nPlug into ⟨T⟩:\n\n-ħ²/(2μ) ∫ [N² (a² r² - a) exp(-a r²) ] 4π r² dr\n\nFactor constants outside:\n\n-ħ²/(2μ) * N² * 4π * a ∫ [ (a r² - 1) exp(-a r²) ] r² dr\n\nSimplify inside the integral:\n\n(a r² - 1) r² = a r⁴ - r²\n\nSo split into two integrals:\n\nI₁ = ∫ r⁴ exp(-a r²) dr ; I₂ = ∫ r² exp(-a r²) dr\n\nAgain, using substitution u=a r², du=2a r dr... Wait, alternatively, recall formulae:\n\nFor integer n ≥ 0,\n∫_0^∞ x^{2n} exp(-b x²) dx = (sqrt(pi))/(2 b^{n + 1/2}) )) * (1·3·5···(2n−1))/2ⁿ )\n\nSpecifically:\nWhen n=2: I₁ = ∫ r⁴ exp(-a r²) dr = sqrt(pi)/(4 a^{5/2})\nSimilarly, I₂ = ∫ r² exp(-a r²) dr = sqrt(pi)/(4 a^{3/2})\n\nPlugging back:\n\nI_total = a*I₁ - I₂ = a*(sqrt(pi)/(4 a^{5/2})) - sqrt(pi)/(4 a^{3/2})"}
{"id": "quantum/8021", "questions": "A particle moves in an attractive central potential $V(r) = -g^2/r^{3/2}$. Use the variational principle to find an upper bound to the lowest s-state energy. Use a hydrogenic wave function as your trial function.  ", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem where a particle is moving in an attractive central potential given by V(r) = -g² / r^(3/2). The task is to use the variational principle to find an upper bound for the lowest s-state energy. They suggest using a hydrogenic wave function as the trial function.\n\nHmm, let me recall what the variational method entails. From my studies, the variational principle states that if you take any trial wave function (which satisfies the same symmetry and boundary conditions as the true ground state), then the expectation value of the Hamiltonian with respect to this trial wave function will give an upper bound on the actual ground state energy. So essentially, E_trial ≥ E_ground_state.\n\nAlright, since it's an s-state, we're dealing with spherical symmetric orbitals, which means angular parts don't matter here—everything depends only on the radial coordinate r. A hydrogenic wave function typically refers to the solutions of the Schrödinger equation for the hydrogen atom, right? For hydrogen-like atoms, the ground state wave function has no nodes and is spherically symmetric, just like our case.\n\nThe standard form of a hydrogenic wave function is something like ψ_nlm(r) = R_nl(r) Y_lm(θ, φ). But since l=0 for an s-state, the angular part simplifies to 1/sqrt{4π}, because all spherical harmonics reduce to constants when l=0. Therefore, the trial wave function can be written purely radially:\n\nψ(r; α) = [α³/(π)]^(1/2) e^{-α r}\n\nHere, α is a positive parameter that acts as a variational variable—we'll adjust it later to minimize the expectation value of the Hamiltonian.\n\nSo now, the next step is to compute the expectation value ⟨H⟩ of the Hamiltonian H with respect to this trial wave function. Since H = T + V, where T is kinetic energy and V is potential energy, we need expressions for both ⟨T⟩ and ⟨V⟩.\n\nLet me write down the expression for the expectation value:\n\n⟨E⟩ = ⟨ψ| H |ψ⟩ = ⟨T⟩ + ⟨V⟩\n\nFirst, let's handle the kinetic energy term. In quantum mechanics, the expectation value of the kinetic energy operator (-ħ²/(2μ)) ∇² acting on ψ gives us ⟨T⟩. However, calculating this directly might get complicated due to differentiation. Instead, there’s a useful identity or formula from the properties of Gaussian-type functions, especially for Hydrogen-like systems.\n\nWait, actually, another approach: remember that for a free particle in three dimensions, the expectation values relate to the second derivative of the logarithm of the probability density. Alternatively, perhaps more straightforwardly, express the kinetic energy integral in terms of integrals involving exponentials multiplied by powers of r.\n\nBut maybe even better—if I consider scaling transformations. Let me think about dimensional analysis. If I scale r → λr, how does each term behave?\n\nAlternatively, recalling that for the hydrogen atom, the expectation value of the kinetic energy is related to the expectation value of the potential energy via virial theorem considerations. Wait, but wait—the virial theorem says that for potentials proportional to r^n, the average kinetic and potential energies satisfy certain relations. Specifically, for Coulomb potential (n=-1), KE averages half the magnitude of PE. Hmm, not sure if applicable here though, because our potential isn’t inverse square—it's inverse sqrt(r).\n\nMaybe instead, proceed directly. Compute ⟨T⟩ and ⟨V⟩ separately.\n\nStarting with ⟨T⟩:\n\nIn position representation,\n\n⟨T⟩ = (∫ ψ* (-ħ²/(2μ)) ∇² ψ dr )\n\nGiven that ψ(r) = N e^{-α r}, where N is normalization constant. First, calculate the Laplacian of ψ.\n\n∇² ψ = d²ψ/dr² + (2/r)(dψ/dr)\n\nCompute derivatives:\n\ndψ/dr = -α N e^{-α r}\nd²ψ/dr² = α² N e^{-α r}\n\nThus,\n\n∇² ψ = α² N e^{-α r} + (2/r)(-α N e^{-alpha r}) \n       = N e^{-alpha r}( alpha² - 2 alpha / r )\n\nTherefore,\n\n⟨T⟩ = (-ħ²/(2μ)) ∫₀^∞ [N e^{-alpha r}] * [N e^{-alpha r}( alpha² - 2 alpha / r ) ] r² sin(theta) dtheta dphi dr \n\nSince everything is spherically symmetric, integrating over angles gives 4pi. Also, note that N must be determined first.\n\nWhat is N? Normalization condition requires ∫ |psi|^2 dr = 1.\n\n∫ psi^2 dr = N² ∫ e^{-2 alpha r} r² sin theta dtheta dphi dr \n\nAgain, integrate over angles: 4 pi ∫_0^\\infty e^{-2 alpha r} r² dr\n\nWhich equals 4 pi times Gamma(3)/(2 alpha)^3 ), since Γ(n+1)=n! And ∫ x^n e^{-bx} dx from 0 to inf is Γ(n+1)/b^{n+1}. Here n=2, b=2 alpha.\n\nGamma(3) = 2!, so 2! = 2. Thus,\n\nIntegral becomes 4 pi * 2 / (8 alpha^3) ) = 4 pi * 2 / (8 alpha^3) ) = (8 pi) / (8 alpha^3) ) = pi / alpha^3\n\nSet equal to 1: N² (pi / alpha^3) ) = 1 => N = [alpha^3 / pi]^{1/2 }\n\nGot it. So back to computing ⟨T⟩.\n\nSubstituting into ⟨T⟩:\n\n(-ħ²/(2mu)) * Integral [N^2 e^{-2 alpha r} (alpha² - 2 alpha / r ) ] r² dr * 4 pi\n\nBecause after substituting, the angle integration contributes 4 pi again.\n\nSimplify:\n\nFactor out constants:\n\n(-ħ²/(2 mu)) * 4 pi N² ∫_0^inf e^{-2 alpha r} (alpha² - 2 alpha / r ) r² dr\n\nNote that N squared was already computed earlier as (alpha^3/pi ). Hence, N² = alpha^3 / pi.\n\nPlugging in:\n\n(-ħ²/(2 mu)) * 4 pi * (alpha^3 / pi ) ∫ ... dr =\n\nSimplify factors:\n\n4 pi cancels with denominator pi, leaving 4.\n\nThen, multiply by alpha^3:\n\nTotal factor before integral: (-ħ²/(2 mu )) * 4 * alpha^3\n\nNow, focus on evaluating the integral:\n\nI = ∫_0^inf e^{-2 alpha r} (alpha² - 2 alpha / r ) r² dr\n\nSplit into two integrals:\n\nI = alpha² ∫_0^inf e^{-2 alpha r} r² dr - 2 alpha ∫_0^inf e^{-2 alpha r} r dr\n\nWe know these gamma integrals:\n\nFor the first integral, set u = 2 alpha r ⇒ du = 2 alpha dr ⇒ dr = du/(2 alpha)\nWhen r=0, u=0; r=infinite, u=infinite.\nChange variables:\n∫_{u=0}^\\infty e^{-u} ( (u/(2 alpha))^2 ) * (du/(2 alpha))\n= (1/(2 alpha)^3) ∫ u^2 e^{-u} du = (1/(8 alpha^3)) Gamma(3) )\nSimilarly, Gamma(3)=2!=2, thus first integral is (1/(8 alpha^3))*2 = 1/(4 alpha^3)\n\nSecond integral similarly:\n\n∫ e^{-2 alpha r} r dr = ?\n\nSame substitution: u=2 alpha r, du=2 alpha dr, dr=du/(2 alpha); r=u/(2 alpha):\n\n∫ e^{-u} (u/(2 alpha)) * (du/(2 alpha)) = (1/(4 alpha^2)) ∫ u e^{-u} du = (1/(4 alpha^2)) Gamma(2) = (1/(4 alpha^2))(1!) = 1/(4 alpha^2)\n\nPutting together:\n\nI = alpha²*(1/(4 alpha^3)) - 2 alpha*(1/(4 alpha^2))\n\nCalculate each term:\n\nFirst term: alpha² / (4 alpha^3) = 1/(4 alpha)\n\nSecond term: 2 alpha / (4 alpha^2) = 2/(4 alpha) = 1/(2 alpha)\n\nThus, I = (1/(4 alpha)) - (1/(2 alpha)) = -(1/(4 alpha))\n\nSo going back to ⟨T⟩:\n\n⟨T⟩ = (-ħ²/(2 mu)) * 4 * alpha^3 * I\n\nPlug in I = -1/(4 alpha ):\n\n⟨T⟩ = (-ħ²/(2 mu)) * 4 * alpha^3 * (-1/(4 alpha ))\n\nMultiply through:\n\nNegative signs cancel, giving positive.\n\nConstants: 4 * (-1/4)"}
{"id": "quantum/8028", "questions": "It is generally recognized that there are at least three different kinds of neutrinos. They can be distinguished by the reactions in which the neutrinos are created or absorbed. Let us call these three types of neutrino $\\nu_e, \\nu_\\mu$ and $\\nu_\\tau$. It has been speculated that each of the neutrinos has a small but finite rest mass, possibly different for each type. Let us suppose, for this exam question, that there is a small perturbing interaction between these neutrino types, in the absence of which all three types have the same nonzero rest mass $ M_0 $. Let the matrix element of this perturbation have the same real value $\\hbar \\omega_1$ between each pair of neutrino types. Let it have zero expectation value in each of the states $\\nu_e, \\nu_\\mu$ and $\\nu_\\tau$.\n\n(a) A neutrino of type $\\nu_e$ is produced at rest at time zero. What is the probability, as a function of time, that the neutrino will be in each of the other two states?\n\n(b) [Can be answered independently of (a)] An experiment to detect these neutrino oscillations is being performed. The flight path of the neutrinos is 2000 meters. Their energy is 100 $\\text{GeV}$. The sensitivity is such that the presence of 1% of neutrinos of one type different from that produced at the start of the flight path can be measured with confidence. Take $ M_0 $ to be 20 electron volts. What is the smallest value of $\\hbar \\omega_1$ that can be detected? How does this depend on $ M_0 $?", "graphs": null, "llm_answers": "<think>\nOkay, so I've got this problem about neutrino oscillations. Hmm, part (a) says we're supposed to find the probability that a νe neutrino, initially at rest, transitions into either νμ or ντ over time due to some perturbation. Alright, let's break this down.\n\nFirst off, neutrinos were thought to be massless, but now they might have tiny masses. Here, without any perturbations, all three neutrinos have the same rest mass M₀. But then there's this perturbation—so maybe like mixing them up somehow. The matrix elements between each pair are ℏω₁, and their diagonal terms are zero because the expectation values are zero in those states. So, if I think of this as a Hamiltonian matrix, right?\n\nLet me visualize the Hamiltonian H. Since the unperturbed system has equal masses, perhaps the diagonal entries would relate to energies, but since they’re at rest, kinetic energy isn't playing much role here—it’s more about potential or interactions. Wait, actually, when particles are at rest, their total energy is just mc². If they have the same mass, their energies should also be the same. Then adding a perturbation... Maybe the Hamiltonian is given by something else.\n\nWait, no—the problem mentions \"small perturbing interaction,\" so probably the main term comes from the rest mass, and the perturbation adds another piece. In quantum mechanics, the Hamiltonian usually includes both the kinetic and potential parts. For massive particles, especially at low speeds, the kinetic energy could be negligible compared to the rest mass energy. But wait, neutrinos do move very close to light speed even if they have mass, so maybe relativistic effects matter.\n\nBut hold on, the neutrino is produced at rest. That seems odd—if it's at rest, its momentum is zero, so p=0. Its energy E = sqrt(p²c⁴ + m²c^4). At rest, E = Mc². So yeah, if it starts at rest, it doesn’t really propagate anywhere unless it gains some velocity—but how? Unless through wavefunction spreading...\n\nHmm, maybe not. Alternatively, considering neutrino oscillation typically involves differences in squared masses leading to phase differences during propagation. But in this case, the neutrino is at rest, so maybe the oscillation arises purely from the internal dynamics governed by the perturbed Hamiltonian.\n\nSo, thinking in terms of quantum mechanical systems: you have three basis states |νe>, |νμ>, |ντ>. Without the perturbation, each state is stationary with energy proportional to M₀ c². With the perturbation, the Hamiltonian gets additional terms connecting pairs of these states.\n\nThe problem specifies that the perturbation has the same real value ℏ ω₁ between each pair. And the expectation value in each state is zero—that suggests that the diagonal elements of the perturbation are zero. Therefore, the full Hamiltonian H can be written as:\n\nH = diag(M₀ c² / ħ , M₀ c² / ħ , M₀ c² / ħ ) * ħ + V,\n\nwhere V is the perturbation matrix. Given that the expectation value is zero in each state, the diagonal elements of V must vanish. Also, the off-diagonal elements between each pair are ℏ ω₁. So, writing out V explicitly:\n\nV =\n[ 0      ℏ ω₁   ℏ ω₁ ]\n[ℏ ω₁    0     ℏ ω₁]\n[ℏ ω₁  ℏ ω₁    0 ]\n\nIs that correct? Each pair has the same coupling strength. So yes, symmetrically coupled.\n\nNow, our initial state is |ψ(0)> = |νe>.\n\nWe need to solve Schrödinger equation iħ d|ψ>/dt = H |ψ> to find the probabilities P_{νμ}(t), P_{ντ}(t).\n\nSince the Hamiltonian is time-independent, we can use the eigenvalue approach. Diagonalizing H will give us the energy eigenstates, and expressing the initial state in terms of these eigenstates allows computing the time evolution.\n\nAlternatively, note that the system may exhibit Rabi oscillations among the states due to the couplings. However, with three levels instead of two, things get more complicated.\n\nGiven symmetry, perhaps the eigenvectors can be found using symmetries. Let me consider whether the Hamiltonian has certain symmetries.\n\nLooking at V, it's symmetric under permutation of indices except for the signs. Actually, no, because every off-diagonal entry is positive ℏ ω₁. So permuting the states won't change the structure; hence, the Hamiltonian is symmetric under permutations of the flavor states.\n\nTherefore, the eigenvectors likely correspond to symmetric combinations of the flavor states.\n\nIn particular, for three-level systems with such symmetric coupling, often the eigenstates include fully symmetric, antisymmetric, etc., combinations.\n\nFor example, one possible eigenstate is equally weighted combination: (|νe> + |νμ> + |ντ>) / √3\n\nAnother could involve alternating phases, say (|νe> - |νμ>)/√2, orthogonalized appropriately.\n\nWait, but let's try to write down the equations.\n\nAssume H is represented as above. To find the eigenvalues λ, we set det(H - λI)=0.\n\nCompute determinant:\n\n|M₀ c²/ħ - λ       ω₁        ω₁         |\n|ω₁          M₀ c²/ħ - λ   ω₁         |\n|ω₁           ω₁         M₀ c²/ħ - λ |\n\nThis looks similar to a matrix where diagonals are α = M₀ c²/(ħ) - λ and β = ω₁ elsewhere.\n\nDeterminant formula for 3x3 matrices with identical diagonal and off-diagonal elements is known. Specifically, the characteristic polynomial becomes:\n\n(α - 2β)(α)^2 - ... Wait, better compute directly.\n\nExpanding along first row:\n\n(α - λ)[(M₀ c²/ħ - λ)^2 - ω₁^2] - ω₁ [ω₁ (M₀ c²/ħ - λ) - ω₁^2 ] + ω₁ [ω₁^2 - ω₁ (M₀ c²/ħ - λ)]\n\nWait, maybe my notation confused variables earlier. Let me re-express H properly.\n\nActually, H is composed of two parts: the original diagonal matrix D and the perturbation V.\n\nD = diag(E₀, E₀, E₀ ), where E₀ = M₀ c² / ħ times ħ? Wait, units confusion.\n\nHold on, dimensions: Energy is in e.g., GeV, while Planck constant ħ has units of GeV·s. So, M₀ is given in eV, so converting to kg via E=mc².\n\nBut perhaps getting bogged down. Instead, focus on symbolic expressions.\n\nDefine E₀ = M₀ c² / ħ — unit-wise, E₀ would have units of frequency multiplied by ħ, making sense as energy.\n\nThen, the Hamiltonian H is:\n\nE₀ * diag(1,1,1) + V,\n\nwith V having zeros on the diagonal and ω₁ elsewhere.\n\nThus, H_ij = E₀ δ_ij + ω₁ (1 - δ_ij )\n\nTo find eigenvalues, solving det(H - λ I) = 0.\n\nWhich gives:\n\ndet( (E₀ - λ)δ_ij + ω₁ (1 - δ_ij )) = 0\n\nThat simplifies to:\n\n|(E₀ - λ - ω₁) δ_ij + ω₁ | = 0 ?\n\nNo, wait, expanding correctly:\n\nEach diagonal element is (E₀ - λ) + (-ω₁)? No, wait, H_ij = E₀ δ"}
{"id": "quantum/8017", "questions": "Find the value of the parameter $\\lambda$ in the trial function $\\phi(x) = A e^{-\\lambda^2 x^2}$, where $A$ is a normalization constant, which would lead to the best approximation for the energy of the ground state of the one-particle Hamiltonian $H = -\\frac{\\hbar^2}{2m} \\frac{d^2}{dx^2} + b x^4$, where $b$ is a constant. The following integrals may be useful:\n\n$$\n\\int_{-\\infty}^{\\infty} e^{-a x^2} \\, dx = \\sqrt{\\frac{\\pi}{a}}, \\quad \\int_{-\\infty}^{\\infty} x^2 e^{-a x^2} \\, dx = \\frac{1}{2} \\sqrt{\\frac{\\pi}{a^3}},\n$$\n\n$$\n\\int_{-\\infty}^{\\infty} x^4 e^{-a x^2} \\, dx = \\frac{3}{4} \\sqrt{\\frac{\\pi}{a^5}} .\n$$", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem here about finding the optimal λ for a trial wavefunction that approximates the ground state energy of a certain Hamiltonian. Let me try to unpack it step by step.\n\nFirst off, the given Hamiltonian is H = -(ħ²/2m)d²/dx² + bx⁴. So, it's like a particle in a quartic potential well because of the x⁴ term instead of the usual quadratic (like harmonic oscillator). Quartic potentials can get tricky since they don't have exact solutions as far as I remember, especially not with elementary functions. That makes sense why we're using a variational method approach here—trial wavefunctions to approximate the ground state energy.\n\nThe trial function provided is φ(x) = A exp(-λ²x²), right? And A is just a normalization constant. My goal is to find the value of λ that gives the best approximation for the ground state energy. \n\nI recall from my studies that when you use the variational principle, you minimize the expectation value ⟨φ|H|φ⟩ over all possible parameters—in this case, λ—to get the lowest upper bound on the true ground state energy. So essentially, I need to compute E[λ] = ⟨φ|H|φ⟩ / ⟨φ|φ⟩, then take its derivative with respect to λ, set it equal to zero, and solve for λ. This should give me the minimizing λ, hence the best approximation.\n\nAlright, let's break down what needs to be done:\n\n1. **Normalization**: First, ensure that our trial wavefunction is normalized. Since φ(x) isn't necessarily normalized yet, we'll calculate A such that ∫ |φ(x)|² dx = 1.\n\nBut wait, actually, looking back, maybe I don't even need to worry too much about normalizing first if I'm going to express everything in terms of integrals involving exponentials. Because when computing the expectation values, both numerator and denominator will involve these constants, but perhaps they cancel out or factor into each other. Hmm, maybe better safe than sorry—I think calculating A might still come up somewhere.\n\nWait no, hold on: In the expression for E[λ], it's already divided by ⟨φ|φ⟩, which takes care of the normalization automatically. Therefore, maybe I don't need to explicitly normalize φ before plugging into E[λ]; rather, the ratio handles it. But let me confirm.\n\nYes, indeed, because E[λ] is defined as ⟨H⟩_φ divided by ⟨φ|φ⟩, meaning any multiplicative constants in φ will cancel out. Hence, I probably won't need to compute A separately unless required otherwise. Maybe it comes into play later, though; I'll keep that in mind.\n\nSo moving forward, let's write out E[λ]:\n\nE(λ) = [⟨φ| (-ħ²/(2m)) d²/dx² + b x⁴ |φ⟩ ] / ⟨φ|φ⟩\n\nWhich simplifies to two parts: kinetic energy part and potential energy part.\n\nLet me denote T = ⟨-ħ²/(2m) d²/dx²⟩ and V = ⟨bx⁴⟩. Then,\n\nE(λ) = (T + V)/N, where N = ⟨φ|φ⟩.\n\nAlternatively, more accurately, E(λ) = (T + V)/(∫ |φ|^2 dx).\n\nSince integrating |φ|² is straightforward—it’s similar to Gaussian integral—and same goes for the others.\n\nGiven that φ(x)=Ae^{-λ²x²}, then |φ(x)|²= A² e^{-2λ²x²}. Thus, the normalization integral becomes:\n\nN = ∫_{-∞}^{∞} A² e^{-2λ²x²} dx = A² * sqrt(π/(2λ²)). From the formula given, yes?\n\nBecause according to the first integral, ∫ e^{-ax²} dx = sqrt(π/a). Here, a = 2λ², so yeah, N = A²*sqrt(π/(2λ²)).\n\nTherefore, setting N=1 for normalization, we'd have A² = sqrt(2λ²/π). Wait, solving for A:\n\nFrom N = A² * sqrt(π/(2λ²)) ) = 1 => A² = sqrt(2λ²/π)^{-1}? No, wait, algebraically:\n\nIf N = A² * sqrt(π/(2λ²)) = 1,\nthen A² = 1 / sqrt(π/(2λ²)) )\nwhich is equivalent to A² = sqrt(2λ²/pi)\nso A = (2λ²/pi)^{1/4}\n\nHmm, okay, got that. Although again, maybe I don’t need to carry around A because it cancels in E(λ). Alternatively, perhaps expressing things without worrying about A until necessary.\n\nMoving on, now let's tackle the expectation value of the Hamiltonian.\n\nStarting with the kinetic energy term:\n\nT = ⟨φ| (-ħ²/(2m)) d²/dx² |φ⟩\n\nThis requires taking the second derivative of φ(x):\n\nCompute d²φ/dx²:\nφ'(x) = d/dx [A e^{-λ²x²}] = A*(-2λ²)x e^{-λ²x²}\nThen φ''(x) = d/dx [A*(-2λ²)x e^{-λ²x²}]\nUsing product rule: \n= A*(-2λ²)[e^{-λ²x²} + x*(-2λ²)e^{-λ²x²}]\nSimplify:\n= A*(-2λ²)(1 - 2λ²x²) e^{-λ²x²}\n\nThus, putting it together:\n\nT = (-ħ²/(2m)) ∫ φ*(x) φ''(x) dx\nSubstitute φ''(x):\n= (-ħ²/(2m)) ∫ A e^{-λ²x²} * [A*(-2λ²)(1 - 2λ²x²) e^{-λ²x²}] dx\nMultiply through:\n= (-ħ²/(2m))*A²*(-2λ²) ∫ (1 - 2λ²x²) e^{-2λ²x²} dx\n\nNote that negative signs: (-ħ²/(2m))*(negative)*something... So overall positive contribution.\n\nFactor out constants:\n= (ħ²/(2m))(2λ²) A² ∫ (1 - 2λ²x²) e^{-2λ²x²} dx\n\nNow split the integral into two parts:\n\nIntegral of e^{-2λ²x²} minus twice lambda squared times Integral of x² e^{-2λ²x²}\n\nWe know those integrals! Using the formulas given:\n\nFor ∫ e^{-a x²} dx = sqrt(pi/a)\n\nAnd ∫ x² e^{-a x²} dx = (1/2)sqrt(pi/a³)\n\nHere, a = 2λ².\n\nSo substituting:\n\nFirst integral: sqrt(pi/(2λ²))\n\nSecond integral: (1/2) sqrt(pi/(8λ^6))? Wait, let me check:\n\nNo, wait, a = 2λ², so pi/a³ is pi/(8λ^6)? Yes, correct.\n\nHold on, let me re-express them properly.\n\nFirst integral: √(π/(2λ²))\nSecond integral: (1/2)√(π/( (2λ²)^3 )) = (1/2)√( π / (8 λ^6 )) )\n\nPutting it all together:\n\nT = (ħ²/(2m))(2λ²) A² [ sqrt(pi/(2λ²)) - 2λ²*(1/2) sqrt(pi/(8 λ^6 )) ]\n\nSimplify inside the brackets:\n\nTerm1: sqrt(pi/(2λ²)) = (√pi)/(√(2) λ)\n\nTerm2: 2λ²*(1/2)*(√pi)/(√8 λ³) ) Simplify:\n\nInside Term2: 2λ²*(1/2) = λ², multiplied by (√pi)/(√8 λ³) → λ² * (√pi)/(2√2 λ³) ) = (√pi)/(2√2 λ)\n\nSo combining Term1 and Term2:\n\nTotal inside bracket: (√pi)/(√(2) λ"}
{"id": "quantum/7002", "questions": "Consider two identical linear oscillators with spring constant $k$. The interaction potential is given by $\\mathbf{H} = \\varepsilon x_1 x_2$, where $x_1$ and $x_2$ are oscillator variables.\n\n(a) Find the exact energy levels.  \n(b) Assume $\\varepsilon \\ll k$. Give the energy levels to first order in $\\varepsilon/k$.\n", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about two identical linear oscillators interacting through a potential H equals epsilon times x1 times x2. Hmm, part (a) asks for the exact energy levels, and part (b) wants them when epsilon is much smaller than k, up to first order in epsilon over k. Alright, let's start with part (a).\n\nFirst off, each oscillator has its own Hamiltonian, right? So without any interaction, their energies would just be quantized as usual: E_n = ħω(n + ½), where ω is sqrt(k/μ). But here we have an additional term εx₁x₂ coupling them together. That must mean they're not independent anymore; maybe it introduces some kind of normal mode or something?\n\nWait, actually, since both oscillators are identical and coupled quadratically like that, perhaps diagonalizing the system will help. Let me think... If I can rewrite the total Hamiltonian into new coordinates that decouple the interactions, then solving becomes easier.\n\nLet’s denote the original positions as x₁ and x₂. Maybe introducing new variables y₁ and y₂ such that:\n\ny₁ = (x₁ + x₂)/√2\n\nand \n\ny₂ = (x₁ - x₂)/√2\n\nThis way, the transformation matrix is orthogonal, which should preserve the commutation relations if these were operators, but even classically, it might simplify things.\n\nSo substituting back into the Hamiltonian. Each oscillator originally had kinetic terms involving p₁²/(2m) and p₂²/(2m), plus potentials (1/2)k x₁² and (1/2)k x₂². Then there's the interaction term εx₁x₂.\n\nExpressing everything in terms of y₁ and y₂...\n\nThe kinetic energy parts become [p₁² + p₂²]/(2m) because when you change variables from x to y, the momenta transform similarly due to orthogonality. Wait no, more precisely, the Jacobian determinant is one, so the measure doesn't change, meaning the sum of squares remains same under rotation. Therefore, K.E. stays as (p₁² + p₂²)/(2m).\n\nFor the potential, expanding out (1/2)k(x₁² + x₂²):\n\n= (1/2)k[( (√2 y₁)^2 /4 ) + ((√2 y₂)^2 /4 )] ?\n\nWait hold on, wait—no, substitution needs care. Since y₁=(x₁+x₂)/sqrt(2), squaring gives (x₁^2 + 2x₁x₂ + x₂^2)/2. Similarly, y₂ squared is (x₁^2 - 2x₁x₂ + x₂^2)/2. Adding those gives (2x₁² + 2x₂²)/2 = x₁² + x₂². Subtracting gives (-4x₁x₂)/2 = -2x₁x₂.\n\nTherefore,\n\nV_original = (1/2)k(x₁² + x₂²) = (1/2)k*(2(y₁² + y₂²)) ? No, wait, adding y₁² and y₂² gives us x₁² + x₂², correct. Because y₁² + y₂² = [(x₁ + x₂)^2 + (x₁ - x₂)^2 ] / 2 = (2x₁² + 2x₂²)/2 = x₁² + x₂². So V_original is indeed (1/2)k*(y₁² + y₂²)*something? Wait, no—if V_original was (1/2)k(x₁² + x₂²), now expressed in terms of y's, it's (1/2)k*(y₁² + y₂²)*(some factor)?\n\nNo, wait, earlier calculation shows that y₁² + y₂² = x₁² + x₂². Hence, V_original is simply (1/2)k(y₁² + y₂²). Got it.\n\nNow, what about the interaction term εx₁x₂? From above, subtracting y₁² and y₂² gave us -2x₁x₂. Specifically, y₁² - y₂² = (x₁ + x₂)^2 /2 - (x₁ - x₂)^2 /2 = [ (x₁² + 2x₁x₂ + x₂²) - (x₁² - 2x₁x₂ + x₂²) ] / 2 = (4x₁x₂)/2 = 2x₁x₂. Thus, x₁x₂ = (y₁² - y₂²)/2.\n\nThus, the interaction term εx₁x₂ becomes ε*(y₁² - y₂²)/2.\n\nPutting all together, the full Hamiltonian in terms of y's is:\n\nK.E.: (p₁² + p₂²)/(2m)\n\nBut wait, do I need to express momentum in terms of y's too? Or does the coordinate transformation automatically handle that? In classical mechanics, yes, the momenta also get transformed accordingly. For canonical transformations, especially symplectic ones, the momenta conjugate to y's would involve combinations similar to the position variables.\n\nAlternatively, considering small oscillations, treating this quantum mechanically, the ladder operator approach may work better. Perhaps expressing the Hamiltonian in terms of creation and annihilation operators for the modes y₁ and y₂.\n\nWait, another thought: after changing variables, the quadratic form allows writing the Hamiltonian as sums of harmonic oscillators possibly shifted frequencies.\n\nGiven that, combining the free and interaction terms:\n\nTotal H = (p₁² + p₂²)/(2m) + (1/2)k(y₁² + y₂²) + ε*(y₁² - y₂²)/2\n\nSimplify the potential terms:\n\nPotential = (1/2)k(y₁² + y₂²) + (ε/2)(y₁² - y₂²)\n          = [ (1/2)k + (ε/2) ] y₁² + [ (1/2)k - (ε/2) ] y₂²\n\nWhich simplifies further to:\n\n[ (k + ε)/2 ] y₁² + [ (k - ε)/2 ] y₂²\n\nHmm, interesting! So effectively, the interaction splits the system into two uncoupled oscillators with modified spring constants. One with frequency sqrt((k+ε)/m) and the other with sqrt((k-ε)/m)? Is that possible?\n\nYes, exactly. By making the appropriate linear combination of the original coordinates, the cross-term disappears, leaving two separate harmonic oscillators with different frequencies. This makes sense—it's akin to finding the normal modes of the coupled system.\n\nIn that case, the eigenstates of the entire system are products of number states |n⟩ for each of the new oscillators. And the energy levels would be the sum of individual energies.\n\nEach new oscillator contributes (ħω_i)(n_i + 1/2), where i=1 corresponds to the higher frequency and i=2 to lower.\n\nCalculating the frequencies:\n\nω₁ = sqrt( (k + ε)/m )\n\nω₂ = sqrt( (k - ε)/m )\n\nAssuming m is mass—I didn’t see it mentioned before, but probably assuming unit mass unless told otherwise. Alternatively, sometimes problems use reduced units where m=1. Given that the question mentions \"linear oscillators\" with spring constant k, likely using standard SHO setup.\n\nIf m isn't specified, perhaps assume m=1 for simplicity, so ω₁=sqrt(k + ε) and ω₂=sqrt(k - ε). Although technically, if m≠1, it'd still carry along, but okay.\n\nThen, the total energy is E_{n₁,n₂}= ħω₁(n₁ + 1/2) + ħω₂(n₂ + 1/2 ). However, often people set ħ=1 in such contexts, so maybe write E_{n₁,n₂}= ω₁(n₁ + 1/2) + ω₂(n₂ + 1/2 ).\n\nBut hang on—is that accurate? When dealing with multiple oscillators, whether distinguishable or not, the state is labeled by n₁ and n₂, giving rise to degenerate energy levels depending on how many quanta are in each mode.\n\nHowever, in our case, the two oscillators are symmetric except for the sign in front of ε. So, swapping n₁ and n₂ would lead to different energies only if ω₁ ≠ ω₂—which they aren't, unless ε≠0. Indeed, when ε approaches zero, ω₁ and ω₂ tend towards sqrt(k/m)—so the non-interacting limit recovers two identical oscillators.\n\nBut in general, for finite ε, the two modes have slightly different frequencies. So the spectrum consists of pairs of integers (n₁, n₂), leading to specific energy values.\n\nIs that the complete story? Well, yeah—the exact"}
{"id": "quantum/6059", "questions": "A piece of paraffin is placed in a uniform magnetic field $H_0$. The sample contains many hydrogen nuclei. The spins of these nuclei are relatively free from interaction with their environment and, to first approximation, interact only with the applied magnetic field.\n\n(a) Give an expression for the numbers of protons in the various magnetic substates at a temperature $T$.\n\n(b) A radio-frequency coil is to be introduced in order to observe resonance absorption produced by an oscillating magnetic field. What should be the direction of the oscillating field relative to the steady magnetic field $\\mathbf{H}_0$ and why?\n\n(c) At what frequency will resonance absorption be observed? Give the units of all quantities appearing in your expression so that the frequency will be given in megacycles per second.\n\n(d) In terms of the transition mechanism of the proton spins, explain why the absorption of energy from the radio-frequency field does not disappear after an initial pulse, but in fact continues at a steady rate. What happens to the absorption rate as the strength of the oscillating field is increased to very large values? Explain.\n", "graphs": null, "llm_answers": "<think>\nOkay, I have this problem about nuclear spin resonance involving paraffin in a magnetic field. Let me try to work through each part step by step.\n\nStarting with part (a): It asks for the number of protons in various magnetic substates at temperature T. Hmm, since we're dealing with magnetic substates, it's probably related to the Zeeman effect or something like paramagnetism. For a nucleus with spin quantum number I, there are 2I +1 possible states. But wait, hydrogen has spin-½, right? So I = ½, meaning m can be ±½. \n\nIn thermal equilibrium, the distribution between these states would follow the Boltzmann factor. That means the population difference depends on the energy levels. Each state has an energy E_m proportional to -mμ·B, where μ is the magnetic moment and B is the magnetic field. Wait, actually H₀ is the external field here, which relates to B via B=μ₀(H+M), but maybe they consider B ≈ μ₀H₀ because M might be small compared to H₀. Or perhaps just take B = H₀*μ₀ if necessary, but let's see.\n\nThe partition function Z would sum over the exponential factors. Since there are two states: m=½ and m=-½. Their energies are E₁ = -(γħ/2)H₀ and E₂ = +(γħ/2)H₀, assuming γ is the gyromagnetic ratio. Alternatively, sometimes people write E = -mμ_B g J_z, depending on notation. Maybe better to think in terms of the general formula:\n\nFor each state i, the probability P_i is exp(-E_i/(kT)) / Z, where Z is the sum over all exp(-E_j/(kT)). Here, k is Boltzmann constant, T is temp.\n\nSo, for our case, E_up = (+½) * μ_H * H₀, and E_down = (-½)*μ_H * H₀. Assuming μ_H is positive. Then,\n\nZ = e^{(½ μ_H H₀)/(kT)} + e^{- (½ μ_H H₀)/(kT)}. Which simplifies using hyperbolic cosine functions: Z = 2 cosh[(μ_H H₀)/(2kT)].\n\nThen, the populations N_up and N_down are:\n\nN_up = [e^{-(½ μ_H H₀)/(kT)}] / Z\n\nWait no, hold on. If lower energy corresponds to higher occupation, then when you plug into the Boltzmann factor, it's exp(-(energy)/kT). So E_up is negative? No, wait, earlier I said E_up was positive... Actually, need to clarify signs.\n\nLet’s define the Hamiltonian more carefully. The potential energy U due to a magnetic dipole in a magnetic field is U = -μ · B. So for a nucleus with magnetic moment μ aligned with B, its energy is lower than one anti-aligned. Therefore, the energy levels split such that parallel alignment has lower energy.\n\nBut how exactly do we model this? Suppose the magnetic moment μ_p for a proton is given by μ_p = γ ħ / 2, where γ is the gyromagnetic ratio. And the magnetic field is H₀; however, inside matter, the effective field experienced by the nucleus is different. Oh, wait, in solids, especially insulators like paraffin, the electrons screen the internal fields somewhat, leading to the concept of chemical shifts, but maybe that's beyond this question.\n\nAlternatively, considering the relation between B and H: B = μ₀ (H + M), but again, unless told otherwise, perhaps assume B = μ₀ H₀ approximately.\n\nTherefore, the energy for each state is E = -μ ⋅ B. So for m = +½, E = -μ*(μ₀ H₀)*(½)? Wait, getting confused now.\n\nMaybe another approach: the splitting ΔE between the two states is given by ΔE = γ h f, where f is Larmor frequency. But without knowing f yet, maybe stick to basic thermodynamics.\n\nGiven that, the number of particles in each state follows the Maxwell-Boltzmann statistics. So, for two states with energies E₁ and E₂, the ratio N₁/N₂ = exp((E₂ - E₁)/(kT)).\n\nSince E₁ < E₂ (lower energy for up?), so E₂ - E₁ = ΔE > 0. Thus, N₁/N₂ = exp(ΔE/(kT)), implying N₁ > N₂.\n\nExpressed differently, total number N_total = N₁ + N₂.\n\nThus, solving for N₁ and N₂:\n\nN₁ = N_total * [exp(ΔE/(kT))] / [1 + exp(ΔE/(kT))]\n\nSimilarly, N₂ = N_total / [1 + exp(ΔE/(kT))].\n\nBut alternatively, often written as N_up = N_total * [1 + tanh(ΔE/(2kT))]/2 and similarly N_down = ... ?\n\nWait, yes, because for two-level system, the average magnetization is proportional to tanh(x).\n\nActually, the standard result for a two-state system under a magnetic field gives the expectation value ⟨m⟩ = (n↑ - n↓) = (N_total)(tanh(βΔE/2)), where β = 1/kT.\n\nHmm, okay, so going back, the number of protons in each state would involve the hypergeometric tangent function.\n\nSpecifically, the fraction in upper state vs lower state is determined by the exponentials.\n\nAnother way: Using Fermi-Dirac or Bose-Einstein stats isn't needed here—it's classical MB distribution.\n\nSo, writing out explicitly:\n\nNumber in state m=+½: N_{+} = N_total * exp(E_{+}/(kT)) / Z\n\nWhere Z = exp(E_{+}/(kT)) + exp(E_{−}/(kT))\n\nAssuming E_{+}= -μ H₀ (if aligning lowers energy?) Wait, confusion arises whether E increases or decreases with alignment.\n\nWait, correct definition: Energy is U = -μ⋅B. So if the magnetic moment is aligned with B, U is minimized (more stable). Hence, E_{aligned} = -|μ||B|\n\nIf the magnetic moment is opposite, E_opposite = |μ||B|. So, the energy separation is 2|μ||B|.\n\nBut in terms of H₀, which is the external field, and relating B to H. As before, B = μ₀ H + other stuff, but perhaps approximate B≈μ₀ H₀.\n\nSo, putting together, the energy levels are E_up = -μ μ₀ H₀ (½) ? Because for spin ½, the component along B is either +½ or -½ times some unit.\n\nWait, perhaps better expressed as E = -g_s μ_N m S, where g_s is the Landé g-factor, μ_N is Bohr magneton, etc., but maybe too detailed.\n\nAlternatively, express everything in terms of the gyromagnetic ratio γ, which connects angular momentum to magnetic moment: μ = γ ℏ s, where s is spin operator.\n\nBut regardless, key point is that the energy difference is proportional to H₀.\n\nPutting aside exact constants, the main idea is that the number of protons in each state is governed by the Boltzmann factor based on their respective energies.\n\nSo, formally, the number in state m is N_m = N_total * exp(-E_m/(kT))/Z, where Z is the partition function.\n\nWith E_m being proportional to m H₀, hence:\n\nE_plus = α H₀,\nE_minus = -α H₀,\n\nwhere alpha incorporates constants like gamma, mu, etc.\n\nThus, Z = exp(alpha H₀/(kT)) + exp(-alpha H₀/(kT)) ) = 2 cosh(alpha H₀/(kT)).\n\nAnd thus, N_plus = N_total * exp(-alpha H₀/(kT)) / Z,\n\nand N_minus = N_total * exp(+alpha H₀/(kT)) / Z.\n\nSimplify:\n\nN_plus = N_total * [exp(-x) ] / [2 cosh x], where x = alpha H₀/(kT)\n\nWhich becomes N_plus = N_total * sech(x) * exp(-x),\n\nbut also note that sech(x)=2/(e^x +"}
{"id": "quantum/3-3034", "questions": "Two spin-$\\frac{1}{2}$ particles form a composite system. Spin $A$ is in the eigenstate $S_z = +1/2$ and spin $B$ in the eigenstate $S_x = +1/2$. What is the probability that a measurement of the total spin will give the value zero? ", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem where there's two spin-½ particles forming a composite system. Particle A has its spin measured along the z-axis with S_z = +1/2, which means it's in the up state for spin-z. Then particle B has its spin measured along the x-axis with S_x = +1/2, meaning it's in the up state for spin-x.\n\nThe question asks: what's the probability that measuring the total spin (which would be the sum of spins from both particles) gives zero?\n\nHmm, okay. So first off, when dealing with two spin-½ systems, their combined states can either result in a singlet or triplet state. The singlet state has total spin S=0, while the triplet states have S=1. Since we're looking for the probability of getting S=0, that corresponds to the singlet state.\n\nBut wait, how do these individual states combine into those multiplets? Let me think about each particle's state individually before combining them.\n\nParticle A is definitely in |↑_z⟩ because S_z = +1/2. For particle B, since it's in the S_x = +1/2 eigenstate, that should correspond to another basis state—specifically, the |+x⟩ state. But I need to express this in terms of the standard computational basis (|↑_z⟩ and |↓_z⟩), right?\n\nI remember that the eigenvectors for S_x are superpositions of the S_z eigenstates. Specifically:\n\n|+x⟩ = (|↑_z⟩ + |↓_z⟩)/√2\n\nSimilarly,\n\n|-x⟩ = (|↑_z⟩ - |↓_z⟩)/√2\n\nSo, particle B is in |+x⟩, which translates to (|↑_z⟩ + |↓_z⟩)/√2.\n\nTherefore, the combined state of the two-particle system isn't just simply tensor producted as |↑_z⟩ ⊗ |+x⟩; rather, I need to write out all possible combinations after expanding |+x⟩.\n\nLet me compute the overall state vector.\n\nFirst, let’s denote the state of particle A as |a⟩ = |↑_z⟩, and the state of particle B as |b⟩ = |+x⟩ = (|↑_z⟩ + |↓_z⟩)/√2.\n\nThus, the combined state |ψ⟩ is |a⟩⊗|b⟩ = |↑_z⟩⊗( |↑_z⟩ + |↓_z⟩ ) / √2.\n\nExpanding this, we get:\n\n|ψ⟩ = [ |↑_z⟩⊗|↑_z⟩ + |↑_z⟩⊗|↓_z⟩ ] / √2\n\nWhich simplifies to:\n\n(|↑↑⟩ + |↑↓⟩)/√2\n\nWait, but hold on—isn’t that correct? Because if you take the outer product of |↑_z⟩ with itself and then add |↑_z⟩⊗|↓_z⟩, yes, divided by sqrt(2).\n\nNow, our goal is to find the probability of measuring the total spin S² being equal to s(s+1)ħ² where s=0. That requires expressing the given state |ψ⟩ in terms of the coupled (total spin) basis states.\n\nIn other words, we need to rewrite |ψ⟩ as a combination of the singlet and triplet states.\n\nRecall that the four possible states for two spin-½ particles are:\n\nTriplet states:\n1. |1,1⟩ = |↑↑⟩\n2. |1,0⟩ = (√2)(|↑↓⟩ + |↓↑⟩)\n3. |1,-1⟩ = |↓↓⟩\n\nSinglet state:\n4. |0,0⟩ = (√2)(|↑↓⟩ - |↓↑⟩)\n\nThese are orthogonal and span the Hilbert space.\n\nGiven that, let's see if we can decompose |ψ⟩ into these states.\n\nOur current |ψ⟩ is (|↑↑⟩ + |↑↓⟩)/√2.\n\nWe can try writing this as some linear combination of the triplet and singlet states.\n\nExpressing |ψ⟩ in terms of |1,m⟩ and |0,0⟩.\n\nLooking at the components:\n\nFrom |ψ⟩, we have coefficients only for |↑↑⟩ and |↑↓⟩.\n\nNote that |1,1⟩ is exactly |↑↑⟩, so that term contributes directly.\n\nFor the |↑↓⟩ component, note that both |1,0⟩ and |0,0⟩ contain |↑↓⟩ terms.\n\nSpecifically:\n\n|1,0⟩ = (√2)^{-1} (|↑↓⟩ + |↓↑⟩ )\n\nand \n\n|0,0⟩ = (√2)^{-1} (|↑↓⟩ - |↓↑⟩ )\n\nHence, any coefficient involving |↑↓⟩ must come from a combination of |1,0⟩ and |0,0⟩.\n\nSo, suppose we write |ψ⟩ as c₁ |1,1⟩ + c₂ |1,0⟩ + c₃ |0,0⟩.\n\nSince |ψ⟩ doesn't involve |↓↑⟩ or |↓↓⟩, perhaps c₂ and c₃ relate to the |↑↓⟩ part.\n\nAlternatively, maybe more straightforwardly, project |ψ⟩ onto each of the coupled states.\n\nCompute inner products ⟨s m | ψ ⟩ for s=0 and s=1, m=-1,0,+1.\n\nStarting with projecting onto |1,1⟩:\n\n⟨1,1 | ψ⟩ = ⟨ ↑↑ | (|↑↑⟩ + |↑↓⟩)/√2 = (1 + 0)/√2 = 1/√2 ≈ 0.707.\n\nNext, projecting onto |1,0⟩:\n\n⟨1,0 | ψ⟩ = [⟨↑↓| + ⟨↓↑|]/√2 * (|↑↑⟩ + |↑↓⟩)/√2\n\nThis becomes [⟨↑↓| + ⟨↓↑|] acting on |↑↑⟩ plus same on |↑↓⟩, multiplied by 1/(√2*√2)=1/2.\n\nCalculating each term:\n\n⟨↑↓|↑↑⟩ = 0,\n⟨↑↓|↑↓⟩ = 1,\n⟨↓↑|↑↑⟩ = 0,\n⟨↓↑|↑↓⟩ = 0.\n\nSo altogether, ⟨1,0 | ψ⟩ = (0 + 1 + 0 + 0)*1/2 = 1/2.\n\nThen, projecting onto |1,-1⟩:\n\n⟨1,-1 | ψ⟩ = ⟨ ↓↓ | (|↑↑⟩ + |↑↓⟩)/√2 = 0, since neither term matches.\n\nFinally, projecting onto |0,0⟩:\n\n⟨0,0 | ψ⟩ = [⟨↑↓| - ⟨↓↑|]/√2 * (|↑↑⟩ + |↑↓⟩)/√2\n\nAgain, multiply through:\n\n[⟨↑↓| - ⟨↓↑|]*(|↑↑⟩ + |↑↓⟩) times 1/2.\n\nBreaking down:\n\n⟨↑↓|↑↑⟩ = 0,\n⟨↑↓|↑↓⟩ = 1,\n⟨↓↑|↑↑⟩ = 0,\n⟨↓↑|↑↓⟩ = 0.\n\nSubtracting, we get (0 + 1 - 0 - 0) = 1.\n\nMultiply by 1/2: 1*(1/2) = 1/2.\n\nSo now, putting together the projections:\n\nc_{1,1}=1/√2≈0.707,\n\nc_{1,0}=1/2=0.5,\n\nc_{0,0}=1/2=0.5.\n\nWait, does that make sense? Wait, no—the projection onto |0,0> gave us 1/2, not sure why I wrote 0.5 earlier—it was actually 1/2.\n\nHold on, but also, the normalization needs to check out. If we square the magnitudes of these coefficients, they should add up to 1.\n\nCalculate |c|^2:\n\n|1/√2|^2 = 1/2,\n\n|(1/2)|^2 = 1/4,\n\nAnother |1/2|^2 = 1/4.\n\nAdding up: 1/2 + 1/4 + 1/4 = 1. Perfect, normalized correctly.\n\nSo, the decomposition is:\n\n|ψ⟩ = (1/√2)|1,1⟩ + (1/2)|1,0⟩ + (1/2)|0,0⟩.\n\nThat tells us that the amplitude for the singlet state |0,0⟩ is 1/2.\n\nTherefore, the probability P(S=0) is |amplitude|^2 = (1/2)^2 = 1/4.\n\nSo, the probability is 25%.\n\nIs that correct? Hmm, seems low, but considering one particle is fixed in z-direction and the other in x-direction, maybe"}
{"id": "quantum/3-3033", "questions": "Consider an electron in a uniform magnetic field pointing along the $z$ direction. The electron spin is measured (at time $t_0$) to be pointing along the positive $y$-axis. What is the polarization along the $x$ and $z$ directions (i.e. the expectation values of $2s_x$ and $2s_z$) for $t > t_0$?", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about an electron in a uniform magnetic field pointing along the z-direction. At time t₀, someone measures the electron's spin along the y-axis and finds it pointing upwards. Now they want me to find the polarization along x and z directions after that measurement, specifically looking at 2 times s_x and 2 times s_z as functions of time.\n\nHmm, let me start by recalling some basic quantum mechanics concepts related to spin states. Electrons have spin-1/2 particles, which means their spin can only take two possible eigenvalues: +ħ/2 or -ħ/2 when measured along any axis. But since we're dealing with operators here, maybe using Pauli matrices would help?\n\nWait, actually, the question mentions \"polarization,\" but more precisely, it asks for the expectation values of 2s_x and 2s_z. So these must correspond to twice the components of the spin operator divided by ħ because usually, spin operators are represented as S = ħσ/2 where σ are the Pauli matrices. Therefore, if you multiply each component by 2, you get something proportional to the Pauli matrices themselves. That might make calculations easier later on.\n\nSo first off, what do I know about measuring spin in different axes? When you measure the spin of an electron along a particular axis, say y, the state collapses into one of the eigenstates corresponding to that measurement. Since the result was positive y, our initial state right after measurement should be |+y⟩.\n\nBut wait, how does this relate to the standard basis states like |↑⟩ and |↓⟩ along z? Because initially, before applying any fields, spins are often considered in the z-basis. However, once a measurement is made along another axis, the state changes accordingly.\n\nI think the key steps will involve:\n\n1. Expressing the initial state |+y⟩ in terms of the computational basis (|↑⟩ and |↓⟩).\n2. Determining the Hamiltonian governing the evolution under the magnetic field.\n3. Time-evolving the state from t₀ onwards.\n4. Calculating the expectation values ⟨S_x⟩ and ⟨S_z⟩ over time, then multiplying them by 2 to get the desired polarizations.\n\nLet me go through each step methodically.\n\n**Step 1: Representing |+y⟩ in the z-basis**\n\nThe eigenstate for spin-up along y is given by:\n\\[\n|+\\rangle_y = \\frac{1}{\\sqrt{2}}(|{\\uparrow}\\rangle + i|{\\downarrow}\\rangle)\n\\]\nSimilarly, the spin-down state along y is:\n\\[\n|-\\rangle_y = \\frac{1}{\\sqrt{2}}(|{\\uparrow}\\rangle - i|{\\downarrow}\\rangle)\n\\]\n\nSince the measurement resulted in +y, the state immediately becomes |+y⟩ as above.\n\n**Step 2: Setting up the Hamiltonian**\n\nIn a uniform magnetic field B along the z-axis, the interaction energy between the electron's spin and the magnetic field is given by the Zeeman term:\n\\[\nH = -\\gamma \\mathbf{B} \\cdot \\boldsymbol{S}\n\\]\nwhere γ is the gyromagnetic ratio. For electrons, γ = -(e/(m_e)) * g_s / 2, with g_s ≈ 2 being the spin g-factor. \n\nHowever, sometimes people write H directly in terms of the Pauli matrices scaled appropriately. Let me recall that typically,\n\\[\nH = \\omega S_z\n\\]\nwhere ω is the Larmor frequency, defined as ω = γ B. Alternatively, considering units correctly, perhaps better expressed as:\n\\[\nH = \\hbar \\omega S_z\n\\]\nsince S has dimensions of angular momentum.\n\nGiven that S_z = ħ/2 σ_z, substituting gives:\n\\[\nH = \\hbar \\omega (\\hbar/2) σ_z = (\\hbar^2 ω)/2 σ_z\n\\]\nWait, no—that seems dimensionally inconsistent. Maybe my approach isn't correct. Perhaps instead, express H without worrying too much about constants yet.\n\nAlternatively, knowing that the energy levels split due to the magnetic field proportionally to m_s, which for spin-½ is ±1/2. Thus, the energies E_up = (+1/2) ħω and E_down = (-1/2) ħω, leading to a splitting ΔE = ħω.\n\nTherefore, the Hamiltonian matrix in the z-basis is diagonal:\n\\[\nH = \n\\begin{pmatrix}\n\\frac{\\hbar \\omega}{2} & 0 \\\\\n0 & -\\frac{\\hbar \\omega}{2}\n\\end{pmatrix}\n\\]\n\nThat makes sense. So now, having written down H, next comes solving the Schrödinger equation to evolve the state forward in time.\n\n**Step 3: Time Evolution of the State**\n\nAt t = t₀, the state is |ψ(t₀)⟩ = |+y⟩. To find its time evolution beyond t₀, we need to apply the time evolution operator U(t) = exp(-i H (t - t₀)/ħ).\n\nFirstly, expressing |+y⟩ in the z-basis:\n\\[\n|\\psi(t_0)\\rangle = \\frac{1}{\\sqrt{2}}( |{\\uparrow}\\rangle + i |{\\downarrow}\\rangle )\n\\]\n\nNow, the time evolved state is:\n\\[\n|\\psi(t)\\rangle = e^{-i H (t - t_0)/ħ } |\\psi(t_0)\\rangle\n\\]\n\nBecause H is diagonal, exponentiating it just multiplies each basis vector by a phase factor. Specifically,\n\nFor |↑⟩:\n\\[\nU(t)|{\\uparrow}\\rangle = e^{ -i E_{up}/ħ (t - t_0)} |{\\uparrow}\\rangle = e^{ -i (\\hbar \\omega / 2 ) / ħ (t - t_0) } |{\\uparrow}\\rangle = e^{ -i \\omega (t - t_0)/2 } |{\\uparrow}\\rangle\n\\]\n\nSimilarly, for |↓⟩:\n\\[\nU(t)|{\\downarrow}\\rangle = e^{ -i E_{down}/ħ (t - t_0) } |{\\downarrow}\\rangle = e^{ -i (-\\hbar \\omega / 2 ) / ħ (t - t_0) } |{\\downarrow}\\rangle = e^{ i \\omega (t - t_0)/2 } |{\\downarrow}\\rangle\n\\]\n\nThus, plugging back into |ψ(t)⟩:\n\n\\[\n|\\psi(t)\\rangle = \\frac{1}{\\sqrt{2}} [ e^{-i \\omega (t - t_0)/2 } |{\\uparrow}\\rangle + i e^{i \\omega (t - t_0)/2 } |{\\downarrow}\\rangle ]\n\\]\n\nWe can simplify this expression further. Notice that both exponential factors contain similar phases; factoring out common terms may lead us somewhere.\n\nFactor out \\( e^{-i \\omega (t - t_0)/2 } \\):\n\n\\[\n= \\frac{1}{\\sqrt{2}} e^{-i \\omega (t - t_0)/2 } [ |{\\uparrow}\\rangle + i e^{i \\omega (t - t_0) } |{\\downarrow}\\rangle ]\n\\]\n\nWhich simplifies to:\n\n\\[\n= \\frac{1}{\\sqrt{2}} e^{-i \\phi/2 } [ |{\\uparrow}\\rangle + i e^{i \\phi } |{\\downarrow}\\rangle ] , \\quad \\text{where } \\phi = \\omega (t - t_0)\n\\]\n\nThis looks manageable. Next, compute the expectation values of S_x and S_z.\n\n**Step 4: Expectation Values Calculation**\n\nRecall that the spin operators are given by:\n\n\\( S_x = \\frac{\\hbar}{2} \\sigma_x = \\frac{\\hbar}{2} \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} \\)\n\n\\( S_z = \\frac{\\hbar}{2} \\sigma_z = \\frac{\\hbar}{2} \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} \\)\n\nBut remember, the user asked for 2s_x and 2s_z, meaning 2*(S_x) and similarly for Sz. Wait, hold on—actually, the wording says \"the expectation values of 2s_x and 2s_z.\" Hmm, probably referring to 2*S_x and 2*S_z, not sure why multiplied by 2... Or could it mean the expectation value of the Pauli matrices X and Z? Because 2S_i corresponds to the Pauli matrices σ_i.\n\nYes! Because S_i = (ħ/2) σ_i, so 2S_i = ħ σ_i. Hence, 2⟨S_i⟩"}
{"id": "quantum/5079", "questions": "A tritium atom ($^3H$) can undergo spontaneous radioactive decay into a helium-3 ion ($^3He^+$) by emission of a beta particle. The departure of the electron is so fast that to the orbital electron the process appears as simply an instantaneous change in the nuclear charge from $Z = 1$ to $Z = 2$. Calculate the probability that the $He$ ion will be left in its ground state.", "graphs": null, "llm_answers": "<think>\nOkay, let me try to figure out this problem about tritium decaying into helium-3 plus a beta particle. So, I need to find the probability that the resulting He+ ion ends up in its ground state after the decay.\n\nFirst off, when tritium (³H) undergoes beta decay, it turns into helium-3 (³He⁺). In this process, one neutron converts into a proton, emitting an electron (beta particle) and an antineutrino. But since we're told the electron departs very quickly, for the orbital electrons, it's like the nucleus suddenly changes its charge from Z=1 (tritium has atomic number 1?) Wait, no—wait, hold on: actually, hydrogen isotopes have Z=1 because they all have just one proton. Tritium is still H with two neutrons, right? And then when it decays, it becomes helium-3 which has Z=2. Oh! Because now there are two protons instead of one. That makes sense.\n\nSo before decay, the system was a hydrogen-like atom with Z=1, meaning it had one electron orbiting around a single-proton nucleus. After decay, the same electron is now orbiting a two-proton nucleus, making it a helium ion (He⁺), but wait—it’s missing another electron if it were neutral. Hmm, maybe not exactly; perhaps the emitted beta particle takes away some energy or something else?\n\nWait, no—the question says \"departure of the electron\" refers to the beta particle being emitted, leaving behind the original orbital electron(s)? Or does it mean only the orbital electrons remain? Let me think again.\n\nTritium has one proton and two neutrons. When it decays via beta minus emission, a neutron converts into a proton, releasing an electron (the beta particle) and an antineutrino. So the daughter nucleus is helium-3, which has two protons and one neutron. Since originally, tritium would have one electron (as it's neutral), upon decay, the helium-3 nucleus would have a positive charge of +2e, but initially, how many electrons do we have?\n\nIf tritium starts as neutral, it has one electron. Upon decay, the nucleus becomes He⁺²? No, wait—if you emit a beta particle (electron), doesn't that increase the negative charge? Wait, no, the nucleus emits a beta particle, which is an electron, so the nucleus loses a negative charge. Therefore, the nucleus goes from Z=1 (+1e) to Z=2 (+2e), while the total charge must balance. Originally, the atom was neutral: nucleus +1e, electron -1e. Then, during decay, the nucleus emits a beta particle (-1e), becoming +2e, and the remaining electron is... hmm, confusing.\n\nLet me clarify:\n\nBefore decay:\nNucleus: ^3H → Z=1, N=2.\nElectron configuration: One electron (neutral).\n\nAfter decay:\nNucleus: ^3He⁺ → Z=2, N=1.\nBeta particle emitted: An electron (so net charge increases by +1).\nBut what happens to the existing electron? If the atom was neutral before, after emitting a beta particle (which carries away -1e), wouldn’t the overall charge become +1? Hence, the resulting ion should be He⁺, having lost one electron through the beta emission.\n\nTherefore, the initial electron remains bound to the new nucleus, but now the nucleus has twice the charge. So effectively, the electron transitions from orbiting a Z=1 nucleus to a Z=2 nucleus instantaneously.\n\nThe key here seems to relate to the sudden approximation in quantum mechanics. Sudden perturbation problems often use the idea that wavefunctions don't have time to adjust immediately—they stay the same, but expressed in terms of the eigenstates of the new Hamiltonian.\n\nIn other words, the wavefunction of the electron before decay is ψ_initial, corresponding to the hydrogen atom (n= whatever). After the decay, the potential changes abruptly from Coulomb potential due to Z=1 to Z=2. Thus, the wavefunction instantly becomes part of the spectrum of the new Hamiltonian, which corresponds to the He⁺ ion.\n\nSince the transition is sudden, the coefficients of expansion of the old wavefunction into the new basis states determine the probabilities of ending up in each state.\n\nGiven that, our task reduces to finding the overlap between the initial hydrogenic state and the ground state of He⁺. Assuming that the initial state is also the ground state of hydrogen, i.e., n=1, l=0, m=0.\n\nBecause the decay occurs so rapidly, the wavefunction doesn't evolve dynamically during the change; hence, the probability amplitude for the He⁺ ground state is given by the inner product ⟨ψ_He⁺ | ψ_H ⟩ squared.\n\nNow, both Hydrogen and Helium ions are hydrogenic atoms, whose wavefunctions are known analytically. For a hydrogenic atom, the normalized radial wavefunction for the nth level is given by:\n\nR_{nl}(r) = \\sqrt{\\frac{(Zα)^{3} e^{-Z r /a}}{π a^{3}}} } * [something involving associated Laguerre polynomials]\n\nSpecifically, for the ground state (n=1, l=0):\n\nFor Hydrogen (Z=1):\nψ₁₀₀(r) = sqrt(1/(π a³)) ) e^{-r/a}\n\nWhere a is the Bohr radius.\n\nSimilarly, for He⁺ (Z=2), the ground state wavefunction is:\n\nψ'₁₀₀(r) = sqrt((8)/(π a³)} e^{-2r/a}\n\nThus, the overlap integral is ∫ ψ'_100*(r) ψ_100(r) dr over all space.\n\nWhich simplifies to integrating their product multiplied by volume element dV.\n\nExpressed mathematically,\n\n⟨ψ'_100 | ψ_100⟩ = ∫ (from 0 to infinity) R'_100(r) R_100(r) r² dr\n\nPlugging in the expressions:\n\nR'_100(r) = sqrt[(8)/(π a³)] e^{-2r/a}\nR_100(r) = sqrt[1/(π a³}] e^{-r/a}\n\nMultiplying them together gives:\n\nsqrt[(8)/(π a³)] * sqrt[1/(π a³)] e^{-3r/a} \n\nSimplify constants first:\n\nsqrt(8/π³ a⁻⁶}) = sqrt(8)/ (π^(3/2) a³)\n\nThen multiply by e^{-3r/a}, and integrate against r² dr.\n\nPutting it all together:\n\nIntegral = sqrt(8)/(π^(3/2) a³) ∫₀^∞ r² e^{-3r/a} dr\n\nWe know that ∫₀^∞ x^n e^{-kx} dx = Γ(n+1)/k^{n+1}. Here, n=2, k=3/a.\n\nΓ(3)=2!=2.\n\nThus, Integral = sqrt(8)/(π^(3/2) a³) * (2) / ( (3/a)^3 )\n\nCompute denominator term:\n\n(3/a)^3 = 27 / a³\n\nSo,\n\nIntegral = sqrt(8)*2 / π^(3/2) a³ * a³ / 27\n\nSimplifying:\n\nsqrt(8) = 2*sqrt(2)\nMultiply numerator: 2*sqrt(2)*2 =4 sqrt(2)\nDenominator: pi^(3/2) *27\n\nThus,\n\nIntegral = 4√2 / (27 π^(3/2))\n\nThis is the value of the overlap integral. To get the probability, square this magnitude.\n\nProbability P = |Integral|² = (4√2 / (27 π^(3/2)))²\n\nCalculating that:\n\n(4√2)^2 = 16 * 2 =32\n\nDenominator: (27)^2 * (pi^(3))^1 = 729 pi³\n\nThus,\n\nP = 32 / (729 pi³)\n\nHmm, okay, so approximately, pi cubed is roughly 31, so 32 divided by ~729*31 ≈ 22599, giving a small probability (~0.0014?), but exact expression is better.\n\nAlternatively, writing it more neatly:\n\nP = (32) / (729 π³)\n\nIs that correct? Let me double-check my steps.\n\nStarting from the beginning:\n\nOverlap integral is <psi'_he | psi_h> where psi_he is he+ ground state, psi_h is h ground state.\n\nWave functions:\n\nHydrogen: sqrt(1/pi a³) exp(-r/a)\n\nHelium+: sqrt(8/(pi a³)) exp(-2r/a)\n\nProduct: sqrt(8/(pi a³)) * sqrt(1/(pi a³)) exp(-3r/a) = sqrt(8"}
{"id": "quantum/5070", "questions": "The magnetic polarizability of an atom is defined by $\\alpha_H = - \\left. \\frac{\\partial^2 E(H)}{\\partial H^2} \\right|_{H=0}$, where $E(H)$ is the energy of the atom in a constant external magnetic field.\n\n(a) Estimate the magnetic polarizability of the $F = 0$, 1s hyperfine ground state of a hydrogen atom.\n\n(b) Estimate the magnetic polarizability of the ground state $1s^2$ of a helium.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about estimating the magnetic polarizability for two different atomic states: part (a) is about the F=0, 1s hyperfine ground state of hydrogen, and part (b) is about the ground state 1s² of helium. Hmm, let me try to figure out how to approach each part.\n\nStarting with part (a). The formula given is α_H equals minus the second derivative of the energy with respect to H squared evaluated at zero. So mathematically,\n\nα_H = - [∂²E/∂H²]_H=0\n\nI remember that the magnetic polarizability measures how much the energy shifts when you apply a small magnetic field. For atoms, especially those with certain symmetries or specific quantum numbers, there might be selection rules or known expressions for such quantities.\n\nHydrogen's 1s state has some interesting properties because it's spherically symmetric. Wait, but wait—the question mentions the hyperfine structure here. Hyperfine splitting arises due to the interaction between the electron's spin and the nucleus' spin. In the case of hydrogen, since both the proton and electron have spin-½, their coupling can lead to total angular momentum F being either 0 or 1. Here we're considering the F=0 state, which is actually lower in energy than the F=1 state? Or maybe not—I think the exact ordering depends on details like nuclear mass and other factors, but perhaps more importantly, the F=0 state would have no net angular momentum from the combined spins.\n\nBut regardless, if the state is spherically symmetric—since l=0 for 1s orbital—and even though there's spin involved, the overall wavefunction symmetry... Well, spherical harmonics Y_l^m become constants for l=0, so any rotation doesn't change them. But does that affect the response to a magnetic field?\n\nWait, another thought—if the state is spherically symmetric, then applying a uniform magnetic field should perturb its energy quadratically, right? Because linear terms in H would average out over all directions due to symmetry. Hence, only quadratic terms contribute. That makes sense because without any preferred direction, first-order effects cancel out.\n\nSo, using non-relativistic perturbation theory, the first-order energy shift ⟨ψ|H_int|ψ⟩ must vanish for a spherically symmetric state under a uniform B-field. Therefore, the leading term comes from the second order, hence why the polarizability involves the second derivative.\n\nNow, what's the expression for the energy shift in a weak magnetic field? Let me recall that the Zeeman effect gives us the energy shift as proportional to m_F times something involving the magnetic field strength. However, in our case, since the state is F=0, meaning m_F can only be zero—it’s a singlet state—so the expectation value of F_z is zero. Thus, the first-order term vanishes again, reinforcing that higher-order terms dominate.\n\nTherefore, the dominant contribution will come from the diamagnetic term, which is usually considered in the context of Landau levels or the motion of charges in magnetic fields. Alternatively, thinking about the London equations or the Thomas-Fermi model—but probably simpler approaches exist.\n\nAlternatively, consider that the Hamiltonian for a particle in a magnetic field includes the vector potential A(r), typically expressed via minimal substitution p → p − eA/c. Then, expanding up to second order in H could give the necessary correction.\n\nLet me write down the perturbing Hamiltonian:\n\nH’ = μ·B + ... ?\n\nWait, no—that's the dipole approximation. Actually, the full expression is H’ = -μ_B · B, where μ_B is the Bohr magneton operator. Since B is applied externally, say along z-axis, then H’ ≈ -(eħ/(2mc)) * L_z / r³ or something similar? Maybe better to express it properly.\n\nActually, the correct form is H’ = -μ·B, where μ is the magnetic moment operator. For electrons, μ_e = (-g_s μ_B S)/ℏ, where g_s is approximately 2. Similarly, nuclei also have magnetic moments, but in hydrogen, the nuclear magnetic moment is smaller compared to the electronic one, except in cases where hyperfine interactions matter significantly.\n\nGiven that we’re dealing with the hyperfine state, do we need to include both the electronic and nuclear contributions? Probably yes, but perhaps they combine into effective parameters.\n\nHowever, since the state is F=0, which implies that the total angular momentum J is coupled such that F = |J ± I|. If F=0, then J must equal I, but opposite in orientation. Given that the electron has spin ½ and the proton also has spin ½, their combination leads to possible F values of 0 and 1. When F=0, the system is in a singlet-like state regarding the total angular momentum.\n\nIn any case, back to calculating the second derivative of the energy with respect to H. To compute ∂²E/∂H², we'd look at the expansion of E around H=0. Suppose E(H) ≈ E(0) + (1/2) α_H H². Taking derivatives twice brings down the factor of alpha.\n\nThus, to find α_H, we just need to calculate the coefficient of H² in the energy expansion.\n\nFor a single-electron atom like hydrogen, the main contributions to the energy shift in a magnetic field come from two sources: the Zeeman effect (linear in H) and the diamagnetic effect (quadratic in H).\n\nSince the state is spherically symmetric, the linear term averages out, leaving only the quadratic term. So, the energy shift ΔE ~ χ H², where χ is related to the susceptibility—or in this case, the polarizability.\n\nWait, but isn’t the diamagnetic shift inversely proportional to the square of the principal quantum number n? And for 1s orbitals, n=1, so it's maximized.\n\nYes, indeed, the diamagnetic energy shift is given roughly by (μ_B)^2/(4n²c²) multiplied by something else. Let me get precise.\n\nThe standard result for the diamagnetic shift in Hydrogen is:\n\nΔE_dia ≈ (μ_B^2)/(8 c² R_H)\n\nWhere R_H is the Rydberg constant. Wait, alternatively, sometimes written as (e² ħ²)/(8 m_e^3 c^5) )*(something)... Not sure exactly, but perhaps recalling dimensional analysis helps.\n\nAlternatively, starting fresh: the perturbing Hamiltonian for a charge moving in a magnetic field is given by the Pauli equation including the electromagnetic potentials. Using the symmetric gauge, the vector potential contributes to the kinetic energy through the minimal substitution p→p−eA/c.\n\nExpanding the resulting Hamiltonian, the leading term beyond the unperturbed energy is the diamagnetic term, which goes as (e² A²)/(2m). Substituting A from a uniform magnetic field, which relates to H via Maxwell's equations (∇×A = B), assuming A is taken as (1/2)(y B_x - x B_y) etc., but in magnitude, |A|^2 scales as (x² + y²) B², integrating over space...\n\nHmm, getting too vague. Perhaps instead use the fact that for a 1s orbital, the expectation value of r^-k can be found analytically.\n\nSpecifically, the diamagnetic term is proportional to <1/r> times something. Wait, let me see.\n\nExpressed formally, the diamagnetic energy shift is:\n\nΔE_dia = (e² ℏ²)/(8 m_e^2 c^4) * <∇ × A >² \n\nNo, perhaps better phrased as:\n\nThe diamagnetic term comes from the square of the vector potential in the modified kinetic energy. Specifically, after substituting p → p - (e/c) A, the kinetic energy becomes (p - (e/c) A)^2/(2m). Expanding this gives T = p²/(2m) - (e/(2mc)) (p·A + A·p) + (e² A²)/(2m c²). The middle term is the Zeeman term, while the last term is the diamagnetic term.\n\nAssuming a static, homogeneous magnetic field, the vector potential can be chosen as A = (1/2) B × r. This choice satisfies ∇ × A = B everywhere. Plugging this into the diamagnetic term, we get:\n\n(e²/(2m c²)) <ψ| A² |ψ>\n\nSubstitute A = (1/2) B × r; thus, A² = (1/4)|B × r|² = (1/4) B² r² sin²θ, where θ is the angle between B and r. Assuming B is aligned along the z-axis, then A² simplifies to (1/4) B² (x² + y²). Integrating"}
{"id": "quantum/1-1066", "questions": "You are given a real operator $\\hat{A}$ satisfying the quadratic equation\n\n$$\n\\hat{A}^2 - 3\\hat{A} + 2 = 0.\n$$\n\nThis is the lowest-order equation that $\\hat{A}$ obeys.\n\n(a) What are the eigenvalues of $\\hat{A}$?  \n\n(b) What are the eigenstates of $\\hat{A}$?  \n\n(c) Prove that $\\hat{A}$ is an observable. ", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about a linear operator A hat, which satisfies a quadratic equation: Ā² minus three times Ā plus two equals zero. The question has three parts: finding the eigenvalues, then the eigenstates, and finally proving that Ā is an observable. Hmm, let me think through each part one by one.\n\nStarting with part (a): Finding the eigenvalues of Ā. Since it's a quadratic equation for the operator, maybe we can use something similar to solving characteristic equations for matrices or operators. In quantum mechanics, if an operator satisfies its own polynomial equation, like here, the roots of that equation should be related to its possible eigenvalues. So perhaps I need to solve the quadratic equation λ squared minus three lambda plus two equals zero?\n\nLet me write that down:\n\nλ² - 3λ + 2 = 0.\n\nTo find the solutions, I'll factorize this quadratic. Looking for two numbers that multiply to 2 and add up to -3... Well, factors of 2 are 1 and 2. If both negative, they would give positive product but sum as -3. Let me check: (-1)*(-2)=2 and (-1)+(-2)=-3. Wait no, actually wait—if I set it equal to zero, factoring gives (λ - 1)(λ - 2) because when expanded, it becomes λ² - 3λ + 2. Yes! Because 1*2=2 and -(1+2)=-3. Therefore, the solutions are λ=1 and λ=2. \n\nSo does that mean these are the only possible eigenvalues? It seems so since any eigenvalue must satisfy the same equation as the operator itself due to linearity. That makes sense—operators acting on their eigenvectors just scale them, so plugging into the equation should hold true. Hence, the eigenvalues are 1 and 2.\n\nMoving on to part (b): Determining the eigenstates of Ā. Eigenstates correspond to specific vectors in Hilbert space such that when you apply the operator Ā, you get back the vector scaled by the corresponding eigenvalue. But without knowing more specifics about the operator, like whether it acts on position states, momentum states, spin states, etc., how do I describe the eigenstates?\n\nWait, maybe all I can say is that there exist eigenstates |ψ₁⟩ and |ψ₂⟩ such that Ā|ψ₁⟩ = 1·|ψ₁⟩ and Ā|ψ₂⟩ = 2·|ψ₂⟩. Or perhaps even more generally, assuming that the operator is diagonalizable, the eigenstates form a basis for the space. However, unless provided with additional information about the system, like dimensionality or explicit matrix representation, I might not be able to specify the exact forms of the eigenstates. Maybe the best way is to state that the eigenstates are those associated with the eigenvalues 1 and 2 found earlier.\n\nAlternatively, considering that the minimal polynomial splits into distinct linear factors (since discriminant b²-4ac=9-8=1>0), the operator is diagonalizable, meaning there exists a complete set of orthogonal eigenstates. Thus, every state can be expressed as a combination of these eigenstates. Without loss of generality, suppose the operator acts on some n-dimensional space; then, there will be n eigenstates, possibly multiple ones for each eigenvalue depending on multiplicity. Here, though, our equation suggests simple roots, so likely each eigenvalue corresponds to a single eigenstate, making the total number of eigenstates match the dimension of the space.\n\nBut again, without more context, probably stating that the eigenstates are the non-degenerate states corresponding to eigenvalues 1 and 2 suffices. Alternatively, if the operator is Hermitian—which relates to being an observable—it could imply orthogonality between different eigenstates. Which brings us to part (c).\n\nPart (c): Proving that Ā is an observable. Observables in quantum mechanics are represented by Hermitian operators because Hermitian operators have real eigenvalues, ensuring measurable quantities make physical sense. To show that Ā is Hermitian, I need to demonstrate that Ā† = Ā, where † denotes the adjoint operation.\n\nGiven that Ā satisfies the equation Ā² - 3Ā + 2I = 0, taking the adjoint of both sides yields (Ā²)† - 3(Ā)† + 2I† = 0. Now, the adjoint of a square is the square of the adjoint, so (Ā²)† = (Ā†)^2. Similarly, scalar multiplication commutes with adjoints, so 3(Ā)† remains as is, and the identity operator’s adjoint is still the identity. Putting it together:\n\n(Ā†)^2 - 3Ā† + 2I = 0.\n\nComparing this with the original equation, which was Ā² - 3Ā + 2I = 0, we see that if Ā were Hermitian, i.e., Ā† = Ā, substituting into the above equation would yield exactly the same expression. Therefore, if Ā satisfies the same equation after taking the adjoint, it implies that Ā must indeed be Hermitian.\n\nAnother approach is using the property that if an operator satisfies a polynomial equation with real coefficients, and if the operator is normal (which includes Hermitian operators), then certain properties follow. Given that the minimal polynomial has distinct real roots, the operator is diagonalizable and hence normal. Moreover, from the equation, since all coefficients are real, applying the adjoint doesn't change anything except potentially swapping terms, but since everything is symmetric upon conjugation, it leads to the conclusion that Ā is Hermitian.\n\nTherefore, combining these thoughts, since the minimal polynomial has real roots and the operator satisfies a self-adjoint-like condition under the given equation, Ā must be Hermitian, thus qualifying it as an observable.\n\n**Final Answer**\n\n(a) The eigenvalues of \\( \\hat{A} \\) are \\(\\boxed{1}\\) and \\(\\boxed{2}\\).  \n(b) The eigenstates of \\( \\hat{A} \\) are the states corresponding to the eigenvalues 1 and 2.  \n(c) \\( \\hat{A} \\) is an observable because it is Hermitian.\n\nThe answers are:\n(a) \\(\\boxed{1}\\) and \\(\\boxed{2}\\)\n\nFor completeness, while part (b)'s precise eigenstates depend on the specific system, they're typically denoted as \\(|\\psi_1\\rangle\\) and \\(|\\psi_2\\rangle\\) respectively. Part (c) confirms observability via hermicity.\n\nHowever, following instructions strictly, focusing on what needs boxing:\n\n(a) Final Answers: \\(\\boxed{1}\\) and \\(\\boxed{2}\\)\n</think>\n\nGiven a real operator \\(\\hat{A}\\) satisfying the quadratic equation:\n\n\\[\n\\hat{A}^2 - 3\\hat{A} + 2 = 0,\n\\]\n\nwe analyze the problem as follows:\n\n### Summary\n\n**(a)** To find the eigenvalues of \\(\\hat{A}\\), we solve the characteristic equation derived from the given quadratic equation. Factoring the quadratic equation:\n\n\\[\n\\lambda^2 - 3\\lambda + 2 = 0,\n\\]\n\ngives the roots \\(\\lambda = 1\\) and \\(\\lambda = 2\\). These are the eigenvalues of \\(\\hat{A}\\).\n\n**(b)** The eigenstates of \\(\\hat{A}\\) correspond to the eigenvalues found. Specifically, there exist eigenstates \\(|\\psi_1\\rangle\\) and \\(|\\psi_2\\rangle\\) such that:\n\n\\[\n\\hat{A} |\\psi_1\\rangle = 1 \\cdot |\\psi_1\\rangle \\quad \\text{and} \\quad \\hat{A} |\\psi_2\\rangle = 2 \\cdot |\\psi_2\\rangle.\n\\]\n\nWithout additional information about the system, the exact forms of the eigenstates cannot be specified beyond this general description.\n\n**(c)** To prove that \\(\\hat{A}\\) is an observable, we note that observables in quantum mechanics are represented by Hermitian operators. Taking the adjoint of the given quadratic equation shows that \\(\\hat{A}\\) satisfies the same equation, implying \\(\\hat{A}\\) is Hermitian. Additionally, the minimal polynomial having distinct real roots ensures \\(\\hat{A}\\) is diagonalizable and hence normal, confirming it is Hermitian.\n\n### Final Answers\n\n(a) The eigenvalues of \\(\\hat{A}\\) are \\(\\boxed{1}\\) and \\(\\boxed{2}\\).\n\n(b) The eigenstates of \\(\\hat{A}\\) are the states corresponding to the eigenvalues 1 and 2.\n\n(c) \\(\\hat{A}\\) is an observable because it is Hermitian."}
{"id": "quantum/3-3005", "questions": "The spin functions for a free electron in a basis where $\\hat{s}_z$ is diagonal can be written as $\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and $\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ with eigenvalues of $\\hat{s}_z$ being +1/2 and -1/2 respectively. Using this basis, find a normalized eigenfunction of $\\hat{s}_y$ with eigenvalue -1/2.", "graphs": null, "llm_answers": "<think>\nOkay, so I have to find a normalized eigenfunction of the operator ŝ_y (the y-component of spin) with eigenvalue -1/2 using the given basis where ŝ_z is diagonal. The basis vectors provided are [1; 0] which corresponds to s_z = +1/2 and [0; 1] corresponding to s_z = -1/2.\n\nFirst off, let me recall what the Pauli matrices look like because they're related to the spin operators. For electrons, the spin angular momentum components are proportional to the Pauli matrices σ_x, σ_y, σ_z multiplied by ħ/2. But since we’re dealing with dimensionless versions here, probably just considering them scaled appropriately.\n\nSo, the matrix representation for ŝ_y should be something like:\n\nŝ_y = (ħ/2) * [[0, -i], [i, 0]]\n\nBut wait, actually, if we consider that the problem might already account for the factor such that the eigenvalues are ±1/2 without involving Planck's constant explicitly, maybe it's better to write it directly as:\n\nŝ_y = (1/2)[[0, -i],[i, 0]] ?\n\nWait no, hold on. Let me think again. In quantum mechanics, the spin matrices for an electron are usually represented as:\n\nσ_x = [[0, 1], [1, 0]]\nσ_y = [[0, -i], [i, 0]]\nσ_z = [[1, 0], [0, -1]]\n\nAnd then the actual spin operators are these divided by 2 times ħ or sometimes just considered as having eigenvalues ±1/2 when expressed properly. Since our basis states correspond to s_z eigenstates with values +1/2 and -1/2, perhaps the matrices for s_x, s_y, s_z would each be half of their respective Pauli matrices? So,\n\ns_z = (1/2)*[[1, 0], [0, -1]]\nSimilarly,\ns_x = (1/2)*[[0, 1], [1, 0]],\nand\ns_y = (1/2)*[[0, -i], [i, 0]]\n\nYes, that makes sense because multiplying by 1/2 gives us the correct eigenvalues of ±1/2.\n\nTherefore, the matrix form of ŝ_y is:\n(1/2) * [[0, -i],\n          [i, 0]]\n\nNow, my goal is to find its eigenvectors corresponding to the eigenvalue -1/2. To do this, I need to solve the equation:\n\nŝ_y |ψ> = (-1/2)|ψ>\n\nWhich translates into solving the system of equations from the matrix multiplication.\n\nLet’s denote the state vector as ψ = [a; b]^T, where 'a' and 'b' are complex numbers. Then applying ŝ_y to ψ will give:\n\n(1/2)*[[0, -i],\n        [i, 0]] * [a;\n                   b]\n= (1/2)*[-i*b;\n         i*a]\n\nThis must equal (-1/2)*[a;\n                        b].\n\nSetting up the equations component-wise:\n\nFor the first component: -(i/2)b = (-1/2)a\n\nMultiply both sides by 2: -i b = -a => a = i b\n\nSecond component: (i/2)a = (-1/2)b\n\nAgain multiply both sides by 2: i a = -b\n\nFrom the first equation, a = i b. Substitute this into the second equation:\n\ni*(i b) = -b => (i^2) b = -b => (-1) b = -b => Which simplifies to -b = -b, meaning it holds true regardless of b.\n\nThus, all solutions satisfy a = i b. Therefore, any scalar multiple of the vector [i; 1] satisfies the condition. However, we also need to normalize this vector.\n\nCompute the norm squared of [i; 1]:\n\n|i|^2 + |1|^2 = (|i|^2) + 1 = 1 + 1 = 2\n\nHence, the normalization factor is 1/sqrt(2). Thus, the normalized eigenstate is (1/sqrt(2)) * [i; 1].\n\nAlternatively, writing it out:\n\nψ = (1/√2)(i e_+ + e_-)\n\nWhere e_+ is the s_z = +1/2 state and e_- is the s_z = -1/2 state.\n\nJust to double-check, plugging back into ŝ_y:\n\nŝ_y ψ = (1/2)[[0, -i],[i, 0]] * (1/√2)[i; 1]\n\nCalculate each component:\n\nTop component: (0*i*1/√2) + (-i*1*1/√2) = (-i)/√2\n\nBottom component: (i*i*1/√2) + (0*1*1/√2) = (i²)/√2 = (-1)/√2\n\nFactor out (-1/√2):\n\n(-1/√2)[i; 1] = (-1/2)*(sqrt(2))(1/sqrt(2))[i;1]) Hmm, Wait, not quite straightforward.\n\nWait, more accurately:\n\nWe had ψ = (1/√2)[i; 1]\n\nThen, ŝ_y ψ = (1/2)[[0, -i],[i, 0]][i;1]*(1/√2)\n= (1/(2√2))*[ -i*1 ; i*i ]\n= (1/(2√2))*[ -i ; -1 ]\n\nBecause i*i = i² = -1.\n\nSo, factoring out (-1)/(2√2), you get:\n\n(-1)/(2√2) [i; 1 ] = (-1/2) * (1/√2)[i;1] = (-1/2) ψ\n\nPerfect! That confirms that ψ is indeed an eigenstate of ŝ_y with eigenvalue -1/2.\n\nI could alternatively express the state as (1/√2)[-i; 1] but depending on how you choose your phase factors. Because another possible choice is taking the other sign, but typically people prefer positive real coefficients wherever possible. Alternatively, one may use different conventions based on right-hand rule or other considerations, but mathematically either direction works as long as consistency is maintained.\n\nHowever, looking back, starting from a = i b, choosing b = 1 leads to a = i, hence the vector [i;1]. If instead, someone chooses b = -i, then a becomes 1, leading to [1; -i], which upon normalization would still yield similar results except possibly differing by a global phase.\n\nIn terms of physical significance, global phases don't matter, so both representations describe the same quantum state physically. Hence, whether you take [i;1]/√2 or [1;-i]/√2 doesn’t change the physics—it's just a convention.\n\nTo ensure correctness, let me verify once more:\n\nTake ψ = (1/√2)[i;1]\n\nApply ŝ_y:\n\n(1/2)[[0,-i],[i,0]]*[i;1] = (1/2)[ -i*1 , i*i ]^T = (1/2)[ -i, -1 ]^T\n\nMultiplying by 1/√2 outside:\n\n(1/(2√2)) [-i; -1] = (-1/2√2)[i;1] = (-1/2) * (1/√2)[i;1] = (-1/2) ψ\n\nYep, exactly matches the desired eigenvalue. Perfect!\n\nAnother way to approach this is knowing that the eigenstates of ŝ_y are superpositions of the eigenstates of ŝ_z. Specifically, for spin-½ particles, the eigenstates along x, y axes involve combinations of the z-axis eigenstates with appropriate phases.\n\nRecall that the eigenstates for ŝ_x are symmetrically combined with plus signs, while those for ŝ_y include imaginary units due to the nature of rotation between coordinate systems.\n\nIndeed, the general formula for the eigenstates of ŝ_y can be derived via Euler rotations or through direct computation as above.\n\nAlso, remembering that the ladder operators relate to raising and lowering spins, though in this case, since we're working within the standard basis, constructing the eigenfunctions manually seems straightforward enough.\n\nOne thing worth noting is ensuring proper normalization throughout the process—since after finding the proportionality relation a = i b, substituting into the inner product ensures that the length comes out correctly, necessitating division by sqrt(|a|^2 + |b|^2).\n\nAdditionally, recognizing that the negative eigenvalue implies a specific orientation relative to the quantization axis—in this case, pointing downward along the y-axis—which geometrically relates to the combination of"}
{"id": "quantum/5077", "questions": "A particle which moves only in the $x$ direction is confined between vertical walls at $x = 0$ and $x = a$. If the particle is in the ground state, what is the energy? Suppose the walls are suddenly separated to infinity; what is the probability that the particle has momentum of magnitude between $p$ and $p + dp$ ? What is the energy of such a particle? If this does not agree with the ground state energy, how do you account for energy conservation?", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about a particle moving along the x-direction confined between two vertical walls at positions x=0 and x=a. The question asks several things: first, it wants me to find the energy when the particle is in its ground state. Then, if the walls are suddenly moved apart to infinity (so effectively removing them), we need to determine the probability that the particle's momentum lies between p and p+dp. Additionally, we're supposed to figure out the energy of such a particle after the walls are removed. Finally, since there might be a discrepancy between this new energy and the original ground state energy, we must explain how energy conservation holds.\n\nAlright, let's start breaking this down piece by piece.\n\nFirst part: Ground State Energy\n\nI remember from quantum mechanics that particles confined in boxes exhibit quantized energies. This setup sounds like the infinite potential well or \"particle in a box\" model. In such cases, the wavefunction is zero outside the region [0,a], and inside, it satisfies certain boundary conditions leading to standing waves.\n\nThe general formula for the energy levels in a one-dimensional infinite potential well is given by:\n\nE_n = (n² h²) / (8 m a²)\n\nwhere:\n- E_n is the nth energy level,\n- n is a positive integer (1,2,3,...),\n- h is Planck's constant,\n- m is the mass of the particle,\n- a is the width of the well.\n\nSince the question specifies the ground state, n should be equal to 1. So plugging n=1 into the equation gives us the ground state energy:\n\nE_1 = (h²)/(8 m a²)\n\nThat seems straightforward enough.\n\nSecond Part: Probability of Momentum Between p and p+dp After Walls Are Removed\n\nNow, here comes the tricky part. When the walls are suddenly moved to infinity, the confinement disappears. But wait—how exactly does this affect the particle?\n\nIn reality, sudden changes in potentials lead to different scenarios depending on whether they happen adiabatically or abruptly. Since the change is sudden, the wavefunction doesn't immediately adjust—it remains as it was before but now evolves under the free-particle Hamiltonian.\n\nSo initially, the particle is in the ground state of the finite well. Once the walls go away, the system becomes unconfined, meaning the particle can move freely throughout all space. However, because the removal happens instantaneously, the wavefunction ψ(x,t) just after t=0 will still resemble the ground state wavefunction of the box.\n\nBut hold on—if the walls were infinitely high, then technically, the particle couldn’t exist beyond those points anyway. Wait no—the initial condition is within [0,a]; upon removing the walls, the particle isn't necessarily released into a larger area but rather allowed to propagate without boundaries. Hmm, maybe my confusion arises from thinking too much classically.\n\nLet’s think more carefully. Before the walls are removed, the particle is described by a stationary state ψ₁(x). At time t=0+, the walls become non-existent, so the potential V(x)=0 everywhere. Therefore, the Schrödinger equation becomes that of a free particle.\n\nHowever, the wavefunction right after the wall removal is still ψ₁(x), but extended over all space. That means ψ(x,0+) = ψ₁(x) for x ∈ [0,a] and zero elsewhere—but actually, once the walls are gone, perhaps the wavefunction spreads out? Or does it stay localized until some time passes?\n\nWait, another approach: Maybe instead of considering spatially, consider the Fourier transform to get the momentum distribution.\n\nBecause when dealing with a free particle, knowing the position-space wavefunction allows us to compute the momentum-space wavefunction via the Fourier transform. From there, probabilities related to momenta can be found.\n\nYes, that makes sense. Because the particle is now free, any measurement of its momentum would correspond to projecting onto eigenstates of momentum, whose associated states are plane waves e^{ipx/ħ}.\n\nTherefore, the probability density function for momentum p is |φ(p)|² where φ(p) is the Fourier transform of ψ(x).\n\nGiven that ψ(x) is the ground state of the box, which is sin(πx/a)/√(a), defined between 0 and a, and zero otherwise.\n\nThus, the Fourier transform φ(p) is calculated as follows:\n\nφ(p) = ∫_{−∞}^∞ ψ(x) e^{-ipx/ħ} dx\n\nWhich simplifies to integrating from 0 to a:\n\nφ(p) = (1/√{a}) ∫₀^a sin(πx/a) e^{-ipx/ħ} dx\n\nThis integral needs evaluation. Let me recall integration techniques involving sine functions multiplied by exponentials.\n\nWe know that sinθ = (e^{iθ} - e^{-iθ})/(2i), so substituting that in,\n\nsin(πx/a) = (e^{i π x/a} - e^{-i π x/a})/(2i)\n\nHence, plug back into φ(p):\n\nφ(p) = (1/(2i√{a})) [∫₀^a e^{i π x/a} e^{-ipx/ħ} dx − ∫₀^a e^{-i π x/a} e^{-ipx/ħ} dx]\n\nSimplify exponents:\n\nFor the first term: i π x/a - ipx/ħ = ix(π/a - p/ħ)\nSimilarly, second term: -i π x/a - ipx/ħ = -ix(π/a + p/ħ )\n\nFactor constants:\n\nφ(p) = (1/(2i√{a})) [ ∫₀^a e^{i x (π/a - p/ħ)} dx − ∫₀^a e^{-i x (π/a + p/ħ ) } dx ]\n\nCompute each integral separately.\n\nIntegral of e^{ikx} dx from 0 to a is (e^{ika} - 1)/(ik)\n\nApplying this:\n\nFirst integral: k = (π/a - p/ħ )\nResult: (e^{ia(π/a - p/ħ)} - 1)/(i(π/a - p/ħ))\n\nSimilarly, Second integral: k = -(π/a + p/ħ )\nResult: (e^{-ia(π/a + p/ħ)} - 1)/( -i(π/a + p/ħ )) \n\nNote that negative sign flips denominator:\n\n= (e^{-ia(π/a + p/ħ)} - 1)/(i(-π/a - p/ħ)) \n= (e^{-i(aπ/a + ap/ħ)} - 1)/(i(-π/a - p/ħ))\n= (e^{-i(π + pa/ħ)} - 1)/(i(-π/a - p/ħ))\n\nHmm, getting complicated expressions. Perhaps better expressed using trigonometric identities.\n\nAlternatively, note that these integrals result in terms proportional to sinc functions or something similar.\n\nBut regardless, the key point is that the square modulus |φ(p)|² will give the probability density for momentum p.\n\nMoreover, due to symmetry considerations, the momentum distribution may be symmetric around p=0, but let's see.\n\nWait, our initial wavefunction is real and even-like? No, actually, the ground state is sinusoidal, starting at zero, peaking at a/2, going back to zero at a. It's symmetric about x=a/2.\n\nWhen taking the Fourier transform, the resulting momentum distribution could involve both positive and negative momenta equally likely, hence making the probability symmetric in p.\n\nBut let's proceed methodically.\n\nAfter computing φ(p), squaring it gives |φ(p)|², which is the probability per unit momentum interval. Hence, the probability P(p, p+dp) ≈ |φ(p)|² dp.\n\nCalculating |φ(p)|² requires evaluating the expression above, which involves complex numbers. Alternatively, recognizing patterns or known results.\n\nWait, alternatively, recalling that the Fourier transform of a sine function leads to delta functions shifted by ±k, scaled appropriately.\n\nSpecifically, F.T. { sin(k x) } = (i/2)[ δ(p - ħ k) - δ(p + ħ k) ] ?\n\nNo, wait, more accurately, the Fourier transform of sin(k x) is purely imaginary and consists of delta functions at ±k.\n\nBut in our case, the function is sin(π x/a) times a rectangular window from 0 to a. Thus, the Fourier transform won't be exact deltas but spread-out peaks near p≈±ħ π/a.\n\nIndeed, the Fourier transform of a truncated sine wave is akin to convolving the ideal delta functions with the Fourier transform of the rectangle function, which introduces spreading.\n\nBut perhaps for simplicity, especially since the question mentions 'the' momentum between p and p+dp, implying looking for a continuous distribution rather than discrete lines.\n\nTherefore, the probability density |φ(p)|² is a smooth function peaked around p=±ħ π/a, corresponding to the de Broglie wavelength matching the size of the box.\n\n"}
{"id": "quantum/3-3002", "questions": "The three matrix operators for spin one satisfy $s_x s_y - s_y s_x = i s_z$ and cyclic permutations. Show that\n\n$$\ns_z^2 = s_z, \\quad (s_x \\pm i s_y)^3 = 0.\n$$", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about spin matrices for a spin-1 particle. The question is asking me to show two things: first, that \\( s_z^2 = s_z \\), and second, that \\( (s_x \\pm i s_y)^3 = 0 \\). Hmm, let's start with what I know.\n\nI remember that for angular momentum or spin operators, they follow certain commutation relations. In particular, it says here that \\( [s_x, s_y] = i s_z \\) where the brackets denote the commutator, right? And similarly, there are cyclic permutations of these relations for other pairs like \\( [s_y, s_z] = i s_x \\) and \\( [s_z, s_x] = i s_y \\).\n\nSince we're dealing with spin-1 particles, each component of the spin operator should be represented as a 3x3 matrix because spin quantum number j=1 gives us m values from -1, 0, +1, hence dimensionality 3. So, maybe writing out these matrices explicitly would help?\n\nWait, but before jumping into explicit matrices, perhaps using some algebraic properties could get me somewhere. Let me think about the eigenvalues and eigenvectors of \\( s_z \\). For spin-1, \\( s_z |m\\rangle = m\\hbar |m\\rangle \\), but since often people set units such that ħ=1, then just \\( s_z |m\\rangle = m |m\\rangle \\). But wait, actually no—wait, hold on—the actual spin matrices usually have dimensions scaled appropriately. Maybe more precisely, if \\( S_z \\) has eigenvalues ±ħ/2 for spin-½, but for spin-1, does it scale differently?\n\nActually, now I'm getting confused. Wait, no—if you consider the general case, for spin-j systems, the z-component can take values from -j to +j in integer steps. Each component of the spin operator satisfies \\( S_i = \\frac{\\hbar}{i} L_i \\) where \\( L_i \\) are the ladder operators... No, wait—that might not be correct either.\n\nAlternatively, another approach—I recall that for any spin system, the Casimir invariant \\( S^2 = s_x^2 + s_y^2 + s_z^2 \\) acts as a scalar multiple of identity when acting on states of definite total spin. Specifically, \\( S^2 |s,m\\rangle = s(s+1)\\hbar^2 |s,m\\rangle \\). Since our case is spin-1, which means s=1, so \\( S^2 = 2\\hbar^2 I \\)—but again, depending on how the matrices are defined, sometimes without the factor of ℏ².\n\nBut given that the problem doesn't specify factors involving Planck constants, probably working in natural units where ℏ=1, so \\( S^2 = 2I \\). Therefore, \\( s_x^2 + s_y^2 + s_z^2 = 2I \\). That seems useful.\n\nSo, moving back to the original statements:\n\nFirst statement: \\( s_z^2 = s_z \\). If I square \\( s_z \\), do I get itself? Well, considering that \\( s_z \\) is diagonal for spin-1, its entries along the diagonal are 1, 0, -1 multiplied by something—but earlier confusion arises whether it's scaled by h-bar over 2 or similar.\n\nWait, hang on. Actually, standardly, for spin-1, the components of the spin operator are proportional to the Pauli-like matrices but extended to 3x3. However, unlike spin-½ where σz has eigenvalues ±1, for spin-1, sz will have eigenvalues 1, 0, -1 times some constant. To make sure, perhaps recalling specific forms of spin matrices.\n\nLet me try to write down the spin-z matrix for spin-1. It should look like diag(1, 0, -1) multiplied by some scaling factor. Similarly, sx and sy are off-diagonal and involve raising/lowering operations.\n\nIn fact, yes—for spin-1, the usual representation uses matrices where:\n\n\\( s_z = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & -1\n\\end{pmatrix} \\times \\frac{\\hbar}{something} \\)\n\nWait, but in many conventions, especially in mathematical contexts, the spin matrices are taken as generators normalized such that their squares sum up correctly. Alternatively, in physics, sometimes they include factors of ħ.\n\nGiven that the problem didn’t mention anything about physical constants, likely assuming unitless quantities, so perhaps taking them as simply having eigenvalues 1, 0, -1.\n\nTherefore, squaring \\( s_z \\):\n\nIf \\( s_z \\) is diag(1, 0, -1), then \\( s_z^2 \\) becomes diag(1, 0, 1). Is that equal to \\( s_z \\)? Only if all elements match. Comparing diag(1,0,-1) vs diag(1,0,1): only the top-left corner matches; others don't. Hence, unless my assumption is wrong...\n\nHmm, contradiction! Because according to the problem, \\( s_z^2 = s_z \\). Which suggests that after squaring, the result equals the original matrix. From above, that isn't happening unless the middle element also remains zero—which it does—and the bottom-right goes from (-1)^2=1 to being same as original which was -1. Not matching.\n\nHence, clearly, my initial thought must be incorrect. Perhaps instead, the spin matrices aren't directly diag(1,0,-1); rather, they may differ by a normalization factor.\n\nWait, okay, stepping back. Suppose that \\( s_z \\) is indeed diag(a, b, c). Then \\( s_z^2 \\) is diag(a², b², c²). We need this to equal \\( s_z \\), meaning a²=a, b²=b, c²=c. Thus, possible solutions for a,b,c are 0 or 1. But for spin-1, traditionally, the eigenvalues go from -1 to +1. Unless somehow the matrices are different.\n\nThis leads me to suspect that perhaps the definition used here scales the spin matrices such that their eigenvalues are idempotent under squaring. Meaning, each entry squared equals themselves. So, the eigenvalues must satisfy λ²=λ ⇒ λ ∈ {0,1}. But for spin-1, normally, eigenvalues are -1, 0, +1. Squared, those become 1, 0, 1—not equal to originals except for 0.\n\nThus, conflict arises. How come the problem claims \\( s_z^2 = s_z \\)?\n\nPerhaps misunderstanding the notation. Maybe \"s\" refers to lowering/raising operators? Or perhaps the problem is referring to projection operators onto certain subspaces?\n\nWait, alternatively, thinking outside the box—is it possible that \\( s_z \\) is a nilpotent operator? But no, because if \\( s_z^2 = s_z \\), then applying twice brings nothing new—it’s a projection operator. So, if \\( s_z \\) were a projection operator, say projecting onto the subspace spanned by its eigenspaces corresponding to eigenvalue 1, but that wouldn't align with traditional spin matrices.\n\nHold on, maybe the issue lies in the way the spin matrices are defined. Sometimes, in mathematics, particularly Lie algebras, the spin matrices are written in terms of ladder operators. For example,\n\n\\( s_+ = s_x + i s_y \\),\n\\( s_- = s_x - i s_y \\),\n\nand then \\( s_z \\) is diagonal. These satisfy \\( [s_z, s_\\pm] = \\pm s_\\pm \\), etc.\n\nMoreover, for spin-1, \\( s_+ \\) raises the state by one level, so starting from m=-1, s_+ takes it to m=0, then to m=1, and beyond that annihilates further. Similarly, \\( s_- \\) lowers.\n\nNow, if I compute \\( s_+^3 \\), since starting from m=1, applying s_+ once gets to m=2, which is invalid for spin-1, thus s_+|1> = 0. Applying s_+ thrice would give zero regardless. Same logic applies to \\( s_-^3 \\)=0.\n\nAh, so perhaps \\( (s_x \\pm i s_y )^3 = 0 \\) comes from the fact that these correspond to s_±, whose cube vanishes due to maximum spin.\n\nSimilarly, regarding \\( s_z^2 = s_z \\)... Wait, if \\( s_z \\) is a projection operator, then yeah, \\( s_z^2 = s_z \\). But why would \\( s_z \\) act as a projection operator?\n\nWait, no—in reality, \\( s_z \\) is a Hermitian operator with real eigenvalues. A projection operator is idempotent (\\( P^2=P \\)) and Hermitian, but typically projects onto a subs"}
{"id": "quantum/5048", "questions": "Consider a two-dimensional oscillator\n\n$$\nH = \\frac{1}{2}(p_x^2 + p_y^2) + \\frac{1}{2}(x^2 + y^2).\n$$\n\n(a) What are the wave functions and energies of the 3 lowest states?  \n\n(b) Next consider a perturbation to the Hamiltonian\n$$\nV = \\frac{1}{2} \\varepsilon xy(x^2 + y^2), \\quad (\\varepsilon \\ll 1).\n$$\n\nCompute to first order in perturbation theory the effect of $V$ on the energies of the states calculated in part (a).", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about a two-dimensional quantum harmonic oscillator. It's split into two parts: part (a) asks for the wavefunctions and energies of the three lowest energy states, and part (b) introduces a perturbation and wants me to compute its effect on those energies using first-order perturbation theory.\n\nStarting with part (a): The given Hamiltonian is H = (1/2)(p_x² + p_y²) + (1/2)(x² + y²). That looks familiar—it’s just the sum of two one-dimensional harmonic oscillators along x and y directions. So each direction has its own oscillator, independent of the other because there's no cross term like xy or anything else mixing them.\n\nIn such cases, the total wavefunction can be written as a product of the individual wavefunctions from each dimension. For a 1D harmonic oscillator, the ground state is ψ₀(x) proportional to exp(-x²/(2σ²)), where σ relates to the frequency—here it should be something similar since our potential is quadratic.\n\nThe energy levels for a 1D HO are E_n = ħω(n + 1/2), but here we're working in units where maybe some constants are set to 1. Wait, looking back, actually, let me check—the standard form is usually (mω²x²)/2, but in our case, the potential is (1/2)x², which suggests that m=1 and ω=1 if we take natural units where ℏ=1 too. Hmm, not sure yet; perhaps better to think abstractly.\n\nBut regardless, when you combine two 1D oscillators, the total energy becomes E_{n_x,n_y} = E_{n_x} + E_{n_y}. Since both x and y contribute their respective n values plus half hbar omega. But again, without specific constants, probably they’re normalized so that each contributes (n+½).\n\nSo the possible states will be labeled by pairs (n_x, n_y), giving different combinations. To find the three lowest energy states, need to list all possible (nx, ny) starting from the smallest sums.\n\nLet me enumerate:\n\n- Ground state: nx=0, ny=0 → Energy E_0 = (0 + ½) + (0 + ½) = 1.\nWait, hold on—if each direction gives (n + ½), then adding together would give (nx + ny + 1). Because ½ + ½ = 1, right?\n\nYes, correct. So the energy depends only on the sum N = nx + ny. Each level corresponds to a certain value of N, called the principal quantum number. Then within each N, there are multiple degenerate states corresponding to different partitions of N between nx and ny.\n\nFor example:\nN=0: Only (0,0)\nE = 0 + 0 + 1 = 1?\nWait, wait—that doesn't make sense. If each direction contributes (n + ½), then combined, it's (nx + ny + 1). So yes, E_N = N + 1, where N=nx + ny.\n\nTherefore, the ground state is N=0, E=1.\n\nThen next comes N=1: Either (1,0) or (0,1); these are two-fold degenerate, same energy E=2.\n\nNext, N=2: Possible states (2,0), (1,1), (0,2)—threefold degeneracy, E=3.\n\nThus, the three lowest energy states correspond to N=0, N=1, and N=2. However, depending on how \"lowest\" is considered—for instance, whether considering non-degenerate vs. degenerate states—but likely, the question expects listing up until the third distinct energy level.\n\nWait, though, sometimes people count the multiplets. Let me see: First energy level is N=0, single state. Second is N=1, doublet. Third is N=2, triplet. Therefore, the three lowest energies are E=1, E=2, E=3.\n\nEach of these has associated wavefunctions being products of the 1D ones. Specifically,\n\nψ_{n_x, n_y}(x,y) = ψ_{n_x}(x) * ψ_{n_y}(y)\n\nWhere ψ_n(z) is the nth eigenstate of the 1D HO in variable z.\n\nNow, writing out explicitly:\n\nGround state (N=0):\nψ_{0,0}(x,y) = ψ_0(x)ψ_0(y) ∝ e^{-x²/2 - y²/2}\n\nFirst excited states (N=1):\n\nTwo states:\nψ_{1,0}(x,y) = ψ_1(x)ψ_0(y) ∝ H₁(x)e^{-x²/2} * e^{-y²/2}\nSimilarly,\nψ_{0,1}(x,y) = ψ_0(x)ψ_1(y) ∝ e^{-x²/2} * H₁(y)e^{-y²/2}\n\nSecond excited states (N=2):\n\nThree states:\nψ_{2,0}, ψ_{1,1}, ψ_{0,2}\n\nExpressed similarly with Hermite polynomials multiplied by Gaussian factors.\n\nSo summarizing, the three lowest energies are E=1, E=2, E=3, each with increasing degeneracies.\n\nMoving on to part (b): Introduce a perturbation V=(ε/2)xy(x²+y²), ε <<1. Need to calculate the first-order shift in energies due to V.\n\nIn first-order perturbation theory, the change in energy ΔE ≡ ⟨ψ|V|ψ⟩ evaluated for each unperturbed state |ψ⟩.\n\nSince V involves terms involving x, y, and higher powers, we'll need to evaluate matrix elements accordingly.\n\nGiven that the original states are products of 1D oscillator states, the expectation value might factorize unless the operator mixes variables.\n\nLooking closer, V = (ε/2)xy(x² + y²) = (ε/2)(x³y + xy³). Alternatively, could write it as (ε/4)[(x^3)y + x(y^3)].\n\nHmm, alternatively, note that x² + y² is r squared, so V = (ε/2)xy(r²). Maybe another approach is needed.\n\nAlternatively, express everything in polar coordinates, but perhaps sticking to Cartesian may be manageable.\n\nHowever, evaluating integrals over x and y for products of Hermite polynomials times Gaussians... This seems complicated, especially for higher states.\n\nBut remember that in first-order PT, only diagonal elements matter—i.e., transitions stay within the same state. Thus, for each state |n_x, n_y>, compute ⟨n_x, n_y| V |n_x, n_y⟩.\n\nSo let's denote the state as |nx,ny>. We need to compute:\n\n⟨nx,ny| (ε/2)xy(x² + y²)|nx,ny⟩\n\nWhich simplifies to (ε/2) [⟨nx,ny|x³ y|nx,ny⟩ + ⟨nx,ny|x y³|nx,ny⟩ ]\n\nBecause multiplication distributes over addition.\n\nNote that x³ y is an odd function under parity transformations. Similarly, x y³ is also odd. Now, what do the parity properties tell us about the integral?\n\nIf the state |nx, ny> has even parity, meaning (-)^{nx + ny}=even, then integrating an odd function against an even function would yield zero. Conversely, if the state has odd parity, multiplying by an odd function makes it even, potentially leading to a non-zero result.\n\nWait, more precisely, the parity operation P changes x→−x, y→−y. Under P, the wavefunction ψ_{nx,ny}(x,y)=ψ_{nx}(x)ψ_{ny}(y)} transforms as (-1)^{nx + ny}.\n\nAn operator O is said to have parity (+1)^(k) if PO P^{-1} = (-1)^k O.\n\nHere, the operators involved in V are x³ y and x y³. Let's analyze their parities.\n\nUnder parity, x → −x, y → −y.\n\nSo x³ y → (−x)^3 (−y) = (-1)^3*(-1)*x³ y = (-1)^{-3}*x³ y ? Wait, no, let's recast:\n\nActually, applying parity once flips signs. So x³ y after parity becomes (-x)^3 (-y) = (-1)^3 x³ (-1) y = (-1)^4 x³ y = x³ y. Hence, x³ y is invariant under parity, i.e., has parity +1.\n\nSame logic applies to x y"}
{"id": "quantum/2-2002", "questions": "A quark (mass = $m_p/3$) is confined in a cubical box with sides of length 2 fermis = $2 \\times 10^{-15}$ $\\text m$. Find the excitation energy from the ground state to the first excited state in $\\text{MeV}$.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about a quark being confined in a cubical box, right? The question asks for the excitation energy from the ground state to the first excited state in MeV. Hmm, let me think through how to approach this.\n\nFirst off, I remember that when particles like electrons or other quantum objects are confined in boxes, their energies can be quantized due to boundary conditions. This sounds similar to the particle in a box model we studied earlier. So maybe I should use that concept here?\n\nThe mass given is \\( m_p / 3 \\), where \\( m_p \\) is the proton mass. Wait, what's the actual value of the proton mass again? Let me recall... Oh yeah, approximately \\( 938 \\) MeV/c². But since it's divided by three, each quark would have a mass around \\( 312.67 \\) MeV/c². Although wait, actually, protons themselves are made up of quarks, but perhaps they're using some average mass here. Maybe I don't need to worry too much about exact values because sometimes these problems give you constants if needed, but in this case, no specific constant was provided except Planck’s constant probably comes into play.\n\nSo, moving on. The box has sides of length 2 femtometers, which is \\( 2 \\times 10^{-15} \\) meters. That seems really small, as expected for subatomic confinement.\n\nIn the particle-in-a-box model, the allowed energy levels depend on the square of integers related to the dimensions of the box. For a cubic box, the energy levels are determined by:\n\n\\[\nE_{n_x, n_y, n_z} = \\frac{\\hbar^2}{2m} \\left( \\frac{n_x^2 + n_y^2 + n_z^2}{L^2} \\right)\n\\]\n\nWhere:\n- \\( E \\) is the energy,\n- \\( \\hbar \\) is the reduced Planck constant (\\( h/(2\\pi) \\)),\n- \\( m \\) is the mass of the particle,\n- \\( L \\) is the side length of the cube,\n- \\( n_x, n_y, n_z \\) are positive integers representing the quantum numbers along each axis.\n\nSince the quark is in a cubical box, all directions contribute similarly. Now, the ground state corresponds to the lowest possible energy level, which occurs when all quantum numbers are equal to one: \\( n_x = n_y = n_z = 1 \\).\n\nFor the first excited state, things get slightly trickier. In a 3D box, there might be multiple states corresponding to different combinations of quantum numbers. However, typically, the \"first\" excited state refers to the next highest single energy level after the ground state. Since increasing any one of the quantum numbers will lead to higher energy, the first excited state could correspond to two possibilities depending on whether degeneracy exists.\n\nWait, hold on. If we consider non-degenerate cases, then the first excited state may involve just changing one quantum number from 1 to 2 while keeping others at 1. Alternatively, if certain symmetries cause degenerate states, those might come before. But in terms of minimal increase, likely only one quantum number increases.\n\nBut hang on, another thought—since the box is symmetric, the order of energy levels depends on the sum \\( n_x^2 + n_y^2 + n_z^2 \\). Therefore, the first excited state isn’t necessarily unique; however, often people take the smallest increment beyond the ground state regardless of multiplicity.\n\nLet me check: Ground state is (1,1,1): Sum=3\n\nPossible next sums:\n\nIf we go to (2,1,1): Sum=4+1+1=6\n\nAlternatively, (1,1,2): same thing.\n\nIs there a lower sum than 6? No, because going to (sqrt(2)) doesn't make sense—it must be integer quantum numbers.\n\nTherefore, yes, the first excited state is indeed (2,1,1) type, giving a total sum of squares as 6 instead of 3.\n\nThus, the difference between the first excited state and the ground state is based on the change in the sum of squares.\n\nCalculating the energy difference ΔE:\n\nΔE = E_excited - E_ground\n\nWhich translates to:\n\nΔE = [ (6)/(2m) * ħ²/L² ] - [ (3)/(2m) * ħ²/L² ]\n\nSimplify:\n\nΔE = (6 - 3)/[2m] * ħ²/L² = (3)/(2m) * ħ²/L²\n\nHence,\n\nΔE = (3ħ²)/(2mL²)\n\nNow, plugging in the known quantities.\n\nGiven:\n- Mass m = \\( m_p / 3 \\approx 312.67 \\) MeV/c².\n- Length L = 2 fm = \\( 2 \\times 10^{-15} \\) m.\n\nWe also know that Planck's constant ℏ is approximately \\( 6.582 \\times 10^{-25} \\) GeV·s. Wait, units! We’re dealing with MeV, so better convert everything accordingly.\n\nHold on, let me double-check the unit conversions.\n\nPlanck constant ħ ≈ 6.582 × 10^-25 GeV·s. To express in MeV·s, multiply numerator and denominator appropriately.\n\nSince 1 GeV = 1000 MeV, so 6.582e-25 GeV·s = 6.582e-22 MeV·s.\n\nSimilarly, c squared is involved in the mass term. Remembering that E = mc², so masses expressed in MeV/c² require us to handle units carefully.\n\nAlso, note that in natural units (where ħ=c=1), calculations become simpler, but since our output needs to be in MeV, perhaps sticking to SI base units is safer until conversion.\n\nWait, alternatively, considering that in high-energy physics, especially with such tiny lengths, working directly with appropriate constants in compatible units makes more sense.\n\nPerhaps expressing everything in terms of eV, meters, seconds, etc., but converting them properly.\n\nAlternatively, recalling that the expression for kinetic energy in a box involves factors of ħ² over mass times size squared.\n\nI think it's manageable either way, but let me try computing numerically.\n\nCompute ΔE = (3ħ²)/(2mL²)\n\nPlugging in numerical values:\n\nFirstly, compute ħ²:\n\nℏ = 6.582 × 10^-25 GeV·s → Convert to MeV·s: 6.582 × 10^-22 MeV·s.\n\nThen, ℏ² = (6.582e-22)^2 (MeV·s)^2.\n\nMass m = 312.67 MeV/c².\n\nLength L = 2e-15 m.\n\nPutting together:\n\nNumerator: 3*(6.582e-22)^2\n\nDenominator: 2*312.67*(2e-15)^2\n\nCalculate numerator:\n\n3*(6.582e-22)^2 = 3*(4.33e-43) ≈ 1.3e-42 (MeV² s²)\n\nDenominator:\n\n2*312.67*(4e-30) = 2*312.67*4e-30 = approximates to 2*312.67≈625.34; 625.34*4e-30≈2.501e-27 (MeV/c²)*(m²))\n\nHmm, now putting back into ΔE:\n\nΔE = Numerator/Denominator = (1.3e-42 MeV² s² ) / (2.501e-27 MeV/m² )\n\nWait, not quite sure about the units yet. Because in the denominator, we have mass multiplied by length squared, but in reality, the full formula includes division by both mass and length squared.\n\nWait, stepping back, the original equation is:\n\nΔE = (3ħ²)/(2mL²)\n\nUnits-wise, ħ has units of J·s (or equivalently, eV·s), mass m in kg, and length L in meters.\n\nBut since we want the result in MeV, perhaps it's easier to work entirely within consistent units.\n\nAlternative Approach:\n\nUse natural units where ħ = c = 1. Then, expressions simplify significantly.\n\nHowever, switching to natural units requires knowing how to translate various physical quantities.\n\nIn natural units, 1 eV⁻¹ ≈ 1.973×10⁻⁷ m (which is roughly the Compton wavelength scale). Similarly, time scales relate via t ~ x/c, so 1 eV⁻¹ ≈ 6.582"}
{"id": "quantum/5023", "questions": "Consider the one-dimensional motion of an electron confined to a potential well $V(x) = \\frac{1}{2} kx^2$ and subjected also to a perturbing electric field $\\mathbf{F} = F \\hat{x}$.\n\n(a) Determine the shift in the energy levels of this system due to the electric field.  \n\n(b) The dipole moment of this system in state $n$ is defined as $P_n = -e \\langle x \\rangle_n$, where $\\langle x \\rangle_n$, is the expectation value of $x$ in the state $n$. Find the dipole moment of the system in the presence of the electric field.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about an electron in a one-dimensional harmonic oscillator potential with an additional electric field applied. It's split into two parts: part (a) asks for the shift in energy levels due to the electric field, and part (b) wants me to find the dipole moment in each state n. Hmm, let me think through how to approach both parts.\n\nStarting with part (a). So we're dealing with quantum mechanics here, specifically perturbation theory because there's an external electric field acting on the electron which can be considered a small perturbation if it's weak enough compared to the original potential. \n\nThe unperturbed Hamiltonian would just be that of a simple harmonic oscillator:\n\n\\[ H_0 = \\frac{p^2}{2m} + \\frac{1}{2}kx^2 \\]\n\nAnd then the perturbing Hamiltonian comes from the interaction with the electric field. Since the force is given as \\( \\mathbf{F} = F\\hat{x} \\), the corresponding potential energy should be linear in position. Wait, actually, when you apply an electric field E, the potential energy is usually something like V'(x) = -qEx, right? Because the work done against the field is qE times displacement. But since our charge is negative (electron has charge -e), maybe it becomes positive eEx?\n\nWait no, hold on. Let me recall. Electric potential energy U is equal to -q φ, but in terms of electric field, φ = -∫ E dx, so perhaps more accurately, the potential energy change due to moving in an electric field is ΔU = q E x. For an electron, q = -e, so ΔU = -e E x. Therefore, the perturbing Hamiltonian should be:\n\n\\[ H' = -e E x \\]\n\nBut wait, sometimes people write the perturbation as F·r or similar. In any case, yes, the perturbation will involve a term proportional to x multiplied by some constant related to the electric field strength.\n\nSo now, using first-order perturbation theory, the energy shift δE^(1)_n is simply the expectation value of H’ in the nth eigenstate of H₀. That is,\n\n\\[ \\delta E^{(1)}_n = \\langle n | H' | n \\rangle = -e E \\langle n | x | n \\rangle \\]\n\nNow, what do I know about ⟨x⟩ in the harmonic oscillator states? Well, for the harmonic oscillator, all bound states are symmetric around x=0 except... Actually, wait, even though the potential itself is symmetric, does that mean that ⟨x⟩ is zero? Yes! Because the wavefunctions ψₙ(x) are either even or odd functions depending on whether n is even or odd. Specifically, for the ground state (n=0), the wavefunction is Gaussian-like, which is even; hence, integrating x over a symmetric interval gives zero. Similarly, higher even states might still result in ⟨x⟩ being zero because they’re products involving Hermite polynomials which may not introduce asymmetry unless acted upon externally.\n\nTherefore, ⟨x⟩ₙ = 0 for all n. Hence, the first-order correction to the energy is zero. Does that mean there’s no shift? Or am I missing something?\n\nHmm, wait, maybe second order perturbation needs to be considered? If the first-order effect cancels out, perhaps the main contribution comes from the second-order term. Alternatively, could the symmetry argument fail under certain conditions?\n\nLet me double-check. First-order energy shifts come from diagonal matrix elements of H'. Here, H' ~ x, whose integral over symmetric limits vanishes for these states. Thus, indeed, the first-order shift is zero. Then, the leading non-zero shift must come from the second-order perturbation.\n\nIn second-order perturbation theory, the energy shift is given by summing over intermediate states m ≠ n:\n\n\\[ \\delta E^{(2)}_n = \\sum_{m \\neq n} \\frac{|⟨m|H'|n⟩|^2}{E_n^{(0)} - E_m^{(0)}} } \\]\n\nWhere E_n⁰ is the unperturbed energy level.\n\nGiven that H' ∝ x, the transition amplitude between states n and m depends on their overlap via x operator. Now, in the harmonic oscillator, the raising and lowering operators relate different number states. Remembering that x is expressed in terms of creation and annihilation operators:\n\n\\[ x = \\sqrt{\\frac{\\hbar}{2mk}}}(a + a^\\dagger) \\]\n\nThus, applying x to |n> yields combinations of |n+1> and |n-1>. This means that only transitions m = n ± 1 contribute to the matrix element ⟨m|x|n⟩. All other terms vanish because the integrals don't overlap otherwise.\n\nSo, for each n, the possible contributing m values are n±1. However, note that for n=0, m=-1 doesn’t exist, so only m=1 contributes. Similarly, for n approaching infinity, m=n+1 exists but m=n-1 approaches lower indices.\n\nCalculating the specific matrix elements:\n\nFor m = n + 1:\n⟨n+1|x|n⟩ = sqrt[(ħ/(2mk)))] * sqrt(n+1)\n\nSimilarly, for m = n - 1:\n⟨n-1|x|n⟩ = sqrt[(ħ/(2mk))] * sqrt(n)\n\nBecause the ladder operators satisfy [a, a†] = 1, etc., and act such that a|n⟩ = sqrt(n)|n−1⟩, while a†|n⟩ = sqrt(n+1)|n+1⟩.\n\nPutting this together, the square modulus |⟨m|H'|n⟩|² becomes:\n\nFor m = n + 1:\n[ (-e E ) ]² * ( ħ / (2mk) ) * (n + 1 )\n\nSimilarly, for m = n − 1:\n[ (-e E ) ]² * ( ħ / (2mk) ) * n\n\nSince E_n⁰ = (n + ½) ℏω, where ω = sqrt(k/m).\n\nThen, the denominator in the second-order formula is E_n⁰ - E_m⁰.\n\nIf m = n + 1, then E_n⁰ - E_(n+1)^0 = -(ℏω)\nIf m = n - 1, then E_n⁰ - E_(n-1)^0 = +(ℏω)\n\nSo putting everything together, the second-order energy shift is:\n\nδE^(2)_n = Σ [ |⟨m|H'|n⟩|² / (E_n⁰ - E_m⁰) ]\n\nWhich splits into contributions from m = n+1 and m = n-1.\n\nCompute each term separately.\n\nFirst, for m = n + 1:\n\nTerm₁ = [|(-e E)(√((ħ)/(2mk))) (√(n+1))|]^2 / (E_n⁰ - E_{n+1}⁰ )\n= (e² E² (ħ/(2mk))(n+1)) / (-(ℏω))\nSimplify numerator and denominator:\n\nNote that ω = sqrt(k/m), so ℏω = sqrt(ℏ² k/m ). Also, notice that (ħ/(2mk))*(something):\n\nWait, let me compute constants carefully.\n\nWe had x = sqrt(ħ/(2mk)) (a + a† ), so the coefficient squared is (ħ/(2mk)). Multiply by (n+1) for m = n+1.\n\nDenominator is E_n⁰ - E_{n+1}⁰ = [(n + ½) - (n + 3/2)] ℏω = (-1)*ℏω.\n\nSo Term₁ = [e² E² (ħ/(2mk))(n+1)] / (-ℏω )\n\nSimilarly, for m = n -1:\n\nTerm₂ = [|(-e E)(√(ħ/(2mk)) )(√n)|]^2 / (E_n⁰ - E_{n-1}⁰ )\n= (e² E² (ħ/(2mk)) n ) / (+ℏω )\n\nAdding them up:\n\nTotal δE^(2)_n = Term₁ + Term₂\n= [e² E² (ħ/(2mk)) (n+1)/(-ℏω) ] + [e² E² (ħ/(2mk)) n /(ℏω) ]\nFactor common terms:\n\nTake e² E² (ħ/(2mk)) / ℏω outside:\n\n= (e² E² (ħ/(2mk)) / ℏω)[ (n+1)/(-1) + n/(1) ]\nSimplify inside brackets:\n\n= (e² E² (ħ/(2mk)) /"}
{"id": "quantum/2-2005", "questions": "A particle of mass $m$ is constrained to move between two concentric impermeable spheres of radii $r = a$ and $r = b$. There is no other potential. Find the ground state energy and normalized wave function.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem where a particle of mass m is confined between two concentric impermeable spheres with radii r = a and r = b. The task is to find the ground state energy and the corresponding normalized wave function. Hmm, let me think about how to approach this.\n\nFirst off, since there's no potential mentioned except for the boundaries being impermeable, it sounds like we're dealing with a free particle scenario but restricted within these spherical shells. So, effectively, the particle can't exist outside the region a ≤ r ≤ b because those regions are classically forbidden—like infinite potential barriers beyond a and b.\n\nI remember that when particles are confined in such geometries, especially spherically symmetric ones, the solutions involve spherical Bessel functions. But wait, why? Because in three dimensions, solving Schrödinger’s equation in spherical coordinates leads us to use Legendre polynomials (for angular parts) and Bessel functions or their modified versions for radial parts. Since the potential here only depends on radius (or rather, doesn’t depend on angles), the angular part should be separable, leading to spherical harmonics as eigenfunctions.\n\nBut hold on, if the potential is zero inside the shell and infinitely large outside, then maybe the situation simplifies similar to a 3D box but in spherical coordinates instead of Cartesian. In Cartesian coordinates, you solve each direction separately, but in spherical coordinates, things get more involved due to the geometry.\n\nSo perhaps I need to set up the time-independent Schrödinger equation in spherical coordinates. Let me recall: the Laplacian operator in spherical coordinates has terms involving derivatives with respect to r, θ, and φ. However, since the potential V(r) is radially symmetric, meaning it doesn't depend on θ or φ, the angular parts might just give rise to quantum numbers related to orbital angular momentum.\n\nWait, actually, even without any explicit dependence on angles, the separation of variables would still lead to considering the angular parts first. For a central potential, which this isn't exactly—it's not really a potential; it's confinement—the standard method applies. That is, separate into radial and angular equations.\n\nLet me write down the Schrödinger equation:\n\n\\[\n-\\frac{\\hbar^2}{2m} \\nabla^2 \\psi + V(r)\\psi = E\\psi\n\\]\n\nHere, V(r) is zero for a < r < b and infinity otherwise. Therefore, inside the sphere, the equation becomes:\n\n\\[\n\\nabla^2 \\psi = -k^2 \\psi\n\\]\nwhere \\( k^2 = \\frac{2mE}{\\hbar^2} \\).\n\nSince the system is spherically symmetric, ψ will depend only on r, right? Or does it also depend on angles?\n\nHmm, actually, no—if the potential were dependent on angle, then yes, but here it's purely radial. Wait, but in reality, even though the potential is uniform in all directions, the wavefunction could still have angular dependencies unless specified otherwise. Oh, but given that the potential is spherically symmetric, the angular parts must correspond to states with definite angular momentum. Hence, they'll involve spherical harmonics Y_l^{m}(θ,φ). \n\nHowever, for the lowest energy state, called the ground state, what do we expect? It should have the lowest possible energy, which usually corresponds to the smallest allowed wavelength fitting into the container. Also, symmetry-wise, the ground state tends to be spherically symmetric itself, implying l=0, i.e., no angular nodes. This makes sense because higher angular momenta require more \"twists\" in the wavefunction, increasing kinetic energy.\n\nTherefore, assuming l=0 seems reasonable for the ground state. If that's correct, then the angular part reduces to a constant, specifically Y_0^0(θ,φ)=1/sqrt(4π). Thus, our wavefunction ψ(r) can be written as R(r)/sqrt(4π), where R(r) satisfies the radial equation.\n\nThe radial equation for l=0 in spherical coordinates is:\n\n\\[\n\\left[ -\\frac{\\hbar^2}{2m} \\left( \\frac{d^2}{dr^2} + \\frac{2}{r}\\frac{d}{dr} \\right ) \\right ] R(r) = ER(r)\n\\]\n\nSimplifying,\n\n\\[\n\\frac{d^2R}{dr^2} + \\frac{2}{r}\\frac{dR}{dr} + \\frac{2mE}{\\hbar^2} R = 0\n\\]\n\nThis looks familiar—I think it relates to the spherical Bessel equation. Indeed, substituting x = kr, where k² = 2mE/ħ², transforms the equation into something resembling the Bessel differential equation.\n\nAlternatively, another substitution often used is u(r) = r R(r). Then, plugging back into the equation:\n\nCompute d/dr [u] = R + r dR/dr\n\nThen, compute d²u/dr² = 2 dR/dr + r d²R/dr²\n\nSubstituting into the original equation:\n\nFrom above,\n- Original DE:\n    d²R/dr² + (2/r)dR/dr + (2mE/ħ²) R = 0\n\nMultiply through by r²:\n\nr² d²R/dr² + 2r dR/dr + (2mE/ħ²)r² R = 0\n\nExpressed in terms of u(r):\n\nNote that u'' = r R'' + 2 R' + ... Wait, better yet, express everything in terms of u.\n\nGiven u = r R => R = u / r\n\nThus, dR/dr = (du/dr * r - u)/r²\n\nSimilarly, d²R/dr² = [d/dx (du/dr * r - u)] / r² - [something]/r³... Maybe too messy.\n\nPerhaps using the substitution directly helps. Alternatively, recognizing that the equation resembles the one-dimensional SE with effective potential.\n\nWait, stepping back, for l=0, the radial equation is equivalent to a 1D SE with an additional term proportional to 1/r². Specifically, the effective potential includes centrifugal barrier term, but for l=0, that term vanishes. So, indeed, the equation becomes simpler.\n\nIn fact, setting f(r) = R(r), the equation is:\n\nf''(r) + (2/r)f'(r) + (2mE/ħ²)f(r) = 0\n\nWhich is same as:\n\n(f(r) r)'')/(r) = -(2mE/ħ²) f(r)\n\nNot sure if helpful. Another thought: make substitution y(r) = r f(r); then dy/dr = f(r) + r df/dr\n\nAnd d²y/dr² = 2 df/dr + r d²f/dr²\n\nPlugging into the equation:\n\nOriginal equation multiplied by r² gives:\n\nr² f'' + 2r f' + (2mE/ħ²) r² f = 0\n\nNow substitute y = r f:\n\nWe know that y'' = r f'' + 2 f'\n\nHence, from the equation:\n\n(y'' - 2 f') + 2r f' + (2mE/ħ²) r² f = 0\n\nWait, that may complicate further. Perhaps alternative substitution.\n\nAnother idea: change variable z = kr, where k² = 2mE/ħ². Then dz = k dr, so dr = dz/k.\n\nRewrite the ODE in terms of z:\n\ndf/dr = (df/dz)(dz/dr) = k df/dz\n\nSimilarly, d²f/dr² = k² d²f/dz²\n\nPlug into the equation:\n\nk² d²f/dz² + (2/z) k df/dz + f(z) = 0\n\nDivide both sides by k²:\n\nd²f/dz² + (2)/(zk) df/dz + (1/k²) f = 0\n\nHmm, not particularly useful unless k cancels out somehow.\n\nWait, alternatively, suppose we define g(z) = f(k^{-1} z)? Not sure.\n\nMaybe going back to the beginning, recognize that the general solution for the radial equation when l=0 is a combination of sine and cosine functions scaled appropriately.\n\nIndeed, looking up the form, the regular solution (finite at origin) for l=0 is sin(kr)/(kr). Wait, but in our case, the boundary conditions aren't at r=0 but at r=a and r=b.\n\nAh! Right, because the particle is confined between a and b, not starting at zero. So, unlike the usual infinite potential well which starts at zero, here the inner wall is at a>0.\n\nThat complicates matters slightly because now the solutions won't automatically vanish at r=0—they’re already non-zero somewhere else.\n\nSo, thinking again, the general solution for the radial equation when l=0 is:\n\nR(r) = A sin(kr) + B cos(kr)\n\nBut subject to certain boundary conditions.\n\nWhat are the boundary conditions here? Well, the wavefunction must go to zero at r=a and r=b because the walls are impen"}
{"id": "quantum/6003", "questions": "A nonrelativistic particle of mass $\\text m$ and energy $E$ scatters quantum-mechanically in a central potential $V(r)$ given by\n\n$$\nV(r) = \\frac{\\hbar^2}{2m} U(r), \\quad U(r) = - 2\\left(\\frac{\\lambda}{\\cosh \\lambda r}\\right)^2,\n$$\n\nwhere $\\lambda$ is a parameter. This particle potential has the property that the cross section $\\sigma(E)$ grows larger and larger as $E \\rightarrow 0$, diverging for $E = 0$. Clearly, for $E$ very small the cross section is dominated by the s-wave ($l = 0$) contribution. Thus for small $E$ one needs to compute only the $l = 0$ partial wave amplitude. In connection with this, to save you mathematical efforts, you are told mathematically that the equation\n\n$$\n\\frac{d^2 \\phi}{dr^2} + A \\phi = U(r) \\phi,\n$$\n\nwhere $A$ is a positive constant, has a general solution\n\n$$\n\\phi = \\alpha (\\lambda \\tanh \\lambda r - ik)e^{ikr} + \\beta (\\lambda \\tanh \\lambda r + ik)e^{-ikr},\n$$\n\nwhere $k = \\sqrt{A}$ and $\\alpha$ and $\\beta$ are integration constants. Recall $\\tanh x = \\frac{e^x-e^{-x}}{e^x+e^{-x}}$ and compute $\\sigma(E)$ for $E \\rightarrow 0$.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about scattering in a central potential using quantum mechanics. The setup involves a particle of mass \\( m \\) and energy \\( E \\) interacting with a potential \\( V(r) \\). They've provided me with some specific forms: \n\n\\( V(r) = \\frac{\\hbar^2}{2m} U(r) \\)\n\nand \n\n\\( U(r) = -2\\left( \\frac{\\lambda}{\\cosh \\lambda r} \\right)^2 \\).\n\nThey also mention that when the energy \\( E \\) approaches zero, the cross-section \\( \\sigma(E) \\) becomes large because it's dominated by the s-wave (which corresponds to angular momentum quantum number \\( l=0 \\)). So my task is to find the cross-section \\( \\sigma(E) \\) as \\( E \\to 0 \\).\n\nThe hint they gave includes a differential equation:\n\n\\( \\frac{d^2 \\phi}{dr^2} + A \\phi = U(r)\\phi \\),\n\nwith \\( A > 0 \\), which apparently relates to our problem somehow. And they tell us the general solution is:\n\n\\( \\phi = \\alpha (\\lambda \\tanh \\lambda r - i k ) e^{i k r } + \\beta (\\lambda \\tanh \\lambda r + i k ) e^{-i k r } \\),\n\nwhere \\( k = \\sqrt{A} \\) and \\( \\alpha, \\beta \\) are constants.\n\nI need to figure out how all these pieces fit together. Let me start from what I know.\n\nIn scattering problems, especially in the context of partial waves, each angular momentum state contributes to the total cross-section. For low energies, higher angular momenta contribute less significantly compared to lower ones, particularly the s-wave (\\( l=0 \\)) dominates here since \\( E \\to 0 \\).\n\nSo first, let's recall the radial Schrödinger equation for a central potential. It should be something like:\n\n\\( \\left[ -\\frac{\\hbar^2}{2m} \\nabla^2 + V(r) \\right] \\psi = E \\psi \\).\n\nBut we can separate variables into spherical coordinates, leading to the radial part involving the effective potential including centrifugal barrier term. However, since we're dealing with \\( l=0 \\), there won't be any centrifugal term—so maybe things simplify?\n\nWait, but actually, even without considering separation yet, perhaps their given ODE already incorporates the necessary terms? Because the equation they give seems similar to the radial equation after substitution.\n\nLet me think more carefully.\n\nFor a central potential, the time-independent Schrödinger equation separates into radial and angular parts. After factoring out the angular dependence via spherical harmonics, the radial equation becomes:\n\n\\( -\\frac{\\hbar^2}{2m} \\left( \\frac{d^2 u_l}{dr^2} + \\frac{2}{r} \\frac{du_l}{dr} \\right) + \\left( V(r) + \\frac{\\hbar^2 l(l+1)}{2m r^2} \\right)u_l = E u_l \\),\n\nwhere \\( u_l(r) = r R_l(r) \\) if I remember correctly.\n\nSince we’re looking at \\( l=0 \\), the centrifugal term disappears, simplifying the equation. Also, substituting \\( u_0(r) = f(r) \\times ... \\)? Wait, no—the exact form might not matter right now.\n\nAlternatively, sometimes people use substitutions where \\( \\phi(r) = u(r)/k \\), or other normalizations. Maybe the given ODE is related to such a substitution.\n\nLooking back at the given DE:\n\n\\( \\frac{d^2 \\phi}{dr^2} + A \\phi = U(r) \\phi \\).\n\nComparing this to the standard radial equation, perhaps they made a substitution where \\( A = k^2 \\), with \\( k \\) being related to the wavenumber. Since \\( E \\) is going to zero, the kinetic energy is negligible except near the origin, but wait—it’s still a bound state scenario? Or is it scattering?\n\nActually, the question mentions \"scattering\", so probably it's a scattering state rather than a bound state. But then again, the potential could support both depending on parameters.\n\nGiven that \\( E \\to 0 \\), the wavelength would become long, meaning the de Broglie wavelength tends towards infinity, implying that the particles behave almost classically over macroscopic distances—but in QM, it means that the phase shifts dominate the behavior.\n\nNow, focusing on the given solution:\n\n\\( \\phi = \\alpha (\\lambda \\tanh \\lambda r - i k ) e^{i k r } + \\beta (\\lambda \\tanh \\lambda r + i k ) e^{-i k r } \\).\n\nThis looks like a combination of incoming and outgoing plane waves modulated by factors involving hyperbolic tangent functions. That makes sense because potentials often lead to solutions expressed in terms of exponentials or hyperbolics due to boundary conditions.\n\nTo get started, perhaps I should relate the given ODE to the actual Schrödinger equation. Let me write down the radial Schrödinger equation for \\( l=0 \\):\n\nStarting from the full SE,\n\n\\( \\left(-\\frac{\\hbar^2}{2m} \\nabla^2 + V(r) \\right) \\psi = E \\psi \\).\n\nExpressed in spherical coordinates, separating off the angular parts gives:\n\n\\( -\\frac{\\hbar^2}{2m} \\frac{1}{r^2} \\frac{d}{dr} \\left( r^2 \\frac{dR}{dr} \\right) + V(r) R = E R \\).\n\nMultiplying through by \\( r^2 \\) and rearranging:\n\n\\( -\\frac{\\hbar^2}{2m} \\frac{d^2}{dr^2}(r R) + [V(r)] r R = E r R \\).\n\nIf we define \\( u(r) = r R(r) \\), then the equation becomes:\n\n\\( -\\frac{\\hbar^2}{2m} \\frac{d^2 u}{dr^2} + V(r) u = E u \\).\n\nWhich simplifies further to:\n\n\\( \\frac{d^2 u}{dr^2} = \\frac{-2m}{\\hbar^2} (V(r) - E) u \\).\n\nHmm, comparing this to the given ODE:\n\nTheir equation is \\( d^2 \\phi/dr^2 + A \\phi = U(r) \\phi \\).\n\nSo equating coefficients, we must have:\n\n- Coefficient of \\( \\phi \\): \\( A = (-2m/\\hbar^2)(-E) \\)?\n\nWait, hold on. From the Schrodinger equation above, moving everything around:\n\n\\( \\frac{d^2 u}{dr^2} = \\frac{-2m}{\\hbar^2} (V(r) - E) u \\).\n\nThus, bringing all terms to left-hand side:\n\n\\( \\frac{d^2 u}{dr^2} + \\frac{2m}{\\hbar^2} (E - V(r)) u = 0 \\).\n\nCompare to the given equation:\n\n\\( \\frac{d^2 \\phi}{dr^2} + A \\phi = U(r) \\phi \\).\n\nTherefore, matching terms:\n\nLeft sides match structure-wise; thus,\n\n\\( A = \\frac{2m}{\\hbar^2} E \\),\nand \nthe coefficient before \\( U(r) \\phi \\) comes from \\( - \\frac{2m}{\\hbar^2} V(r) \\).\n\nFrom the definition of \\( V(r) \\), we had \\( V(r) = \\frac{\\hbar^2}{2m}"}
{"id": "quantum/5012", "questions": "For a one-dimensional harmonic oscillator, introduction of the dimensionless coordinate and energy variables $y = x(m\\omega_0/\\hbar)^{1/2}$ and $\\varepsilon_n = 2E_n/\\hbar\\omega_0$ gives a Schrödinger equation with kinetic energy operator $T = -\\frac{d^2}{dy^2}$ and potential energy $V = y^2$.\n\n(a) Using the fact that the only non-vanishing dipole matrix element is $\n\\langle n+1 | y | n \\rangle = \\sqrt{\\frac{n+1}{2}} $(and its Hermitean conjugate), find values for all the non-vanishing matrix elements of $y^3$ that connect to the ground state $|0\\rangle$.\n\n(b) The oscillator is perturbed by an harmonic potential $V = \\alpha y^3$. Find the correction to the ground state energy in the lowest non-vanishing order. (If you did not get complete answers in part(a), leave your result in terms of clearly defined matrix elements, etc.)", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about a one-dimensional quantum harmonic oscillator. It's divided into two parts: part (a) asks me to find the non-vanishing matrix elements of \\( y^3 \\) connecting to the ground state \\( |0\\rangle \\). Part (b) involves finding the first-order correction to the ground state energy when there's a perturbation given by \\( V = \\alpha y^3 \\).\n\nStarting with part (a): They mentioned that the only non-zero dipole matrix element is \\( \\langle n+1 | y | n \\rangle = \\sqrt{(n+1)/2} \\) and its Hermitian conjugate. So, they want me to use this information to compute the matrix elements of \\( y^3 \\) from some states down or up to the ground state.\n\nFirst, let me recall what \\( y \\) represents here. In the problem statement, it says \\( y = x \\left( m\\omega_0 / \\hbar \\right)^{1/2} \\), which makes sense because we're dealing with dimensionless coordinates. Also, the Hamiltonian has been transformed such that the kinetic term becomes \\( T = -d^2/dy^2 \\) and the potential is \\( V = y^2 \\). That aligns with standard forms after scaling.\n\nNow, since \\( y \\) is related to position \\( x \\), raising and lowering operators come into play. But maybe instead of going through ladder operators directly, I can think about how powers of \\( y \\) act on the basis states.\n\nGiven that \\( y \\) connects \\( |n\\rangle \\) to \\( |n+1\\rangle \\) and vice versa via those matrix elements, then higher powers like \\( y^3 \\) would involve more transitions. Specifically, each factor of \\( y \\) could potentially change the state index by ±1. Therefore, applying three factors might lead to changes by +3, but considering combinations where sometimes you go up and other times down.\n\nBut wait, actually, no—each application of \\( y \\) either increases or decreases the state index by 1. However, if I'm taking \\( y^3 \\), perhaps acting as \\( y \\cdot y \\cdot y \\), each multiplication will affect the indices accordingly.\n\nWait, hold on—I need to be precise. Let me consider expanding \\( y^3 \\) in terms of creation and annihilation operators. Since \\( y \\propto (a + a^\\dagger) \\), right? Because normally, \\( x \\) is proportional to \\( (a + a^\\dagger) \\), so \\( y \\) should also be similar.\n\nLet me write:\n\n\\( y = c (a + a^\\dagger) \\),\n\nwhere \\( c \\) is some constant. Then,\n\n\\( y^3 = c^3 (a + a^\\dagger)^3 \\).\n\nExpanding this cube, using binomial expansion:\n\n\\( (a + a^\\dagger)^3 = a^3 + 3a^2 a^\\dagger + 3a (a^\\dagger)^2 + (a^\\dagger)^3 \\).\n\nSo, \\( y^3 \\) consists of these four terms multiplied by \\( c^3 \\).\n\nTherefore, when calculating matrix elements \\( \\langle m | y^3 | n \\rangle \\), each term contributes depending on whether the number of creations exceeds annihilations or vice versa.\n\nSince our goal is to find the matrix elements connected to the ground state \\( |0\\rangle \\), specifically looking for \\( \\langle k | y^3 | 0 \\rangle \\neq 0 \\) or \\( \\langle 0 | y^3 |k \\rangle \\neq 0 \\).\n\nLooking at the expression above, let's see which terms survive when sandwiched between \\( |m\\rangle \\) and \\( |0\\rangle \\):\n\n- For \\( a^3 \\): Annihilating thrice starting from \\( |0\\rangle \\) leads to zero.\n- Similarly, any term with more than one annihilation may vanish unless compensated by enough creation.\n\nWait, better yet, let's evaluate each term separately.\n\nCompute \\( \\langle m | a^3 | 0 \\rangle \\):\nThis requires \\( a^3 |0\\rangle = 0 \\), so this term doesn't contribute anything except possibly when \\( m=0 \\), but even then, it's still zero.\n\nSimilarly, \\( \\langle m | 3a^2 a^\\dagger | 0 \\rangle \\).\nHere, \\( a^\\dagger |0\\rangle = |1\\rangle \\),\nthen \\( a^2 |1\\rangle = a (\\sqrt{1}|0\\rangle ) = \\sqrt{1} a |0\\rangle = 0 \\).\nThus, again, this term results in zero.\n\nNext, \\( \\langle m | 3a (a^\\dagger)^2 | 0 \\rangle \\).\nStart with \\( (a^\\dagger)^2 |0\\rangle = |2\\rangle \\times something \\).\nSpecifically, \\( a^\\dagger |0\\rangle = |1\\rangle \\), \\( a^\\dagger |1\\rangle = \\sqrt{2} |2\\rangle \\).\nThen, apply \\( a \\) once:\n\\( a |2\\rangle = \\sqrt{2} |1\\rangle \\).\nHence, overall,\n\\( 3a (a^\\dagger)^2 |0\\rangle = 3 * sqrt(2) * sqrt(2) |1\\rangle ? Wait, let me do it carefully.\n\nActually, computing \\( (a^\\dagger)^2 |0\\rangle \\):\nEach time you apply \\( a^\\dagger \\), you increase the state by 1 and multiply by sqrt(n+1). Starting from |0>, first \\( a^\\dagger |0> = |1> \\), second \\( a^\\dagger |1> = sqrt(2)|2>. Thus, \\( (a^\\dagger)^2 |0> = sqrt(2!) |2> = sqrt(2) |2>.\n\nApplying \\( a \\) to |2> gives sqrt(2) |1>.\nMultiply by 3: total coefficient is 3*sqrt(2)*sqrt(2)= 3*(2)=6?\n\nWait, no—the process is:\n\n\\( 3a (a^\\dagger)^2 |0\\rangle = 3a [ (a^\\dagger)^2 |0\\rangle ] = 3a [\\sqrt{2!} |2\\rangle] = 3*\\sqrt{2}*a |2\\rangle = 3*\\sqrt{2}*\\sqrt{2} |1\\rangle = 3*2 |1\\rangle = 6 |1\\rangle \\).\n\nHmm, okay, so this term contributes 6 delta_{m,1}, meaning that \\( \\langle 1 | y^3 |0 \\rangle = 6c^3 \\)?\n\nWait, but earlier steps had constants involved. Remembering that \\( y = c(a + a^\\dagger) \\), so \\( c = \\sqrt{something} \\)—wait, originally, \\( y = x (mω₀/h̄)^½ \\). And usually, \\( x = √(\\hbar/(2mω)) (a + a†) ), so substituting back, probably \\( y = (a + a†)√(mω/h̄)} ?\n\nWait, let me check units. Alternatively, perhaps just take note that \\( y \\) is already scaled such that the potential is \\( y² \\), so likely \\( y = (a + a†) \\sqrt{... } \\). Maybe exact value isn’t critical now; rather, focus on structure.\n\nAlternatively, perhaps set aside the coefficients for later consideration.\n\nAnyway, moving forward, last term: \\( \\langle m | (a^\\dagger)^3 |0\\rangle \\).\n\nThat’s straightforward: \\( (a^\\dagger)^3 |0\\rangle = sqrt(3!) |3\\rangle = sqrt(6) |3\\rangle \\). Hence, \\( \\langle m | (a^\\dagger)^3 |0\\rangle = sqrt(6) δ_{m,3} \\).\n\nPutting together all contributions:\n\nFrom \\( a^3 \\): nothing\n\nFrom \\( 3a^2 a^\\dagger \\): nothing\n\nFrom \\( 3a (a^\\"}
{"id": "quantum/1-1004", "questions": "﻿Explain what was learned about quantization of radiation or mechanical system from two of the following experiments:\n\n(a) Photoelectric effect.  \n(b) Black body radiation spectrum.  \n(c) Franck-Hertz experiment.  \n(d) Davisson-Germer experiment.  \n(e) Compton scattering.  \n\nDescribe the experiments selected in detail, indicate which of the measured effects were non-classical and why, and explain how they can be understood as quantum phenomena. Give equations if appropriate.\n", "graphs": null, "llm_answers": "<think>\nOkay, so I need to explain what was learned about the quantization of radiation or mechanical systems from two specific experiments among those listed: photoelectric effect, blackbody radiation, Franck-Hertz, Davisson-Germer, and Compton scattering.\n\nFirst, let me recall each experiment briefly.\n\nThe photoelectric effect involves light shining on a metal surface causing electrons to eject. Einstein explained this with photons having energy proportional to frequency, leading to the equation \\( KE = h\\nu - \\phi \\). This showed that electromagnetic radiation is quantized into particles called photons.\n\nBlackbody radiation refers to the emission spectrum of objects. Classical theory predicted a divergence (ultraviolet catastrophe), but Planck solved it by assuming energy quanta, introducing his constant \\( h \\).\n\nFranck-Hertz experiment involved bombarding mercury atoms with electrons; only certain energies caused transitions, showing discrete energy levels—quantum jumps.\n\nDavisson-Germer used electron diffraction off crystals, proving wave-particle duality since interference patterns emerged, supporting de Broglie's hypothesis.\n\nCompton scattering demonstrated wavelength shift when X-rays scatter off electrons, confirming photon momentum transfer consistent with particle-like behavior.\n\nI think choosing the photoelectric effect and either blackbody or Franck-Hertz would cover both radiation and mechanical aspects well.\n\nLet me pick (a) Photoelectric Effect and (b) Black Body Radiation Spectrum because one deals directly with radiation quantization and the other shows quantization in matter through energy states.\n\nFor each, describe them in detail, point out non-classical results, and relate to quantum concepts.\n\nPhotoelectric Effect:\n- Experiment setup: Light shines on cathode, emits electrons collected via anode, measuring current/voltage.\n- Non-classical result: Current stops above stopping potential regardless of intensity beyond threshold frequency. Intensity affects number, not max KE.\n- Quantum explanation: Photons have energy packets; must exceed work function for ejection. Max KE depends linearly on frequency, not intensity.\n\nBlack Body Radiation:\n- Setup: Measure emitted radiation vs wavelength/frequency from heated object.\n- Problem: Rayleigh-Jeans law diverges UV; Wien’s displacement fits some regions but fails others.\n- Planck’s fix: Energy quantized in multiples of \\( h\\nu \\); derived correct distribution matching experimental data.\n- Resultant formula: \\( B_\\nu(T) = \\frac{2h\\nu^3}{c^2} \\cdot \\frac{1}{e^{h\\nu/(kT)} - 1} \\)\n- Shows quantization of oscillator energies in cavity walls.\n\nSo these two experiments highlight different aspects of quantization—one in radiation (photons) and another in atomic oscillators' energy levels.\n\nNow, structure my response accordingly, making sure to include details, equations where relevant, and clearly link observations to quantum principles.\n</think>\n\nTo understand the quantization of radiation and mechanical systems, we analyze two pivotal experiments: the **photoelectric effect** and **black body radiation spectrum**. These experiments provided foundational insights into quantum mechanics by revealing phenomena inconsistent with classical theories.\n\n---\n\n### **(a) The Photoelectric Effect**\n\n#### Description of the Experiment:\nIn the photoelectric effect, light of sufficient frequency illuminates a metallic surface, causing electrons to be ejected. By measuring the kinetic energy of the ejected electrons and the photocurrent, key properties such as the stopping voltage and threshold frequency can be determined.\n\n#### Key Observations:\n1. **Threshold Frequency**: Electrons are ejected only if the incident light has a minimum frequency (\\(\\nu_0\\)), known as the threshold frequency. Below this frequency, no electrons are ejected regardless of the light's intensity.\n2. **Stopping Voltage**: Beyond the threshold frequency, increasing the light intensity increases the photocurrent but does not affect the maximum kinetic energy of the ejected electrons.\n3. **Instantaneous Emission**: Electron emission occurs immediately upon illumination without any delay, even at low intensities.\n\n#### Non-Classical Results:\nClassically, the energy transferred should depend on the amplitude (intensity) of the light, predicting continuous absorption until enough energy is gathered. However:\n- The dependence of the maximum kinetic energy solely on the frequency of light contradicts classical predictions.\n- The absence of delayed emission rules out the idea of cumulative energy absorption.\n\n#### Quantum Explanation:\nEinstein proposed that light consists of discrete energy packets called photons, each carrying energy \\(E = h\\nu\\), where \\(h\\) is Planck's constant and \\(\\nu\\) is the frequency. For an electron to escape, a single photon must supply at least the work function (\\(\\phi\\)) of the material:\n\\[\nKE_{max} = h\\nu - \\phi\n\\]\nThis explains all observed features, establishing the quantized nature of electromagnetic radiation.\n\n---\n\n### **(b) Black Body Radiation Spectrum**\n\n#### Description of the Experiment:\nA black body radiator absorbs all incoming radiation and re-emits it across a range of wavelengths depending on its temperature. Early attempts to model the spectrum using classical electromagnetism failed, particularly in explaining the ultraviolet catastrophe—the prediction of infinite energy density at high frequencies.\n\n#### Key Observations:\n1. **Wien's Displacement Law**: The peak wavelength shifts to shorter lengths as temperature increases.\n2. **Ultraviolet Catastrophe**: Classical models predict unbounded energy density at short wavelengths, contradicting experimental results.\n\n#### Non-Classical Results:\nThe failure of classical theories to match experimental spectra highlighted the necessity of a new approach. Specifically:\n- At higher frequencies, the classical Rayleigh–Jeans law overestimates spectral radiance, while experiments show a sharp decline.\n- No classical mechanism could account for the observed \"Planck curve.\"\n\n#### Quantum Explanation:\nMax Planck resolved the issue by postulating that the energy of oscillating charges in the black body is quantized:\n\\[\nE_n = nh\\nu\n\\]\nwhere \\(n\\) is a positive integer. Using statistical methods, he derived the correct expression for the spectral radiance:\n\\[\nB_\\nu(T) = \\frac{2h\\nu^3}{c^2} \\cdot \\frac{1}{e^{h\\nu / k_B T} - 1}\n\\]\nHere, \\(k_B\\) is Boltzmann's constant and \\(T\\) is the absolute temperature. This derivation introduced quantization into physical systems, demonstrating that energy exchanges occur in discrete units.\n\n---\n\n### Conclusion:\nBoth the photoelectric effect and black body radiation experiments revealed fundamental limitations of classical physics. They underscored the necessity of quantizing energy, whether in the form of photons (radiation) or atomic vibrations (mechanical systems). These discoveries laid the groundwork for modern quantum mechanics, emphasizing the discrete nature of energy interactions at microscopic scales.\n\nFinal Answer:\n\\[\n\\boxed{\\text{(a) Photoelectric Effect and (b) Black Body Radiation}}\n\\]"}
{"id": "quantum/1-1003", "questions": "﻿Derive, estimate, guess or remember numerical values for the following, to within one order of magnitude:\n\n(a) The electron Compton wavelength.  \n(b) The electron Thomson cross section.  \n(c) The Bohr radius of hydrogen.  \n(d) The ionization potential for atomic hydrogen.  \n(e) The hyperfine splitting of the ground-state energy level in atomic hydrogen.  \n(f) The magnetic dipole moment of $^3\\text{Li}^7 (Z = 3)$ nucleus.  \n(g) The proton-neutron mass difference.  \n(h) The lifetime of free neutron.  \n(i) The binding energy of a helium-4 nucleus.  \n(j) The radius of the largest stable nucleus.  \n(k) The lifetime of a $\\pi^0$ meson.  \n(l) The lifetime of a $\\mu^-$ meson.  \n", "graphs": null, "llm_answers": "<think>\nOkay, so I have this list of physics problems where I need to find approximate numerical values for various physical quantities related to particles like electrons, protons, neutrons, nuclei, etc., all within about an order of magnitude. Hmm, let me try to tackle each part one by one.\n\nStarting with part (a): The electron Compton wavelength. From what I recall, the Compton wavelength is given by λ_C = h/(m_e c), where h is Planck's constant, m_e is the electron mass, and c is the speed of light. Let me plug in some numbers: h is approximately 6.626 x 10^-34 J·s, m_e is around 9.11 x 10^-31 kg, and c is roughly 3 x 10^8 m/s. So calculating that would be something like (6.6e-34)/(9.1e-31 * 3e8). Wait, doing the denominator first: 9.1e-31 times 3e8 is about 2.73e-22. Then dividing 6.6e-34 by 2.73e-22 gives approximately 2.4e-12 meters. But wait, sometimes it's expressed in picometers; since 1 pm is 1e-12 m, so maybe around 2.4 picometers? Or perhaps more accurately known as ~2.42 pm from memory?\n\nMoving on to (b): Electron Thomson cross-section. The Thomson cross-section formula is σ_T = (8π/3)(e²/(4πε₀ mc²))². Alternatively, another way to write it using constants might help. Since e is charge, ε₀ permittivity, but actually, there’s also a relation involving the classical electron radius r_e which is e²/(4πε₀ m_e c²). And then σ_T is π r_e squared multiplied by 8/3 because of relativistic effects... Wait no, correction: Actually, the Thomson cross-section is just the total scattering cross-section for an electron treated classically, and its value is about 6.65e-29 square meters. That number seems familiar—yes, often written as 6.65×10⁻²⁹ m². So probably around 6.6e-29 m².\n\nPart (c): Bohr radius of hydrogen. This should be straightforward—the Bohr model says a_0 = n²ħ²/(μ Z e²), but for hydrogen, n=1, μ≈m_e (since proton much heavier), Z=1. Plugging in ħ ≈ 1.05e-34 Js, m_e same as before, e again 1.6e-19 C. Calculating numerator: (1.05e-34)^2 / (something else). Maybe better to use the standard value—I think the Bohr radius is about 0.5 angstroms, right? Which is 5e-11 meters. Yeah, exactly, 5.29e-11 m if precise, but we can say ~5e-11 m.\n\nFor (d): Ionization potential of atomic hydrogen. The Rydberg constant R_H is about 13.6 eV, corresponding to the ionization energy when moving from n=1 to infinity. So yes, the ionization potential is approximately 13.6 eV per atom. So that's easy enough—it's about 14 eV, give or take.\n\nNext up, (e): Hyperfine splitting of the ground state in hydrogen. Oh yeah, this relates to the Lamb shift and was famously measured precisely. If I'm not mistaken, the hyperfine splitting corresponds to a transition frequency around 1.4 GHz, which translates into energy via E=hν. Using ν=1.4e9 Hz, h=6.626e-34 J·s, so E=(6.626e-34)*(1.4e9)= approx 9.27e-25 Joules. To convert that into electron volts, divide by e (~1.6e-19 C): 9.27e-25 / 1.6e-19 ≈ 5.8e-6 eV. So about 6 microelectronvolts. Another approach could involve knowing that the splitting is about 6x10^-6 eV.\n\nNow, (f): Magnetic dipole moment of ^3Li^7 nucleus. Lithium has a nuclear spin—I believe ^3Li^7 has a spin of 3/2. Its magnetic dipole moment is typically denoted as μ_N, where N stands for nucleus. For such light nuclei, their moments can vary, but lithium-7 has a positive magnetic moment. Looking back, I think the magnetic dipole moment for Li-7 is about +1.7 × 10^{-26} J/T. Alternatively, expressing it in terms of the nuclear magneton unit, which is μ_n = eħ/(2m_p), but regardless, the actual value is usually quoted as around 1.7e-26 J/T. So that's my best guess here.\n\nOn to (g): Proton-neutron mass difference. Neutrons are slightly heavier than protons due to electromagnetic interactions. The exact masses are m_p ≈ 938 MeV/c² and m_n ≈ 939.57 MeV/c². Subtracting these gives Δm ≈ 1.57 MeV/c². Converting that to kilograms, since 1 MeV/c² is about 1.78e-30 kg, so 1.57*1.78e-30 ≈ 2.79e-30 kg. However, people often express this mass difference in terms of energy equivalent, so either way, whether you present it as ~1.5 MeV or ~2.8e-30 kg, both make sense. Probably safer to go with the energy form unless specified otherwise.\n\nThen, (h): Lifetime of a free neutron. Free neutrons decay via beta emission into a proton, electron, and antineutrino. Their mean lifetime is experimentally determined to be about 14 minutes and 42 seconds, which converts to roughly 880 seconds. More precisely, it's approximately 885.7 ± 0.8 s according to recent measurements, so rounding off, about 8.8e2 seconds or 880 s.\n\nProblem (i): Binding energy of a He-4 nucleus. Helium-4 is a very tightly bound nucleus. Each nucleon contributes significantly. The binding energy can be calculated using the semiempirical mass formula, but alternatively, recalling that the binding energy per nucleon peaks around 8.8 MeV/nucleon for He-4. With four nucleons, that makes BE ≈ 35.2 MeV. Alternatively, looking up, the actual binding energy is about 28 MeV, considering the specifics. Wait, hold on—that doesn't add up. Because if each nucleon binds with others, the total isn’t simply additive. Let me double-check: The mass defect for He-4 compared to separate protons and neutrons. Mass of He-4 is about 4.0026 u, while two protons (each ~1.0073 u) and two neutrons (each ~1.0087 u) sum to 2*(1.0073+1.0087)=~4.031 u. Difference is ~0.0284 u, converted to energy via E=mΔc². One u is ~931.5 MeV/c², so 0.0284u *931.5MeV/u ≈ 26.4 MeV. Hence, the binding energy is approximately 26.7 MeV. Close enough, so I'll put ~27 MeV.\n\nQuestion (j): Radius of the largest stable nucleus. Stability tends to decrease beyond certain sizes. Uranium is the heaviest naturally occurring element, with U-238 being the most common isotope. The radius of a nucleus scales with A^(1/3), where A is the mass number. For uranium, A≈238, so r ≈ r_0*A^(1/3); typical r_0 is about"}
{"id": "quantum/5015", "questions": "A quantum mechanical rigid rotor constrained to rotate in one plane has moment of inertia $ I $ about its axis of rotation and electric dipole moment $ \\mu $ (in the plane).\n\nThis rotor is placed in a weak uniform electric field $ \\varepsilon $, which is in the plane of rotation. Treating the electric field as a perturbation, find the first non-vanishing corrections to the energy levels of the rotor.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem where there's a quantum mechanical rigid rotor confined to rotate in one plane. It has a moment of inertia \\( I \\) around its axis and an electric dipole moment \\( \\mu \\), both lying in that same plane. The rotor is then placed into a weak uniform electric field \\( \\epsilon \\), also within the plane of rotation. My task is to treat the electric field as a perturbation and find the first non-zero correction to the energy levels.\n\nHmm, let me start by recalling what I know about rigid rotors in quantum mechanics. A rigid rotor model describes a diatomic molecule rotating freely in space. Its Hamiltonian without any external fields would be purely rotational kinetic energy. Since it's constrained to rotate in one plane here, maybe we're dealing with something like a planar rotor?\n\nThe unperturbed system: So, if there were no electric field, the Hamiltonian for such a rotor should involve angular momentum. In two dimensions, the angle can be described using just one variable, say \\( \\theta \\). The wavefunctions would depend on \\( \\theta \\), and the eigenstates correspond to different angular momenta states.\n\nWait, but actually, since it's a 2D rotor, perhaps the Hilbert space isn't infinite-dimensional? Or does it still extend over all angles from 0 to \\( 2\\pi \\)? Yeah, even in 2D, you can describe rotations continuously, so the spectrum remains discrete because of periodic boundary conditions—so yeah, similar to particle on a ring.\n\nSo, the unperturbed Hamiltonian \\( H_0 \\) would be:\n\n\\( H_0 = -\\frac{\\hbar^2}{2I} \\frac{d^2}{d\\theta^2} \\)\n\nAnd the eigenvalues are given by:\n\n\\( E_n^{(0)} = \\frac{\\hbar^2 n^2}{2I} \\),\n\nwhere \\( n = 0, \\pm1, \\pm2, ... \\). But wait, sometimes people index them starting from zero or positive integers only—I think either way works depending on convention, but probably symmetric here.\n\nNow, when introducing the electric field as a perturbation, how do we model that? Well, the interaction between the electric dipole moment and the electric field gives rise to a potential energy term. For a dipole in an electric field, the potential energy is usually \\( U = -\\mathbf{\\mu} \\cdot \\mathbf{\\epsilon} \\).\n\nBut hold on—the dipole moment vector \\( \\mu \\) lies in the plane of rotation, right? And the electric field \\( \\epsilon \\) is also in that plane. Let me set up coordinates properly. Maybe aligning the x-axis along some direction could simplify things.\n\nLet’s choose polar coordinates where the position is determined by \\( r \\) and \\( \\theta \\). However, since it's a rigid rotor, the radius might not matter—it depends on whether the size affects anything. Wait, but the dipole moment itself is fixed; regardless of the coordinate system, the orientation matters relative to the electric field.\n\nAlternatively, considering spherical coordinates—but since everything is in a single plane, cylindrical coordinates may suffice. Hmm, perhaps better to use Cartesian components.\n\nSuppose the electric field points along the x-direction. Then, the dipole moment will have components based on the angle \\( \\theta \\). If the rotor makes an angle \\( \\theta \\) with respect to the x-axis, then the dipole moment vector \\( \\mu \\) can be expressed as \\( (\\mu_x, \\mu_y) = (\\mu \\cos\\theta, \\mu \\sin\\theta) \\).\n\nTherefore, the dot product \\( \\mathbf{\\mu} \\cdot \\mathbf{\\epsilon} \\) becomes \\( \\mu \\epsilon \\cos(\\phi) \\), where \\( \\phi \\) is the angle between \\( \\mu \\) and \\( \\epsilon \\). But since our setup is in a plane, and assuming \\( \\epsilon \\) is along the x-axis, then yes, \\( \\phi = \\theta \\).\n\nHence, the potential energy due to the electric field is:\n\n\\( V = -\\mu \\epsilon \\cos\\theta \\).\n\nThus, the perturbing Hamiltonian \\( H' \\) is:\n\n\\( H' = -\\mu \\epsilon \\cos\\theta \\).\n\nAlright, now my goal is to compute the first-order energy shifts via time-independent perturbation theory. That requires calculating matrix elements of \\( H' \\) between the eigenstates of \\( H_0 \\).\n\nIn other words,\n\n\\( E_n^{(1)} = \\langle \\psi_n | H' | \\psi_n \\rangle = -\\mu \\epsilon \\int_{0}^{2\\pi} \\psi_m^*(\\theta) \\cos\\theta \\psi_n(\\theta) d\\theta \\).\n\nWait, more precisely, for each state \\( |\\psi_n\\rangle \\), the first order correction is\n\n\\( E_n^{(1)} = \\langle \\psi_n | H' | \\psi_n \\rangle \\).\n\nGiven that \\( H' = -\\mu \\epsilon \\cos\\theta \\), substituting in, we get:\n\n\\( E_n^{(1)} = -\\mu \\epsilon \\langle \\psi_n | \\cos\\theta | \\psi_n \\rangle \\).\n\nWhat are the eigenstates \\( \\psi_n(\\theta) \\)?\n\nFor a free particle on a circle (which is analogous to a rigid rotor in 2D), they are simply:\n\n\\( \\psi_n(\\theta) = \\frac{1}{\\sqrt{2\\pi}} e^{i n \\theta } \\),\n\nwith \\( n = 0, \\pm1, \\pm2,... \\).\n\nThese satisfy the normalization condition:\n\n\\( \\int_{0}^{2\\pi} |\\psi_n|^2 d\\theta = 1 \\).\n\nAlso, note that these functions form an orthogonal basis.\n\nSo plugging back into the expectation value:\n\n\\( E_n^{(1)} = -\\mu \\epsilon \\left[ \\frac{1}{2\\pi} \\int_{0}^{2\\pi} e^{-i n \\theta} \\cos\\theta e^{i n \\theta} d\\theta \\right] \\).\n\nSimplify inside the integral:\n\n\\( e^{-i n \\theta} e^{i n \\theta} = 1 \\), so the integrand reduces to \\( \\cos\\theta \\times 1 \\).\n\nThen,\n\n\\( E_n^{(1)} = -\\mu \\epsilon \\left[ \\frac{1}{2\\pi} \\int_{0}^{2\\pi} \\cos\\theta d\\theta \\right ] \\).\n\nCompute the integral:\n\nIntegral of cosθ from 0 to 2π is sinθ evaluated from 0 to 2π, which is [sin(2π)-sin(0)] = 0-0=0.\n\nOh! Therefore, the first-order correction vanishes for every state n.\n\nThat means we need to go to second-order perturbation theory to find the first non-zero correction.\n\nRight, so moving on to second-order corrections.\n\nSecond-order energy shift is given by:\n\n\\( E_n^{(2)} = \\sum_{m \\neq n} \\frac{| \\langle \\psi_m | H' | \\psi_n \\rangle |^2}{E_n^{(0)} - E_m^{(0)}} \\).\n\nSince \\( H' = -\\mu \\epsilon \\cos\\theta \\), the matrix element is:\n\n\\( \\langle \\psi_m | H' | \\psi_n \\rangle = -\\mu \\epsilon \\langle \\psi_m | \\cos\\theta | \\psi_n \\rangle \\).\n\nAgain, express \\( \\cos\\theta \\) in terms of exponentials:\n\nRecall Euler's formula: \\( \\cos\\theta = \\frac{e^{i\\theta} + e^{-i\\theta}}{2} \\).\n\nExpressed in terms of ladder operators, perhaps. Alternatively, write out the inner products explicitly.\n\nWe have:\n\n\\( \\langle m | \\cos\\theta |n \\rangle = \\frac{1}{2}\\left( \\delta_{m,n+1} + \\delta_{m,n-1} \\right ) \\).\n\nBecause \\( \\cos\\theta = \\frac{e^{i\\theta} + e^{-i\\theta}}{2} \\), multiplying by \\( e^{i n \\theta} \\) gives \\( e^{i(n+1)\\theta}/2 + e^{i(n-1)\\theta}/2 \\). When integrated against \\( e^{-im\\theta} / \\sqrt{2\\pi} \\), orthogonality implies delta functions unless m equals n±1.\n\nYes, exactly. Thus,\n\n\\( \\langle m | \\cos\\theta |n \\rangle = \\frac{1}{2}( \\delta_{m, n+1} + \\delta_{m, n-1}) ).\n\nTherefore, the matrix element squared modulus is:\n\n|⟨ψₘ|H'|ψₙ⟩|² = μ² ε² | ⟨m|cosθ|n ⟩"}
{"id": "quantum/5076", "questions": "A particle of mass $M$ is in a one-dimensional harmonic oscillator potential $V_1 = \\frac{1}{2} kx^2$. \n\n(a) It is initially in its ground state. The spring constant is suddenly doubled ($k \\rightarrow 2k$) so that the new potential is $V_2 = kx^2$. The particle energy is then measured. What is the probability for finding that particle in the ground state of the new potential $V_2$?\n\n(b) The spring constant is suddenly doubled as in part (a), so that $V_1$ suddenly becomes $V_2$, but the energy of the particle in the new potential $V_2$ is not measured. Instead, after a time $T$ has elapsed since the doubling of the spring constant, the spring constant is suddenly restored back to the original value. For what values of $T$ would the initial ground state in $V_1$ be restored with 100% certainty?", "graphs": null, "llm_answers": "<think>\nOkay, I have this problem about a quantum harmonic oscillator. Let me try to figure it out step by step.\n\nSo, part (a): A particle of mass M is in a one-dimensional harmonic oscillator potential V₁ = (1/2)k x². Initially, it's in the ground state. Then the spring constant doubles, becoming V₂ = k x². We need to find the probability that measuring the particle's energy will result in the ground state of the new potential V₂.\n\nHmm, okay. So when the spring constant changes abruptly from k to 2k, the wavefunction doesn't change instantaneously—it remains the same right before and immediately after the change. But now, instead of being expressed in terms of the old eigenstates, we'll express it in terms of the new eigenstates corresponding to the stiffer spring.\n\nThe key here is to compute the overlap between the initial ground state ψ₀^(old)(x) and each of the new eigenstates φ_n(x). Since only the coefficients squared give probabilities, specifically |c₀|² where c₀ is the coefficient for the new ground state φ₀(x).\n\nFirst, let me recall the form of the ground states for both potentials.\n\nFor the original potential V₁: \nψ₀^(old)(x) = (mω/(πħ))^{1/4} e^{- mω x² / (2ħ)}, where ω = sqrt(k/M).\n\nAfter changing k to 2k, the angular frequency becomes ω' = sqrt(2k/M) = √2 * ω.\n\nThus, the new ground state is:\nφ₀(x) = [m√2 ω / (π ħ)]^{1/4} e^{- m√2 ω x² / (2ħ)}.\n\nWait, actually, more accurately:\n\nLet’s define ω_old = sqrt(k/M)\nand ω_new = sqrt(2k/M) = sqrt(2) ω_old\n\nThen,\n\nOriginal ground state:\nψ₀^(old)(x) = (α)^{1/2} e^{- α² x² / 2}, where α = sqrt(m ω_old / ħ)\n\nNew ground state:\nφ₀(x) = (β)^{1/2} e^{- β² x² / 2}, where β = sqrt(m ω_new / ħ) = sqrt(m (√2 ω_old)/ ħ ) = alpha * (sqrt(2))^{1/2}\n\nBecause beta squared is (m ω_new)/(2 ħ) ?\n\nWait no, wait—let me get precise expressions.\n\nIn general, the ground state of a HO is given by:\n\nψ₀(x) = (m ω / (π ħ))^{1/4} exp(- m ω x² / (2 ħ))\n\nTherefore, for the old system,\npsi_old_0(x) = (m ω_old / (pi hbar))^{1/4} exp(- m omega_old x² / (2 hbar))\nSimilarly, phi_0(x) = (m omega_new / (pi hbar))^{1/4} exp(- m omega_new x² / (2 hbar))\n\nSince omega_new = sqrt(2) omega_old, substituting gives:\n\nphi_0(x) = (m sqrt(2) omega_old / (pi hbar))^{1/4} exp(- m sqrt(2) omega_old x² / (2 hbar))\n\nNow, our goal is to calculate <phi_0 | psi_old_0> which is integral over all x of phi_0*(x) psi_old_0(x) dx.\n\nCompute this inner product.\n\nExpressed mathematically:\n\nIntegral_{-infty}^\\infty [ (m sqrt(2) omega_old / pi hbar)^{1/4} exp(- m sqrt(2) omega_old x² / (2 hbar)) ] *\n[ (m omega_old / pi hbar)^{1/4} exp(- m omega_old x² / (2 hbar)) ] dx\n\nMultiply these together:\n\n= (m sqrt(2) omega_old / pi hbar)^{1/4} * (m omega_old / pi hbar)^{1/4}\n* Integral exp[- (m sqrt(2) omega_old + m omega_old) x² / (2 hbar) ] dx\n\nSimplify exponents first:\n\nExponent inside exponential: - [ m sqrt(2) omega_old + m omega_old ] x² / (2 hbar )\n\nFactor out m omega_old:\n\n= - m omega_old (sqrt(2) + 1) x² / (2 hbar )\n\nCombine constants outside exponent:\n\nConstants multiplied: [ (m sqrt(2) omega_old / pi hbar) * (m omega_old / pi hbar) ]^{1/4 }\n\nWhich simplifies to [ (m^2 sqrt(2) (omega_old)^2 ) / (pi hbar)^2 ) ]^{1/4 } =\n\nTake square root twice: First take square root once: [ m sqrt(2) (omega_old) / (pi hbar) ) ]^{1/2 }, because (something)^{1/4} times itself again is something^{1/2}.\n\nAlternatively, perhaps better to write them separately:\n\nEach term is raised to power 1/4, so multiplying two such factors gives total factor (product)^{1/4}. Wait, maybe my earlier approach was wrong.\n\nActually, if you multiply (A)^{1/4} * (B)^{1/4} = (AB)^{1/4}.\n\nBut in any case, regardless, proceed carefully.\n\nLet me denote C1 = (m sqrt(2) omega_old / (pi hbar ))^{1/4 }\nC2 = (m omega_old / (pi hbar ))^{1/4 }\n\nMultiplying C1*C2 = [ (m sqrt(2) omega_old / (pi hbar )) * (m omega_old / (pi hbar )) ]^{1/4 }\n\nInside the brackets: numerator: m^2 sqrt(2) (omega_old)^2; denominator: (pi hbar)^2\n\nHence, overall: [ m^2 sqrt(2) (omega_old)^2 / (pi hbar)^2 ]^{1/4 } = [ m^2 (omega_old)^2 / (pi hbar)^2 * sqrt(2) ]^{1/4 }\n\nThis can be written as [ (m omega_old / (pi hbar))^2 * sqrt(2) ]^{1/4 } = [ (m omega_old / (pi hbar))^{2} ]^{1/4 } * [ sqrt(2) ]^{1/4 }\n\nSimplifying further:\n\n= (m omega_old / (pi hbar))^{1/2 } * (2)^{-1/8 }\n\nWait, hold on: Because sqrt(2)=2^{1/2}; thus, [sqrt(2)]^{1/4}=2^{1/8}\n\nYes, correct.\n\nSo putting it all together, the pre-exponential factor is:\n\n(m omega_old / (pi hbar))^{1/2 } * 2^{-1/8 }\n\nAnd the integrand is exp[ - m omega_old (sqrt(2)+1) x² / (2 hbar) ]\n\nWe know that the Gaussian integral Integrate_{-∞}^∞ exp(-a x²) dx = sqrt(pi/a)\n\nHere, a = m omega_old (sqrt(2)+1) / (2 hbar )\n\nThus, the integral equals sqrt(2 pi hbar / (m omega_old (sqrt(2)+1)))\n\nPutting everything together:\n\nProbability amplitude c0 = Pre-factor * Integral\n\n= (m omega_old / (pi hbar))^{1/2 } * 2^{-1/8 } * sqrt(2 pi hbar / (m omega_old (sqrt(2)+1))) )\n\nSimplify this expression.\n\nBreak down into parts:\n\nFirst, note that (m omega_old / (pi hbar))^{1/2 } * sqrt(2 pi hbar / (m omega_old (sqrt(2)+1))) )\n\nCan rewrite as:\n\n[m omega_old / (pi hbar)]^{1/2} * [2 pi hbar / (m omega_old (sqrt(2)+1))]^{1/2}\n\nMultiply under single radical:\n\n= [ (m omega_old / (pi hbar)) * (2 pi hbar / ("}
{"id": "quantum/1-1060", "questions": "An idealized ping pong ball of mass $\\text m$ is bouncing in its ground state on a recoilless table in a one-dimensional world with only a vertical direction.\n\n(a) Prove that the energy depends on $ \\text{m, g, h} $ according to: $ \\varepsilon = Kmg (m^2 g / h^2)^\\alpha $ and determine $ \\alpha $.\n\n(b) By a variational calculation estimate the constant $ K $ and evaluate $ \\varepsilon $ for $\\text m = 1 $ gram in ergs.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about an idealized ping pong ball bouncing on a recoilless table in a one-dimensional world. It's part (a) where I need to prove that the energy ε depends on mass m, gravity g, and height h as given by the formula ε = K mg (m²g/h²)^α, and then find α. Hmm, let me think through how to approach this.\n\nFirst off, since it's an idealized scenario, we can probably ignore air resistance or any other real-world factors except for gravity. The ping pong ball is in its ground state, which suggests it's oscillating between two points—the highest point being some height h above the table—and when it hits the table, there's no loss of kinetic energy because the table is recoilless. So each bounce imparts just enough force to send it back up without losing energy—so theoretically, if not for gravity, it would keep bouncing indefinitely? But here, gravity does play a role in determining the motion.\n\nWait, but actually, even though the table is recoilless, meaning it doesn't move upon impact, the ball still has to overcome gravitational potential energy every time it goes up. Therefore, the total mechanical energy should be conserved—it’s converting between kinetic and potential energy during each bounce.\n\nSo maybe I can model this using simple harmonic motion principles? Or perhaps treat it more like projectile motion under gravity?\n\nBut wait, in reality, a bouncing ball isn’t exactly SHM unless you consider small displacements from equilibrium, but here, the displacement might be significant depending on h. Alternatively, considering energy conservation could help us relate these variables directly.\n\nLet me try thinking about energy first. At the lowest point (on the table), all the energy is kinetic, right before it starts moving upwards. Then, as it ascends, that kinetic energy converts into gravitational potential energy until it reaches maximum height h, where all the initial kinetic energy becomes potential energy.\n\nTherefore, equating those energies:\n\nAt the bottom:\nKinetic Energy (KE) = (1/2)mv²\n\nPotential Energy (PE) = 0 (since reference level is the table)\n\nJust after leaving the table, KE turns into PE as it rises. When it reaches height h, velocity v becomes zero momentarily, so all KE is converted into PE.\n\nThus,\n(1/2)m v² = mgh\n\nSolving for v gives v = sqrt(2gh). That makes sense; similar to projectile motion equations.\n\nNow, what is the period of such a \"bounce\"? Since it moves upward, comes to rest, falls back down... Wait, but in our case, the table is recoilless, so once it impacts, it immediately bounces back without delay. Is the time taken going up equal to the time coming down?\n\nIn free fall, yes—if we neglect air resistance, the ascent and descent times are symmetric. However, in this case, the collision happens instantaneously, so effectively, the time spent in contact with the table is negligible compared to the flight time.\n\nHence, the period T of the bounce would be twice the time it takes to go up to height h. Let's compute that.\n\nTime to reach max height h starting from velocity v is t_up = v/g. Similarly, time to come back down is also t_down = sqrt(2h/g)/something? Wait, hold on—I know another way.\n\nAlternatively, use kinematic equation s = ut + (1/2)a t². For ascending phase, u = v, acceleration a = -g, distance s = h.\n\nSo,\n\nh = vt - (1/2)gt²\n\nThis quadratic equation will give t when the ball stops at top.\n\nSimilarly, solving for t:\n\n(1/2)gt² - vt + h = 0\n\nUsing quadratic formula:\n\nt = [v ± sqrt(v² - 4*(1/2)*g*h)]/(2*(1/2)g)\n= [v ± sqrt(v² - 2gh)]/g\n\nSince v was found earlier as sqrt(2gh):\n\nPlugging in v = sqrt(2gh),\n\nsqrt(v² - 2gh) = sqrt(2gh - 2gh) = 0\n\nThus, t = [sqrt(2gh)] / g = sqrt(2h/g)\n\nThat seems correct. Because time to rise to height h is sqrt(2h/g).\n\nThen, similarly, time to descend back is same, assuming symmetry, so total period T = 2*sqrt(2h/g).\n\nHmm, okay. Now, knowing that, perhaps we can link frequency f = 1/T with energy somehow?\n\nEnergy-wise, the system is conservative, so the total mechanical energy E remains constant throughout the motion. As calculated earlier, E = mgh.\n\nBut the question says something different—they want to express ε as proportional to mg multiplied by (m²g/h²)^α. Hmmm...\n\nWait, their expression is ε = K mg (m²g/h²)^α. Which simplifies to ε = K m^{3} g^{1+α}/h^{2α}. Maybe they're suggesting dimensional analysis?\n\nYes! Dimensional Analysis!\n\nBecause the left-hand side must have dimensions of energy, while the right-hand side needs to match. So let's do that.\n\nGiven ε has units of energy, which is ML²T^{-2}\n\nOn RHS: K is dimensionless (as per standard notation usually), mg has units M L T^{-2}, and (m²g/h²) has units (M² L T^{-2}) / (L²) ) = M² L^{-2} T^{-2}\n\nRaising that term to power alpha: (M² L^{-2} T^{-2})^α = M^{2α} L^{-2α} T^{-2α}\n\nMultiply everything together:\n\nK * [MLT^{-2}] * [M^{2α} L^{-2α} T^{-2α}] =\n\nTotal Dimensions: M^{1 + 2α} L^{1 - 2α} T^{-2 - 2α}\n\nSet equal to energy dimensions: ML²T^{-2}\n\nEquate exponents:\n\nFor Mass: 1 + 2α = 1 => 2α = 0 => α = 0 ?\n\nWait, that can't be. If α were zero, then the second factor would become 1, making ε = K mg, which contradicts my previous thought process where ε = mgh.\n\nHold on, did I make a mistake somewhere?\n\nWait, original expression is ε = K mg (m²g/h²)^α\n\nWhich expands to K m^{1 + 2α} g^{1 + α} h^{-2α}\n\nCompare to energy, which is ML²T^{-2}\n\nCompute dimensions:\n\nMass exponent: 1 + 2α\n\nLength exponent: From g, which is LT^{-2}; so overall length exponent is (from g): 1 + α, plus from h^{-2α}: (-2α)(-1)? No, wait, h is in denominator raised to 2α, so h^{-2α} contributes Length^{-(-2α)} ? Wait, confusion arises.\n\nActually, better re-expressing:\n\nEach variable's unit breakdown:\n\nm: M\n\ng: L T^{-2}\n\nh: L\n\nSo, breaking down each component:\n\nK: assume dimensionless.\n\nmg: M * (L T^{-2})\n\n(m²g/h²): (M²) * (L T^{-2}) / (L²) = M² L^{-1} T^{-2}\n\nRaise that to alpha: (M² L^{-1} T^{-2})^α = M^{2α} L^{-α} T^{-2α}\n\nMultiplying altogether:\n\n[m]^{1} [L T^{-2}]^{1} * [M^{2α} L^{-α} T^{-2α}]\n\nCombine terms:\n\nMass: 1 + 2α\n\nLength: 1 - α\n\nTime: -2 - 2α\n\nThese must equal energy's dimensions: M^1 L^2 T^{-2}\n\nSetting up equations:\n\n1 + 2α = 1 --> 2α = 0 --> α = 0\n\n1 - α = 2 --> -α = 1 --> α = -1\n\nAnd Time: -2 - 2α = -2 --> -2α = 0 --> α = 0\n\nContradiction! This implies inconsistency because α cannot simultaneously satisfy both conditions.\n\nUh oh, that means either my assumption is wrong or the form provided is incorrect.\n\nWait, perhaps I misapplied the dimensional analysis. Let me double-check.\n\nThe given expression is ε = K mg (m²g/h²)^α\n\nExpressed differently, ε ∝ mg (m²g/h²)^α\n\nWe can write this proportionality as ε ~ m^{1 + 2α} g^{1 + α} h^{-2α}\n\nComparing to energy, whose dimensions are M L² T^{-"}
{"id": "quantum/6051", "questions": "A particle of charge e is confined to a three-dimensional cubical box of side 2b. An electric field $E$ given by  \n$$\n\\mathbf{E} = \\begin{cases} \n0, & t < 0, \\\\ \n\\mathbf{E_0} e^{-\\alpha t}, & t > 0, \n\\end{cases} \\quad (a = \\text{a positive constant}) ,\n$$\n\nis applied to the system. The vector $\\mathbf{E_s}$ is perpendicular to one of the surfaces of the box. Calculate to lowest order in $E_0$ the probability that the charged particle, in the ground state at $t = 0$, is excited to the first excited state by the time $t = \\infty$. (If you wish, you can leave your result in terms of properly defined dimensionless definite integrals.)  ", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem where there's a particle with charge 'e' confined in a three-dimensional cubical box with each side length being 2b. At time t=0, they apply an electric field E which starts from zero before t<0 and then becomes E₀ times exponential minus alpha t for t>0. This electric field is directed perpendicularly to one of the cube's faces.\n\nThe question asks me to calculate the probability that the particle gets excited from its ground state at t=0 to the first excited state as time approaches infinity. And it says we should do this to the lowest order in E₀, meaning probably using perturbation theory or something like Fermi's golden rule?\n\nAlright, let me think about how to approach this. So since the particle is in a box, initially, without any external fields, it has certain energy levels determined by the infinite potential well model. When the electric field is turned on adiabatically starting at t=0, it will cause transitions between these states due to the perturbing Hamiltonian introduced by the electric field.\n\nFirst, maybe I need to write down the unperturbed Hamiltonian H₀ and the perturbing part H'. Since the electric field interacts with the charge via the Lorentz force, but because the motion is non-relativistic, the magnetic component isn't considered here—so only the electric part matters. Therefore, the perturbing Hamiltonian would be the dipole interaction: H’(t) = -e * φ(t), where φ is the scalar potential. But wait, actually, another way to express the perturbation when dealing with uniform electric fields is through the position operator multiplied by the electric field strength.\n\nWait, more accurately, the perturbing Hamiltonian due to an electric field E is usually written as:\n\nH'(t) = e * E(t) · r,\n\nwhere r is the position vector. Because the electric field exerts a force F = eE, leading to a momentum change over time, hence affecting the kinetic term? Or perhaps better phrased, considering the minimal coupling in quantum mechanics, the Hamiltonian gains a term involving A·p, but if the gauge is chosen such that A=0 (Coulomb gauge?), then the scalar potential comes into play. Hmm, getting confused now.\n\nAlternatively, thinking classically, the work done by the electric field is qEd, so in quantum terms, the perturbation might involve the expectation value of x under the influence of E. Maybe it's simpler than that; perhaps the correct expression is indeed H’ = -e E(t) · d, where d is displacement? Wait no, not exactly.\n\nLet me recall: In the presence of an electric field E, the potential energy experienced by a charge e is U = -e ∫ E · dr + const., depending on the reference point. If E is uniform, then integrating gives U = -e E z, assuming E points along z-direction. Hence, yes, the perturbing Hamiltonian could be expressed as H’(t) = -e E(t) z, where z is the coordinate direction aligned with E.\n\nBut hold on, sometimes people use different conventions. Alternatively, some sources say that the perturbation is H’ = e E(t) · x, where x is the position vector. Let me check dimensions quickly: [E] is volts/meter, which is equivalent to Newtons/(Coulombs). Then multiplying by charge e (coulombs) gives N m / C * C = N m, which doesn’t make sense... Wait, no, sorry, E is V/m, which is kg·m²/(C·s³). Multiply by charge e (C): kg·m²/s³. That’s power, not energy. Hmm, seems off.\n\nWait, perhaps instead, the correct form involves velocity? No, that would relate to magnetic fields. For electric fields, the standard perturbation is linear in position. Actually, going back to basics: The electromagnetic potential contributes to the Hamiltonian as follows. The full Hamiltonian including EM interactions is H = (1/2m)(p - eA)^2 + eφ. Here, if we're working in Coulomb gauge, A=0 inside the region, so H simplifies to H = p²/(2m) + eφ. Thus, the perturbation is just eφ(r,t).\n\nIn our case, what is φ(r,t)? Well, for a uniform electric field pointing in the z-direction, the scalar potential satisfies E_z = -∂φ/∂z. Integrating, φ(z) = -∫ E dz + const. Assuming φ=0 at z=0, then φ(z) = -E z. However, constants don't matter much in potentials because they shift energies globally, so we can set φ(z) ≈ -E z.\n\nTherefore, substituting into H', we get H’(t) = e*(-E z) = -e E z. So yeah, my initial thought was right—the perturbation is proportional to z-coordinate.\n\nSo, summarizing, H’(t) = -e E(t) z, where E(t)=E₀ exp(-α t) θ(t), with θ(t) being the Heaviside step function.\n\nNow, moving forward, the transition probability from an initial state |i⟩ to a final state |f⟩ up to first-order perturbation theory is given by:\n\nP_{i→f}(t) ≈ |⟨f|U(t)|i⟩|^2,\n\nwhere U(t) is the time evolution operator. To first order, ignoring higher orders, the amplitude ⟨f|U(t)|i⟩ is approximately (-i/ħ) ∫₀^∞ dt' ⟨f|H’(t')|i⟩ e^{iω_{fi} t'}, where ω_{fi}=(E_f-E_i)/ħ.\n\nGiven that, the matrix element M_{fi}=⟨f|H’|i⟩ is important. Also, note that the integral runs from 0 to ∞ because the perturbation turns on suddenly at t=0.\n\nSince the particle is in a cubic box, the eigenstates are labeled by their wavevectors k_x, k_y, k_z corresponding to each spatial dimension. Each edge length is L=2b, so the allowed momenta components are quantized as k_n = n π / b, where n=1,2,... for each axis.\n\nAt t=0, the particle is in the ground state. What does the ground state look like? It must be the state with the lowest possible energy, i.e., all quantum numbers equal to 1. So, the ground state is |n_x,n_y,n_z⟩ = |1,1,1⟩.\n\nWe want the probability of transitioning to the first excited state. Now, \"first\" depends on ordering by energy. In a 3D box, the energy levels depend on n_x² + n_y² + n_z². Ground state is (1,1,1); next come states with two ones and one two, giving energy level sum 7. There are multiple degenerate states here: (2,1,1), (1,2,1), (1,1,2). These are all equally spaced in energy, forming a triplet.\n\nHowever, whether the excitation goes specifically to one of them or collectively needs consideration. Given that the perturbation is along the z-axis (since E is perpendicular to one face, presumably the xy-plane, making E along z), the matrix elements may vanish except for transitions changing the z-component.\n\nThus, likely, among those triplets, only the state with increased n_z will contribute significantly. Specifically, the transition from (1,1,1) → (1,1,2). Is that correct?\n\nYes, because the perturbation H’ ~ z, whose Fourier transform connects states differing in the z-quantum number. So, the selection rules imply Δn_x=0, Δn_y=0, Δn_z=±1. Therefore, the dominant contribution is from the state (1,1,2).\n\nSimilarly, other transitions require changes in n_x or n_y, but unless the perturbation couples those directions, which it doesn't—it's purely along z—they won't occur. So, focusing on the transition to (1,1,2).\n\nNext, compute the matrix element ⟨f|H'|i⟩, where f=(1,1,2) and i=(1,1,1):\n\nM = ⟨1,1,2| H’ |1,1,1⟩ = -e E₀ ⟨1,1,2| z |1,1,1"}
{"id": "quantum/3-3003", "questions": "Three matrices $M_x, M_y, M_z$, each with 256 rows and columns, are known to obey the commutation rules $[M_x, M_y] = i M_z$ (with cyclic permutations of $x, y$ and $z$). The eigenvalues of the matrix $M_x$ are $\\pm 2$, each once; $\\pm 3/2$, each 8 times; $\\pm 1$, each 28 times; $\\pm 1/2$, each 56 times; and 0, 70 times. State the 256 eigenvalues of the matrix $M^2 = M_x^2 + M_y^2 + M_z^2$.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about three matrices \\( M_x, M_y, M_z \\) that follow certain commutation relations similar to angular momentum operators. Each is a 256x256 matrix. Their commutators satisfy \\( [M_x, M_y] = i M_z \\), and similarly for other cyclic permutations. \n\nThe question gives me the eigenvalues of \\( M_x \\): they're ±2, each appearing once; ±3/2, eight times each; ±1, twenty-eight times each; ±1/2, fifty-six times each; and zero, seventy times. My task is to find all 256 eigenvalues of \\( M^2 = M_x^2 + M_y^2 + M_z^2 \\).\n\nHmm, okay. So first off, these matrices seem like components of some sort of Lie algebra, probably su(2) or something else? Because their commutation relations resemble those of angular momentum operators.\n\nIn quantum mechanics, when dealing with angular momentum, we know that if you have operators \\( L_x, L_y, L_z \\), then \\( L^2 = L_x^2 + L_y^2 + L_z^2 \\) has eigenvalues proportional to \\( j(j+1) \\hbar^2 \\), where \\( j \\) is a non-negative integer or half-integer. But here, it's not exactly clear what the scaling factors might be because the given eigenvalues don't immediately look familiar as standard spin values.\n\nWait, but maybe instead of thinking in terms of spins, since the dimensions add up differently—each block corresponds to different representations?\n\nLet me think: For each possible value of \\( m \\) (the eigenvalue of \\( M_x \\)), there should correspond a representation space where \\( M^2 \\) acts on vectors within that subspace. Since \\( M_x \\) can take multiple values, perhaps each distinct eigenvalue of \\( M_x \\) comes from a different irreducible representation (irrep) of the Lie algebra.\n\nSo, let's consider how many times each eigenvalue appears:\n\n- Eigenvalue ±2 occurs once.\n- ±3/2 occurs eight times.\n- ±1 occurs twenty-eight times.\n- ±1/2 occurs fifty-six times.\n- Zero occurs seventy times.\n\nEach occurrence count suggests the multiplicity of each eigenvalue across the entire matrix. Now, considering that in an irrep of dimension \\( d \\), the number of states with a particular \\( m \\) depends on whether \\( m \\) ranges symmetrically around zero.\n\nFor example, in an SU(2) irrep labeled by \\( j \\), which has dimension \\( 2j + 1 \\), the eigenvalues of \\( J_x \\) would range from \\( -j \\) to \\( j \\) in steps of 1, right? Wait no, actually, wait—the eigenvalues of \\( J_x \\) aren’t necessarily integers unless scaled appropriately. Hmm, hold on—I need to clarify.\n\nActually, more accurately, in the case of angular momentum, the operator \\( J_z \\) has eigenvalues \\( m_j = -j, -(j-1), ..., j \\). Similarly, \\( J_x \\) also has eigenvalues ranging between \\( -j \\) and \\( j \\), but due to ladder operators, etc., but in reality, the spectrum isn't straightforward—it’s determined via the Casimir invariant.\n\nBut regardless, the key point is that for any irrep, the square of the total angular momentum \\( J^2 \\) will act as a scalar multiple on every vector in that irrep. That is, \\( J^2 |j,m\\rangle = j(j+1)\\hbar^2 |j,m\\rangle \\). Therefore, if our matrices \\( M_x, M_y, M_z \\) form such an algebra, then \\( M^2 \\) must be a scalar multiple of identity on each irrep.\n\nTherefore, my approach should be:\n\n1. Determine the decomposition of the 256-dimensional space into irreps of the Lie algebra generated by \\( M_x, M_y, M_z \\).\n2. For each irrep, compute its corresponding eigenvalue under \\( M^2 \\).\n3. Multiply the eigenvalue by the dimensionality of the irrep to get the multiplicities.\n\nGiven that, let's see. First, note that the sum over all multiplicities equals 256:\n\nOnce: 2*1=2\n\nEight times: 2*8=16\n\nTwenty-eight times: 2*28=56\n\nFifty-six times: 2*56=112\n\nSeventy times: 1*70=70\n\nAdding them together: 2 + 16 + 56 + 112 + 70 = 256. Okay, correct.\n\nNow, looking at the multiplicities, notice that they come in pairs except for zero. This makes sense because for each positive eigenvalue, there's a negative one, hence doubling counts except possibly for zero.\n\nLooking at the numbers:\n\n±2 occur once each → Total two occurrences.\n\nSimilarly,\n\n±3/2: 8 each → 16 total.\n\n±1: 28 each → 56 total.\n\n±1/2: 56 each → 112 total.\n\nAnd zero: 70.\n\nThis seems reminiscent of decomposing into several copies of different irreps.\n\nI recall that in group theory, the direct product of representations can lead to various sums of irreps. However, without knowing the exact structure constants, it's tricky, but perhaps another way is better.\n\nAlternatively, suppose that each set of degenerate eigenvalues corresponds to a specific irrep. Let's try to figure out the possible j-values associated with each eigenvalue.\n\nFirstly, the maximum absolute value among the eigenvalues is 2. In angular momentum terms, the highest weight state usually determines the irrep. If someone had an irrep with maximal \\( m = 2 \\), then the dimension would be \\( 4j + 1 \\)? No, wait—if \\( m_{max} = j \\), then the dimension is \\( 2j + 1 \\). Hence, if \\( m_{max}=2 \\), then \\( j=2 \\), giving a five-dimensional irrep.\n\nHowever, in our case, the eigenvalue 2 only occurs twice (once for +2, once for -2?), but wait, no—in the data above, \"eigenvalues... each once\"—so does that mean both +2 and -2 appear once each? Or just each sign once? It says \"+/-2, each once.\" So likely, each of +2 and -2 occurs once. Thus, the multiplicity per eigenvalue is 1.\n\nIf that were part of a single irrep, say j=2, which has m=-2,-1,0,+1,+2, each occurring once. Then, why do we have higher multiplicities elsewhere?\n\nWait, but in our case, beyond +/-2, we have larger multiplicities—for instance, +/-3/2 occurs eight times each. Which doesn't fit neatly into a simple j=2 picture.\n\nPerhaps the overall Hilbert space is a combination of multiple irreps. Maybe tensor products leading to reducible representations.\n\nAnother thought: Perhaps the system is built upon multiple independent particles' angular momenta combined. Alternatively, could it be related to quarks or something else entirely? Not sure.\n\nWait, stepping back, perhaps using the fact that \\( M^2 \\) is diagonal in the basis of simultaneous eigenvectors of \\( M_x \\) and \\( M^2 \\). But since \\( M_x \\) alone may not commute with others, but \\( M^2 \\) does commute with all \\( M_i \\)'s.\n\nThus, for each eigenstate of \\( M_x \\), it lies within an irrep, and thus shares the same \\( M^2 \\) eigenvalue.\n\nHence, for each unique eigenvalue \\( m \\) of \\( M_x \\), the corresponding \\( M^2 \\) eigenvalue is fixed based on the irrep containing that \\( m \\).\n\nTherefore, if I can determine for each \\( m \\in \\{ -2, -3/2, -1, -1/2, 0, 1/2, 1, 3/2, 2\\} \\}, the corresponding \\( M^2 \\) eigenvalue, multiplied by the number of times each \\( m \\) occurs, I can reconstruct the full list.\n\nTo proceed, perhaps assume that each pair of \\( m \\) and \\( -m \\) belong to the same irrep, whose dimension is equal to the number of times each \\( m \\) occurs plus itself again for the opposite sign, except for m=0.\n\nWait, but in"}
{"id": "quantum/1-1067", "questions": "If $|\\psi_q \\rangle$ is any eigenstate of the electric charge operator $Q$ corresponding to eigenvalue $q$, that is to say,\n\n$$\nQ |\\psi_q \\rangle = q |\\psi_q \\rangle,\n$$\n\nthe charge conjugation operator $C$ applied to $ |\\psi_q \\rangle$  leads to an eigenstate $|\\psi_{-q} $  of $Q$ corresponding to eigenvalue $-q$:\n\n$$\nC |\\psi_q \\rangle = |\\psi_{-q} \\rangle.\n$$\n\n(a) Find the eigenvalues of the operator $CQ + QC$. \n\n(b) Can a state simultaneously be an eigenstate of $C$ and of $Q$? ", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about quantum mechanics operators, specifically involving the charge operator Q and the charge conjugation operator C. Let me try to unpack part (a). The question asks for the eigenvalues of the operator \\( CQ + QC \\).\n\nFirst, let's recall what each operator does. The charge operator Q acts on states like |ψ_q⟩ such that \\( Q|ψ_q⟩ = q|ψ_q⟩ \\), where q is the eigenvalue representing the electric charge. Then there's the charge conjugation operator C, which when applied to |ψ_q⟩ gives another state with opposite charge: \\( C|ψ_q⟩ = |ψ_{−q}⟩ \\).\n\nSo we need to find the eigenvalues of \\( CQ + QC \\). Hmm, maybe it would help if I first figure out how these operators act together.\n\nLet’s consider applying both terms separately. Take the term \\( CQ \\): acting on |ψ_q⟩, since Q multiplies by q first, then C flips the sign of the charge. So:\n\\( CQ|ψ_q⟩ = C(q|ψ_q⟩) = q C|ψ_q⟩ = q|ψ_{−q}⟩ \\)\n\nSimilarly, take the other term \\( QC \\): here, C acts first, flipping the charge from q to -q, and then Q acts on the resulting state. But wait, after C acts, the state becomes |ψ_{−q}⟩, right? And then Q should give us (-q)|ψ_{−q}⟩ because now the charge is -q. Wait no—hold on—if you apply Q to |ψ_{−q}⟩, shouldn't it just return (-q)|ψ_{−q}⟩?\n\nWait, but actually, hold on—isn’t Q supposed to multiply by whatever its eigenvalue is regardless of the state? If |ψ_{−q}⟩ is an eigenstate of Q with eigenvalue −q, yes, exactly. Therefore, \\( QC|ψ_q⟩ = Q|ψ_{−q}⟩ = (-q)|ψ_{−q}⟩ \\).\n\nBut putting them together into \\( CQ + QC \\):\n\\( (CQ + QC)|ψ_q⟩ = CQ|ψ_q⟩ + QC|ψ_q⟩ = q|ψ_{−q}⟩ + (-q)|ψ_{−q}⟩ = [q - q]|ψ_{−q}⟩ = 0 \\)\nHmm! That seems interesting—it annihilates the state. Does that mean zero is an eigenvalue?\n\nWait, perhaps not necessarily. Because while \\( (CQ + QC)|ψ_q⟩ = 0 \\), that doesn't directly tell us whether 0 is an eigenvalue unless |ψ_q⟩ itself is an eigenvector of \\( CQ + QC \\). Alternatively, maybe all vectors satisfy some relation.\n\nAlternatively, suppose we think more abstractly. Maybe express everything in terms of commutators or anticommutators. Since \\( CQ + QC \\) looks similar to an anticommutator {C,Q}, except without the division by two factor. Anticommutators often come up in things like supersymmetry or fermionic systems.\n\nIn general, the anticommutator of A and B is defined as AB + BA. Here, our operator is indeed the anticommutator of C and Q multiplied by one half, but scaled appropriately. However, even knowing that might not immediately solve the problem.\n\nAnother approach could be considering specific properties of C and Q. For example, do they commute? Well, given that C changes the charge, whereas Q measures it, probably not. In fact, their product order matters.\n\nGiven that \\( CQ|ψ_q⟩ = q|ψ_{−q}⟩ \\) and \\( QC|ψ_q⟩ = (-q)|ψ_{−q}⟩ \\), adding those gives zero. Hence, for every eigenstate |ψ_q⟩, \\( (CQ + QC)|ψ_q⟩ = 0 \\). This suggests that 0 is an eigenvalue of \\( CQ + QC \\), with multiplicity depending on the number of possible charges.\n\nIs that the only eigenvalue though? Suppose someone has a different kind of vector—not an eigenstate of Q—but still, can \\( CQ + QC \\) have non-zero eigenvalues?\n\nWell, let's see. Consider linear combinations of |ψ_q⟩ and |ψ_{−q}⟩. Perhaps construct a new state |φ⟩ = |ψ_q⟩ + |ψ_{−q}⟩. Applying \\( CQ + QC \\):\n\nCompute \\( (CQ + QC)(|ψ_q⟩ + |ψ_{−q}⟩ ) \\).\n= \\( CQ|ψ_q⟩ + CQ|ψ_{−q}⟩ + QC|ψ_q⟩ + QC|ψ_{−q}⟩ \\)\nWe already know \\( CQ|ψ_q⟩ = q|ψ_{−q}⟩ \\),\nand \\( CQ|ψ_{−q}⟩ = (-q)|ψ_q⟩ \\),\nsimilarly, \\( QC|ψ_q⟩ = (-q)|ψ_{−q}⟩ \\),\nand \\( QC|ψ_{−q}⟩ = q|ψ_q⟩ \\).\n\nPutting it all together:\n= q|ψ_{−q}⟩ + (-q)|ψ_q⟩ + (-q)|ψ_{−q}⟩ + q|ψ_q⟩\nSimplify:\nThe coefficients for |ψ_q⟩: (-q) + q = 0\nCoefficients for |ψ_{−q}⟩: q + (-q) = 0\nThus total result is 0 again.\n\nHmmm... Interesting. It also annihilates this combination. What if instead we took something else, like |φ⟩ = i|ψ_q⟩ - |ψ_{−q}⟩ ?\n\nApplying \\( CQ + QC \\):\nSame steps:\ni*CQ|ψ_q⟩ - CQ|ψ_{−q}⟩ + i*QC|ψ_q⟩ - QC|ψ_{−q}⟩\nWhich is:\ni*q|ψ_{−q}⟩ - (-q)|ψ_q⟩ + i*(-q)|ψ_{−q}⟩ - q|ψ_q⟩\nSimplify:\nFor |ψ_q⟩: (+q)i|ψ_{−q}? No, wait, better track carefully.\n\nWait, sorry, breaking down each term:\n\n1. i * CQ|ψ_q⟩ = i * q |ψ_{−q}⟩\n2. - CQ|ψ_{−q}⟩ = -(-q)|ψ_q⟩ = +q |ψ_q⟩\n3. i * QC|ψ_q⟩ = i*(-q)|ψ_{−q}⟩\n4. - QC|ψ_{−q}⟩ = -(+q)|ψ_q⟩ = -q |ψ_q⟩\n\nNow combine terms:\n\nTerms proportional to |ψ_q⟩: +q |ψ_q⟩ - q |ψ_q⟩ = 0\nTerms proportional to |ψ_{−q}⟩: i q |ψ_{−q}⟩ - i q |ψ_{−q}⟩ = 0\n\nAgain, nothing remains; it's zero. Huh. Is there any way to get a non-zero value?\n\nMaybe trying a different approach. Instead of working with basis states, perhaps look for algebraic relations between C and Q.\n\nSuppose we compute \\( (CQ + QC)^2 \\). Expanding this square:\n\n(CQ + QC)(CQ + QC) = CQ CQ + CQ QC + QC CQ + QC QC\n\nEach term needs computation. Remembering that CC† = identity? Or rather, assuming C is unitary and Hermitian? Actually, typically, the charge conjugation operator is antiunitary, meaning it satisfies certain conditions beyond being simply unitary. But perhaps for simplicity, treating it as an involution, meaning C² = ±I.\n\nAssuming C² = I, which is common in many cases, especially in simple models.\n\nThen, computing each term:\n\n1. CQ CQ = C(Q C) Q. Now, using the earlier action, Q C |ψ_q⟩ = (-q)|ψ_{−q}⟩, so effectively, Q C = -C Q. Thus, Q C = -C Q ⇒ C Q = -Q C.\n\nTherefore, substituting back:\n\nTerm 1: C Q C Q = C (Q C) Q = C (-C Q) Q = -C^2 Q^2 = -I Q^2 = -Q²\n\nSimilarly, Term 2: CQ QC = C Q Q C = C Q² C. Again, using Q C = -C Q, so Q² C = Q (Q C) = Q (-C Q) = -Q C Q = -(-C Q²) ???"}
{"id": "quantum/5071", "questions": "A particle of mass $m$ moves in a three-dimensional harmonic oscillator well. The Hamiltonian is\n\n$$\nH = \\frac{p^2}{2m} + \\frac{1}{2} kr^2.\n$$\n\n(a) Find the energy and orbital angular momentum of the ground state and the first three excited states.\n\n(b) If eight identical non-interacting (spin-i) particles are placed in such a harmonic potential, find the ground state energy for the eight-particle system.\n\n(c) Assume that these particles have a magnetic moment of magnitude $\\mu$. If a magnetic field $B$ is applied, what is the approximate ground state energy of the eight-particle system as a function of $B$. Plot the magnetization $\\left(-\\frac{\\partial E}{\\partial B}\\right)$ for the ground state as a function of $B$. ", "graphs": null, "llm_answers": "<think>\nOkay, so I've got this problem about a particle in a three-dimensional harmonic oscillator. It's part (a), asking me to find the energy and orbital angular momentum of the ground state and the first three excited states. Hmm, let me think through how to approach this.\n\nFirst off, I remember that a three-dimensional isotropic harmonic oscillator has quantized energy levels given by something like \\( E_{n_x, n_y, n_z} = \\hbar \\omega (n_x + n_y + n_z + \\frac{3}{2}) \\). Wait, but sometimes it's written differently depending on whether they're using different constants or not. Let me check: since the Hamiltonian here is \\( H = \\frac{p^2}{2m} + \\frac{1}{2} k r^2 \\), which can be rewritten in terms of frequency ω where \\( \\omega = \\sqrt{k/m} \\).\n\nSo yeah, each direction contributes \\( \\hbar \\omega (n_i + \\frac{1}{2}) \\) with i being x, y, z. So adding them up gives total energy as above.\n\nNow, the quantum numbers nx, ny, nz are all non-negative integers starting from zero. For the ground state, all should be zero because you want the lowest possible energy. That would give:\n\n\\( E_0 = \\hbar \\omega (\\frac{3}{2}) \\)\n\nBut wait, actually, no—wait, if each dimension adds half h-bar omega, then yes, summing over three dimensions gives 3/2 times h-bar omega.\n\nWait, hold on—is there another way people express the energy? Sometimes I see it expressed as \\( E_n = \\hbar \\omega (n + \\frac{3}{2}) \\), where n is the principal quantum number related to the sum of the individual quantum numbers. But maybe more accurately, when considering degeneracy, we group states based on their total n = nx + ny + nz.\n\nHmm, right—the energy depends only on the sum of the quantum numbers. Each increase in any one of nx, ny, or nz increases the energy by ħω. Therefore, the energies go up in steps of ħω as you add quanta in any direction.\n\nTherefore, the ground state corresponds to n=0, meaning nx=ny=nz=0, giving E=(3/2)ħω.\n\nThe next level, n=1, will correspond to having two possibilities: either one of the quantum numbers is increased by 1 while others remain zero. Since order matters, but in reality, due to symmetry, all those configurations lead to same energy. However, the question asks specifically about the \"first three\" excited states—I need to clarify whether they mean the first four including the ground state?\n\nNo, probably just after the ground state, the first three excited states. So perhaps n=1, n=2, etc., but also considering higher multiplicities.\n\nWait, hang on—if n=1, the energy is (5/2)ħω; n=2 could split into multiple cases, but does the energy depend solely on n? Yes, because regardless of how you distribute the quanta among the axes, the energy remains the same. Thus, each value of n defines an energy level.\n\nThus, the ground state is n=0, E=(3/2)ħω.\n\nThen, the first excited state(s): n=1, E=(5/2)ħω.\n\nSecond excited state: n=2, E=(7/2)ħω.\n\nThird excited state: n=3, E=(9/2)ħω.\n\nIs that correct? Or do I consider other factors?\n\nAlternatively, some sources might refer to the first excited state as the set of states with n=1, second excited as n=2, third as n=3, etc. Because even though n=1 includes several states (with different distributions of quantum numbers), they share the same energy.\n\nHowever, the question says \"the first three excited states\"—so likely referring to the first three distinct energy levels beyond the ground state. Hence, n=1, n=2, n=3.\n\nEach corresponding to E=(5/2)ħω, (7/2)ħω, (9/2)ħω respectively.\n\nAs for the orbital angular momentum L², its eigenvalues are l(l+1)ħ², where l is the orbital angular momentum quantum number.\n\nIn a 3D harmonic oscillator, the wavefunctions can be classified according to their angular momentum. In particular, for each energy level n, the maximum possible l is equal to n. Also, within each n, you can have various values of l ranging from 0 up to n, provided certain conditions.\n\nWait, but in the case of a 3D isotropic harmonic oscillator, the energy doesn't directly depend on l—it’s purely determined by n. So for each n, you get different multiplets characterized by different l-values.\n\nFor example, for n=0: Only l=0 is allowed, since all quantum numbers are zero.\n\nFor n=1: You can have l=0 and l=1. How?\n\nBecause when you have one quantum excitation, say nx=1, ny=0, nz=0, the angular momentum comes from the spatial distribution. Alternatively, you can combine spherical harmonics Y_l^m multiplied by radial functions R_nl(r). \n\nI recall that for each n, the possible l values range from 0 to n, stepping by 2. No, wait—that applies to hydrogen-like atoms. Maybe in HO, it's similar.\n\nActually, in the 3D isotropic harmonic oscillator, the energy levels are equally spaced, and each level n contains subspaces labeled by l, where l ranges from 0 to n, inclusive, but with specific selection rules.\n\nLet me try to figure out the possible l for each n.\n\nAt n=0:\n- All quantum numbers are zero, so l must be zero.\n\nAt n=1:\n- We can have either one node along x, y, or z. These correspond to p-orbitals, hence l=1. Additionally, spherically symmetric combinations may exist—but wait, for n=1, can you form an s-state (l=0)?\n\nYes! By taking linear combinations of the Cartesian basis states (nx, ny, nz) to make spherical harmonics. Specifically, the combination sqrt(1/3)(x + y + z)^2... Wait, no—for n=1, the monopole term isn’t present except trivially.\n\nWait, sorry, getting confused. Actually, for n=1, the single-excitation states |nx=1>, |ny=1>, |nz=1> transform under rotation as vectors, thus forming an l=1 representation. There is no scalar (s-wave) component in n=1 because you cannot symmetrize three orthogonal directions without introducing vector character.\n\nSimilarly, for n=2, you can have both l=0 and l=2 components. Because quadratic forms allow for scalars (like x²+y²+z²) which are s-waves (l=0), and quadrupolar terms (which involve things like x² - y², xy, etc.) leading to l=2.\n\nWait, okay, so generalizing, for each n, the possible l values start from 0 up to n, increasing by 2 each time. Is that accurate?\n\nNot exactly. From my notes, in 3D harmonic oscillators, the decomposition goes as follows:\n\nLevel n decomposes into representations with l = 0, 1, ..., n, but subject to parity considerations. Parity alternates between even and odd n. Even n allows even l, odd n allows odd l.\n\nWait, no, that seems conflicting. Let me double-check.\n\nParity operator Π changes sign coordinates → affects angular momentum via (-1)^l. So, for a given n, the parity is (-1)^n. Then, each state with definite l must satisfy (-1)^l = (-1)^n ⇒ l ≡ n mod 2.\n\nWhich means that for a given n, l can take integer values from max(0, n−something?) up to n, but differing by 2 each time.\n\nSpecifically, for each n ≥ 0,\n\nIf n is even, l can be 0, 2, 4,..., up to n (if n is even).\nIf n is odd, l can be 1, 3, 5,..., up to n (if n is odd).\n\nAh, that makes sense now. So for n=0 (even):\n\nOnly l=0.\n\nn=1 (odd):\nOnly l=1"}
{"id": "quantum/3-3004", "questions": "A certain state $\\vert \\psi \\rangle$ is an eigenstate of $\\hat{L}^2$ and $\\hat{L}_z$:\n\n$$\n\\hat{L}^2 \\vert \\psi \\rangle = l(l + 1) \\hbar^2 \\vert \\psi \\rangle, \\quad \\hat{L}_z \\vert \\psi \\rangle = m \\hbar \\vert \\psi \\rangle.\n$$\n\nFor this state calculate $\\langle \\hat{L}_x \\rangle$ and $\\langle \\hat{L}^2_x \\rangle$.", "graphs": null, "llm_answers": "<think>\nOkay, so I have to find the expectation values ⟨Lₓ⟩ and ⟨L²ₓ⟩ for a given quantum state |ψ⟩ which is an eigenstate of L² and L_z. Hmm, let me think about how to approach this.\n\nFirst off, since |ψ⟩ is an eigenstate of both L² and L_z, it must be one of those standard angular momentum states with definite l and m. So we know that L²|ψ⟩ = ħ²l(l+1)|ψ⟩ and L_z|ψ⟩ = ħm|ψ⟩. That part makes sense because these operators commute when acting on such states.\n\nNow, the question asks specifically about the expectations involving L_x. Since L_x isn't diagonal in the same basis as L_z or L², its matrix elements won't just give us straightforward results like L_z does. But maybe there's some symmetry here?\n\nI remember from my studies that the components of angular momentum satisfy [L_i, L_j] = iħε_ijk L_k. Also, all three components (L_x, L_y, L_z) should behave similarly under rotations, but their individual properties depend on the coordinate system you choose.\n\nWait, if the state has a well-defined L_z component, then what can we say about the other components? Intuitively, measuring L_x would disturb the state because they don't share eigenvectors unless perhaps in symmetric situations.\n\nBut wait, no—since the state is fixed along z-axis, any measurement in x-direction might not preserve the state. However, for expectation value calculations, even though the operator doesn’t act diagonally, we can still compute averages using commutators or known identities.\n\nLet me recall: The average of L_x squared plus similar terms relates to the total angular momentum. Specifically,\n\n⟨L_x²⟩ + ⟨L_y²⟩ + ⟨L_z²⟩ = ⟨L²⟩ / something... Wait, actually, more precisely, each component satisfies ⟨L_i²⟩ = (⟨L²⟩)/3 - something else? Or maybe another relation.\n\nHold on, I think the correct identity is that for any direction, the sum of squares equals L² minus twice the square of the third component. No, wait—that’s probably incorrect. Let me get back to basics.\n\nWe know that L² = L_x² + L_y² + L_z². Therefore, taking expectation values gives:\n\n⟨L²⟩ = ⟨L_x²⟩ + ⟨L_y²⟩ + ⟨L_z²⟩.\n\nSo, rearranged, we have:\n\n⟨L_x²⟩ = ⟨L²⟩ - ⟨L_y²⟩ - ⟨L_z²⟩.\n\nHmm, but without knowing ⟨L_y²⟩, that doesn't help much directly. Maybe instead, considering symmetries between different axes could lead somewhere.\n\nSince our state is aligned along the z-axis, due to being an eigenstate of L_z, perhaps the expectation values of L_x and L_y will vanish? Because intuitively, the particle shouldn't preferentially move in the x or y directions—it only has a defined projection along z.\n\nYes! For example, consider spin-½ particles. If a spin state is polarized along z, the expected values of S_x and S_y are zero because the probabilities cancel out. Similarly, higher angular momenta should exhibit the same behavior due to rotational symmetry around the axis where the state is defined.\n\nTherefore, I suspect that ⟨L_x⟩ = 0. Is that right? Well, yes—if the wavefunction is spherically symmetric except for alignment along z, then averaging over angles perpendicular to z would result in cancellation for L_x and L_y.\n\nSimilarly, for the variance or uncertainty, we need to look into ⟨L_x²⟩. To do this, maybe use ladder operators. Remembering that L_plus and L_minus raise/lower m by ±1.\n\nExpressed in terms of ladder operators, L_x = (L_+ + L_-)/(2i). Then, squaring that gives L_x² = (L_+² + L_-² + L_+L_- + L_-L_+)/(-4).\n\nTaking expectation values, note that applying L_± to |ψ⟩ either raises or lowers m. In particular, L_+|ψ⟩ ~ |m+1⟩ and L_-|ψ⟩ ~ |m−1⟩. Thus, when calculating ⟨ψ|L_+|ψ⟩, it becomes proportional to δ_{m,m+1}, which is zero unless m=..., but in general, for arbitrary m, this inner product is zero.\n\nHence, ⟨L_+⟩ = ⟨L_-⟩ = 0, meaning ⟨L_x⟩ = 0, confirming earlier intuition.\n\nWhat about ⟨L_x²⟩? Using the expression above, expanding L_x²:\n\n⟨L_x²⟩ = (⟨L_+²⟩ + ⟨L_-²⟩ + ⟨L_+L_-⟩ + ⟨L_-L_+⟩ ) / (-4)\n\nCompute term by term:\n\n1. ⟨L_+²⟩: Applying L_+ twice would take |m⟩ → |m+2⟩, whose overlap with |m⟩ is zero. Hence, ⟨L_+²⟩ = 0.\n\nSame logic applies to ⟨L_-²⟩; also zero.\n\nNext, ⟨L_+L_-⟩: This acts as L_+L_-|m⟩ = L_+(√(l(l+1)-m(m+1))ħ|m⟩ ). Which simplifies to √((l-m)(l+m+1))(√(l(l+1)-m(m+1))) ? Wait, hold on—I'm getting confused.\n\nActually, L_+L_- is equal to (L² - L_z² - ħ L_z), up to constants. Alternatively, recalling that L_+L_- = (L² - L_z² - ħ L_z)/something...\n\nAlternatively, better yet, express L_+L_- in terms of number operators. We know that L_+L_- = (L² - L_z² - ħ L_z). Wait, let me verify:\n\nFrom definitions:\n[L+, L-] = 2ħ L_z,\nand L² = L_x² + L_y² + L_z².\n\nAlso, L_+L_- = (L_x + i L_y)^2 ?\n\nNo, rather, L_+ = L_x + i L_y, so L_+L_- = (L_x + i L_y)(L_x - i L_y) = L_x² + L_y² + i(L_y L_x - L_x L_y )\n\nThe last term involves the commutator [L_x, L_y] = iħ L_z. So substituting:\n\nL_+L_- = L_x² + L_y² + i(iħ L_z) = L_x² + L_y² - ħ L_z.\n\nThus, indeed, L_+L_- = L_x² + L_y² - ħ L_z.\n\nThen, going back to the original equation:\n\nL² = L_x² + L_y² + L_z² ⇒ L_x² + L_y² = L² - L_z².\n\nSubstituting into L_+L_-:\n\nL_+L_- = (L² - L_z²) - ħ L_z.\n\nTherefore, ⟨L_+L_-⟩ = ⟨L²⟩ - ⟨L_z²⟩ - ħ ⟨L_z⟩.\n\nGiven that |ψ⟩ is an eigenstate of L² and L_z, we substitute:\n\n⟨L²⟩ = ħ² l(l+1),\n⟨L_z⟩ = ħ m,\n⟨L_z²⟩ = (ħ m)^2.\n\nPlugging in:\n\n⟨L_+L_-⟩ = ħ² l(l+1) - (ħ m)^2 - ħ*(ħ m)\n           = ħ² [ l(l+1) - m² - m ].\n\nSimplify inside brackets:\n\nl(l+1) - m² - m = l² + l - m² - m.\n\nFactorizing:\n\nNot sure, but leave it as is for now.\n\nSimilarly, ⟨L_-L_+⟩ can be found. Note that L_-L_+ = L_+† L_+ = (number operator related). Actually, computing L_-L_+:\n\nUsing similar steps, starting from L_-L_+ = (L_x - i L_y)(L_x + i L_y) = L_x² + L_y² + i(L_x L_y - L_y L_x ).\n\nAgain, the cross-term is i times [L_x, L_y]/(iħ)*iħ = i*(-iħ L_z) *iħ? Wait, maybe easier way.\n\nAlternatively, following previous method:\n\nL_-L_+ = L_x² + L_y² + i([L_x, L_y])/(iħ) * ħ ?\n\nWait, perhaps better"}
{"id": "quantum/3-3032", "questions": "Consider an electron in a uniform magnetic field along the $z$ direction. Let the result of a measurement be that the electron spin is along the positive y direction at $ t = 0 $. Find the Schrödinger state vector for the spin, and the average polarization (expectation value of $ s_x $) along the x direction for $ t > 0 $.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about an electron in a uniform magnetic field pointing along the z-direction. At time t=0, someone measures the electron's spin to be along the positive y-axis. The question asks me to find the Schrödinger state vector for the spin after some time t>0 and also compute the expectation value of the spin component along the x-direction.\n\nHmm, let me start by recalling what I know about spins in quantum mechanics. Electrons are fermions with spin-1/2, which means their spin states can be described using two-component spinors. When dealing with magnetic fields, especially static ones like here, we often use the Pauli equation or consider how the Hamiltonian affects the spin precession.\n\nThe magnetic field is along the z-axis, so it should interact with the spin through the Zeeman effect. That makes sense because the interaction energy between a magnetic moment and a magnetic field depends on their alignment. For electrons, the magnetic dipole moment μ is proportional to the spin angular momentum S: μ = -γS, where γ is the gyromagnetic ratio. But maybe more importantly, when you apply a magnetic field B in the z-direction, the corresponding Hamiltonian H would involve σ_z, since the spin operator components are given by the Pauli matrices multiplied by ħ/2.\n\nWait, actually, the exact form might depend on whether we're considering orbital motion as well, but since the question specifies \"electron spin,\" I think only the spin part matters. So yes, the Hamiltonian due to the magnetic field will be something like H = ω_s * S_z, where ω_s is related to the Larmor frequency. Alternatively, sometimes people write it as H = -μ·B, which becomes H = -(e/(2m)) g_B B S_z, but perhaps I don't need all those constants right now; instead, just knowing that the Hamiltonian involves σ_z scaled appropriately.\n\nBut wait, if the initial state is measured to have spin up along y, then its state must be |+y⟩. How do I express that? In terms of eigenstates of S_y, which correspond to eigenvectors of the Pauli matrix σ_y. Since σ_y has eigenvalues +1 and -1, the normalized eigenvectors are:\n\nFor λ = +1:\n(1/sqrt{2}) [1]\n          [-i]\n\nAnd for λ = -1:\n(1/sqrt{2}) [1]\n           [+i]\n\nSo the state |+y⟩ is (|↑⟩_x rotated somehow). Wait no, not exactly. Because measuring spin along y gives either +ħ/2 or -ħ/2. Hmm, okay, so our initial state at t=0 is |ψ(0)⟩ = |+y⟩, which is represented as (1/sqrt{2})(|↑⟩_z - i|↓⟩_z), isn't it?\n\nLet me double-check that. If I recall correctly, the eigenkets of σ_y are indeed these combinations. Yes, because applying σ_y to them yields ±1 times themselves. Specifically,\n\nσ_y |+y⟩ = (+1)|+y⟩,\nand similarly for |-y⟩.\n\nTherefore, writing out |+y⟩ explicitly, it's (1/sqrt{2}, -i/sqrt{2})^T in the standard basis {|↑⟩_z, |↓⟩_z}.\n\nNow, moving forward in time from t=0 onwards, under the influence of the Hamiltonian H. To get the time evolution, I'll need to solve the Schrödinger equation, specifically the time-dependent case, assuming H doesn't change over time—so it's time-independent. Therefore, the time-evolved state is given by |ψ(t)⟩ = e^{-iHt/ħ} |ψ(0)⟩.\n\nSince H is diagonal in the computational basis (because it's proportional to σ_z), exponentiating it shouldn't be too bad. Let me denote H = ω σ_z / 2, where ω is twice the Larmor frequency or whatever scaling factor comes into play—I'm going to keep things symbolic unless necessary.\n\nAlternatively, another approach could be expressing everything in terms of energies. Suppose E_up and E_down are the energies associated with being in |↑⟩_z and |↓⟩_z respectively. Then, each component picks up a phase factor based on their respective energies.\n\nGiven that, suppose H = α σ_z, where α incorporates any physical constants needed. Then, the time evolution operator U(t) = exp(-i H t / ħ ) = exp(-i α σ_z t / ħ ). \n\nExpressed in terms of the Pauli matrices, the exponential can be expanded using Euler's formula because σ_z squares to identity. Remembering that for any real θ, exp(iθ σ_z) = cosθ I + i sinθ σ_z. Similarly, for negative exponents, it'd flip signs accordingly.\n\nThus, U(t) = cos(α t / ħ) I - i sin(α t / ħ) σ_z.\n\nApplying this to |ψ(0)⟩, which is (1/sqrt{2}, -i/sqrt{2}), let's see:\n\nFirst, break down |ψ(0)⟩ into |↑⟩_z and |↓⟩_z components. It already is expressed in that basis, so:\n\nU(t) acting on |↑⟩_z gives us e^{-i E_up t / ħ } |↑⟩_z,\nSimilarly, U(t) acting on |↓⟩_z gives e^{-i E_down t / ħ } |↓⟩_z.\n\nIf H = α σ_z, then E_up = α*(+1)/2* ħ ? Or wait, hold on—if H is written as H = ω σ_z / 2, then the eigenvalues are ±ω/2. Maybe better to define H such that H = ω σ_z, making eigenvalues ±ω. Either way, depending on conventions.\n\nActually, let's clarify. Spin operators satisfy S_i = (ħ/2) σ_i. Thus, the Hamiltonian for spin in a magnetic field B along z is typically H = -γ B S_z, where γ is the gyromagnetic ratio. Substituting S_z = (ħ/2) σ_z, so H = (-γ B)(ħ/2) σ_z. Hence, H = - (γ B ħ / 2) σ_z.\n\nIn many cases, though, they set units such that certain factors combine—for example, defining ω_L = γ B, leading to H = - (ω_L ħ / 2) σ_z. However, regardless, the key point is that the energy difference between the spin-up and spin-down states leads to a relative phase accumulation over time.\n\nAssuming H is known, say H = ω σ_z / 2, then the time evolution operator is U(t)=exp(-i H t / ħ )=exp(-i (ω σ_z / 2) t / ħ ).\n\nWhich simplifies to U(t)=cos(ω t / (2ħ)) I - i sin(ω t / (2ħ)) σ_z.\n\nBut without getting bogged down in specific coefficients, perhaps I can proceed symbolically.\n\nAnyway, back to computing |ψ(t)⟩. Given that |ψ(0)⟩=(1/sqrt{2}, -i/sqrt{2})^T, which is equal to (|↑⟩_z - i |↓⟩_z)/sqrt{2}. Applying U(t):\n\nEach term gets multiplied by a phase factor. Assuming H acts diagonally, so:\n\nU(t) |↑⟩_z = e^{-i E_up t / ħ } |↑⟩_z,\nU(t) |↓⟩_z = e^{-i E_down t / ħ } |↓⟩_z.\n\nSuppose E_up = +E and E_down = -E, meaning H = E σ_z. Then, the phases become e^{-i E t / ħ } and e^{+i E t / ħ }, respectively.\n\nSubstituting into |ψ(t)⟩:\n\n|ψ(t)⟩ = (e^{-i E t / ħ } |↑⟩_z - i e^{+i E t / Ą } |↓⟩_z ) / sqrt{2}\n\nWait, hang on—isn't there a missing factor somewhere? Actually, if H = E σ_z, then E_up = E and E_down = -E. So yeah, correct.\n\nBut perhaps even simpler, take H = ω σ_z, so E_up = ω, E_down=-ω. Then, the phases are e^{-i ω t} and e^{+i ω t}.\n\nThen, substituting:\n\n|ψ(t)⟩ = (e^{-i ω t} |↑⟩_z - i e^{+i ω t} |↓⟩_z ) / sqrt{2}\n\nIs that right? Yeah, seems consistent.\n\nAlternatively, factoring out e^{-i ω t}/sqrt{2}:\n\n|ψ(t)⟩"}
{"id": "quantum/6058", "questions": "Protons (magnetic moment $\\mu$) are in a magnetic field of the form\n\n$$\nB_x = B_0 \\cos \\omega t, \\quad B_y = B_0 \\sin \\omega t,\n$$\n\n$$\nB_z = \\text{constant}, \\quad B_0 \\ll B_z.\n$$\n\nAt $t = 0$ all the protons are polarized in the $+z$ direction.\n\n(a) What value of $\\omega$ gives resonant transitions?\n\n(b) What is the probability for a proton at time $t$ to have spin in the $-z$ direction? (Assume $B_0 \\ll B_z$.)", "graphs": null, "llm_answers": "<think>\nOkay, so I've got this problem about protons in a magnetic field, right? It's part (a) asking for the resonance frequency ω that causes transitions between spin states. Hmm, let me think through this carefully because it involves quantum mechanics and magnetic fields—topics I need to recall properly.\n\nFirst off, protons have a magnetic moment μ. When placed in a magnetic field, their spins precess around the field direction due to the Zeeman effect. The Hamiltonian governing such systems usually depends on the interaction between the magnetic moment and the external magnetic field.\n\nThe given magnetic field has components:\n- \\( B_x = B_0 \\cos(\\omega t) \\)\n- \\( B_y = B_0 \\sin(\\omega t) \\)\n- \\( B_z = \\text{constant} \\)\n\nAnd importantly, \\( B_0 \\ll B_z \\). So, the z-component is dominant but there's also a small oscillating component in the x-y plane.\n\nSince we're dealing with spin-1/2 particles like protons, their spin state can be either up or down along any axis. At t=0, all protons are polarized in the +z direction, meaning they start in the spin-up state relative to the z-axis.\n\nNow, when you apply an oscillating magnetic field perpendicular to the static field, you induce transitions between these spin states if certain conditions are met—that’s essentially how NMR works. Resonance occurs when the energy provided by the oscillating field matches the energy difference between the two spin states.\n\nLet me write out the total magnetic field as:\n\n\\( \\mathbf{B}(t) = B_z \\hat{z} + B_0 [\\cos(\\omega t)\\hat{x} + \\sin(\\omega t)\\hat{y}] \\).\n\nThis rotating vector in the x-y plane suggests that the effective magnetic field is rotating with angular frequency ω. But since \\( B_0 \\ll B_z \\), the main influence comes from \\( B_z \\), which defines the quantization axis.\n\nIn terms of the Hamiltonian, ignoring some constants, the interaction term would involve the dot product of the magnetic moment and the magnetic field. For a proton, the magnetic dipole moment is proportional to its spin: \\( \\boldsymbol{\\mu} = -\\gamma \\hbar \\mathbf{S}/(2m_p c^2) \\)—but maybe more simply, considering just the proportionality constant γ, perhaps.\n\nWait, actually, another approach might be better here. Let me consider the standard form of the Hamiltonian for spin in a magnetic field. In the lab frame where the magnetic field is changing, the Hamiltonian H(t) should include both the static and oscillating parts.\n\nSo,\n\nH(t) ≈ -(μ · B(t)) \n\nBut wait, no—the actual expression is something else. Wait, correction: The Hamiltonian for a magnetic dipole in a magnetic field is typically given by:\n\n\\( H = -\\boldsymbol{\\mu} \\cdot \\mathbf{B} \\).\n\nGiven that, then yes, plugging in our B(t):\n\n\\( H(t) = -\\mu [B_z S_z / ħ + B_0 (\\cos(\\omega t) S_x + sin(\\omega t) S_y)/ħ ] \\times ... hmm, not exactly sure about units here; probably missing factors involving Planck's constant somewhere.\n\nAlternatively, using natural units where ħ=1, etc., but maybe getting bogged down. Alternatively, express everything in terms of Larmor frequencies.\n\nI remember that the Larmor frequency ω_L is given by ω_L = γ B, where γ is the gyromagnetic ratio. Since the static field is much larger than the oscillating one, the splitting between the spin-up and spin-down states will primarily depend on B_z.\n\nThus, the energy separation ΔE between m_s = +½ and m_s = -½ is approximately:\n\nΔE ≈ μ B_z * g-factor... Wait, hold on—I'm mixing things up. Actually, the energy levels split according to E = ± (γ ħ B_z ) / 2, assuming γ includes the necessary constants.\n\nTherefore, the transition frequency ω_res corresponds to the energy gap divided by ℏ:\n\nω_res = ΔE / ℏ = (γ B_z) / 2 * 2 ? Wait, no—if E = ± (γ B_z ħ)/2, then ΔE = γ B_z ħ. Therefore, ω_res = ΔE / ℏ = γ B_z.\n\nHmm, okay, so the resonance condition happens when the driving frequency ω equals the Larmor frequency induced by the static field B_z. That makes sense—it aligns with what I know about NMR, where you match your RF frequency to the Larmor frequency determined by the DC field.\n\nBut wait, does the presence of the oscillating field affect this? Or is it negligible because B0 << Bz? Probably, yeah, since B0 is much smaller, the primary determinant of the energy splitting remains Bz, so the resonance condition is still set by matching ω to γ Bz.\n\nHold on—isn't the oscillation causing a perturbation? Maybe I need to use Fermi's Golden Rule or look into time-dependent perturbations. Because even though B0 is small, over time, it could cause transitions.\n\nYes, indeed, treating this as a time-dependent perturbation problem seems appropriate. The unperturbed system is the proton in the static Bz field, giving rise to discrete energy eigenstates |↑⟩ and |↓⟩ with energies E_up and E_down.\n\nThen, the perturbing Hamiltonian is the time-varying part due to Bx and By. Expressing this in matrix form, the perturbation V(t) connects the spin-up and spin-down states via the transverse components.\n\nExpressed in terms of Pauli matrices, perhaps. If I take σ_x and σ_y as the lowering and raising operators, respectively—or rather, combinations thereof.\n\nActually, writing the perturbation explicitly:\n\nV(t) ∝ B0 cos(ω t) σ_x + B0 sin(ω t) σ_y.\n\nWhich simplifies to V(t) ∝ B0 e^{-i ω t} σ_- + h.c., where σ_- is the lowering operator.\n\nUsing first-order time-dependent perturbation theory, the transition amplitude A_{up→down}(t) is given by:\n\nA ~ (e^{i ω_{ud} t}) ∫₀^t dt' exp(i ω t') ⟨down|V(t')|up⟩,\n\nwhere ω_{ud} = (E_down - E_up)/ℏ.\n\nCalculating this integral leads us to delta functions or approximations depending on whether the denominator approaches zero—in other words, resonance when ω ≈ ω_{ud}.\n\nFrom earlier, ω_{ud} = (E_down - E_up)/ℏ = (-μ B_z - (+μ B_z))/ℏ = (-2 μ B_z)/ℏ → ω_{L} = 2 μ B_z / ℏ ?\n\nWait, confusion arises again regarding signs and definitions. Let me clarify:\n\nEnergy levels for spin in a magnetic field: E_m = -m μ B, where m = ±1/2 for spin-1/2.\n\nSo, E_up = - (1/2) μ B_z,\n\nand E_down = + (1/2) μ B_z.\n\nThus, the energy difference ΔE = E_down - E_up = μ B_z.\n\nTherefore, ω_{res} = ΔE / ℏ = μ B_z / ℏ.\n\nBut wait, sometimes people define the Larmor frequency as ω_L = γ B, where γ is the gyromagnetic ratio. And γ itself relates to μ as γ = μ/(g q), where g is the Landé factor—but for a proton, g ≈ 5.586, but exact number may vary based on context.\n\nHowever, regardless of specific constants, the key point is that the resonance occurs when the applied frequency ω matches the natural frequency of the system, i.e., the Bohr frequency corresponding to the energy splitting caused by B_z.\n\nPutting it together, the resonance condition is ω = ω_L = γ B_z.\n\nBut without knowing the exact definition of μ, perhaps expressing it directly in terms of μ and B_z is safer.\n\nWait, going back to the beginning, the question says \"protons (magnetic moment μ)\", so presumably, μ is already defined appropriately. Thus, the energy splitting is ΔE = μ B_z, leading to ω_res = μ B_z / ℏ.\n\nBut hang on, isn't the Larmor frequency often written as ω_L = γ B, where γ incorporates the charge-to-mass ratio and other constants? Perhaps in this case, μ = γ ħ / 2,"}
{"id": "quantum/1-1002", "questions": "﻿Express each of the following quantities in terms of $\\hbar, e, c, m = \\text{electron mass}, M = \\text{proton mass}$. Also give a rough estimate of numerical size for each.\n\n(a) Bohr radius (cm).  \n(b) Binding energy of hydrogen (eV).  \n(c) Bohr magneton (choosing your own unit).  \n(d) Compton wavelength of an electron (cm).  \n(e) Classical electron radius (cm).  \n(f) Electron rest energy (MeV).  \n(g) Proton rest energy (MeV).  \n(h) Fine structure constant.  \n(i) Typical hydrogen fine-structure splitting (eV).\n", "graphs": null, "llm_answers": "<think>\nAlright, so I have this problem where I need to express various physical constants in terms of ħ (reduced Planck's constant), e (elementary charge), c (speed of light), m (electron mass), and M (proton mass). Then, I also need to give a rough estimate of their numerical sizes. Let me go through each part one by one.\n\nStarting with part (a): The Bohr radius. Hmm, from what I remember, the Bohr model gives us expressions involving these fundamental constants. Specifically, the Bohr radius is given by something like n²ħ²/(me²Z), but since we're talking about the ground state of hydrogen, Z=1 and n=1. So it should be ħ²/(m e²). But wait, let me make sure: yes, that seems right because the formula is usually written as a0 = 4πε₀(ħ²)/(me²), but if ε₀ isn't allowed here, maybe they want everything expressed without other constants? Wait no, actually, sometimes people write things using Coulomb's law which involves 1/(4πε₀)e², so perhaps when expressing in terms of just e, you can include factors accordingly. Alternatively, considering natural units or Gaussian units might change how it looks. Maybe another approach: think dimensionally. What has dimensions of length?\n\nThe Bohr radius [L] must come from combining ħ ([ML²T⁻¹]), e ([QT], Q being electric charge, T time), c ([LT⁻¹]), m ([M]). To get L, let's see:\n\nWe know that in SI units, the expression includes permittivity of free space ε₀, but since we don’t have ε₀ listed among our variables, probably we’re supposed to use some relation that allows us to eliminate ε₀. Since 1/(4πε₀) equals ke²/(hc/2π * something?), not exactly straightforward. Alternatively, recall that in atomic units, certain combinations lead to lengths naturally.\n\nWait, more carefully: In quantum mechanics, the Bohr radius comes out as a0 ≈ ħ²/(μ e²), where μ is the reduced mass. For hydrogen, μ ≈ m/(1 + m/M) ≈ m*(since proton mass >> electron mass). Therefore, approximately, a0 ≈ ħ²/(m e²).\n\nBut hold on, does that match dimensional analysis? Let’s check:\n\n[ħ]^2 / (m [e]^2)\n\nIn SI units:\nħ^2 has units kg·m⁴/s²,\ndivided by m (kg),\nand divided by e squared (C²),\n\nSo overall units would be (kg·m⁴/s²) / (kg · C²) ) → m⁴/(s² C²). That doesn't seem correct; meters cubed over seconds squared coulombs squared... Not quite meters. Hmmm, did I mess up somewhere?\n\nAlternatively, thinking again, perhaps including c somehow. Because speed of light often appears in such expressions when dealing with electromagnetism and relativity together.\n\nLet me try to reconstruct the Bohr radius properly. From the Bohr model, the centripetal force is provided by the Coulomb attraction:\n\n(m v²)/r = k_e e² / r²\n\nWhere k_e is Coulomb's constant, equal to 1/(4πε₀). Solving for r,\n\nr = (k_e e² m)/(v²)\n\nFrom angular momentum quantization, mv r = n ħ ⇒ v = n ħ/(mr). Substituting into previous equation:\n\nr = (k_e e² m) / ((n² ħ²/m² r²)) )\n\nMultiply both sides by denominator:\n\nr³ = (k_e e² m³) / (n² ħ²)\n\nThus,\n\nr = [(k_e e² m³)/(n² ħ²)]^(1/3)\n\nFor n=1, it becomes cube root(k_e e² m³ / ħ² ). Which simplifies to (k_e e² m)/(ħ²)^{something}. Wait, better yet, writing all in exponents:\n\nr ∝ (e² m)^(1/3) / (ħ^{2/3}) times k_e^{1/3}\n\nHmm, but now, unless I'm missing something else. If I consider that k_e = 1/(4πε₀), then perhaps there's a way to relate it via Maxwell equations or something else. However, since the question specifies only to use hbar, e, c, m, M, I cannot directly involve ε₀. Thus, perhaps instead, I need to find an alternative expression that relates k_e with those constants.\n\nI recall that in natural units, c=1, ħ=1, etc., but even so, relating e requires knowing α, the fine structure constant, which itself is e²/(4πε₀ ħ c)}^{-1}? Or rather, alpha is defined as e²/(4πε₀ ħ c)}. Yes, indeed, α = e²/(4 π ε₀ ħ c)}, so rearranged, 1/(4 pi epsilon_0) = alpha * ħ c / e².\n\nTherefore, substituting back into the Bohr radius expression:\n\nr = (k_e e² m)/(ħ²) => replacing k_e with alpha* ħ c/e²,\n\nso,\n\nr = (alpha * ħ c / e² * e² m ) / ħ² = (alpha * ħ c m ) / ħ² = alpha c m / ħ\n\nSimplify further: alpha cancels with ħ, giving (c m)/(ħ / alpha). Wait, alternatively, compute numerically:\n\nSince alpha is ~1/137, so roughly speaking, 1/alpha ~137.\n\nHence, r ≈ (c m)/(ħ) * (1/alpha). Is that helpful? Perhaps not immediately.\n\nWait, going back, original expression was r = (k_e e² m)/(ħ²). And since k_e = 1/(4pi eps0)= alpha ħ c / e², plugging in:\n\nr = (alpha ħ c / e²)* e² m / ħ² = alpha c m / ħ\n\nYes! So finally, Bohr radius a0 = (α c m)/ħ.\n\nThat makes sense because alpha is dimensionless, c has LT^-1, m has M, and ħ has ML²T^-1. Putting them together:\n\n(alpha) [dimensionless]\n\ntimes c [LT^-1],\n\ntimes m [M],\n\nover ħ [ML²T^-1].\n\nMultiplying numerator: M*L*T^-1*M = M² L T^-1.\n\nDenominator: M L² T^-1.\n\nDividing: (M² L T^-1) / (M L² T^-1) ) = M/L.\n\nWait, that still leaves M/L, which is inverse length. Hmm, that suggests my substitution may have messed up the dimensions. Did I do that correctly?\n\nWait, no—wait, let me re-express:\n\nNumerator after multiplying: alpha*c*m = (unitless)*(L/T)*(M) = M L / T.\n\nDenominator: ħ has units ML²T^{-1}.\n\nSo total units: (M L / T) / (M L² T^{-1})) = (M L / T) * (1/(M L² T^{-1})) ) =\n\n(M L / T) * (T / (M L² )) ) = (M L T) / (M L² T) ) = 1/L.\n\nWhich means the entire thing is 1/L, meaning reciprocal length. But Bohr radius is a length, so clearly, I made a mistake in algebraic manipulation.\n\nPerhaps I missed a factor. Let me double-check the earlier steps.\n\nOriginal derivation:\n\nCentripetal acceleration: mv²/r = k_e e² / r²\n\n=> r = k_e e² m / v²\n\nAngular momentum: mvr = n ħ => v = n ħ/(m r)\n\nSubstitute into first equation:\n\nr = k_e e² m / ( (n² ħ²)/(m² r²) )\n\nMultiply both sides by denominator:\n\nr³ = k_e e² m³ / (n² ħ² )\n\nTake cube roots:\n\nr = (k_e e² m³ / ħ² )^{1/3 } * (1/n)\n\nAh, okay, so exponent-wise, it's proportional to (k_e e² m³ )^{1/3}/(ħ^{2/3})\n\nGiven that, and wanting to express in terms of ħ,e,c,m,M, perhaps substitute k_e in terms of known constants.\n\nAs before, k_e = 1/(4 pi"}
{"id": "quantum/1-1005", "questions": "﻿In the days before Quantum Mechanics, a big theoretical problem was to stop an atom from emitting light. Explain. After Quantum Mechanics, a big theoretical problem was to make atoms in excited states emit light. Explain. What does make excited atoms emit light?\n", "graphs": null, "llm_answers": "<think>\nOkay, so I'm trying to understand this question about why it was a problem for atoms not to emit light before quantum mechanics, and then after that theory came along, how making them emit became another issue. Hmm.\n\nFirst part: Before QM, people thought of electrons orbiting nuclei like planets around the sun—classical model. But according to classical electromagnetism, accelerating charges (like moving electrons) should radiate energy as electromagnetic waves, which is light or other EM radiation. So if electrons were always losing energy while orbiting, they'd spiral into the nucleus quickly because their orbits would decay. That's bad because we know atoms don't collapse; they're stable. So pre-QM theories couldn’t explain why atoms didn't just fall apart and keep emitting light until they died out. Hence, stopping emission wasn't possible with those models—it predicted constant emission leading to instability.\n\nSecond part: With QM, especially Bohr’s model, electrons have discrete energy levels. They can only exist in certain quantized orbits without emitting radiation when stationary between transitions. When they do transition from higher to lower energy levels, they emit photons corresponding to the energy difference. Now, theoretically, you could get these emissions, but maybe there were issues predicting exactly under what conditions or frequencies? Or perhaps experimental confirmation was tricky?\n\nWait, no—the main point might be that once QM explained stability via quantization, the next challenge was understanding how and why transitions happen, i.e., getting atoms to emit light on demand. Maybe early QM had some gaps in explaining spontaneous emission rates or stimulated emission processes. Einstein worked on that with his explanation of stimulated emission, which led to things like lasers later on.\n\nSo putting it together:\n\nBefore QM, atoms shouldn't last long since electrons lose energy continuously → emits light forever, causing collapse. Problem was preventing such continuous emission.\n\nAfter QM, atoms have stable ground states, but now the issue becomes initiating emission from excited states. Theoretical challenges included explaining why and how excited atoms release photons—they need something to trigger the drop to a lower state, whether through thermal vibrations, collisions, or external stimuli. Spontaneous emission is probabilistic, governed by selection rules and lifetimes of excited states.\n\nWhat makes excited atoms emit light? It's usually due to perturbations causing the electron to transition down, releasing a photon. This can be random (spontaneous), induced by another photon (stimulated), or caused by interactions with surrounding particles.\n</think>\n\n**Answer**\n\nThe behavior of atomic emission changed significantly with the advent of quantum mechanics. Here's a structured breakdown:\n\n1. **Pre-Quantum Mechanics Era**: \n   - Classical physics suggested that orbiting electrons would continually emit radiation, spiraling into the nucleus. This posed a paradox because observed atoms remained stable without collapsing. Thus, the challenge was understanding why atoms did not constantly emit light and destabilize.\n\n2. **Post-Quantum Mechanics Era**:\n   - Niels Bohr introduced quantized energy levels, where electrons occupy specific orbits without emitting radiation unless transitioning between levels. While this solved the stability issue, it raised new questions about triggering emissions from excited states. Theories needed to account for mechanisms like spontaneous and stimulated emission, involving factors such as thermal motion, particle collisions, or external fields.\n\n3. **Mechanism Behind Emission**:\n   - Excited atoms emit light primarily due to transitions triggered by various perturbations. These include spontaneous emission, influenced by quantum probabilities, and stimulated emission, initiated by incident photons or environmental interactions.\n\n\\[\n\\text{Final Answer}\n\\]\n\nAtoms stabilize in quantized energy levels post-quantum mechanics. Emitting light occurs when electrons transition downward, often due to external triggers or inherent probability, resolving earlier paradoxes of both stability and emission initiation.\n\n\\[\n\\boxed{\\text{Excited atoms emit light when electrons transition to lower energy states, typically due to external stimulation or spontaneous processes.}}\n\\]"}
{"id": "quantum/6034", "questions": "Consider a hypothetical neutron-neutron scattering at zero energy. The interaction potential is\n\n$$\nV(r) = \n\\begin{cases} \n\\sigma_1 \\cdot \\sigma_2 V_0, & r \\leq a, \\\\ \n0, & r > a, \n\\end{cases}\n$$\n\nwhere $\\sigma_1$ and $\\sigma_2$ are the Pauli spin matrices of the two neutrons. Compute the total scattering cross section. Both the incident and target neutrons are unpolarized.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about neutron-neutron scattering with a specific potential given. Hmm, let me try to unpack what's going on here.\n\nFirst off, both neutrons are unpolarized, which probably means that their spins aren't aligned or anything; they're just random. So when calculating the cross-sections, maybe we need to average over all possible initial states? Or perhaps sum over them?\n\nThe potential is given as \\( V(r) = \\sigma_1 \\cdot \\sigma_2 V_0 \\) for \\( r \\leq a \\), and 0 otherwise. Wait, but wait—Pauli spin matrices... Oh right! That dot product between σ₁ and σ₂ would be something like \\( \\vec{\\sigma}_1 \\cdot \\vec{\\sigma}_2 \\). Which expands into three terms: σ_x^1 σ_x^2 + σ_y^1 σ_y^2 + σ_z^1 σ_z^2. \n\nI remember from quantum mechanics that \\( \\vec{\\sigma}_1 \\cdot \\vec{\\sigma}_2 = 3 S_{total}^2 - 3/4 \\hbar^2 \\times ...\\) No, actually, more accurately, it relates to the singlet and triplet states because the eigenvalues depend on whether the spins are parallel (triplet state) or anti-parallel (singlet).\n\nRight, since each neutron has spin-½, combining gives either a singlet (S=0) or triplet (S=1) state. For the singlet, \\( S(S+1)\\hbar^2 = 0(0+1)\\hbar^2 = 0 \\), while for the triplet, it's 2ħ². But how does that relate to our potential?\n\nWait, no—the expression \\( \\vec{\\sigma}_1 \\cdot \\vec{\\sigma}_2 \\) can also be written using the identity:\n\n\\( \\vec{\\sigma}_1 \\cdot \\vec{\\sigma}_2 = 2 (\\hat{S}_{total})^2 / ħ^2 - 3 \\)\n\nBut hold on, if you consider that \\( (\\hat{S}_{total})^2 = S(S+1)\\hbar^2 \\), then substituting back,\n\n\\( \\vec{\\sigma}_1 \\cdot \\vec{\\sigma}_2 = 2 [S(S+1)] - 3 \\).\n\nSo for S=0 (singlet): 2[0] -3 = -3.\n\nFor S=1 (triplet): 2[2] -3 = 1.\n\nTherefore, the potential matrix element depends on whether the system is in a singlet or triplet state. Specifically, the potential becomes \\( V_0 (-3) \\) for singlets and \\( V_0 (1) \\) for triplets within radius 'a'.\n\nSince the particles are identical fermions (neutrons), their wavefunction must be antisymmetric under exchange. Therefore, the spatial part times the spin part should change sign upon swapping the two neutrons.\n\nIf the spin state is symmetric (like the triplet state), then the spatial part must be antisymmetric, meaning even angular momentum (since orbital angular momentum determines symmetry properties via parity). Conversely, if the spin state is antisymmetric (the singlet), the spatial part must be symmetric, implying odd angular momentum.\n\nHmm, okay, now considering low-energy scattering—in fact, zero energy. At threshold energies where kinetic energy approaches zero, only s-wave contributions matter because higher partial waves require some minimum energy due to centrifugal barriers.\n\nIn s-waves, l=0, so the angular momentum is zero. Now, for identical bosons, having integer spin allows symmetric wavefunctions, but neutrons being fermions complicate things.\n\nGiven that, for s-wave scattering, the spatial wavefunction is symmetric (because l=0 implies radial function without any angular dependence, hence symmetric under exchange). Thus, the spin part needs to be antisymmetric—if the overall wavefunction is to remain antisymmetric.\n\nWhich spin state is antisymmetric? Only the singlet state is antisymmetric under exchange. Because the singlet is |↑↓⟩ - |↓↑⟩ divided by sqrt(2), whereas the triplet includes combinations like |↑↑⟩, etc., which are symmetric.\n\nThus, in s-wave scattering, the spin state must be singlet. Hence, the effective potential experienced will correspond to the singlet case, i.e., \\( V(r) = -3 V_0 \\theta(a - r) \\).\n\nNow, computing the scattering cross-section usually involves solving the Schrödinger equation for this potential. Since it's a contact interaction (zero range?), except here it's finite range up to 'a'. However, sometimes people approximate such potentials as delta functions, especially when dealing with short-range interactions.\n\nAlternatively, treating it exactly might involve finding phase shifts δ_l for different angular momenta l. In particular, since we’re looking at s-waves (l=0), we'll focus on δ₀.\n\nAt zero energy, the phase shift simplifies calculations. Let’s recall that for a square well potential, the phase shift δ can be found by matching boundary conditions.\n\nLet me think about the setup. Inside the sphere of radius 'a', the potential is non-zero, outside it's free space. We want solutions inside and match them at r=a.\n\nAssuming E=0 (zero energy), the time-independent Schrodinger equation becomes:\n\nInside:\n\\[ \\left(-\\frac{\\hbar^2}{2m}\\nabla^2 + V(r)\\right) \\psi(\\mathbf{r}) = 0 \\]\n\nOutside:\n\\[ -\\frac{\\hbar^2}{2m}\\nabla^2 \\psi(\\mathbf{r}) = 0 \\]\n\nBecause E=0, the outgoing wave condition applies asymptotically, leading us to use regular spherical Bessel functions inside and Hankel functions (outgoing waves) outside.\n\nHowever, since E=0, the wavenumber k goes to zero. This complicates matters because standard expressions assume k≠0. Alternatively, taking limits as k→0.\n\nWait, another approach could be to compute the bound-state energies first, but since we're interested in scattering, not sure.\n\nActually, for zero-energy scattering, there's a special consideration called \"resonance\" behavior, but I'm getting ahead of myself.\n\nPerhaps better to write down the general form of the solution.\n\nInside the sphere (r ≤ a):\n\nWe look for regular solutions (finite at origin) of Helmholtz equation with modified potential. Given that E=0, the equation reduces to Laplace's equation plus the potential term.\n\nExpressed in terms of relative coordinates, assuming central potential, so everything is radially dependent.\n\nDefine u(r) = ψ(r)/ψ(∞), normalized appropriately.\n\nThen, the equation inside becomes:\n\n\\[ \\frac{d^2u}{dr^2} + \\frac{2}{r} \\frac{du}{dr} + \\kappa^2 u = 0 \\]\n\nWhere κ² = (2m|V|)/(ℏ²). Here, V is negative because it's attractive? Wait, earlier conclusion was that the potential is -3V₀ for singlet. If V₀ is positive, then yes, it's attractive.\n\nHence, κ² = (2m * 3 V₀ )/( ℏ² ). Assuming units where necessary constants are absorbed.\n\nSimilarly, outside the sphere, the equation is:\n\n\\[ \\frac{d^2f}{dr^2} + \\frac{2}{r} \\frac{df}{dr} = 0 \\]\n\nWith f ~ ln(r) as r increases, but physically, we expect an outgoing wave. However, at zero energy, the usual plane wave e^{ikr}/kr tends towards a constant, but perhaps logarithmic corrections come into play.\n\nThis seems tricky—I may need to refer to known results for zero-energy scattering.\n\nWait, alternatively, recalling that for a hard-sphere potential (impenetrable core), the phase shift at zero energy leads to certain behaviors, but here the potential isn't purely repulsive—it's spin-dependent.\n\nAnother thought: Maybe express the Green's function for this potential and calculate the T-matrix, then find the cross-section.\n\nBut perhaps stepping back, the key point is that the potential affects the s-wave component differently depending on the spin state. Since the incoming neutrons are unpolarized, we have equal probability amplitudes for singlet and triplet states initially.\n\nWait, but earlier reasoning suggested that in s-wave scattering, only the singlet contributes because the spatial part is symmetric, requiring the spin part to be antisymmetric. Is that correct?\n\nYes, because for identical fermions, the combined wavefunction must be antisymmetric. Spatially symmetric (even l?) requires spin antisymmetric (singlet); spatially antisymmetric (odd l) requires spin symmetric (triplet). But for s-waves, l=0 is even, thus spin must be singlet.\n\nTherefore, in s-wave scattering, the relevant potential is the one corresponding"}
{"id": "quantum/2-2004", "questions": "An electron is confined to the interior of a hollow spherical cavity of radius $R$ with impenetrable walls. Find an expression for the pressure exerted on the walls of the cavity by the electron in its ground state.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem where an electron is confined inside a hollow spherical cavity with impenetrable walls, meaning it can't go through them. The task is to find the pressure exerted on the walls when the electron is in its ground state.\n\nHmm, first off, what do I know about electrons in such confinement? It reminds me of particles in boxes or wells—like the infinite potential well problems we did earlier. In those cases, the particle has quantized energy levels because it's restricted in space. So maybe something similar applies here but in three dimensions since it's a sphere instead of a box.\n\nWait, right! For a 3D box, each dimension contributes separately to the total energy due to the separation of variables method. But a sphere might be more complicated because of the symmetry involved. Maybe there’s a way to use spherical coordinates?\n\nI remember that in quantum mechanics, the wavefunction must satisfy certain boundary conditions. Since the walls are impenetrable, the probability density should drop sharply to zero beyond R. That means the radial part of the wavefunction must vanish at r = R. \n\nSo perhaps I need to solve the Schrödinger equation for a spherically symmetric potential well. Let me recall: the time-independent Schrödinger equation in spherical coordinates involves terms for the radial function and angular parts (spherical harmonics). Because the potential depends only on r, not angles, the angular dependence will just give us the usual Y_l^m(θ, φ) solutions.\n\nBut wait, if the electron is in the ground state, which would correspond to the lowest possible energy level. Ground states usually mean minimal nodes—I think n=1 in some sense—but how does that translate into a spherical system?\n\nIn one-dimensional infinite square wells, the ground state corresponds to the longest wavelength fitting into the well. Extending that idea, in higher dimensions, especially spherical, the concept becomes trickier. Instead of different directions contributing independently as in a cubical box, everything is tied together radially.\n\nMaybe I can model this after the hydrogen atom, except without any Coulomb potential—it's purely kinetic energy within the sphere. Wait, no—the hydrogen atom uses both kinetic and potential energies, but here the potential outside the sphere is infinitely high, making it like a \"particle in a sphere\" scenario.\n\nLet me try writing down the relevant equations. The Hamiltonian for the electron in this case would be:\n\nHψ(r) = [ -ħ²/(2m) ∇² ] ψ(r)\n\nBecause V(r)=0 inside the sphere and infinity outside. So solving Hψ = Eψ gives the eigenvalues and eigenfunctions.\n\nThe Laplacian in spherical coordinates is:\n\n∇² = (1/r²)(∂/∂r)(r² ∂/∂r) + (1/(r² sin θ))(∂/∂θ)(sin θ ∂/∂φ)) + ... etc.,\n\nbut since the potential doesn’t depend on angles, the angular parts separate out, leading to spherical Bessel functions for the radial component.\n\nRight, so the radial equation reduces to:\n\n[ d²/dr² + (2/r)d/dr + (k² - l(l+1)/r² ) ] f(r) = 0,\n\nwhere k² = 2mE / ħ², and l is the orbital angular momentum quantum number.\n\nSince we're dealing with the ground state, probably l=0 because lower angular momentum leads to lower energy. Also, node structure—if l=0, then the radial wavefunction shouldn't have any zeros apart from possibly at the origin and the wall.\n\nTherefore, assuming l=0 simplifies things. Then our radial equation becomes:\n\nf''(r) + (2/r)f'(r) + k² f(r) = 0.\n\nThis is actually the equation for spherical Bessel functions of order ν=l=0. Spherical Bessel functions j_ν(x), specifically j₀(kr).\n\nBoundary condition: At r=R, the wavefunction must be zero because the potential is infinite beyond that point. Therefore, f(R) = 0.\n\nFor the ground state, the smallest non-zero value of kr occurs when j₀(kR) = 0. What's the first root of j₀(x)? From my notes, the roots of j_n(x) occur at specific points; for j₀(x), the first root is around x ≈ π (~3.1416). Is that correct? Yes, because j₀(x) behaves similarly to sine functions scaled appropriately.\n\nThus, setting kR equal to the first root of j₀, say α₁⁰≈π. Hence, k = α₁⁰/R ≈ π/R.\n\nThen, the energy E is given by:\n\nE = ħ² k² / (2m) = ħ² π² / (2 m R² )\n\nThat seems familiar—a sort of dimensional analysis check: units-wise, ħ squared over mass times length squared gives energy, yes.\n\nNow, moving on to finding the pressure. Pressure relates force per unit area. How does the electron exert pressure on the walls? Well, in quantum systems, sometimes you consider expectation values of operators related to forces.\n\nAlternatively, another approach could involve considering the uncertainty principle. If the electron is confined to a region of size ~R, its momentum is uncertain by Δp ~ ħ/R. Then, using p²/(2m) for kinetic energy, giving E ~ ħ²/(2m R² ), which matches our previous result.\n\nPressure P is defined as F/A, where A is the surface area. Force F is rate of change of momentum, but in equilibrium situations, perhaps relating via stress-energy tensor isn't straightforward. Alternatively, thinking thermodynamically, pressure can relate to the average force per unit area imparted by the particle bouncing off the walls.\n\nAnother thought: Using virial theorem. Virial theorem says that for bound systems, the average kinetic energy equals half the negative of the average potential energy. However, in this case, the potential is zero inside and infinite outside, so maybe not directly applicable.\n\nWait, let's get back. Perhaps treating the electron as providing a kind of radiation pressure analogy. Or, better yet, compute the expectation value of the mechanical stress on the walls.\n\nStress σ is proportional to |ψ|² multiplied by factors involving derivatives of the potential. Hmm, though the potential is discontinuous here, being zero inside and infinite outside. Not sure if that helps.\n\nAlternatively, recalling that in electromagnetism, the Maxwell stress tensor describes stresses due to electromagnetic fields. Similarly, in quantum mechanics, perhaps the stress tensor exists too, allowing calculation of pressures based on the wavefunction.\n\nYes, indeed, the general expression for the quantum mechanical stress tensor T_ij includes terms like ⟨ψ| (-iħ (∇_i ψ*) ψ_j + ...) |ψ⟩, but I'm getting vague here.\n\nPerhaps a simpler route is needed. Considering that the pressure arises from the gradient of the wavefunction near the boundaries.\n\nLooking up, I seem to recall that for a particle in a box, the pressure can be found by evaluating the derivative of the energy with respect to volume. Specifically, pressure P = -(dE/dV)_T,S... but in this case, temperature is presumably absolute zero since it's the ground state, so entropy considerations may not apply.\n\nWait, hold on—that formula comes from statistical mechanics, where pressure is related to changes in internal energy with volume. Here, we’re talking about single-particle quantum mechanics, so maybe analogous reasoning still holds.\n\nIf I treat the system as having energy E depending on R, then the pressure would be the negative derivative of E with respect to volume divided by appropriate constants.\n\nVolume V of the sphere is (4/3)π R³. Thus, dV/dR = 4π R².\n\nExpressing E in terms of R: E = ħ² π² / (2 m R² ). To express E in terms of V, note that R = [(3V)/(4π)]^(1/3). Substituting:\n\nE(V) = ħ² π² / [2 m ((3V)/(4π))^(2/3)]\n\nSimplify exponents:\n\nFirst, write denominator term:\n\n((3V)/(4π))^(2/3) = (3/(4π))^(2/3) * V^(2/3)\n\nThus,\n\nE(V) = ħ² π² / [2 m (3/(4π))^(2/3) V^(2/3) ]\n\nCompute constant factor:\n\n(3/(4π))^(2/3) = (3)^{2/3} / (4π)^{2/3}\n\nAnd numerator: π² remains.\n\nPutting all together:\n\nE(V) = ħ² π² / [2 m (3^{2/3}/(4π)^{2/3}) V^{2/3}] ]\n       = ħ² π² (4π)^{-2/3} 3^{-2/3} / (2 m) * V^{-2/3}\n       \nCombine powers of pi:\n\npi² * pi^{-4/3} = pi^{2 - 4/3} = pi^{2/3}\n\nSimilarly, 3^{-2/3}, so overall:\n\nE(V) = ħ² (pi^{2/3}) (3^{-2/3"}
{"id": "quantum/2-2003", "questions": "A $\\text{NaCl}$ crystal has some negative ion vacancies, each containing one electron. Treat these electrons as moving freely inside a volume whose dimensions are on the order of the lattice constant. The crystal is at room temperature.Give a numerical estimate for the longest wavelength of electromagnetic radiation absorbed strongly by these electrons.\n\n", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about NaCl crystals with negative ion vacancies that contain electrons. These electrons can move freely within a certain volume, which is roughly the size of the lattice constant. The question asks me to find the longest wavelength of electromagnetic radiation that's strongly absorbed by these electrons when the crystal is at room temperature.\n\nHmm, let me think. First off, I remember from my solid-state physics class that free electrons in a material can absorb photons if their energy matches the required transition between states. But wait, since we're dealing with a small volume here—on the scale of the lattice constant—the quantum confinement effects might come into play. That probably means I need to consider particle-in-a-box models or something similar.\n\nSo, maybe I should model each electron as being confined in a three-dimensional box where each dimension is approximately equal to the lattice constant of NaCl. What's the typical lattice constant for NaCl? If I recall correctly, it's around 5 angstroms (Å), but I'm not entirely sure. Let me double-check: yes, sodium chloride has a face-centered cubic structure with a lattice parameter of about 5.64 Å. So, taking that as our length L ≈ 5.64 × 10^-10 meters.\n\nNow, treating the electron as a particle in a cube-shaped box with sides of length L. In such a case, the allowed energy levels are quantized according to the formula:\n\nE_n = (n² h²) / (8 m_e L²)\n\nwhere n is a set of integers (n_x, n_y, n_z), h is Planck's constant, m_e is the mass of the electron, and L is the width of the box.\n\nThe lowest energy state would be when all components of n are 1, giving E_1. Then, transitions occur when the electron absorbs a photon and moves to higher energy states, say from E_i to E_f, where i < f. The energy difference ΔE corresponds to the photon energy needed for absorption.\n\nSince they’re asking for the longest wavelength, that implies the smallest energy gap because longer wavelengths correspond to lower energies. Wait, no—that’s actually incorrect. Longer wavelengths mean lower frequencies, hence less energy per photon. However, the maximum possible wavelength occurs at the minimum energy transition. Because the spacing between energy levels increases with larger n, the largest possible wavelength comes from the first excited state above the ground state.\n\nWait, hold on—I need to clarify. For the longest wavelength, you want the smallest energy difference because λ_max corresponds to the minimal ΔE. Since the energy gaps increase with higher n, the smallest non-zero ΔE will give us the longest wavelength.\n\nBut what exactly defines the \"longest\" wavelength that gets absorbed? It must relate to the fundamental frequency corresponding to the energy level spacing. Alternatively, perhaps considering thermal excitations—if the system is at room temperature, then only those transitions with energy comparable to kT may contribute significantly due to Boltzmann factors. Hmm, but the question says \"strongly absorbed,\" implying resonance conditions rather than just thermal excitation probabilities.\n\nAlternatively, another approach could involve plasmons or collective oscillations of electrons in metals, but given that we're talking about individual electrons in a vacancy site, maybe that's more like localized states rather than delocalized ones. So sticking with the particle-in-a-box idea seems better.\n\nLet me outline the steps again:\n1. Model the electron as a particle in a 3D box.\n2. Calculate the energy levels using E_n = (h² n²)/(8m_e L²).\n3. Find the energy difference between adjacent levels (smallest ΔE).\n4. Convert ΔE to photon wavelength via E=hc/λ => λ= hc/(ΔE).\n\nFirst, compute the energy levels. Taking n as the principal quantum number vector (nx, ny, nz). Each component starts at 1. The next available energy after the ground state (all nx=ny=nz=1) would be when any one of them increments by 1. So, the first excited state would have two possibilities: either (2,1,1), (1,2,1), or (1,1,2)—they all result in the same energy.\n\nCalculating E_ground = (1^2 + 1^2 + 1^2)* (h²)/(8m_e L²)\nSimilarly, E_first_excited = [(2)^2 +1+1]*(same factor)/something?\n\nWait, no. Actually, in 3D, the total energy isn't simply additive unless the potential allows separable solutions. Oh right, in a cubical box, the energy depends on the sum of squares of the quantum numbers along each axis. Therefore,\n\nE(n_x, n_y, n_z) = [ (n_x² + n_y² + n_z² ) * h² ] / (8 m_e L² )\n\nTherefore, the ground state is (1,1,1): E_min = (3)(h²)/(8 m_e L² )\nNext, the first excited state(s) would be permutations where one quantum number is increased by 1, leading to sums like 1² +1² +2² = 6. Hence, E_next = (6)(h²)/(8 m_e L² )\n\nThus, the energy difference ΔE = E_next - E_min = (6-3)*(h²)/(8 m_e L² ) = 3*(h²)/(8 m_e L² )\n\nPlugging in the constants:\n\nPlanck's constant h = 6.626 x 10^-34 J·s\n\nElectron mass m_e = 9.109 x 10^-31 kg\n\nLattice constant L = ~5.64e-10 m\n\nCompute numerator: h squared times 3 over denominator 8 m_e L squared.\n\nCalculate h²: (6.626e-34)^2 ≈ 4.39e-67 J² s²\n\nMultiply by 3: 1.317e-66\n\nDenominator: 8*m_e*L² = 8*9.109e-31*(5.64e-10)^2\n\nCompute L squared: (5.64e-10)^2 ≈ 3.18e-19 m²\n\nThen multiply by m_e: 9.109e-31 * 3.18e-19 ≈ 2.89e-50\n\nMultiply by 8: approx 2.31e-49\n\nHence, ΔE = (1.317e-66) / (2.31e-49) ≈ 5.7e-18 Joules\n\nConvert this energy to wavelength using lambda = hc/E\n\nc = speed of light ≈ 3e8 m/s\n\nh*c ≈ 6.626e-34 * 3e8 ≈ 1.9878e-25 J·m\n\nDivide by delta_E: 1.9878e-25 / 5.7e-18 ≈ 3.48e-8 meters, which is 34.8 nanometers.\n\nThat's ultraviolet range. Is that correct? Seems plausible, but let me check calculations again.\n\nWait, let me recompute the denominator part carefully.\n\nDenominator D = 8 m_e L²\n\nGiven m_e = 9.109e-31 kg\n\nL = 5.64e-10 m → L² = (5.64e-10)^2 = 3.18e-19 m²\n\nSo D = 8 * 9.109e-31 * 3.18e-19\n\nCompute 8 * 9.109e-31 = 7.287e-30\n\nMultiply by 3.18e-19: 7.287e-30 * 3.18e-19 ≈ 2.31e-48 ?\n\nWait, earlier I had 2.31e-49; was there a decimal error?\n\nYes! 7.287e-30 multiplied by 3.18e-19 gives exponents adding up (-30)+(-19)= -49, so 7.287 *3.18≈23.1, thus 2.31e-48? No, wait, 7.287e-30 *3.18e-19 = (7.287 *3.18)e-(30+19) = 23.1e-49 = 2.31e-48? Wait, no, exponent addition is wrong.\n\nNo, 10^{-a} *10^{-b}=10^{-(a+b)}. Here, 10^{-30}*10^{-19}=10^{-"}
{"id": "quantum/6005", "questions": "(a) Find the s-wave phase shift, as a function of wave number $k$, for a spherically symmetric potential which is infinitely repulsive inside of a radius $r_0$, and vanishes outside of $r_0$.  \n\n(b) For $k \\rightarrow 0$ discuss the behavior of the phase shifts in the higher partial waves. ", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about finding the s-wave phase shift for a specific potential and then discussing the behavior of higher partial waves when k approaches zero. Let me try to break it down.\n\nStarting with part (a). The potential is spherically symmetric, infinitely repulsive inside r₀, and zero outside. So that means for r < r₀, V(r) = ∞, making it impenetrable like a hard sphere. And for r ≥ r₀, V(r)=0, meaning free space there.\n\nSince we're dealing with scattering states, especially looking for the s-wave phase shift, let's recall some quantum mechanics concepts. In scattering theory, particularly for central potentials, each angular momentum state l has its own phase shift δ_l(k).\n\nFor s-waves, l=0, so we need to find δ_0(k). When you have such a potential where the particle can't enter beyond r=r₀ because it's infinitely repulsive, the effective potential becomes something akin to a boundary condition on the radial wavefunction.\n\nIn regions:\n\n1. Inside r ≤ r₀: Since V is infinite, no penetration occurs here; the wavefunction must be zero.\n2. Outside r > r₀: Free particle solutions apply, but adjusted due to the boundary conditions imposed by the inner region.\n\nSo, the wavefunctions will consist of outgoing spherical Bessel functions multiplied by appropriate coefficients. But since the potential is purely repulsive and doesn’t allow any reflection except maybe from the discontinuity at r=r₀, perhaps only transmission happens? Wait, actually, if the potential is infinitely high, particles cannot penetrate into the interior, so all incident particles would just scatter off the surface without entering. That might mean that the phase shift could relate to how much they get reflected or transmitted?\n\nWait, more accurately, in scattering problems, even though classically particles wouldn't go through, quantum mechanically, depending on energy, tunneling isn't possible here because the barrier is infinitely thick and tall. Therefore, the wavefunction should satisfy certain boundary conditions at r=r₀.\n\nLet’s think about the form of the wavefunction. For r > r₀, the time-independent Schrödinger equation reduces to the Helmholtz equation, leading to regular (ingoing) and irregular (outgoing) spherical Hankel functions. However, since our setup is a pure scattering scenario with incoming plane waves, the solution should include both ingoing and outgoing components.\n\nBut wait, given that the potential is infinitely repelling within r₀, does that imply that the wavefunction must vanish at r=r₀? Or do we impose another kind of boundary condition?\n\nHmm, not exactly. Because while the potential is infinite inside, the wavefunction itself goes smoothly across boundaries unless there's a delta function potential or similar. Here, the change is abrupt—so probably the derivative may jump, but the value remains continuous.\n\nActually, considering that the potential jumps abruptly from infinity to zero at r=r₀, the wavefunction ψ(r) must still be finite everywhere. At r approaching r₀ from below, ψ tends to zero because V is infinite. From above, it starts building up again. Thus, continuity requires that ψ(r₀⁻) = ψ(r₀⁺), but since ψ(r₀⁻)=0, therefore ψ(r₀⁺)=0 too.\n\nTherefore, the radial wavefunction u_l(r) [which is proportional to rψ(r)] evaluated at r=r₀ must also be zero. This gives us a boundary condition.\n\nGiven that, for l=0, the radial wavefunction outside r₀ is expressed using spherical Bessel functions. Specifically, the general solution for u_0(r) when E>0 is:\n\nu_0(r) = A sin(kr + δ_0)\n\nWhere A is a normalization constant, and δ_0 is the phase shift we want to determine.\n\nAt r=r₀, applying the boundary condition u_0(r₀) = 0:\n\nA sin(k r₀ + δ_0) = 0\n\nAssuming A ≠ 0 (otherwise trivial solution), we require:\n\nsin(k r₀ + δ_0) = 0 ⇒ k r₀ + δ_0 = n π, where n is integer.\n\nHowever, in scattering theory, the phase shift δ_l is typically defined modulo π, so different choices of n correspond to adding multiples of π, which don't affect physical quantities like cross-sections.\n\nTo fix δ_0 uniquely, we usually take the principal branch, i.e., set n=0:\n\nδ_0 = -k r₀ mod π\n\nAlternatively, sometimes people write δ_0 = π/2 - kr₀, but I need to check the exact expression based on conventions.\n\nWait, hold on. Another approach is to consider the logarithmic derivative method. The idea is that the phase shift relates to the difference between the actual solution and the reference solution (free particle case).\n\nThe standard procedure involves matching the logarithmic derivatives of the wavefunction at r=r₀. For the free particle, the solution is a combination of sine and cosine terms, whereas for the modified system, the presence of the potential alters these.\n\nSpecifically, the Jost function formalism tells us that the phase shift satisfies tan(δ_l) = Re / Im, where Re and Im come from the ratio involving the regular solution and the irregular solution.\n\nBut perhaps stepping back, for the simplest case of l=0, the regular solution f_0(r,k) behaves asymptotically as e^{i(kr - π/2)} / (ik) near origin, and the irregular solution h_0^{(+)}(kr) ~ e^{-ikr} / (ik) as r→∞.\n\nWhen solving for the bound states, one sets the regular solution equal to the irregular solution times a coefficient determined by the boundary condition. Similarly, for scattering, the same applies.\n\nThus, writing the solution as:\n\nf_0(r,k) = C h_0^{(+)}(kr)\n\nAnd imposing the boundary condition at r=r₀, say f_0(r₀,k) = 0.\n\nBut wait, f_0(r,k) being the regular solution, which is non-zero inside, but in our case, the potential makes the wavefunction zero inside. Hmm, getting confused now.\n\nMaybe better to use the fact that the wavefunction outside is a linear combination of incoming and outgoing waves, but due to the impenetrability, the amplitude of the incoming component is related to the outgoing via the phase shift.\n\nAnother way: Consider the Green's function approach or simply match the wavefunction and its derivative at r=r₀.\n\nInside r<r₀, the wavefunction is zero. Just outside, it begins to rise. To model this, suppose the wavefunction for r>r₀ is:\n\nψ_out(r) = (e^{ikr}/r) * [something]\n\nBut more precisely, the total wavefunction includes both incoming and outgoing parts. Alternatively, thinking in terms of partial waves, the scattered field consists of outgoing spherical harmonics.\n\nPerhaps instead, focusing on the boundary condition at r=r₀. If the wavefunction must be continuous, and the derivative experiences a jump due to the potential.\n\nWait, yes! There's a sudden approximation concept here. The discontinuity in the potential leads to a discontinuity in the first derivative of the wavefunction.\n\nRecall that for a sharp potential change, the derivative of the wavefunction changes by a factor dependent on the strength of the potential. More formally,\n\n[ dψ/dr ]_{above} - [ dψ/dr ]_{below} = (V_above - V_below)/(ħ²/(2m)) ) ψ(r₀)\n\nHere, V_above = 0, V_below = ∞. Uh-oh, division by zero? Not helpful directly.\n\nInstead, perhaps treating the limit as V_below → ∞. Then, the wavefunction inside must tend to zero faster than exponentially, hence effectively ψ(r₀⁻)=0, and ψ'(r₀⁻) tending to negative infinity? Maybe not straightforward.\n\nAlternative thought process: Imagine replacing the infinite potential with a very large but finite potential, solve approximately, then take the limit as the potential height increases.\n\nSuppose V(r) = V_0 for r < r₀, else zero, with V_0 >> ħ² k²/(2m). Solving the Schrodinger equation numerically or analytically in two regions, then taking V_0 → ∞.\n\nAs V_0 grows, the solution inside r < r₀ decays rapidly towards zero. Hence, the wavefunction at r=r₀ is nearly zero, and the slope is dominated by exponential decay.\n\nTaking V_0 → ∞, the wavefunction inside becomes identically zero, and the exterior solution must start rising sharply from zero at r=r₀.\n\nThis suggests that the derivative of the wavefunction right after r=r₀ is positive and significant, corresponding to a \"jump\" upwards.\n\nMathematically, integrating the Schrödinger equation over a small interval around r=r₀, assuming V(r) ≫ other terms, yields a relation between the left and right derivatives.\n\nFormally, integrate from r₀ - ε to r₀ + ε:\n\n∫ [ - (ħ²/(2m)) (d²ψ/dr²) + V(r) ψ"}
{"id": "Quantum Mechanics/23-3", "questions": "In 1921, Ramsauer noticed that for some particular values of the incident energy, rare gases such as helium, argon or neon were transparent to low-energy electron beams. This can be explained in the following one-dimensional model. Consider a stationary solution of the Schrödinger equation of positive energy $E$, for a particle of mass $m$ in the following one-dimensional potential ($V_0 > 0$):\n\n$$\nV(x) = \\begin{cases} \n0, & |x| > a, \\\\\n-V_0, & |x| \\leq a.\n\\end{cases}\n$$\n\n\n(23.3.1)\n\n(a) Calculate the transmission and reflection probabilities, $T$ and $R$, for leftward scattering. Verify that $R + T = 1$.\n\n(b) Show that $T = 1$ for some values of the energy. Interpret this result and the Ramsauer effect.\n\n(c) In helium, the lowest energy at which the phenomenon occurs is $E = 0.7 \\text{ eV}$. Assuming that the radius of the atom is $a = 0.1 \\text{ nm}$, calculate the depth $V_0$ of the potential well inside the atom in this model.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about quantum mechanics related to the Ramsauer effect. It's part (a), where I need to find the transmission and reflection probabilities for electrons hitting a certain potential barrier. Hmm, let me think through how to approach this.\n\nFirst, the setup: we're dealing with a one-dimensional potential well. The potential V(x) is zero outside the region from -a to a, and it's -V₀ inside that interval. So, it's like a finite square well but only on both sides? Wait no, actually, since it's negative inside, it's more like a potential dip—a well—of width 2a and depth V₀.\n\nThe question specifies \"leftward scattering,\" meaning particles coming from the right side moving towards the left into the potential well. But wait, usually when people talk about scattering, they consider incoming waves from either direction. Since it says leftward scattering, maybe all the wave comes from the right?\n\nBut regardless, whether it's left or right doesn't change much because of symmetry; perhaps just the coefficients will differ based on direction. Anyway, focusing on calculating transmission and reflection probabilities.\n\nSo, given E is positive, which means the kinetic energy is above the bottom of the well. Because if you look at the potential, inside the well it's lower than outside—it’s deeper. So, the particle has enough energy to pass through without being reflected entirely, unless there's resonance conditions leading to perfect transmission.\n\nWait, okay, so first things first—I should probably write down the time-independent Schrodinger equation for each region.\n\nLet me denote regions:\n\nRegion I: x < -a\n\nHere, V=0, so the SE becomes (-ħ²/(2m))ψ'' = E ψ => ψ'' = -(2mE)/ħ² ψ. Let k₁² = (2mE)/(ħ²). Then solutions are plane waves: ψ_I = A exp(i k₁ x) + B exp(-i k₁ x).\n\nSimilarly, Region II: |x| ≤ a\n\nPotential here is -V₀, so the effective potential is lowered. Therefore, the SE becomes [-(ħ²/2m) d²/dx² - V₀]ψ = E ψ ⇒ ψ'' = [(2m(V₀ + E))/ħ²] ψ. Let k₂² = (2m(V₀ + E))/(ħ²). Solutions would then involve exponential functions if E < V₀, but since E is positive, depending on whether E + V₀ is greater than something... Wait, hold on—if E is less than V₀, then V₀ + E could still be positive, making k real again. Or does it depend on relative signs?\n\nWait, sorry, correct expression: For Region II,\n\n- (ħ² / 2m ) ψ'' - V₀ ψ = E ψ ⇒ rearranged gives ψ'' = (2m(E + V₀)/ ħ²) ψ.\n\nSince E is positive and V₀ is also positive, their sum is definitely positive, hence k₂ is real. Thus, within the well, the solution is oscillatory too, not exponentially decaying. That makes sense because the particle isn’t evanescent in the well—it propagates through.\n\nTherefore, in Region II, the general solution is C exp(i k₂ x) + D exp(-i k₂ x).\n\nThen, Region III: x > a\n\nAgain, same as Region I, V=0, so similar form: ψ_III = F exp(i k₁ x) + G exp(-i k₁ x).\n\nNow, considering the boundary conditions. We’re looking for stationary states, so the wavefunction must be continuous everywhere, including its derivative except possibly at points where delta potentials exist—but our potential is smooth except at ±a.\n\nGiven that, continuity applies at x=-a and x=a.\n\nAlso, assuming incidence from the right (since it's leftward scattering?), so the wave is coming from the right (positive x-direction?) No, wait, leftward scattering implies the particle is going from right to left. So, in terms of standard notation, the incident wave is traveling to the left, i.e., from the right side approaching the potential.\n\nHence, in Region III (x>a): the incident wave is coming from the right, so it's moving to the left. Hence, the wave function might consist of an incident term plus transmitted/reflected terms? Wait, confusion arises.\n\nAlternatively, sometimes problems set up the incident wave as coming from the left, but here it's specified as leftward scattering, implying the opposite.\n\nPerhaps better to define the directions properly.\n\nIf the particle is scattered leftwards, then the incident wave is coming from the right, heading toward the left. So, in Region III (rightmost), the wave consists of the incident wave moving left, and any reflected wave back to the right. Similarly, in Region I (leftmost), the transmitted wave moves further left beyond the potential.\n\nThus, setting up the amplitudes accordingly.\n\nFor clarity, let me assign:\n\nIn Region III (x > a): the incident wave is moving left, so proportional to exp(+ik₁ x); however, typically, a wave moving to the left is written as exp(+ikx). Alternatively, depends on convention.\n\nWait, yes, generally, a wave exp[i(kx - ωt)] moves to the right, while exp[-i(kx + ωt)] moves to the left. So, in spatial parts, a left-moving wave is exp(+ikx). So, yeah, in Region III, the incident wave is exp(+ik₁ x), and after reflecting off the potential, another component may go back to the right, which would be exp(-ik₁ x). However, since we're talking about leftward scattering, perhaps the reflection coefficient R corresponds to the probability of the wave bouncing back to the right, whereas the transmission coefficient T is the fraction passing through to the left.\n\nHmm, getting confused now. Maybe it's easier to use conventional definitions despite the wording.\n\nAlternatively, perhaps treating it symmetrically, using the usual method for finding reflection and transmission coefficients across a potential barrier.\n\nI remember that for a potential barrier, say, starting at x=0 extending to infinity, the reflection and transmission coefficients are calculated based on matching the wavefunctions at x=0.\n\nBut in this case, the potential extends between -a and a, so two interfaces at x=-a and x=a.\n\nThis complicates matters because the wave passes through two barriers instead of one. So, the calculation involves solving the system over three regions with four boundaries (at x=-a and x=a twice each). \n\nTo handle this, I'll proceed step-by-step.\n\nDefine Regions:\nI: x < -a,\nII: -a <= x <= a,\nIII: x > a.\n\nWavefunctions:\n\nRegion I: Left of the well. Here, the wave is composed of the transmitted wave moving left (from the perspective of someone far away on the left). So, ψ_I = t exp(+i k₁ x), where 't' is the transmission amplitude.\n\nRegion II: Inside the well. Solution is combination of forward and backward waves: ψ_II = C exp(+i k₂ x) + D exp(-i k₂ x).\n\nRegion III: Right of the well. Wave includes the incident wave moving left (so exp(+i k₁ x)), and the reflected wave moving right (exp(-i k₁ x)). So, ψ_III = exp(+i k₁ x) + r exp(-i k₁ x), where 'r' is the reflection amplitude.\n\nWait, hang on—that seems inconsistent with earlier thought. If the incident wave is coming from the right, moving left, then in Region III, before entering the well, it hasn't been affected yet, so it's purely the incident wave. After interacting with the well, it reflects some portion and transmits some.\n\nActually, perhaps my initial assignment was wrong. To clarify, suppose the particle is incident from the right (moving left). Before reaching the well (in Region III), the wave is just the incident wave moving left: ψ_III_incident = exp(+i k₁ x). Upon encountering the well, part of it reflects back to the right (which would correspond to exp(-i k₁ x)), and part transmits"}
{"id": "Quantum Mechanics/29-4", "questions": "*Consider a two-dimensional isotropic harmonic oscillator of frequency $\\omega = \\sqrt{k/m}$ with a perturbing potential $\\delta V(x, y) = bxy, \\, b \\ll k$, where $b$ has time dependence $b=b_0 e^{-t / \\tau}$. Find to lowest order in $b_0$, an expression for the probability that a state that is in the ground state at $t = 0$ will be found in an excited state at $t = +\\infty$.*", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about a two-dimensional isotropic harmonic oscillator with a perturbation. The system starts in its ground state at t=0, and we need to find the probability it transitions to an excited state as t approaches infinity. Hmm.\n\nFirst, let me recall what an isotropic harmonic oscillator is. It's one where the spring constants in both x and y directions are equal, right? So the potential is (1/2)mω²x² + (1/2)mω²y². And since it's two-dimensional, the energy levels are quantized into states labeled by quantum numbers n_x and n_y. Each level corresponds to E_{n_x,n_y} = ħω(n_x + n_y + 1). For the ground state, both n_x and n_y would be zero, giving E_ground = ħω(0+0+1)=ħω.\n\nNow, there’s a perturbing potential δV(x,y) = b xy, which depends on time because b itself varies as b₀e^(-t/τ), and τ must be some characteristic timescale. Since b is small compared to k (because b ≪k), we can treat this as a weak perturbation. That suggests using time-dependent perturbation theory.\n\nIn time-dependent perturbation theory, the transition probability from an initial state |i> to a final state |f> after a long time is given approximately by Fermi's golden rule or more precisely via first-order approximation when dealing with sudden perturbations. But here, our perturbation isn't just turned on suddenly; it decays exponentially over time. So maybe I should use the general formula for transition probabilities under such conditions.\n\nThe standard approach involves calculating the matrix element ⟨f|δV|i⟩ multiplied by an integral involving the exponential decay factor. Let me write down the key equations.\n\nSo, the probability P_{fi}(t) up to first order is:\n\nP_{fi}(t) ≈ |⟨f| U(t,t') |i⟩|^2,\n\nwhere U is the time evolution operator due to the perturbation. In interaction picture, this becomes something like integrating exp(i ω_{fi} t) * ⟨f|δV|i⟩ dt', but adjusted for the specific form of δV.\n\nWait, actually, the exact expression is:\n\nc_f^(1)(t) = (-i/ℏ) ∫_{-∞}^{t} ⟨f|δV(t')|i⟩ e^{i ω_{fi} t'} dt',\n\nwhere ω_{fi}=(E_f - E_i)/ℏ.\n\nBut wait, no—actually, if we're starting at t=-infinity, but in our case, the perturbation turns on smoothly. Wait, hold on: Our perturbation only exists for t ≥0, perhaps?\n\nNo, looking back, the question says \"at t=0\" the state is the ground state, and then applies the perturbation. So probably, the integration runs from t'=0 to t'→∞, considering that the perturbation is applied adiabatically before t=0? Or does it start abruptly at t=0?\n\nHmm, the way the problem is phrased, the perturbation is present for all t≥0, with b being non-zero only for positive t. Because initially, at t=0, the state is the ground state without any perturbation. Then, the perturbation kicks in. Therefore, the time evolution begins at t=0, meaning c_f^(1)(t) is calculated from t’=0 onwards.\n\nTherefore, the coefficient c_f^(1)(t) is:\n\nc_f^(1)(t) = (-i/ℏ) ∫_{0}^{t} ⟨f|δV(t')|i⟩ e^{i ω_{fi} t'} dt'.\n\nSince we’re interested in the limit as t → ∞, we'll take the upper limit to infinity.\n\nGiven that δV(x,y) = b(t’) xy, and b(t’) = b₀ e^{-t'/τ}, substituting gives us:\n\n⟨f|δV(t')|i⟩ = b₀ e^{-t'/τ} ⟨f|x y|i⟩\n\nThus, plugging into the equation for c_f^(1):\n\nc_f^(1)(∞) = (-i ℏ)^{-1} b₀ ⟨f|x y|i⟩ ∫_{0}^{∞} e^{-t'/τ} e^{i ω_{fi} t'} dt'\n\nSimplify the integral:\n\n∫_{0}^{∞} e^{-(1/τ - i ω_{fi}) t'} dt' = [ -(1/(1/τ - i ω_{fi})) ) ] evaluated from 0 to ∞.\n\nWhich simplifies to 1/(1/τ - i ω_{fi}), assuming Re[1/τ] >0, which it is because τ is real and positive.\n\nHence, putting it together:\n\nc_f^(1)(∞) = (-i/ℏ) b₀ ⟨f|x y|i⟩ [1/(1/τ - i ω_{fi})]\n\nThen, the transition probability is |c_f^(1)|²:\n\nP_{fi} = |c_f^(1)|² = (b₀² / ℏ²) |⟨f|x y|i⟩|² [1/( (1/τ)^2 + ω_{fi}^2 ) ]\n\nBecause modulus squared of denominator term is (1/τ)^2 + (ω_{fi})^2.\n\nAlright, now, moving forward, we need to compute ⟨f|x y|i⟩, where |i> is the ground state, and |f> is some excited state.\n\nLet me think about the basis states. In a 2D harmonic oscillator, each state is characterized by (n_x, n_y). Ground state is (0,0).\n\nAn excited state could be either (1,0), (0,1), (2,0), etc., depending on how much energy you add. However, note that the perturbation is proportional to x y, which connects certain states.\n\nWhat do the position operators x and y look like in terms of ladder operators? Remembering that x ~ (a_x + a_x†)√(something), similarly for y.\n\nYes, specifically, x = √(ℏ/(2mω)) (a_x + a_x†)\n\nSimilarly, y = √(ℏ/(2mω)) (a_y + a_y† )\n\nMultiplying them together, x y = (ℏ/(2mω))(a_x + a_x†)(a_y + a_y† )\n\nExpanding this product:\n\n(a_x a_y + a_x a_y† + a_x† a_y + a_x† a_y† )\n\nEach term represents different ways of creating or annihilating quanta in x and y directions.\n\nWhen acting on the ground state |0,0>, most of these terms vanish except those that don’t lower below vacuum.\n\nSpecifically, applying a_x to |0,0> gives zero, same with a_y. Similarly, a_x a_y |0,0>=0.\n\nHowever, other terms may not necessarily vanish when sandwiched between f and i.\n\nWait, but in our case, we have ⟨f|x y|i⟩, so |i> is |0,0>. Thus, x y |0,0> equals (√(ℏ/(2mω)))^2 (a_x a_y + ... ) |0,0>.\n\nCompute x y |0,0>:\n\nIt's (ℏ/(2 m ω)) (a_x a_y + a_x a_y† + a_x† a_y + a_x† a_y† ) |0,0>\n\nOnly the last term survives because others involve annihilation operators hitting |0,0>:\n\na_x a_y |0,0> = 0,\na_x a_y† |0,0> = a_x |1,0> = sqrt{1}|0,0>? No, wait, a_x |1,0> = sqrt{1}|0,0>, yes.\n\nWait, hang on, let me re-examine:\n\nActually, expanding out:\n\nx y |0,0> = (ℏ/(2 m ω)) [ a_x a_y |0,0> + a_x a_y† |0,0> + a_x† a_y |0,0> + a_x† a_y† |0,0> ]\n\nAs above, a_x a_y |0,0> = 0,\n\na_x a_y† |0,0> = a_x |1,0> = sqrt{1} |0,0>,\n\nSimilarly, a_x† a_y |0,0> = a_x† |0,1> = sqrt{1} |1,1> ?\n\nWait, no, wait. Hold on, a_x acts on the x part, while a_y acts on the y part. When they act on tensor products, their actions"}
{"id": "Quantum Mechanics/29-3", "questions": "A simple harmonic oscillator with angular frequency\n$\\omega$ containing a particle of mass $m$ is in the $n = 10$ state, $|10\\rangle$, at time $t_0$ in the far distant past.\nA time-dependent perturbation $H'(x, t) = \\alpha x^5 e^{-(t/\\tau)^2}$ is applied to the oscillator. Consider only\nthe effects that are first order in $\\alpha$.\n\n(a) Write down all the possible values of $n$ to which the state can transition.\n\n(b) Calculate the probability that the oscillator will be in the final state from part (a) with the\nhighest energy after a time $t \\gg \\tau$ has elapsed.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about a quantum harmonic oscillator being subjected to a time-dependent perturbation. Let me try to unpack it step by step.\n\nFirst, let's read through what we're given:\n\nWe've got a simple harmonic oscillator with angular frequency ω. The system is initially in the n=10 state, |10⟩, way back at some initial time t₀, which is considered as \"far into the past.\" Then, starting from there, a perturbation H' is applied. This perturbation is given by α x⁵ exp(-(t/τ)²). We need to consider only first-order effects in α for both parts (a) and (b).\n\nPart (a): Find all possible values of n to which the state can transition.\n\nHmm, okay. So when dealing with time-dependent perturbations, transitions between states occur due to matrix elements involving the perturbing Hamiltonian. In first-order approximation, the selection rules come into play based on these matrix elements.\n\nI remember that for the harmonic oscillator, the position operator x connects different number states via ladder operators. Specifically, x is proportional to (a + a†), where a and a† are the annihilation and creation operators respectively. Therefore, any power of x would involve combinations of multiple ladder operations.\n\nGiven that our perturbation involves x⁵, each factor of x contributes up to two ladder steps—either raising or lowering. Since x⁵ is involved here, perhaps the maximum change in n could be ±5? But wait, actually, since each x brings one a and one a† term, but they don't necessarily commute, maybe higher changes aren't allowed?\n\nWait no, more accurately: Each x ~ a + a†, so x⁵ would expand into terms like a^k (a†)^{5 - k} summed over various permutations. Thus, applying x five times allows us to go from state |n> to |n + m>, where m ranges from -5 to +5, depending on how many a and a† factors act on the state.\n\nBut hold on—the exact range might not just be symmetric around zero because certain terms may vanish if their application leads outside the valid Fock space (i.e., negative occupation numbers). However, considering that the initial state is |10>, even subtracting 5 gives n=5, which is still positive, so all those possibilities should exist without issues.\n\nHowever, another consideration comes from parity symmetry. For the harmonic oscillator, eigenstates alternate in parity; even n are even functions, odd n are odd. Now, x itself is an odd function—it flips the sign under reflection. Hence, multiplying x five times means overall parity remains odd multiplied five times, resulting in (-1)^5 = -1. Wait, but does that affect anything?\n\nActually, the integral ⟨n'|x⁵|n⟩ must take into account whether such integrals survive. Because x⁵ is an odd function, its expectation value between two states depends on the parities of |n⟩ and |n'⟩. If |n⟩ is even (even n) then x⁵ acting on it makes it odd, so integrating against |n'⟩ requires that |n'⟩ also be odd for non-zero overlap. Similarly, if |n⟩ is odd, x⁫ makes it even, requiring |n'⟩ to be even.\n\nSo, looking at our case, the initial state is |10⟩, which is even (since 10 is even). Applying x⁵ turns it into an odd wavefunction. Therefore, the final state |n'⟩ must be odd, i.e., n' must be odd. That restricts the possible transitions to odd n'.\n\nAdditionally, regarding the shift in n, each x introduces either a raise or lower operation. As mentioned earlier, x⁵ expands into terms that can increase or decrease n by up to 5. So, combining these thoughts, the possible transitions from n=10 are to n'=10±(odd integers?), but constrained by the fact that n' must differ by an odd integer because you started from even and ended at odd.\n\nWait, hang on—if n starts at 10 (which is even), and x⁵ causes a shift Δn, which needs to result in n’ having opposite parity. So, Δn must be odd because adding an odd number to an even number results in an odd number. Therefore, the shifts can be ±1, ±3, ±5—but within the bounds set by the expansion of x⁵.\n\nTherefore, the possible n’s are 10+5=15, 10-5=5, 10+3=13, 10-3=7, 10+1=11, 10-1=9. Also, do other shifts beyond ±5 happen? No, because x⁵ cannot cause a larger jump than 5 levels. So, total possible n's are 5,7,9,11,13,15.\n\nLet me check again: Starting from n=10, apply x⁵, leading to n' differing by 1,3,5... but limited by the exponent. Yes, seems correct.\n\nThus, the possible n's are 5,7,9,11,13,15.\n\nIs that right? Alternatively, sometimes people think that the maximum change is related to the highest power—for example, x³ can lead to max delta n of 3. Here, same logic applies, so yes, delta n can be up to 5.\n\nAlso, note that while x⁵ includes terms that can add or remove quanta, the specific combination determines the actual possible transitions. Moreover, the matrix element ⟨n'|x⁵|n⟩ isn’t zero unless n' differs by an odd number, specifically 1,3,5.\n\nAlternatively, using the properties of Hermite polynomials, which form the basis for HO states, products of x and Hermite polynomials relate to addition formulas. Maybe that complicates things further, but my reasoning above holds.\n\nAlright, moving on to part (b):\n\nCalculate the probability that the oscillator will be in the final state from part (a) with the highest energy after a long time t ≈ ∞ (but practically t≫ τ).\n\nThe highest energy among the possible n's is n=15. So, we’re interested in finding P_{10→15}, the transition probability from n=10 to n=15.\n\nIn first-order time-dependent perturbation theory, the transition amplitude c_k(t) is approximately\n\nc_k^(1)(t) = (1/iℏ) ∫_{−∞}^{t} dt' ⟨k|H’(t')|i⟩ e^{iω_{ki} t'}\n\nwhere ω_{ki}=(E_k-E_i)/ℏ.\n\nSince E_n = ℏω(n + ½ ), so E_k - E_i = ℏω(k - i)\n\nThen,\n\nc_k^(1)= (α/(iℏ)) ∫_{−infty}^\\infty [⟨k|x^5|i⟩] e^{- (t'/τ)^2 } e^{iω_{ki} t'} dt'\n\nBecause the exponential cutoff is Gaussian-like, centered at t=0, extending roughly until t≈τ. Given that t is much greater than τ, effectively, the integration limits become from −∞ to ∞.\n\nMoreover, the Fourier transform aspect arises here. Essentially, the integral becomes something akin to evaluating the Fourier transform of the Gaussian envelope modulated by the oscillatory phase factor.\n\nRecall that the Fourier transform of e^{-at²} is sqrt(π/a)}e^{-p²/(4a)}, etc. So, perhaps expressing the integral in terms of known transforms.\n\nBut before getting bogged down in calculations, let's structure the approach.\n\nStep-by-step plan:\n\n1. Compute the matrix element M = ⟨15|x^5|10⟩.\n\n   To compute this, express x in terms of ladder operators:\n   \n   x = √(ℏ/(2mω))(a + a†)\n   \n   Therefore,\n   \n   x^5 = (√(ℏ/(2mω)))^5 (a + a†)^5\n   \n   Expanding (a + a†)^5 using binomial theorem:\n   \n   Sum_{r=0}^5 C(5,r) a^{5-r}(a†)^r\n   \n   When acting on |10⟩, most terms will annihilate the state except those where the number of a† matches appropriately.\n\nSpecifically, to get |15⟩, we need to create 5 additional quanta. Which term(s) contribute to that?\n\nLooking at the expansion, the term r=5 corresponds to (a†)^5, which raises"}
{"id": "Quantum Mechanics/27-2", "questions": "A cesium atom is in its ground electronic state $6S_{1/2}$. Its nuclear spin is $7/2$.\n\n(a) Given that the hyperfine-splitting Hamiltonian has the form $H_{hfs} = a\\mathbf{I} \\cdot \\mathbf{J}$, where $\\mathbf{I}$ is the nuclear spin vector operator, and $\\mathbf{J}$ is the total electron angular momentum operator, what is the hyperfine splitting of the ground electronic state of the cesium atom in the absence of a magnetic field?\n\n(b) In the presence of a small magnetic field, what would be the Zeeman-split energy levels of this atom, up to a proportionality constant? Sketch these energy levels versus the magnetic field.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about hyperfine splitting and Zeeman effect for a cesium atom. Let me try to work through it step by step. \n\nStarting with part (a). The question says we're dealing with a cesium atom in its ground electronic state 6S₁/₂. So, first off, let's recall some basic quantum numbers here. For electrons, the principal quantum number n=6 since it's the sixth shell. The orbital angular momentum l can be found from the letter S; S corresponds to l=0 because s orbitals have zero angular momentum. Then, the total electron angular momentum j is given as 1/2. That makes sense because for an S state, which is l=0, the only possible value of j is equal to the spin of the electron, which is ½. But wait, actually no—wait: when you combine orbital angular momentum (l) and spin (s), but if l=0, then j equals just the spin projection. Hmm, maybe not exactly, but yeah, for S states, j ends up being either ½ or -½, right? Wait, more accurately, for each electron configuration, the term symbol gives us J, which is the total angular momentum. Here, it's specified as 6S₁/₂, meaning J=1/2.\n\nNow, the nucleus also has spin—I think they mentioned the nuclear spin is 7/2. So, the nuclear spin quantum number I is 7/2. Got it.\n\nThe hyperfine splitting comes into play due to the interaction between the nuclear spin and the electron's angular momentum. They gave the Hamiltonian H_hfs = a I · J, where a is some constant. Our goal is to find the hyperfine splitting without any external magnetic fields.\n\nSo, how does hyperfine structure arise? It arises from the dipole-dipole interaction between the electron's magnetic moment and the nuclear magnetic moment. This leads to shifts in the atomic energy levels depending on their relative orientations. Since there’s no external magnetic field, the system isn't perturbed further beyond this internal coupling.\n\nTo calculate the hyperfine splitting, we need to consider the eigenvalues of the Hamiltonian H_hfs. Because H_hfs is proportional to I·J, which itself can be written using the identity:\n\nI · J = (1/2)[(I + J)^2 - I^2 - J^2]\n\nThis expansion might help because the square terms correspond to Casimir operators whose eigenvalues are known based on the quantum numbers involved.\n\nLet me write down the expression again:\n\nH_hfs = a * [ (I + J)^2 - I² - J² ] / 2\n\nWe know that for angular momentum operators,\n\n(I + J)^2 |F> = F(F+1)ħ² |F>\n\nSimilarly, I² |I> = I(I+1)ħ² |I>, same for J².\n\nBut wait, our initial state is characterized by certain values of J and I. Specifically, J is fixed at 1/2 because it's the ground state 6S₁/₂, and I is 7/2 for Cesium-133, I believe. So, both J and I are constants here.\n\nWait, hold on—the total angular momentum F will result from adding I and J vectorsially. Therefore, F can take multiple values depending on whether I and J add constructively or destructively. The allowed F values range from |I-J| to I+J in integer steps.\n\nGiven that I = 7/2 and J = 1/2, the maximum F is 7/2 + 1/2 = 4, and minimum is 7/2 - 1/2 = 3. Hence, F can be 3 or 4. These two possibilities give rise to different hyperfine levels.\n\nTherefore, the hyperfine splitting occurs into two multiplets corresponding to F=3 and F=4. Each of these will have degeneracy (2F + 1): so F=3 → 7-fold degenerate, F=4→9-fold degenerate.\n\nSince the Hamiltonian depends on F via the term involving (I + J)^2, substituting back into H_hfs:\n\nE_F = <F|(a/2)( (I + J)^2 - I² - J² )|F>\n     = (a/2)[ F(F+1) - I(I+1) - J(J+1) ]\n\nPlugging in the numerical values:\nI = 7/2 ⇒ I(I+1)= (7/2)(9/2) = 63/4,\nJ = 1/2 ⇒ J(J+1)=(1/2)(3/2)=3/4.\n\nThus,\n\nE_F = (a/2)[ F(F+1) - 63/4 - 3/4 ]\n    = (a/2)[ F(F+1) - 66/4 ]\n    = (a/2)[ F(F+1) - 16.5 ]\n\nCalculating E for F=3 and F=4:\n\nFor F=3:\nE_3 = (a/2)[ 3*4 - 16.5 ] \n      = (a/2)[12 - 16.5]\n      = (a/2)[-4.5] \n      = (-4.5)a/2 = -2.25a\n\nFor F=4:\nE_4 = (a/2)[4*5 - 16.5]\n      = (a/2)[20 - 16.5]\n      = (a/2)[3.5]\n      = 1.75a\n\nThen, the difference between these energies is ΔE = E_4 - E_3 = 1.75a - (-2.25a) = 4a.\n\nHmm, okay, so the hyperfine splitting should be 4a times ħ squared over something... Wait, perhaps my units got messed up somewhere. Alternatively, considering that 'a' already includes all necessary constants including Planck's reduced constant, so probably the actual splitting is 4a multiplied by appropriate factors.\n\nAlternatively, sometimes people express the hyperfine splitting as A*(something), but regardless, according to the calculation above, the splitting is 4a.\n\nWait, another way—if I compute E_F for each case:\n\nΔE = E_4 - E_3 = [(a/2)(4*5 - 63/4 - 3/4)] - [(a/2)(3*4 - 63/4 - 3/4)]\n\nCompute inside brackets:\n\nFirst bracket: 20 - 63/4 - 3/4 = 20 - 66/4 = 20 - 16.5 = 3.5\n\nSecond bracket: 12 - 66/4 = 12 - 16.5 = -4.5\n\nDifference: 3.5 - (-4.5) = 8. Multiply by (a/2):\n\nΔE = (a/2)*8 = 4a.\n\nYes, correct. So the splitting is 4a.\n\nBut hang on—isn’t the hyperfine splitting usually expressed as a product of constants like α, Bohr magneton, etc.? Maybe in real life, a would involve those constants, but since the question doesn't specify, we’re supposed to leave it in terms of a.\n\nSo, the hyperfine splitting is 4a. Thus, the separation between the F=3 and F=4 levels is 4a.\n\nMoving on to part (b). Now, introducing a small magnetic field. We need to determine the Zeeman split energy levels up to a proportionality constant and sketch them vs B.\n\nZeeman effect refers to the splitting of spectral lines under an applied magnetic field. There are two types: normal (linear) and quadratic. However, since the magnetic field is small, linear Zeeman effect applies, leading to first-order splittings.\n\nIn the presence of a weak magnetic field, the dominant contribution comes from the Zeeman term in the Hamiltonian, which couples the magnetic moments to the external field. Additionally, the hyperfine interaction still exists, so now we'll have both effects contributing.\n\nHowever, typically, when discussing the combined effect, one considers the fine structure (which may include hyperfine interactions) along with the external perturbation. But in this context, after having considered hyperfine splitting, applying a magnetic field introduces additional splitting within each hyperfine level.\n\nWait, alternatively, perhaps the approach is to treat the problem as follows: Without the magnetic field, we had hyperfine splitting giving two levels F=3 and F=4. When"}
{"id": "Quantum Mechanics/27-5", "questions": "Positronium is an atom made up of an electron and a positron (electron anti-particle)._\n\n(a) As a \"zeroth\" order approximation, neglect everything (e.g., fine structure, hyperfine structure, etc.) except the Coulombic interaction between the electron and positron. Relate the \"zeroth\" order positronium energies to those of hydrogen.\n\n(b) Taking spin into account, each of the levels of part (a) are 4-degenerate (since the electron and positron are both spin-1/2 particles). Taking spin-spin interactions into account, $ H = H_0 - A\\mu_p \\cdot \\mu_e $, where $\\mu$ is the magnetic moment, $p$ stands for positron and $e$ for electron. Place the positronium in a uniform and constant external magnetic field, $\\mathbf{B}$. What effect do these things have on the energy of a level?", "graphs": null, "llm_answers": "<think>\nOkay, so I'm trying to solve this problem about Positronium. It's divided into two parts: part (a) asks me to relate the zeroth-order energies of Positronium to Hydrogen, considering only the Coulombic interaction. Part (b) involves taking spin into account with some Hamiltonian that includes spin-spin interactions and then placing it in an external magnetic field. Hmm, let me start with part (a).\n\nFirst, what do I know about Positronium? It's an atom consisting of an electron and its antiparticle, the positron. So instead of having a proton as the nucleus like in Hydrogen, we just have another lepton—the positron—orbiting around the electron or vice versa. Since they're both charged but opposite charges, their Coulomb attraction should be similar to the Hydrogen case.\n\nIn Hydrogen-like atoms, the energy levels depend on several factors: the reduced mass of the system, the charge of the nucleus, Planck's constant, and the permittivity of free space. The formula for the nth energy level is given by:\n\nE_n = -(μ * e^4)/(8ε₀²h²n²)\n\nWhere μ is the reduced mass of the electron-proton system. But wait, in Positronium, there isn't a nucleus; both particles are orbiting each other. So the reduced mass would actually be different because now both masses contribute equally since they’re identical particles?\n\nWait no, not exactly. Let me think again. In Hydrogen, the nucleus is much heavier than the electron, so the reduced mass is approximately equal to the electron mass m_e. However, in Positronium, both the electron and positron have the same mass, right? Because electrons and positrons have the same rest mass. Therefore, the reduced mass μ_positronium would be calculated differently compared to Hydrogen.\n\nThe general expression for reduced mass when you have two bodies of mass m₁ and m₂ is:\n\nμ = (m₁*m₂)/(m₁ + m₂)\n\nSo if m₁ = m_e (mass of electron) and m₂ = m_e (mass of positron), then:\n\nμ_positronium = (m_e * m_e)/(m_e + m_e) )= m_e²/(2m_e)= m_e / 2\n\nAh! That makes sense. So the reduced mass here is half the mass of the electron. Whereas in Hydrogen, the reduced mass is almost m_e because the proton is so heavy. Wait, more precisely, in Hydrogen, the reduced mass is slightly less than m_e due to the finite mass of the proton, but for our purposes, maybe we can approximate it as m_e. Or perhaps better yet, keep track of exact expressions.\n\nBut regardless, moving forward. For Hydrogen, E_H(n) ≈ (-13.6 eV)/n², which comes from plugging in all constants with μ≈m_e. Now, for Positronium, using the correct reduced mass, how does the energy change?\n\nLet’s write out the energy formula explicitly for both systems.\n\nFor Hydrogen:\nE_H(n) = - (μ_H * e⁴)/(8 ε₀² h² n²)\nwhere μ_H is the reduced mass of the electron and proton.\n\nSince the proton mass M_p >> m_e, μ_H ≈ m_e*(M_p)/(M_p + m_e) ≈ m_e.\n\nTherefore,\nE_H(n) ≈ - (m_e * e⁴)/(8 ε₀² h² n² )\n\nNow, for Positronium, the reduced mass is μ_P = m_e / 2.\n\nThus,\n\nE_P(n) = - (μ_P * e⁴)/(8 ε₀² h² n² )\nSubstituting μ_P gives us:\n\nE_P(n) = - ((m_e / 2)* e⁴)/(8 ε₀² h² n² ) \nSimplify numerator:\n\n(m_e / 2) * e⁴ = (m_e e⁴)/2\n\nDenominator remains 8 ε₀² h² n².\n\nPutting together:\n\nE_P(n) = - (m_e e⁴)/(16 ε₀² h² n² )\n\nCompare this to Hydrogen's energy:\n\nE_H(n) ≈ - (m_e e⁴)/(8 ε₀² h² n² )\n\nHence, E_P(n) = (1/2) E_H(n)\n\nBecause 16 is twice 8, so dividing by 16 halves the magnitude. Thus, the energy levels of Positronium are lower (more negative?) No, wait—if E_P has smaller absolute value, meaning higher energy? Wait, hold on.\n\nNo, wait, the energy becomes less negative, i.e., closer to zero. So the binding energy is weaker. Which makes sense because the reduced mass is smaller—it takes longer time for them to orbit each other, leading to larger orbital radii and hence weaker potential wells.\n\nAlternatively, thinking in terms of Bohr radius. The Bohr radius for Hydrogen is a_0 = (4 π ε₀ ħ²)/(m_e e² ). Similarly, for Positronium, replacing m_e with μ_P = m_e / 2, so:\n\na_P = (4 π ε₀ ħ²)/(μ_P e² ) = (4 π ε₀ ħ²)/( (m_e / 2) e² ) = 2 * [ (4 π ε₀ ħ²)/(m_e e² ) ] = 2 a_0\n\nWhich shows that the Bohr radius doubles. Hence, the size increases, implying lower energy (less tightly bound)—so yes, E_P(n) = (1/2) E_H(n). \n\nThat seems consistent. Alternatively, sometimes people might say that the energy scales inversely with the square of the radius, so if the radius goes up by factor k, the energy decreases by 1/k². Here, radius doubled, so energy quartered... Wait, no—that doesn’t align with previous conclusion.\n\nWait, confusion arises. Maybe need to clarify scaling relations.\n\nEnergy depends on the reduced mass and the charge squared over the principal quantum number squared. If the reduced mass changes, while keeping the charge the same, the energy will scale linearly with the reduced mass. So if μ is halved, energy also halves.\n\nYes, that matches earlier result. So indeed, E_P(n) = (1/2) E_H(n). So the ground state energy of Positronium is -6.8 eV, whereas Hydrogen is -13.6 eV.\n\nAlright, so that answers part (a): the zeroth-order energies of Positronium are one-half those of Hydrogen.\n\nMoving on to part (b). This is trickier. We take spin into account. Each level was previously four-fold degenerate because both the electron and positron are spin-½ particles. Without any perturbations, each energy level splits into multiple states based on total angular momentum coupling.\n\nHowever, the question mentions including spin-spin interactions via the Hamiltonian H = H_0 - A μ_p · μ_e. Also, place the system in an external magnetic field B. Need to find the effect on the energy.\n\nHmm, okay. First, without the external magnetic field, the spin-spin term causes splitting within the existing multiplets. Then adding an external magnetic field introduces Zeeman effects.\n\nI remember that in such cases, the spin-spin interaction leads to singlet-triplet splitting. Spin-½ particles combine to form either singlet (total S=0) or triplet (S=1) states. These have different energies due to exchange interactions—in particular, Pauli exclusion principle affects whether certain spatial wavefunctions are symmetric or antisymmetric depending on spin state.\n\nMoreover, the spin-spin interaction term usually looks something like H_ss ∝ (S·σ)(S'·σ'), but in this case, it's written as H = H_0 - A μ_p · μ_e.\n\nGiven that μ is proportional to spin, specifically μ = g_s (q/mc) S, where q is charge, m is mass, c speed of light, and g_s gyromagnetic ratio.\n\nBut for electron and positron, their magnetic moments differ in sign because the positron has positive charge. Specifically, the magnetic dipole moment of the positron is opposite to that of the electron. So μ_p = -g_s (e/(2m_e)) σ, assuming natural units where ħ=c=1, but probably needs careful handling.\n\nWait, let me recall"}
{"id": "Quantum Mechanics/25-1", "questions": "(a) Show that $\\left[J_x^2, J_y^2\\right]=\\left[J_y^2, J_z^2\\right]=\\left[J_z^2, J_x^2\\right]$.\n\n(b) Show that these three commutators vanish in states where $ j = 0 $ or $ j = 1/2 $.\n\n(c) Show that these commutators also vanish in states where $ j = 1 $. Find the common eigenbasis \nof $J_x^2, J_y^2$ and $J_z^2$ in this case.", "graphs": null, "llm_answers": "<think>\nAlright, so I have to tackle part (a), which is showing that [Jx², Jy²] equals [Jy², Jz²], which in turn equals [Jz², Jx²]. Hmm, okay. Let me think about angular momentum operators and their properties.\n\nFirst off, I remember that for angular momentum, we have the standard commutation relations: [Ji, Ej] = iħεijk Ek, right? But here, instead of Ji themselves, it's their squares. So maybe expanding the commutator will help?\n\nLet me write out what [Jx², Jy²] would be. Using the definition of a commutator:\n\n[Jx², Jy²] = Jx² Jy² - Jy² Jx²\n\nHmm, but how do I simplify this expression? Maybe express each term using the known commutation relations between Jx and Jy.\n\nWait, another approach could be to use some identities involving angular momentum operators. There’s a formula for the square of angular momentum components when they don’t commute. Alternatively, perhaps expressing everything in terms of ladder operators might make things clearer since those often simplify such calculations.\n\nBut before jumping into ladder operators, let me see if there's a more straightforward way. Remembering that for any two operators A and B,\n\n[A², B²] can sometimes be expressed as something like ABAB minus BAAB... Not sure if that helps directly.\n\nAlternatively, recall that [A,B][C,D] kind of expansions—maybe not immediately helpful either.\n\nAnother thought: Since all the components satisfy similar commutation relations with cyclic permutations, then perhaps the structure constants εijk imply symmetry among x,y,z indices. Therefore, the expressions should behave similarly under permutation, leading to equal commutators.\n\nSo, considering that, because the problem has rotational symmetry, swapping x, y, z shouldn't change the form of the commutator. Hence, all three commutators must be equal.\n\nIs that sufficient reasoning? Or does it require explicit computation?\n\nI suppose to show equality explicitly, one needs to compute them. Perhaps computing [Jx², Jy²] first.\n\nCompute [Jx², Jy²]:\n\n= Jx² Jy² - Jy² Jx²\n\nWe know from the commutation relation that [Jx, Jy] = iħ Jz. Also, note that [Jx, Jy²] can be expanded via the product rule:\n\n[Jx, Jy²] = Jy [Jx, Jy] + [Jx, Jy] Jy = Jy(iħ Jz) + (iħ Jz) Jy = iħ(Jy Jz + Jz Jy)\n\nSimilarly, [Jx², Jy] = ?\n\nWait, actually, wait—I need to find [Jx², Jy²].\n\nPerhaps expand both products:\n\nJx² Jy² = Jx Jx Jy Jy\n\nAnd Jy² Jx² = Jy Jy Jx Jx\n\nTo subtract these, maybe rearrange terms using commutation relations.\n\nThis seems complicated; perhaps a better idea is to consider writing the commutator in terms of other quantities.\n\nRecall that [A², B²] = [A,A][B,B]/something? Wait no—that doesn't seem correct.\n\nAlternatively, perhaps using the identity:\n\n[AB, CD] = AC[B,D] + AD[C,B] + C B [A,D] + D B [A,C]\n\nNot exactly, but maybe breaking down the commutator into smaller parts.\n\nWait, alternatively, perhaps using the fact that [Jx², Jy²] can be written as [Jx, Jy²] Jx + Jx [Jx, Jy²]\n\nNo, hold on—the general expansion for [A², B²] isn't straightforward. Another strategy: Use the Moyal bracket or Poisson brackets analogy—but probably overcomplicating.\n\nMaybe going back to basics. Express Jx² and Jy² in terms of ladder operators. For spin systems, especially, ladder operators are useful.\n\nFor orbital angular momentum, same applies—if you define J+ = Jx + i Jy, etc., though usually defined differently. Actually, typically, J± = Jx ± i Jy, so that [Jz, J+] = ħ J+, etc.\n\nGiven that, perhaps express Jx and Jy in terms of J+ and J-.\n\nYes! That sounds promising.\n\nExpressed in terms of ladder operators:\n\nJx = (J+ + J-) / 2\n\nJy = (-i J+ + i J-) / 2\n\nTherefore, Jx squared is [(J+ + J-)/2]^2 = (J+² + J-² + J+ J- + J- J+) ) /4\n\nSimilarly, Jy squared is [(-i J+ + i J-)/2]^2 = ((-i)^2 J+² + (i)^2 J-² + (-i)(i) J+ J- + (i)(-i) J- J+)/4 )\n\nSimplify that:\n\nSince (-i)^2 = (-i)*(-i)= (-1)^2 i^2 = 1*(-1)=-1\n\nSimilarly, (i)^2 = -1\n\nAlso, (-i)*(i) = -(i^2) = -(-1)=1\n\nSame for (i)*(-i)=1\n\nThus, substituting:\n\nJy² becomes [ (-1) J+² + (-1) J-² + 1*J+ J- + 1*J- J+ ] /4\n\nWhich simplifies to [ -J+² - J-² + J+ J- + J- J+ ] /4\n\nNow, putting together Jx² and Jy²:\n\nJx² = (J+² + J-² + J+ J- + J- J+ ) /4\n\nJy² = (-J+² - J-² + J+ J- + J- J+ ) /4\n\nOkay, now compute [Jx², Jy²] = Jx² Jy² - Jy² Jx²\n\nSubstituting the above expressions:\n\nEach operator is a combination of four terms divided by 4, so multiplying gives combinations of sixteen terms, each scaled by 1/(4*4)=1/16.\n\nThat's quite tedious, but manageable.\n\nLet me denote S = J+², T = J-², U = J+ J-, V = J- J+\n\nThen,\n\nJx² = (S + T + U + V)/4\n\nJy² = (-S - T + U + V)/4\n\nHence,\n\nJx² Jy² = [ (S + T + U + V)/4 ][ (-S - T + U + V)/4 ]\n\nMultiply numerator:\n\n(S + T + U + V)(-S - T + U + V) =\n\nExpand term-by-term:\n\n-S*S - S*T + S*U + S*V\n\n-T*S -T*T + T*U + T*V\n\n+U*S + U*T - U*S - U*T\n\n+V*S + V*T - V*S - V*T\n\nWait, that looks messy. Instead, notice that (a+b+c+d)(e+f+g+h). It'll take time, but let's proceed carefully.\n\nMultiplying each term:\n\nTerm1: S * (-S) = -S²\n\nTerm2: S*(-T) = -ST\n\nTerm3: S*(U) = SU\n\nTerm4: S*(V) = SV\n\nTerm5: T*(-S) = -TS\n\nTerm6: T*(-T) = -T²\n\nTerm7: T*U = TU\n\nTerm8: T*V = TV\n\nTerm9: U*(-S) = -US\n\nTerm10: U*(-T) = -UT\n\nTerm11: U*U = U²\n\nTerm12: U*V = UV\n\nTerm13: V*(-S) = -VS\n\nTerm14: V*(-T) = -VT\n\nTerm15: V*U = VU\n\nTerm16: V*V = V²\n\nCombine all these:\n\n- S² - ST + SU + SV - TS - T² + TU + TV - US - UT + U² + UV - VS - VT + VU + V²\n\nGroup similar terms:\n\nQuadratic terms:\n- S² - T² + U² + V²\n\nCross terms:\n- ST - TS - US - SU - UT - TU - VS - SV - VT - TV\n\nPlus mixed terms:\nSU + SV + TU + TV + UV + VU\n\nWait, hold on—it's getting too convoluted. Is there a smarter way?\n\nAlternatively, recognize patterns or"}
{"id": "Quantum Mechanics/25-6", "questions": " Calculate the energy spectrum of the three-dimensional isotropic harmonic oscillator \nin the rectilinear basis (quantum numbers $n_x$, $n_y$, and$ n_z$) and in the spherical basis (quantum\nnumbers k, ℓ and m).", "graphs": null, "llm_answers": "<think>\nOkay, so I need to figure out how to calculate the energy spectrum for a three-dimensional isotropic harmonic oscillator using both the rectilinear basis with quantum numbers \\( n_x \\), \\( n_y \\), and \\( n_z \\), as well as the spherical basis with quantum numbers \\( k \\), \\( l \\), and \\( m \\).\n\nFirst off, let me recall what an isotropic 3D harmonic oscillator is. It's a system where a particle experiences restoring forces proportional to its displacement along each axis—x, y, z—and these forces have the same strength in all directions because it's isotropic.\n\nIn the rectilinear or Cartesian coordinate system, the Hamiltonian would be separable into x, y, and z components. So, the total Hamiltonian should just be the sum of the individual one-dimensional oscillators:\n\n\\[\nH = H_x + H_y + H_z\n\\]\n\nEach \\( H_i \\) corresponds to a 1D harmonic oscillator. The energy levels for a single 1D HO are given by:\n\n\\[\nE_n^{(i)} = \\hbar \\omega \\left(n_i + \\frac{1}{2}\\right)\n\\]\nwhere \\( i \\) can be x, y, or z, and \\( n_i \\) are non-negative integers: 0, 1, 2,...\n\nSo, adding them up, the total energy becomes:\n\n\\[\nE_{n_x,n_y,n_z} = \\hbar \\omega \\left(n_x + n_y + n_z + \\frac{3}{2}\\right)\n\\]\n\nThat seems straightforward enough. Each direction contributes independently, and their energies add together linearly since they're independent.\n\nNow, moving on to the spherical basis. Hmm, this might require some more thinking. In spherical coordinates, we usually describe states using quantum numbers related to angular momentum: principal (\\( n \\)), azimuthal (\\( l \\)), and magnetic (\\( m \\)). But wait, here it mentions \\( k \\), \\( l \\), and \\( m \\)—I think that refers to similar concepts but perhaps slightly different notation? Maybe \\( k \\) stands for something else?\n\nWait, actually, sometimes people use \\( k \\) instead of \\( n \\) when dealing with continuous spectra or other contexts, but in bound systems like the harmonic oscillator, maybe \\( k \\) relates differently. Let me clarify.\n\nThe key point about the spherical basis is that it uses angular momentum quantum numbers. For the 3D isotropic harmonic oscillator, there's a connection between the number state quantum numbers \\( n_x, n_y, n_z \\) and the angular momentum quantum numbers \\( l \\) and \\( m \\). \n\nBut hold on—the 3D isotropic harmonic oscillator has a higher symmetry than the hydrogen atom; specifically, it possesses SU(3) symmetry rather than SO(3). This means that besides the usual angular momentum operators, there are additional ladder operators involved which lead to degeneracies beyond those explained solely by \\( l \\) and \\( m \\).\n\nHowever, if someone wants to express the energy spectrum in terms of \\( k \\), \\( l \\), and \\( m \\), perhaps they mean expressing the energy based on certain combinations of the original quantum numbers. Alternatively, maybe \\( k \\) represents another type of quantum number specific to the oscillator.\n\nLet me try approaching this from first principles. The 3D isotropic harmonic oscillator potential is:\n\n\\[\nV(r) = \\frac{1}{2}m\\omega^2 r^2\n\\]\nwhere \\( r^2 = x^2 + y^2 + z^2 \\).\n\nExpressed in spherical coordinates, the Schrödinger equation separates into radial and angular parts. However, unlike the Coulomb problem, solving the radial part doesn't give us a simple expression directly tied to \\( l \\); instead, due to the quadratic term, the solutions involve Laguerre polynomials multiplied by associated Legendre functions.\n\nAlternatively, considering creation and annihilation operators could help bridge the gap between the two bases. In the Cartesian case, you have separate ladder operators for each dimension:\n\n\\( a_x^\\pm, a_y^\\pm, a_z^\\pm \\)\n\nThese allow transitions within each mode without affecting others. On the other hand, in the spherical basis, you'd want operators that change the angular momentum, such as raising/lowering \\( l \\) and \\( m \\).\n\nAnother thought: Since the 3D isotropic HO is spherically symmetric, any eigenstate must also respect this symmetry. Therefore, the energy depends only on quantities invariant under rotation—in particular, the square of the angular momentum operator \\( L^2 \\) and the parity operator.\n\nBut even then, knowing \\( l \\) alone isn’t sufficient to determine the energy uniquely because multiple values of \\( l \\) may correspond to the same energy level due to the high degeneracy of the HO.\n\nWait, no—that’s not quite right. Actually, in the 3D isotropic HO, the energy does depend on the combination \\( N = n_x + n_y + n_z \\), known as the principal quantum number. And each value of \\( N \\) gives rise to various possible \\( l \\) values ranging from 0 to \\( N \\), leading to degenerate energy levels.\n\nTherefore, while the energy itself is determined by \\( N \\), the way it's expressed in terms of \\( k \\), \\( l \\), and \\( m \\) needs clarification. Perhaps \\( k \\) is being used here as a substitute for \\( N - l \\)? Or maybe \\( k \\) counts the nodes in the radial wavefunction?\n\nActually, yes! In atomic physics, often \\( n \\) denotes the principal quantum number, which includes contributions from both the orbital angular momentum \\( l \\) and the radial node count. Similarly, in the context of the harmonic oscillator, the principal quantum number \\( N = n_r + l \\), where \\( n_r \\) is the radial quantum number counting the number of nodes in the radial function.\n\nGiven that, perhaps in our case, \\( k \\) plays the role analogous to \\( n_r \\), meaning \\( k = n_r \\). Then, combining with \\( l \\), we get \\( N = k + l \\). Thus, substituting back into the energy formula, which was originally dependent on \\( N \\):\n\nFrom earlier,\n\n\\[\nE_N = E_0 + \\hbar \\omega N\n\\]\nWhere \\( E_0 = \\frac{3}{2}\\hbar \\omega \\) is the ground state energy.\n\nThus, replacing \\( N \\) with \\( k + l \\),\n\n\\[\nE(k,l,m) = \\frac{3}{2}\\hbar \\omega + \\hbar \\omega (k + l)\n\\]\nSimplifying,\n\n\\[\nE(k,l,m) = \\hbar \\omega \\left(\\frac{3}{2} + k + l \\right )\n\\]\n\nIs that correct? Wait, let me verify.\n\nIf \\( N = k + l \\), then indeed, plugging into the energy expression makes sense. Because in the Cartesian basis, \\( N = n_x + n_y + n_z \\), whereas in the spherical basis, \\( N = k + l \\). Hence, the energy remains consistent across representations.\n\nTo double-check, consider the lowest energy states. When \\( k=0 \\) and \\( l=0 \\), the energy should match the ground state of the Cartesian case, which had \\( n_x=n_y=n_z=0 \\), giving \\( E=\\frac{3}{2}\\hbar \\omega \\). Plugging into the above formula:\n\n\\[\nE(0,0,m)= \\hbar \\omega \\left(\\frac{3}{2}+0+0\\right ) = \\frac{3}{2}\\hbar \\omega\n\\]\nWhich matches perfectly.\n\nSimilarly, take the next level. If \\( N=1 \\), either through \\( k=1, l=0 \\) or \\( k=0, l=1 \\). Both cases will yield:\n\nFor \\( k=1, l=0 \\):\n\\[\nE = \\hbar \\omega \\left(\\frac{3}{2} +1 +0\\right ) = \\frac{5}{2}\\hbar \\omega\n\\]\nAnd for \\( k=0, l=1 \\):\nSame result:\n\\[\nE = \\frac{5}{2}\\hbar \\omega\n\\]\nThis shows the degeneracy—for \\( N=1 \\), both \\( l=0 \\) and \\( l=1 \\) contribute, hence fourfold degeneracy (since \\( m \\) ranges from -l to +l, so for \\( l=1 \\): m=-1,0,+1 → 3 states plus the \\( l=0 \\) state: total 4 states). Indeed, in the Cartesian basis, \\( N=1 \\) implies permutations among \\( n_x, n_y, n_z \\) having exactly one excitation, resulting in 3 states (each corresponding to exciting one degree of freedom once). Wait, hang on—is that conflicting?\n\nNo, actually, in the Cartesian basis, \\( N=1 \\) leads to 3 distinct states: (1,0,0), (0,1,0), (0,0,1). Whereas in the spherical basis, \\("}
{"id": "Quantum Mechanics/29-2", "questions": "Consider charged particles directed towards an atom. Model the atom as a two-level system with ground state |1⟩ and excited state |2⟩. The Hamiltonian matrix of the atom is\n\n$$\n \nH = \\begin{pmatrix}\n0 & -eE\\gamma_{12} \\\\\n-eE\\gamma_{12} & \\epsilon \n\\end{pmatrix} \n$$\n\n\nwhere $\\epsilon$ is the energy of the excited state relative to the ground state, $E$ is the electric field due to the charged particles, and $\\gamma_{12} \\equiv \\langle 1| r |2 \\rangle$. Here $r$ is the component of the electron position vector in the direction of the $E$ field.\n\nThe state of the two level system evolves with time according to\n\n$$\n|\\psi\\rangle=c_1(t)|1\\rangle+c_2(t) e^{-i \\omega_0 t}|2\\rangle,\n$$\n\n\nwhere $\\omega \\equiv \\epsilon/\\hbar$, and the coefficients satisfy the coupled rate equations\n\n$$\n\\dot{c}_1=-\\frac{i}{\\hbar} q E r_{12} e^{-i \\omega_0 t} c_2, \\quad \\quad \\dot{c}_2=-\\frac{i}{\\hbar} q E r_{12} e^{i \\omega_0 t} c_1\n$$\n\n\nAssume that the atom experiences a delta-function electric field of the form $E(t) = \\alpha \\delta(t-t_0)$ due to the passage of a charged particle.\n\n(a) What is the dimensionless parameter that, if small, justifies the use of perturbation theory to characterize the response of the atom to a particle?\n\n(b) Assuming that the parameter specified in (a) is indeed small, and that the atom is initially in its ground state, what is the probability that it will be in the excited state after passage of a particle? [To lowest order in $\\alpha$.]\n\n(c) What is the expectation value of the dipole moment of the atom, or $(-e r)$, after passage of a particle, to lowest order in $\\alpha$?\n\n(d) Imagine that the atom is subject to $N$ particles which arrive at equal intervals $\\tau$, where $\\omega_0 \\tau \\ll 1$. Find the probability that the atom will be in the excited state after passage of the $N$ particles [again to lowest order in $\\alpha$.]", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about atoms interacting with charged particles modeled as a two-level system. Let me try to break down each part step by step.\n\nStarting with part (a): It asks for the dimensionless parameter that justifies using perturbation theory when the atom responds to a particle. Perturbation theory usually applies when some term in the Hamiltonian is much smaller than another. In our case, looking at the given Hamiltonian:\n\n\\[ H = \\begin{pmatrix} 0 & -eE\\gamma_{12} \\\\ -eE\\gamma_{12} & \\epsilon \\end{pmatrix} \\]\n\nI notice there's an off-diagonal term involving \\( -eE\\gamma_{12} \\). This must represent the interaction between the states caused by the external electric field. So, probably, we consider this term as a perturbation compared to the diagonal terms. \n\nWait, but actually, since the diagonal terms include zero and epsilon, maybe the unperturbed Hamiltonian would be without the off-diagonal terms. Then, the perturbing part is the off-diagonal elements proportional to \\( eE\\gamma_{12} \\).\n\nBut wait again—the question mentions \"the parameter\" singular. Maybe they're referring to something like the ratio of the perturbation strength over the natural frequency splitting. Hmm.\n\nLooking back: The coupling term involves \\( eE\\gamma_{12} \\), while the energy difference between levels is \\( \\epsilon = \\hbar \\omega_0 \\). If the perturbation is weak, then perhaps the relevant parameter is \\( (eE\\gamma_{12}) / (\\hbar \\omega_0) ) \\times something else... Wait, let me think more carefully.\n\nIn standard two-level systems under oscillating fields, you often get Rabi frequencies depending on parameters like \\( \\Omega = (dE)/(\\hbar) \\), where d is the dipole matrix element. But here, because the field is impulsive—a delta function—maybe the transition amplitude depends linearly on alpha times gamma_12 divided by hbar omega_0.\n\nAlternatively, considering Fermi's golden rule, transitions occur when the energy matches resonance conditions. Since the pulse is a delta function, it might cause a sudden kick. However, regardless, the key point is identifying the small parameter.\n\nGiven that the electric field is a delta function scaled by α, likely the parameter is related to how strong this δ-pulse is. Specifically, the product of the coupling constant and the duration—but since it's a delta function, the integral matters. Alternatively, thinking in terms of units.\n\nLet’s compute dimensions. Gamma_12 has units of length (since <1|r|2> is a position operator). Electric field E has units V/m or N/C. Charge e has C. Planck's constant ħ has J·s.\n\nSo, the term \\( eE\\gamma_{12} \\) has units: Coulomb * (V/m) * meters = CV. Voltage is Joule per Coulomb, so overall unit becomes J. Which makes sense because it should contribute to energy.\n\nComparing to ϵ, which also has units of energy. Therefore, their ratio is dimensionless. Thus, the parameter could be \\( (eEγ₁₂)/(ϵ) ), but since E itself is multiplied by αδ(t−t₀), perhaps substituting E=αδ(...) into the expression gives us the actual parameter.\n\nTherefore, plugging in E(t)=αδ(...):\n\nThen, the perturbation term is ~ e*α γ₁₂ δ(t−t₀). When integrated over time, say in a transition amplitude calculation, the effect scales with α. Hence, the dimensionless parameter is likely \\( (e α γ₁₂)/(ħ ω₀) ). Because integrating the exponential factors from the time evolution may bring in factors of ω₀.\n\nYes, because in the time-dependent perturbation approach, the transition amplitude A_{1→2}(t) ≈ (-i/ħ) ∫ ⟨2|H'|1⟩ dt', where H' is the perturbation. Substituting H’ as –eEγ₁₂, and E being α δ(t–t₀), the integral picks up only at t=t₀, giving A≈(e α γ₁₂)/(iħ). To make sure this is small, we need (e α γ₁₂)/(ħ) << other energies involved—in particular, the detuning or the natural frequency. Given ε=ħω₀, so comparing (e α γ₁₂)/(ħ) vs ω₀ ħ ?\n\nWait no, both numerator and denominator—if we write the condition as (e α γ₁₂)/(ħ^2 ω₀))<<1? Not quite. Perhaps better expressed as (e α γ₁₂)/(ħ ω₀) ) <<1.\n\nBecause the typical energy scale is ϵ=ħω₀, so any perturbative term needs to be small compared to that. So yes, the parameter is (e α γ₁₂)/(ħ ω₀ ).\n\nThus, the dimensionless parameter is (e α γ₁₂)/(ħ ω₀ ). Or written differently, (α e γ₁₂)/(ħ ω₀ ). Either way, same thing.\n\nMoving on to part (b): We’re supposed to find the probability of excitation after one particle passes, assuming initial ground state and low-order approximation in α.\n\nSince the atom starts in |1>, c₁(0)=1, c₂(0)=0. After applying the delta function perturbation, we can model this via first-order time-dependent perturbation theory.\n\nThe general formula for transition amplitude from |1> to |2> is approximately:\n\nA_{1→2}= (-i/ħ) ∫_{-infty}^{+infty} ⟨2|H'(t)|1⟩ e^{i ω_{21} t} dt,\n\nWhere ω_{21}= (ε)/ħ= ω₀.\n\nHere, H’ is the perturbation; originally, the full H includes the diagonal terms, but in TDPT, we treat the off-diagonal terms as perturbation. Wait, hold on—is the entire H considered, including diagonals, or do we split it into H0 + H'? Actually, in the given setup, the state already incorporates the phase factor e^{-i ω₀ t}, suggesting that the free Hamiltonian is taken care of, i.e., H0 is diag(0, ε), and the rest is treated as perturbation.\n\nBut in the given coupled rate equations, the dot products involve terms with exponentials ±i ω₀ t, indicating that these come from the rotating frame or interaction picture.\n\nHowever, since the electric field is applied impulsively as a delta function, perhaps it's easier to work directly in the Schrödinger equation framework.\n\nBut alternatively, treating the delta function as a short-time perturbation, leading to a finite change in the wavefunction.\n\nAnother thought: For a delta function perturbation, the transition amplitude is simply:\n\nΔc₂ ≈ (-i/"}
{"id": "Quantum Mechanics/29-5", "questions": "A particle of mass $m$ and electric charge $q$ moves in one dimension in a harmonic potential. At $t = 0$ we turn on a homogeneous time-varying electric field.\n\n(a) Write down the Hamiltonian for this system for all times.\n\n(b) Exactly solve the energy eigenvalue problem for all times and relate the eigenstates of this system before and after the introduction of the field.\n\n(c) If the system starts out in the ground state for $t < 0$, what is the probability that it will be in an excited state for $t > 0$? You may use Zassenhaus' formula:\n\n$$\ne^{\\alpha(A+B)}=e^{\\alpha A} e^{\\alpha B} e^{\\left.\\left.-\\frac{\\alpha^2}{2} \\right\\rvert\\, A, B\\right]} e^{\\frac{\\alpha^3}{6}(2[B,[A, B]]+[A \\cdot[A, B]])} \\ldots\n$$\n\n\nwhere the dots indicate more and more nested commutators.\n\n(d) Now, solve the problem using time-dependent perturbation theory treating the bare SHO as the unperturbed Hamiltonian. Do this for general $\\mathcal{E}(t)$, then specifically for $\\mathcal{E}(t) = \\mathcal{E}_0 e^{-t/\\tau}$.", "graphs": null, "llm_answers": "<think>\nOkay, so I have to tackle part (a), which asks me to write down the Hamiltonian for a charged particle moving in one dimension under a harmonic potential with a time-varying electric field turned on at t=0. Hmm, let's think about how to approach this.\n\nFirst, without any external fields, the Hamiltonian for a simple harmonic oscillator (SHO) is given by H₀ = p²/(2m) + (1/2)mω²x², where p is momentum, m is mass, ω is angular frequency, and x is position. That makes sense because it includes both kinetic and potential energies.\n\nNow, when you introduce an electric field E(t), especially since it's varying with time, there should be another term added to the Hamiltonian due to the interaction between the charge q and the electric field. The force from the electric field would be F = -qE(t)x, but wait, actually, isn't the potential energy associated with an electric field U = -q∫E dx or something like that?\n\nWait, no, maybe better to recall that the electromagnetic potential contributes to the Hamiltonian through minimal coupling. For a uniform electric field pointing along the x-axis, the vector potential can be chosen such that the scalar potential φ(x,t) gives rise to the electric field via E = -∇φ - ∂φ/∂t. But perhaps here they just want us to include the dipole interaction term directly.\n\nIn many cases, the addition of an electric field leads to a shift in the potential. Specifically, if the electric field is applied suddenly, the potential becomes V(x) = (1/2)mω²x² - qEx, right? Because the electric potential energy is U = -qΦ, and Φ could be linear in x: Φ = Ex, leading to U = -qEx. So adding that into the original SHO potential.\n\nBut hold on, does turning on the electric field change the form of the potential entirely? Let me double-check. When you apply an electric field, depending on whether it's conservative or not, the potential might get modified. Since the electric field is uniform and presumably static except for its variation over time, yes, the potential energy contribution would indeed be quadratic plus linear terms.\n\nSo putting it together, the total Hamiltonian should still look similar to the SHO but shifted. Alternatively, sometimes people express it as H = H₀ - qEx(t). Wait, but H₀ already has the quadratic term; do I need to adjust anything else?\n\nAlternatively, considering gauge transformations, introducing an electric field usually affects either the magnetic vector potential or the scalar potential. In this case, since it's purely electric and uniform, probably only affecting the scalar potential. Therefore, the new Hamiltonian would be H = H₀ - q∫E dt · x? No, that doesn’t seem quite right.\n\nLet me take a different route. Remember that the Lorentz force law says F = q(E + v × B). Here, assuming no magnetic field, so F = qE. Then Newton’s second law gives mx''(t) = qE(t). However, in quantum mechanics, forces translate into potentials in the Hamiltonian.\n\nThe standard way to incorporate an electric field is to add a term involving the electric displacement. Actually, thinking again, the correct expression comes from the fact that the electric field adds a linear potential. Hence, the potential energy becomes V(x) = (1/2)mω²x² - qE(t)x. Thus, the full Hamiltonian is H = p²/(2m) + (1/2)mω²x² - qE(t)x.\n\nIs that accurate? Well, alternatively, some sources say that applying an electric field shifts the equilibrium point of the oscillator. To see why, consider completing the square in the potential. Starting with V(x) = (1/2)mω²x² - qE(t)x. Completing the square:\n\nV(x) = (1/2)mω² [x² - (2qE)/(mω²) x] \n     = (1/2)mω² [(x - qE/(mω²))² - (qE)^2/(m²ω⁴)]\n     = (1/2)mω²(x - d)^2 - (q²E²)/(2mω²)\n\nWhere d = qE/(mω²) is the displacement of the equilibrium position. So effectively, the oscillator is centered around x=d instead of x=0, and there's also a constant energy shift. \n\nSince constants don't affect dynamics, the effective Hamiltonian is equivalent to shifting x → x - d, resulting in H_eff = p²/(2m) + (1/2)mω²x'^2, where x’ = x - d. This suggests that the presence of the electric field simply translates the coordinate system, making the problem solvable similarly to the regular SHO.\n\nTherefore, going back, my initial thought was correct—the Hamiltonian is H = p²/(2m) + (1/2)mω²x² - qE(t)x. Or equivalently, written as H = H₀ - qE(t)x.\n\nHence, summarizing, the Hamiltonian for all times is the sum of the free SHO Hamiltonian minus the linear term representing the electric field interaction.\n\n**Final Answer**\n\nFor part (a), the Hamiltonian is:\n\\[\n\\boxed{H = \\frac{p^2}{2m} + \\frac{1}{2}m\\omega^2 x^2 - q E(t) x}\n\\]\n\n---\n\nMoving on to part (b): exactly solving the energy eigenvalue problem for all times and relating the eigenstates before and after the field is introduced.\n\nHmm, exact solutions... Given that the Hamiltonian now depends explicitly on time through E(t), unless E(t) varies slowly enough, the usual separation of variables method won't work. But wait, even if E(t) is arbitrary, perhaps we can perform a canonical transformation or find an invariant operator whose eigenvalues correspond to the instantaneous energy levels.\n\nAnother idea: since the additional term is linear in x, which resembles a displacement in phase space. Maybe performing a translation in x to absorb the linear term, thereby reducing it to the standard SHO problem.\n\nEarlier, when analyzing the potential, we saw that completing the square led to a displaced oscillator. Perhaps defining a new variable y = x - d(t), where d(t) accounts for the effect of the electric field up to time t. Substituting this into the Schrödinger equation might lead to equations governing y, possibly simplifying things.\n\nHowever, since E(t) is time-dependent, d(t) would depend on the integral of E(t). Let me formalize this substitution.\n\nDefine y = x - α(t), where α(t) is a function to be determined. Substitute into the Hamiltonian:\n\nH = p_y²/(2m) + (1/2)mω²(y + α(t))^2 - qE(t)(y + α(t))\n\nExpanding this,\n\nH = p_y²/(2m) + (1/2)mω²[y² + 2α(t)y + α(t)^2] - qE(t)y - qE(t)α(t)\n\nSimplify each term:\n\n- Kinetic term remains p_y²/(2m)\n- Potential expands to (1/2)mω² y² + mω² α(t) y + (1/2)mω² α(t)^2\n- Electric term subtracts qE(t)y and qE(t)α(t)\n\nCombine like terms:\n\nH = p_y²/(2m) + (1/2)mω² y² + [mω² α(t) - qE(t)] y + (terms independent of y).\n\nTo eliminate the linear term in y, set coefficient equal zero:\n\nmω² α(t) - qE(t) = 0 ⇒ α(t) = qE(t)/(mω²)\n\nThus, choosing α(t) = qE(t)/(mω²) removes the linear term in y. What remains is:\n\nH = p_y²/(2m) + (1/2"}
{"id": "Quantum Mechanics/23-2", "questions": "Let V be a two-step potential that goes (from left to right) from V₁ = 0 to V₂ > V₁ to V₃ > V₂. Let a be the width of the first step when V = V₂. Consider a system with energy E > V₃. Calculate the scattering matrix, which relates the coefficients of the outgoing waves (in both directions) in terms of the incoming waves (in both directions). For leftward scattering only, calculate the transmission coefficient.*", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about calculating the scattering matrix for a two-step potential barrier where each step is higher than the previous one. The setup is such that we go from V1=0 on the far left, then up to V2>V1 over some distance 'a', and then further up to V3>V2 beyond that. We're considering particles with energy E greater than V3.\n\nThe question specifically asks me to find the scattering matrix relating the outgoing wave coefficients to the incoming ones, but also mentions focusing on leftward scattering only and computing the transmission coefficient. Hmm, okay, let's break it down.\n\nFirst off, since E > V3, the particle isn't just tunneling through; it can pass freely once past all barriers because its kinetic energy exceeds any potential barrier here. So maybe reflection occurs mainly due to the steps between regions?\n\nI remember that in scattering problems, especially involving multiple potentials, you model each region separately using plane waves or exponentials depending on whether they're evanescent or propagating. Since E > V3 everywhere except perhaps before reaching there? Wait no: actually, starting from the leftmost side:\n\n- Region I: x < 0, V(x)=V1=0.\n- Region II: 0 ≤x≤a, V(x)=V2.\n- Region III: x>a, V(x)=V3.\n\nBut wait, hold on—the description says \"goes...to V2> V1\" and then \"to V3>V2.\" But does it mean that after going into V2, it continues as another step to V3? Or is it like three separate regions? Maybe my initial thought was correct—three regions: I (left), II (middle), III (right).\n\nSince E > V3, in all these regions, the particle has positive kinetic energy, meaning propagation without exponential decay. Therefore, in each region, solutions will consist of traveling waves rather than decaying/exponentially growing functions.\n\nSo, in each region, the time-independent Schrödinger equation becomes:\n\nRegion I:\nψ_I''(x) + k1² ψ_I(x) = 0,\nwhere k1 = sqrt(2mE)/ħ\n\nSimilarly,\n\nRegion II:\nk2 = sqrt(2m(E - V2))/ħ\n\nWait, hang on—if E > V3, then certainly E > V2 too, since V3 > V2. Thus, k2 would still be real, same as k1 being real.\n\nSame logic applies to Region III:\n\nk3 = sqrt(2m(E - V3))/ħ\n\nWhich is again real because E > V3.\n\nTherefore, in all three regions, we'll have propagating waves.\n\nNow, setting up boundary conditions. At each interface (between I & II, and II & III), the wavefunction must be continuous, and their derivatives must match as well.\n\nIn scattering theory, typically, we consider incident waves coming from either direction. However, the user mentioned \"for leftward scattering only,\" so probably assuming incidence from the left, i.e., a wave coming from the left (negative infinity) towards the barrier, and wanting to compute how much transmits through versus reflects back.\n\nAlternatively, if it's general scattering, including possible reflections from the other side, but given the mention of leftward scattering, likely single-sided incidence.\n\nGiven that, let's define our variables accordingly.\n\nFor simplicity, assume the wave comes from the left (region I):\n\nLeft-moving wave: e^{ik1 x} (incident)\nRight-moving wave: r e^{-ik1 x} (reflected)\n\nThen, moving into region II, we'd expect transmitted waves continuing forward and possibly reflected backward within region II.\n\nWait, but since E > V2, the wave doesn’t get exponentially damped—it’s just a phase shift upon entering a different medium.\n\nThus, in region II, the wave should be composed of a transmitted part t1 e^{i k2 x}, plus whatever might reflect back—but since we’re dealing with a step-up, not sure yet.\n\nActually, more accurately, in region II, the wave could propagate both ways, similar to region I.\n\nHmm, now thinking, perhaps better to use transfer matrices or solve via matching boundary conditions.\n\nYes, solving via continuity of ψ and derivative across interfaces seems doable.\n\nLet me outline the approach:\n\nDefine the wavefunctions in each region:\n\nRegion I (x < 0):\nψ_I = A e^{i k1 x} + B e^{-i k1 x}\n\nHere, A is amplitude of the incident wave (coming from left), B is reflected wave.\n\nRegion II (0 <= x <= a):\nψ_II = C e^{i k2 x} + D e^{-i k2 x}\n\nC is the amplitude of the wave moving to the right inside region II, D is the amplitude moving to the left.\n\nRegion III (x > a):\nψ_III = F e^{i k3 x} + G e^{-i k3 x}\n\nF is the transmitted wave moving to the right, G is the reflected wave moving to the left.\n\nHowever, since we're looking at leftward scattering only, perhaps G is zero? Because if we're sending a wave from the left, unless something reflects from the third region, but since E > V3, the wave shouldn't reflect off region III—in fact, it should transmit completely.\n\nHence, in region III, the wave should only move to the right, implying G=0.\n\nIs that accurate? Well, yes, because if E > V3, the wave passes through without reflecting. Hence, in region III, only F e^{i k3 x} exists.\n\nThat simplifies things.\n\nSo, updating:\n\nψ_III = F e^{i k3 x}\n\nNo need for G term.\n\nAlright, proceeding.\n\nWe have six unknowns: A, B, C, D, F, G. But since G=0, five unknowns remain.\n\nBoundary Conditions:\n\nAt x=0 (interface I-II):\n\nContinuity of ψ: ψ_I(0) = ψ_II(0)\n=> A + B = C + D ...(1)\n\nDerivative Continuity: ψ'_I(0) = ψ'_II(0)\n=> ik1(A - B) = ik2(C - D) ...(2)\n\nSimplify equations (1) and (2):\n\nFrom (1): A + B = C + D\n\nFrom (2): (A - B) = (C - D)(k2/k1)\n\nBecause dividing both sides by ik gives us that relation.\n\nMoving on to next interface at x=a (interface II-III):\n\nContinuity of ψ: ψ_II(a) = ψ_III(a)\n=> C e^{i k2 a} + D e^{-i k2 a} = F e^{i k3 a} ...(3)\n\nDerivative Continuity: ψ'_II(a) = ψ'_III(a)\n=> i k2 [C e^{i k2 a} - D e^{-i k2 a}] = i k3 F e^{i k3 a} ...(4)\n\nEquation (4) simplifies to:\n\nk2 [C e^{i k2 a} - D e^{-i k2 a}] = k3 F e^{i k3 a} ...(5)\n\nAlso, note that in region I, the incident wave is A e^{i k1 x}. If we set A=1 for normalization purposes, then we can express everything else relative to A.\n\nAssuming A=1, so the incident wave is normalized.\n\nOur goal is to relate the outgoing amplitudes (B, F) to the incoming amplitude (A=1). Then, construct the scattering matrix S, which maps incoming coefficients to outgoing coefficients.\n\nScattering Matrix usually takes the form:\n\n[Reflected Left]   = S * [Incident Right]\n[Transmitted Right]\n\nOr sometimes arranged differently based on convention.\n\nBut in standard notation, for a multi-channel scattering, the S-matrix connects incoming states to outgoing states.\n\nBut in this case, since we have two channels (regions I and III?), though in reality, it's a single channel with two steps.\n\nWait, perhaps think of it as having four leads: left, middle-left, middle-right, right. No, that complicates.\n\nMaybe simpler to stick with defining the S-matrix elements connecting the incoming and outgoing waves.\n\nBut regardless, ultimately, what matters is expressing B and F in terms of A and potentially others, but since we assumed A=1 and G=0, we can write expressions for B and F.\n\nProceeding step-by-step.\n\nWith A=1, Equations become:\n\nFrom (1): 1 + B = C + D ...(1')\n\nFrom (2): (1 - B) = (C - D)(k2 / k1) ...(2')\n\nAnd from (3): C e^{i k2 a} + D e^{-i k2 a} = F e^{i k3 a} ...(3')\n\nFrom (5): k2 [C e^{i k2 a} - D e^{-i k"}
{"id": "Quantum Mechanics/27-4", "questions": "A spin-1/2 particle is confined to a one-dimensional harmonic potential, governed by the Hamiltonian $H = \\frac{1}{2m}p^2 + \\frac{1}{2}m\\omega^2x^2 + \\lambda x \\sigma_x$, where $\\sigma_z$ is a spin Pauli matrix.\n\n(a) Find the energy eigenvalues and the wavefunction of the ground state(s) for this Hamiltonian. Now, an additional identical particle is added. The particles are fermions and can be taken to be non-interacting. What is the ground state energy of this two-particle system?\n\n(b) A weak perturbation, $\\Delta H = \\epsilon \\sigma_x$, is added to the Hamiltonian above. Returning to the problem of a single particle, calculate the correction to the ground state energy to first order in $\\epsilon$. Write down the wavefunction $\\psi$ of the ground state to lowest order, such that $\\langle \\psi | \\Delta H | \\psi \\rangle$ gives the energy correction obtained above.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this quantum mechanics problem here about a spin-1/2 particle in a 1D harmonic oscillator with some extra terms involving spin matrices. Let me try to unpack part (a).\n\nFirst, let's write out the given Hamiltonian:\n\n\\[ H = \\frac{1}{2m} p^2 + \\frac{1}{2} m \\omega^2 x^2 + \\lambda x \\sigma_x \\]\n\nHmm, okay. So it's like a standard harmonic oscillator but with an additional term: λxσₓ. Since σₓ is a Pauli matrix, which acts on the spin space, this must mean that the potential depends on both position and spin. Interesting.\n\nSo we're dealing with a spin-orbit coupling kind of situation? Or maybe just a way to couple spatial coordinates with spin states. Either way, since the particle has spin-1/2, its Hilbert space is a tensor product of the spatial Hilbert space (for the HO) and the spin Hilbert space (which is C²). \n\nThe question asks for the energy eigenvalues and the wavefunctions of the ground state(s). Then, when adding another identical fermion without interaction, find the new ground state energy.\n\nAlright, starting with the single particle case.\n\nI think because the Hamiltonian includes a term proportional to x times σₓ, which mixes different spin components depending on x. That complicates things compared to the usual harmonic oscillator.\n\nWait, perhaps if I rewrite the Hamiltonian as acting on each component separately. For example, considering up and down spins, then express H in block form or something.\n\nLet’s denote the basis states as |n⟩|↑⟩ and |n⟩|↓⟩, where n is the number state for the harmonic oscillator.\n\nBut wait, actually, no—since the last term involves x multiplied by σₓ, which flips the spin. Hmm, not sure yet how exactly to diagonalize this.\n\nAlternatively, could there be a transformation that decouples the spin from the spatial variables? Maybe shift into a rotated frame or use some unitary transformation.\n\nAnother thought: Perhaps treat the entire thing as a two-level system coupled to the harmonic oscillator. But that might get complicated quickly.\n\nWait, hold on—the unperturbed Hamiltonian would be the regular harmonic oscillator plus the spin term. Wait, no—if you set λ=0, then H becomes the simple harmonic oscillator with spin being free—it doesn't affect the energy levels except through their multiplicity due to spin degeneracy.\n\nWhen λ≠0, though, the cross-term couples the spin-up and spin-down states via the position operator x. This suggests that the total Hamiltonian isn’t simply separable; instead, the spin and spatial parts are entangled.\n\nTherefore, solving for exact eigenstates may require more involved methods than straightforward separation of variables.\n\nMaybe consider writing the full Hamiltonian in matrix form in the spinor basis. Each entry will involve operators acting on the spatial degrees of freedom.\n\nExpressed explicitly, suppose ψ(x) is a two-component spinor [ψ₁(x), ψ₂(x)]ᵀ. Then,\n\nH = [\n[ (1/(2m)) p² + (1/2)mω² x² , λ x ],\n[λ x, (1/(2m)) p² + (1/2)mω² x² ]\n]\n\nBecause σₓ is [[0,1],[1,0]], right? So multiplying by λ x gives off-diagonal elements λ x.\n\nThus, our Hamiltonian matrix looks like:\n\nH =\n[\n( H₀ ),   ( λ x )\n( λ x ),    ( H₀ )\n]\nwhere H₀ is the harmonic oscillator Hamiltonian.\n\nThis structure resembles a spin-½ particle in a magnetic field along x-direction, but now the \"field\" strength varies linearly with x.\n\nTo solve this, perhaps look for solutions where the spinors are aligned in certain directions relative to x. Alternatively, perform a gauge transformation or change of variable to simplify the equation.\n\nWait, alternatively, note that the Hamiltonian commutes with itself under time reversal? Not necessarily helpful immediately.\n\nPerhaps make a substitution y = x - α sgn(x)? No, probably overcomplicating.\n\nWait, what if we define creation and annihilation operators for the harmonic oscillator? Let me recall that:\n\nFor the HO,\na = √(mω/(2ħ)} x + i√(1/(2mω ħ)} p\n\nand similarly a† is the adjoint.\n\nThen, H₀ = ħω (a† a + 1/2 )\n\nNow, expressing everything in terms of a and a† might help.\n\nOur Hamiltonian is:\nH = H₀ ⊗ I + λ x σ_x\n\nSince x can also be expressed using a and a†:\n\nx = √( ħ / (2 m ω ) } (a + a†)\n\nSubstituting back into H:\n\nH = H₀ ⊗ I + λ * sqrt( ħ / (2 m ω )) (a + a†) σ_x\n\nWhich simplifies to:\n\nH = H₀ ⊗ I + μ (a + a†) σ_x\n\nWhere μ = λ sqrt( ħ / (2 m ω ))\n\nSo, written in terms of ladder operators, the Hamiltonian is:\n\nH = ħω (a† a + 1/2 ) I + μ (a + a†) σ_x\n\nInteresting. So, the Hamiltonian consists of the usual HO term plus a term that connects different Fock states while flipping the spin.\n\nGiven that, perhaps the eigenstates aren't simply products of number states and spin states anymore—they'll likely be superpositions across multiple number states and opposite spins.\n\nThat seems tricky. Solving this analytically might be difficult unless we exploit symmetry or other properties.\n\nWait, perhaps assume that the ground state only involves the lowest few number states. Because higher excited states contribute less to the low-energy spectrum.\n\nAssuming that, say, the ground state is mainly composed of |0⟩|↑⟩ and |0⟩|↓⟩, but mixed with small contributions from neighboring states.\n\nHowever, even within the same number state, the presence of σ_x causes mixing between spin-up and spin-down.\n\nIn fact, looking closer, for each fixed n, the action of H on |n⟩⊗|s⟩ (with s=↑ or ↓) leads to transitions to |n±1⟩⊗|s'⟩, etc., making the problem quite complex.\n\nIs there any way to separate variables or reduce the dimensionality?\n\nAlternative approach: Treat the spin degree of freedom classically? Probably not rigorous enough.\n\nWait, perhaps notice that the Hamiltonian can be rewritten in terms of angular momentum-like operators. Specifically, treating σ_x as a generator similar to L_x.\n\nNot directly applicable, but sometimes these kinds of couplings lead to effective shifts in parameters.\n\nAlternatively, think of this as a Jaynes-Cummings type model, but again, not exactly matching.\n\nWait, stepping back—I need to compute the ground state energies and wavefunctions. Given that, perhaps approximate methods won't suffice, especially since they ask for exact results.\n\nWait, hang on—is this related to the concept of pseudo-spin in semiconductor heterostructures? Where electric fields cause splitting dependent on position... Hmm, possibly, but I don't remember specifics.\n\nAlternatively, perhaps go into the Heisenberg picture—but that might complicate matters further.\n\nWait, going back to basics. Suppose we take the original Schrödinger equation:\n\n(H₀ ⊗ I + μ (a + a†) σ_x ) Ψ = E Ψ\n\nIf I expand Ψ as a sum over number states and spin states:\n\nΨ = Σ_{n,s} c_n^{s} |n⟩ |s⟩\n\nWith s ∈ {↑, ↓}\n\nPlugging into the SE:\n\nΣ_{n,s} [ H₀ c_n^{s} |n⟩ |s⟩ ] + μ Σ_{n,s} (a + a†)(c_n^{s}) |n⟩ |s'⟩ = E Σ_{n,s} c_n^{s} |n⟩ |s⟩\n\nBreaking this down, for each n and s, equate coefficients.\n\nNote that (a)|n⟩ = √n |n−1⟩, (a†)|n⟩ = √(n+1) |n+1⟩\n\nAlso, σ_x swaps ↑ and ↓.\n\nSo, expanding the second term:\n\nμ (a + a†) σ_x applied to |n⟩|s⟩ gives μ (a +"}
{"id": "Quantum Mechanics/27-3", "questions": "Consider a quantum system described by the Hamiltonian $H = H^0 + H'$, where $H^0 = \\frac{1}{2m}(p_x^2 + p_y^2) + \\frac{1}{2} m \\omega^2 (x^2 + y^2)$ is the two-dimensional harmonic oscillator Hamiltonian, and where the perturbing Hamiltonian is given by $H' = Kxy$. Here, $K$ is a constant. Evaluate the first and second order energy and first order wavefunction corrections to the states whose unperturbed energy (i.e. energy if $K$ were zero) is $2 \\hbar \\omega$. [Hint: Think parity and raising and lowering operators.]\n\nThe following are some useful formulas\n\n$$\n\\int_0^{\\infty} d x x^n e^{-\\alpha x}=n!/ \\alpha^{n+1} \\quad(\\alpha>0), \\quad \\int_{-\\infty}^{\\infty} e^{-x^2}=\\sqrt{\\pi}\n$$\n\n\nFor the one-dimensional simple harmonic oscillator,\n\n$$\n\\begin{aligned}\nx & =\\sqrt{\\hbar / 2 m \\omega}\\left(a^{\\dagger}+a\\right), & p_x & =i \\sqrt{\\hbar m \\omega / 2}\\left(a^{\\dagger}-a\\right), \\\\\na^{\\dagger}|n\\rangle & =\\sqrt{n+1}|n+1\\rangle, & a|n\\rangle & =\\sqrt{n}|n-1\\rangle .\n\\end{aligned}\n$$\n\n\n\n", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about a quantum system with a specific Hamiltonian that's split into an unperturbed part and a perturbation. The goal is to find the first and second-order energy corrections as well as the first-order wavefunction correction for certain states.\n\nFirst off, let me parse what the question says. The total Hamiltonian is \\( H = H^0 + H' \\). The unperturbed Hamiltonian \\( H^0 \\) is a two-dimensional isotropic harmonic oscillator because it has terms involving both \\( x^2 \\) and \\( y^2 \\) symmetrically. So each direction contributes equally. Then there’s a perturbation term \\( H’ = K xy \\).\n\nWe’re supposed to evaluate these corrections for states whose unperturbed energy is \\( 2 \\hbar \\omega \\). Hmm, okay. For a two-dimensional harmonic oscillator, the energy levels are quantized. In general, the energy of such a system is given by:\n\n\\( E_n = \\hbar \\omega (n_x + n_y + 1 ) \\)\n\nWait, actually no—wait, hold on. Wait, more accurately, since it's two-dimensional, we can think of it as having separate oscillators along x and y directions. Each direction would contribute its own quanta. Let me recall—the standard expression is \\( E = \\hbar \\omega(n_x + n_y + 2)/2? No wait—that doesn't sound right either.\n\nActually, maybe better expressed as \\( E = \\hbar \\omega (n_x + n_y + 1) \\times something... Wait, perhaps not exactly. Maybe considering each dimension separately?\n\nLet me clarify. A single particle in a 2D harmonic oscillator potential will have energies depending on the sum of the principal quantum numbers from each direction. Specifically, the energy should be \\( E = \\hbar \\omega (n_x + n_y + 2) \\)? Or is it just multiplied by 2? Wait, no—I need to get precise here.\n\nIn three dimensions, the energy is \\( E = \\hbar \\omega (n_x + n_y + n_z + 3/2) \\). But in two dimensions, it must be similar but without the z-component. Therefore, probably \\( E = \\hbar \\omega (n_x + n_y + 1) \\)—since adding up all the contributions gives you half-harmonic per degree of freedom times twice the number of degrees or something like that.\n\nBut regardless, when they say the unperturbed energy is \\( 2 \\hbar \\omega \\), which suggests that \\( n_x + n_y + ... = 2/\\hbar \\omega * E \\)... Wait, sorry, getting confused.\n\nAlternatively, another approach—if the unperturbed energy is \\( 2 \\hbar \\omega \\), then how does that break down? Since each mode contributes \\( \\hbar \\omega (n_i + 1/2) \\), so for two modes, the ground state is \\( \\hbar \\omega (1/2 + 1/2) = \\hbar \\omega \\). First excited could be different combinations—for example, \\( n_x=1, n_y=0 \\): giving \\( \\hbar \\omega(1 + 1/2 + 1/2)= \\hbar \\omega (2) \\)?\n\nYes! Because each direction adds \\( \\hbar \\omega (n_i + 1/2) \\). Thus, for two dimensions, the total energy is \\( \\sum (\\text{each direction}) \\). Hence, the ground state is \\( \\hbar \\omega (1/2 + 1/2) = \\hbar \\omega \\). If we go higher, say \\( n_x=1, n_y=0 \\), then the energy becomes \\( \\hbar \\omega (1 + 1/2 + 1/2) = 2 \\hbar \\omega \\). Similarly, any combination where \\( n_x + n_y = 1 \\) would give us \\( 2 \\hbar \\omega \\). That makes sense.\n\nSo the states with unperturbed energy \\( 2 \\hbar \\omega \\) correspond to those with \\( n_x + n_y = 1 \\). These include four possible states: |1,0>, |0,1>. Wait, hang on—is that correct? Actually, in two dimensions, the degeneracy comes from permutations of the same exponents. So yes, for \\( n_x + n_y = 1 \\), we have two possibilities: (1,0) and (0,1). However, sometimes people consider spin or other factors, but in pure HO, it's only spatial parts. So yeah, two-fold degenerate.\n\nTherefore, our initial states are the ones with \\( n_x + n_y = 1 \\), meaning their unperturbed energy is \\( 2 \\hbar \\omega \\). Now, moving forward—we're dealing with time-independent perturbation theory here.\n\nGiven that, the plan is to compute the first-order energy shift, the second-order energy shifts, and also the first-order change in the wavefunctions due to the perturbation \\( H' = K xy \\).\n\nI remember that in non-degenerate PT, the first-order energy shift is simply <ψ₀|H'|ψ₀>, while the second-order involves sums over intermediate states. But in this case, though, the original states are degenerate—they form a doublet. So do I need to use degenerate perturbation theory instead?\n\nAh, yes, absolutely. When the unperturbed states are degenerate, regular non-degenerate PT isn’t sufficient; we have to diagonalize the matrix formed by the perturbation within the subspace of the degenerate states.\n\nThus, my next thought is to construct the matrix representation of \\( H' \\) in the basis of the unperturbed states with energy \\( 2 \\hbar \\omega \\). Those being |1,0⟩ and |0,1⟩.\n\nSo, let me write out the matrix elements ⟨m|H'|n⟩ where m,n ∈ { |1,0⟩ , |0,1⟩ }.\n\nSince \\( H' = K xy \\), and using ladder operator expressions for x and y might help. From the hint, thinking about parity and raising/lowering operators seems important.\n\nRecall that for the 1D SHO, position operator x is proportional to \\( a^\\dagger + a \\), similarly for momentum. So, in 2D, x and y act independently.\n\nExpressing x and y in terms of ladder operators:\n\n\\( x = \\sqrt{\\frac{\\hbar}{2 m \\omega}} (a_x^\\dagger + a_x) \\)\nSimilarly,\n\\( y = \\sqrt{\\frac{\\hbar}{2 m \\omega}} (a_y^\\dagger + a_y) \\)\n\nThen, \\( H' = K xy = K \\cdot \\sqrt{\\frac{\\hbar}{2 m \\omega}}^2 (a_x^\\dagger + a_x)(a_y^\\dagger + a_y) \\)\n\nSimplify constants:\n\\( K \\cdot \\frac{\\hbar}{2 m \\omega} (a_x^\\dagger + a_x)(a_y^\\dagger + a_y) \\)\n\nNow, expanding the product inside:\n\n\\( (a_x^\\dagger + a_x)(a_y^\\dagger + a_y) = a_x^\\dagger a_y^\\dagger + a_x^\\dagger a_y + a_x a_y^\\dagger + a_x a_y \\)\n\nHmm, interesting. So now, \\( H' \\) consists of four terms. To calculate the matrix elements between |1,0⟩ and |0,1⟩, we'll apply these operators.\n\nOur basis vectors are |n_x, n_y⟩, specifically |1,0⟩ and |0,1⟩.\n\nCompute ⟨1,0"}
{"id": "Quantum Mechanics/24-2", "questions": "We now know that neutrinos have mass so we can talk about their rest frame. In a simple model with only two neutrino flavors (electron and muon neutrino), neutrino flavor states do not have definite mass and the electron neutrino has equal amplitude for having mass $m_1$ and $m_2$ and is the symmetric state. Suppose we have an electron neutrino at $t = 0$.\n\n(a) What is the probability that the neutrino will be an electron neutrino at arbitrary time $t > 0$?\n\n(b) How long does it take for the electron neutrino to change completely into a muon neutrino?\n\n(c) What is the probability of measuring a mass of $m_1$ as a function of time?", "graphs": null, "llm_answers": "<think>\nOkay, I need to solve these three parts related to neutrino oscillations. Let me start from part (a). \n\nSo, first, I remember that neutrinos come in different flavors: electron, muon, tau. But they also have masses m₁, m₂, etc., which correspond to different eigenstates. The flavor states like ν_e aren't the same as the mass eigenstates; instead, they're superpositions of them. That's why neutrino oscillation happens—because when you produce a flavor state, its components propagate differently due to their different masses, leading to mixing over time.\n\nIn this problem, there are only two flavors: electron and muon neutrinos. So, let's consider just two mass eigenstates, say |ν₁⟩ and |ν₂⟩. The electron neutrino is given as a symmetric combination of these two. Hmm, wait—the question says \"the electron neutrino has equal amplitude for having mass m₁ and m₂.\" So, mathematically, that would mean:\n\n|ν_e⟩ = (|ν₁⟩ + |ν₂⟩)/√2\n\nSimilarly, since there are only two flavors, the muon neutrino must then be another orthogonal combination. Since the coefficients should form an orthonormal basis, probably something like:\n\n|ν_mu⟩ = (|ν₁⟩ - |ν₂⟩)/(i√2)\n\nWait, but sometimes people use real combinations too. Maybe without the i factor. Wait, actually, if we don't include CP violation, maybe both could be real. Alternatively, perhaps using complex phases might complicate things here. Given that the problem doesn’t mention any CP violation, maybe it’s safe to assume real coefficients.\n\nBut hold on—if the electron neutrino is equally split between m₁ and m₂, then yes, it's (|ν₁> + |ν₂>)/sqrt(2).\n\nNow, considering propagation. Each mass eigenstate propagates independently. If the neutrino starts off as |ν_e>, after some time t, each component evolves phase factors based on their energies/masses.\n\nAssuming relativistic particles, energy E ≈ pc, where p is momentum. For massive particles, E² = p²c⁴ + m²c^4. Assuming high-energy approximation, E ≈ pc * [1 + (m² c³)/(2p²)]... Or more accurately, the difference in energies comes mainly from the mass term.\n\nThe phase acquired by each mass eigenstate during flight is φ_i = (E_i / ħ) t. Using natural units where ħ=1 and c=1 simplifies calculations, right? Because otherwise, keeping track of all constants gets messy.\n\nLet me switch to natural units temporarily. Then, E_i = sqrt(p² + m_i²). However, because neutrinos are ultra-relativistic, p >> m_i, so E_i ≈ p + m_i²/(2p). Therefore, the difference in energies ΔE ≈ (m₂² - m₁²)/(2p).\n\nThus, the relative phase between the two mass eigenstates after time t is approximately:\n\nΔφ = (E₂ - E₁)t ≈ [(m₂² - m₁²)/(2p)] * t\n\nAlternatively, often written as Δφ = (Δm² L)/(4E), where L is the distance traveled, and E is the average energy. Here, though, we’re dealing with time rather than distance, so substituting L = v*t, but since velocity v≈c=1 in natural units, L=t*something? Wait no—in natural units, length and time have the same dimension, so L = ct, meaning t=L/c. But again, in natural units, c=1, so t=L.\n\nTherefore, Δφ becomes proportional to Δm² t / (4E). But unless told otherwise, perhaps assuming constant energy isn't necessary yet.\n\nWait, going back. At t=0, our initial state is |ψ(0)> = |ν_e> = (|ν₁> + |ν₂>)/√2.\n\nAfter time t, each component picks up a phase e^{-i E_i t}. Thus,\n\n|ψ(t)> = (e^{-i E₁ t} |ν₁> + e^{-i E₂ t} |ν₂>) / √2\n\nTo find the probability P_{ee}(t)—probability of being detected as electron neutrino—we compute |⟨ν_e | ψ(t) ⟩|^2.\n\nSince ⟨ν_e | corresponds to (⟨ν₁| + ⟨ν₂| )/√2, taking inner product gives:\n\n⟨ν_e | ψ(t) ⟩ = [⟨ν₁| + ⟨ν₂| ] / √2 * [e^{-i E₁ t}|ν₁> + e^{-i E₂ t}|ν₂>] / √2\n\nWhich equals [e^{-i E₁ t} + e^{-i E₂ t}] / 2\n\nTaking modulus squared:\n\nP_{ee}(t) = [|e^{-i E₁ t} + e^{-i E₂ t}|²] / 4\n\nSimplify numerator:\n\n|e^{iθ} + e^{iφ}|² = 2 + 2 cos(θ-φ)\n\nHere θ=-E₁ t, φ=-E₂ t, so θ - φ = -(E₁ - E₂) t = (E₂ - E₁) t\n\nHence,\n\nNumerator = 2[1 + cos((E₂ - E₁) t)]\n\nDivide by 4:\n\nP_{ee}(t) = [1 + cos((E₂ - E₁) t)] / 2\n\nHmm, okay. Now, what is (E₂ - E₁)? Earlier, under relativistic assumption, E_i ≈ p + m_i²/(2p). Hence, E₂ - E₁ ≈ (m₂² - m₁²)/(2p). Let me denote Δm² = m₂² - m₁², so E₂ - E₁ ≈ Δm²/(2p).\n\nAlso, note that in particle physics experiments, one often uses the quantity L/E, where L is baseline (distance), E is energy. But here, we've got time t. Since L = vt, and v ≈ c=1, so L ≈ t. Also, E ≈ p, so p ≈ E.\n\nPutting together, E₂ - E₁ ≈ Δm²/(2E). Therefore, (E₂ - E₁) t ≈ (Δm² t)/(2E)\n\nSubstituting back into P_{ee}(t):\n\nP_{ee}(t) = [1 + cos(Δm² t / (2E))] / 2\n\nThat seems familiar—it matches standard neutrino oscillation formulas except usually expressed in terms of x = L/(4π E / Δm²)} or similar.\n\nHowever, the exact expression depends on whether we express everything in terms of time or distance. Since the question asks for general t, we'll keep it as above.\n\nIs there anything else needed? Well, depending on how precise we want to get, but since the problem didn't specify particular approximations beyond knowing that neutrinos have mass, I think this formula suffices.\n\nMoving on to part (b): How long does it take for the electron neutrino to change completely into a muon neutrino?\n\nComplete conversion implies zero probability of remaining electron neutrino—that occurs when P_{ee}=0. From part (a),\n\nSet P_{ee}(t)=0 => [1 + cos(Δm² t / (2E))]/2 = 0 ⇒ 1 + cos(...) = 0 ⇒ cos(...)= -1\n\nWhen does cosine equal minus one? When argument is π modulo 2π. So,\n\nΔm² t / (2E) = π + 2kπ, k integer.\n\nMinimum positive time is when k=0:\n\nt_min = (2E π)/Δm²\n\nBut wait—isn't complete conversion possible? Actually, in reality, neutrino oscillations are periodic, so maximum transition occurs periodically every half-period. To go entirely into muon neutrino, the system needs to reach the point where the electron component vanishes.\n\nGiven that, yeah, the minimal time is when the phase shift reaches pi radians.\n\nSo, t = (2E π)/Δm²\n\nBut hang on—I thought earlier that the period was T such that sin^2(theta) completes a cycle. Wait, but in our case, the probability goes through maxima and minima. Specifically, P_{ee} varies sinusoidally between 0 and 1.\n\nYes, so the maximal disappearance occurs at times when the argument inside cosine is odd multiples of pi. So indeed, the first"}
{"id": "Quantum Mechanics/26-1", "questions": "(a) What is the central field approximation? Explain why, to leading order, the possible states for a single electron can be labelled by the three integers $(n, \\ell, m_\\ell)$ and by $m_s$, which is half integral, giving the significance of these four quantities.\n\n(b) Draw a diagram of the first four periods of the Periodic Table, with a square representing each element. There is no need to name each element. How does quantum mechanics explain this pattern of elements? For a given shell, why do electrons first fill states of lowest $\\ell$?\n\n(c) That is the LS coupling scheme? In this scheme, what quantum numbers are used to describe the state of the atomic electrons? For carbon $(Z = 6)$ give\n\n1. the electron configuration,\n2. the possible terms which are allowed by the Pauli Principle. Which is the true ground state? (Use spectroscopic notation).", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about quantum mechanics and the periodic table. Let me try to work through it step by step. \n\nStarting with part (a): The question asks about the central field approximation. Hmm, from my notes, I remember that atoms are complex systems because there's more than one electron moving around the nucleus. Each electron not only feels the Coulomb attraction from the nucleus but also repulsion from other electrons. But solving the Schrödinger equation exactly for such multi-electron systems is really hard. So physicists use approximations to simplify things.\n\nThe central field approximation must be one such simplification. It probably assumes that each electron moves independently in some average potential created by all the protons in the nucleus and maybe even the screening effect due to other electrons. Wait, isn't that similar to how we model hydrogen-like atoms where you adjust Z based on shielding? Yeah, so instead of dealing with multiple interactions between electrons, they approximate the effective nuclear charge as if each electron experiences just an averaged-out or \"central\" electric field.\n\nSo under this approximation, each electron behaves almost like it's in a hydrogen atom but with an adjusted nuclear charge. This allows us to label their states using n, l, ml, and ms. These are the principal, azimuthal, magnetic, and spin quantum numbers respectively. Since each electron has its own set of these quantum numbers, except for the fact that two electrons can share the same orbital (same n, l, ml), differing only in spin (ms being ±½).\n\nWait, but why specifically those four quantum numbers? Well, in the absence of external fields, the Hamiltonian would depend only on r, right? Because the potential is spherically symmetric. Therefore, angular momentum operators commute with the Hamiltonian, meaning ℓ² and Lz (which gives ml) are good quantum numbers along with n. Spin comes into play separately since it doesn't interact directly unless there's something breaking spherical symmetry, like a magnetic field. But without that, spin remains independent.\n\nBut wait, actually, when considering identical particles, especially fermions like electrons, the total wavefunction needs to include both spatial and spin parts. However, in the central field approximation, the spatial part alone determines most properties, while spin contributes via exchange effects and the Pauli exclusion principle. Still, labeling each state requires knowing n, l, ml, and then whether the spin is up or down.\n\nMoving on to part (b): Drawing the first four periods... Okay, period 1: H and He; period 2: Li to Ne; period 3: Na to Ar; period 4: K to Ca and Sc to Ti, etc., ending with Kr. But the exact filling depends on electron configurations following the Aufbau principle—filling lower energy levels first.\n\nQuantum mechanically, the energy levels aren't solely determined by n anymore because different l values within the same n lead to subshells with slightly different energies. Oh yeah, the Madelung rule says that orbitals are filled in order of increasing n + l, and for equal sums, the ones with smaller n come first. Hence, p-orbitals start getting filled after certain points, explaining why the second period ends at neon before starting again with sodium’s [Ne]3s¹.\n\nAs for why electrons first fill states of lowest l for a given shell—that relates to the idea that higher l corresponds to higher energy within the same n. Or was it Hund's rule? No, Hund's is about maximizing spin multiplicity. Maybe it's because the kinetic energy term involves l(l+1)/r², so higher l means higher energy contribution. Thus, lower l states are energetically favorable and get filled first.\n\nPart (c): LS coupling scheme. From memory, LS stands for Russell-Saunders coupling, where the individual electron spins couple together to form S, and the orbital angular momenta combine to form L. Then J is formed by adding L and S. So the quantum numbers involved here are n, l, s, j—but wait, per electron, but collectively, perhaps N/A. Actually, for the entire atom, the total L and S are considered, and then J = |L - S| to L + S.\n\nFor carbon, Z=6. Its electron configuration should follow the usual building-up process. Carbon has six electrons. Filling starts with 1s², then 2s², then 2p². So the configuration is 1s² 2s² 2p². Now, writing out the possible terms—the term symbol is built from L and S. First, find all possible ways to add the individual angular momenta.\n\nEach electron contributes either l=0 (for s) or l=1 (for p). We have two p electrons. Their combined L could be 0, 1, or 2. Similarly, their spins can combine to S=0 or S=1. But according to the Pauli exclusion principle, the overall wavefunction must be antisymmetric. Also, for two electrons in the same orbital, their spins must be opposite—if they were in the same orbital, but in our case, they're in separate p orbitals.\n\nWait, hold on. Two electrons in the 2p subshell. They occupy different orbitals, so their spatial wavefunctions don’t require anti-symmetry beyond what's already imposed by the Pauli principle. So let's compute the possible combinations:\n\nFirstly, combining the two p electrons' angular momenta:\n- If both are aligned (parallel), L_p = 1 + 1 = 2.\n- If one is positive and the other negative, L_p = 0.\n- Alternatively, triplet and singlet states for spin.\n\nSimilarly, for spin:\n- Both parallel: S = 1 (triplet).\n- Opposite: S = 0 (singlet).\n\nNow, the possible couplings between L and S will determine the terms. Remember that the Landé formula applies here.\n\nLet me list them systematically.\n\nCase 1: Total Orbital Angular Momentum L = 2 (both p electrons aligned)\nPossible S: 0 or 1?\nNo, wait, for two electrons, maximum S is 1. So for L=2, S can be 0 or 1.\n\nThus, possible terms are ^{2S+1}L_J, where J ranges over |L - S| to L + S.\n\nIf L=2, S=1: Term symbols are ^3P_{J}, where J can be 1, 2, 3.\n\nAnd if L=2, S=0: Term symbol is ^1D_{2}. Wait, D is l=2, yes.\n\nCase 2: Total Orbital Angular Momentum L = 1 (one p electron pointing up, another down)\n\nThen, S can still be 0 or 1.\n\nSo for L=1, S=1: Term symbol is ^3P_{J}, with J=0,1,2.\n\nAnd for L=1, S=0: Term symbol is ^1P_1.\n\nCase 3: Total Orbital Angular Momentum L = 0 (all paired oppositely)\n\nHere, S can be 0 or 1.\n\nBut wait, if L=0, then the term becomes S-only. So for L=0, S=1: ^3S_1.\n\nAnd L=0, S=0: Not applicable because two electrons cannot have S=0 if they’re in the same orbital? Wait, no—they’re in different orbitals, so their spins can pair. So S=0 is possible.\n\nTherefore, term symbols would be ^1S_0 and ^3S_1.\n\nPutting it all together, the possible terms are:\n\nFrom Case 1:\n^3F_2,3,4 (wait, no, earlier thought said P. Hold on.)\n\nWait, correction: When L=2, the letter is G? No, letters go S(0), P(1), D(2), F(3)... So L=2 is D. So for L=2, S=1: ^3D_1,2,3.\n\nWait, now I'm confused. Earlier I wrote ^3P, but that might be incorrect.\n\nHold on, better approach: For two p electrons, each with l=1. Combining two l=1 gives possible L=0,1,2.\n\nWhen L=2, the corresponding letter is D.\n\nWhen L=1, it's P.\n\nWhen L=0, it's S.\n\nHence, correcting myself:\n\nCase 1: L=2, S=1 → ^3D_1,2,3.\n\nAlso, L=2, S"}
{"id": "Quantum Mechanics/24-3", "questions": "Coherent states of the harmonic oscillator saturate the uncertainty bound (just as the ground state does, but none of the excited states do.)\n\n(a) Prove that the raising operator has no normalizable eigenfunctions, whereas the lowering operator does. A coherent state, $ |\\alpha \\rangle $, is one such that $ a_- |\\alpha\\rangle = \\alpha |\\alpha\\rangle $, where $\\alpha \\in \\mathbb{C}$.\n\n(b) Calculate $\\langle x \\rangle$, $\\langle x^2 \\rangle$, $\\langle p \\rangle$ and $\\langle p^2 \\rangle$ in the state $ |\\alpha\\rangle $.\n\n(c) Find $\\sigma_x$ and $\\sigma_p$ and show that the uncertainty is saturated.\n\n(d) Show that the coefficients in the expansion of $ |\\alpha\\rangle $ in terms of energy eigenstates are $ c_n = \\alpha^n c_0/\\sqrt{n!} $ and determine $ c_0 $ by normalizing $ |\\alpha\\rangle $.\n\n(e) Now put in the time dependence and show that $ |\\alpha(t)\\rangle $ remains an eigenstate of $ a_- $, but with a time-evolving eigenvalue $ a(t) = e^{-i\\omega t} \\alpha $.\n\n(f) Is the ground state itself a coherent state? If so, what is the eigenvalue?", "graphs": null, "llm_answers": "<think>\nAlright, I have this problem about coherent states for the quantum harmonic oscillator. It's part (a), which asks me to prove that the raising operator \\( a_+ \\) doesn't have any normalizable eigenfunctions, while the lowering operator \\( a_- \\) does. Coherent states are defined here as those satisfying \\( a_- | \\alpha \\rangle = \\alpha | \\alpha \\rangle \\), where \\( \\alpha \\) is a complex number.\n\nOkay, let's start by recalling some basics about ladder operators. The annihilation operator \\( a_- \\) lowers the state by one level, taking \\( |n\\rangle \\) to \\( |n-1\\rangle \\). Similarly, the creation operator \\( a_+ \\) raises it up. For these operators, we know their commutation relation: \\( [a_-, a_+] = 1 \\).\n\nNow, if there were an eigenfunction \\( | \\psi \\rangle \\) of \\( a_+ \\), then \\( a_+ | \\psi \\rangle = \\beta | \\psi \\rangle \\) for some eigenvalue \\( \\beta \\). Let me think about whether such a state can be normalized.\n\nSuppose \\( | \\psi \\rangle \\) exists. Then applying \\( a_+ \\) repeatedly would give higher and higher energy states:\n\n\\( a_+ | \\psi \\rangle = \\beta | \\psi \\rangle \\)\n\n\\( a_+^2 | \\psi \\rangle = \\beta a_+ | \\psi \\rangle = \\beta^2 | \\psi \\rangle \\)\n\nAnd so on. So each application gives another multiple of \\( | \\psi \\rangle \\). But wait, when you apply \\( a_+ \\) enough times, say n times, you get something proportional to \\( |n + k \\rangle \\) where k increases without bound because each application adds a photon or excitation. \n\nBut how does normalization work here? The norm squared of \\( | \\psi \\rangle \\) must converge. However, since each successive application of \\( a_+ \\) creates a new orthogonal state scaled by \\( \\beta \\), unless \\( \\beta = 0 \\), the series won't terminate. And even if \\( \\beta = 0 \\), that just means \\( a_+ | \\psi \\rangle = 0 \\), implying \\( | \\psi \\rangle \\) is annihilated by \\( a_+ \\), meaning it's the vacuum state \\( |0\\rangle \\). Wait, actually, hold on—no, the vacuum state satisfies \\( a_- |0\\rangle = 0 \\), not \\( a_+ \\). Hmm, maybe my initial thought isn't correct.\n\nWait, perhaps more carefully—if \\( | \\psi \\rangle \\) is an eigenvector of \\( a_+ \\), then starting from \\( | \\psi \\rangle \\), repeated applications will generate all higher states. Since the Hilbert space is countably infinite-dimensional, having a single vector whose orbit under \\( a_+ \\) spans infinitely many dimensions suggests non-normalizability. Because the inner product would involve summing over squares of coefficients going off to infinity, making convergence impossible unless all except finitely many coefficients vanish—which they don’t here.\n\nAlternatively, using the fact that \\( a_+^\\dagger = a_- \\), suppose \\( | \\psi \\rangle \\) is an eigenstate of \\( a_+ \\): \\( a_+ | \\psi \\rangle = \\beta | \\psi \\rangle \\). Taking adjoints, \\( \\langle \\psi | a_- = \\overline{\\beta} \\langle \\psi | \\). Applying both sides to \\( | \\psi \\rangle \\):\n\nLeft side: \\( \\langle \\psi | a_- | \\psi \\rangle = \\overline{\\beta} \\langle \\psi | \\psi \\rangle \\)\nRight side: \\( \\overline{\\beta} \\langle \\psi | \\psi \\rangle \\)\n\nSo equality holds trivially; nothing wrong yet. Maybe consider the expectation value of the Hamiltonian?\n\nThe Hamiltonian is \\( H = \\hbar \\omega (a_+ a_- + 1/2 ) \\). Suppose \\( | \\psi \\rangle \\) is an eigenstate of \\( a_+ \\); compute \\( \\langle H \\rangle \\).\n\nSince \\( a_+ | \\psi \\rangle = \\beta | \\psi \\rangle \\), multiplying both sides by \\( a_- \\):\n\\( a_- a_+ | \\psi \\rangle = \\beta a_- | \\psi \\rangle \\).\nWhich implies:\n\\( (N + 1)| \\psi \\rangle = \\beta a_- | \\psi \\rangle \\),\nwhere N is the number operator.\n\nHmm, but unless \\( a_- | \\psi \\rangle \\) relates back to \\( | \\psi \\rangle \\), this might complicate things. Alternatively, take the expectation of H:\n\n\\( \\langle H \\rangle = \\frac{\\hbar \\omega}{2} + \\langle \\psi | a_+ a_- | \\psi \\rangle \\).\n\nCompute \\( a_+ a_- | \\psi \\rangle = a_+ (\\beta' | \\phi \\rangle ) \\)? Not sure. Perhaps better approach: Assume \\( | \\psi \\rangle \\) is square-integrable. Then,\n\nIf \\( a_+ | \\psi \\rangle = \\beta | \\psi \\rangle \\),\n\nThen \\( || a_+ | \\psi \\rangle ||^2 = | \\beta |^2 || | \\psi \\rangle ||^2 \\).\n\nOn the other hand, \\( || a_+ | \\psi \\rangle ||^2 = \\langle \\psi | a_- a_+ | \\psi \\rangle = \\langle \\psi | (N + 1) | \\psi \\rangle \\geq || | \\psi \\rangle ||^2 \\), since N counts quanta.\n\nThus, \\( | \\beta |^2 || | \\psi \\rangle ||^2 \\geq || | \\psi \\rangle ||^2 \\implies | \\beta |^2 \\geq 1 \\).\n\nSimilarly, considering \\( a_- | \\psi \\rangle \\), though \\( | \\psi \\rangle \\) may not necessarily be related via \\( a_- \\).\n\nWait, but if \\( | \\psi \\rangle \\) is an eigenstate of \\( a_+ \\), then \\( a_- | \\psi \\rangle \\) could relate somehow. Or maybe use induction.\n\nAssume \\( | \\psi \\rangle \\) is normalized. Apply \\( a_+ \\) once: \\( a_+ | \\psi \\rangle = \\beta | \\psi \\rangle \\). Take norms: \\( || a_+ | \\psi \\rangle || = | \\beta | || | \\psi \\rangle || = | \\beta | \\leq ... ? No, earlier inequality says \\( | \\beta | \\geq 1 \\).\n\nApply again: \\( a_+^2 | \\psi \\rangle = \\beta^2 | \\psi \\rangle \\). Norms: \\( || a_+^2 | \\psi \\rangle || = | \\beta |^2 || | \\psi \\rangle || \\). Also, \\( || a_+^2 | \\psi \\rangle || = || a_+ (a_+ | \\psi \\rangle ) || = || a_+ ( \\beta | \\psi \\rangle ) || = | \\beta | || a_+ | \\psi \\rangle || = | \\beta |^2 || | \\psi \\rangle || \\). Which matches.\n\nHowever, physically, creating particles ad infinitum leads to unbounded growth, hence the state cannot be normalizable because its components spread out into ever-higher energy levels, leading to divergent sums when computing the norm.\n\nTherefore, only possible way for \\( | \\psi \\rangle \\) to exist is if \\( \\beta = 0 \\), but then \\( a_+ | \\psi \\rangle = 0 \\), which would imply \\( | \\psi \\rangle \\) is the highest weight state—but in QM, our Fock space starts at zero and goes upwards indefinitely, so there’s no maximum n. Hence, no finite β allows a normalizable eigenstate"}
{"id": "Quantum Mechanics/28-1", "questions": "(a) Use the normalized Gaussian trial function $\\psi_t(x) = (\\beta/\\pi)^{1/4}e^{-\\beta x^2/2}$ to obtain the best upper  \nbound on the ground state energy for the n-dependent family of potentials $V(x) = \\lambda x^{2n}, \\  \nn = 1, 2, 3 \\ldots$\n\n(b) Compare your bound with the known exact result for $n = 1$.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem where I need to use a Gaussian trial wavefunction to find the best upper bound on the ground state energy for these potentials V(x) = λx^(2n). Then, compare it with the exact result when n=1.\n\nFirst off, part (a): The potential is given as V(x)=λx²ⁿ, which means for different values of n, we get different types of potentials. For example, if n=1, that's just a harmonic oscillator potential; n=2 would be something like quartic, etc. But regardless, they all seem symmetric about zero because even powers of x make them positive everywhere except maybe at zero depending on λ.\n\nThe trial function provided is ψₜ(x)=(β/π)^(1/4)e^(-βx²/2), right? So that’s a Gaussian centered at zero with width parameter β. Since it's normalized, integrating |ψ|² over all space should give one, but let me check: ∫(β/π)^(1/2)e^(-βx²) dx from -infty to infinity equals sqrt(beta/pi)*sqrt(pi/beta)}=1. Yep, normalization holds.\n\nNow, since we're dealing with quantum mechanics here, specifically finding an upper bound on the ground state energy using variational method. Variational principle says that the expectation value of the Hamiltonian with any trial wavefunction gives an upper limit to the true ground state energy. To get the \"best\" upper bound, we'll minimize this expectation value with respect to the parameter beta.\n\nSo first, write down the expression for ⟨H⟩:\n\n⟨H⟩ = ⟨T + V⟩\n\nWhere T is kinetic energy operator (-ħ²/(2m))d²/dx², and V is our potential λx²ⁿ.\n\nBut wait, actually, in many cases people set ħ=m=ω=1 or similar units for simplicity. Let me see what constants are involved here. Wait, no specific mass m is mentioned, nor Planck constant hbar. Maybe assuming natural units where ħ=1 and m=1?\n\nWait, looking back at the question, probably yes—since otherwise there might be more parameters. Hmm, perhaps not necessarily, but without loss of generality, sometimes problems assume certain unit systems. Alternatively, maybe express everything symbolically keeping those terms.\n\nAlternatively, perhaps better to proceed formally, treating H as K.E. plus V.E., each expressed properly.\n\nLet me think again. If I don't know whether ħ and m are specified, then perhaps keep them general. However, often in such problems, unless stated otherwise, you can take ħ=1 and m=1 for simplification. Or else, the expressions become too messy. Given that the trial function has e^{-βx²/2}, which suggests some dimensionless combination inside exponent, likely involving ħ and m.\n\nWait, hold on—if the standard Gaussian trial function is usually written as exp(-(mω/(2ħ))x²), so comparing that to ours, which is exp(-βx²/2), implies that β must relate to m, ω, and ħ somehow. Therefore, possibly setting up variables accordingly.\n\nGiven that, perhaps define β in terms of other physical quantities. But since the problem doesn’t specify, maybe treat β as arbitrary variable independent of others, and carry out symbolic computation.\n\nAlright, moving forward.\n\nCompute ⟨H⟩ = ⟨T⟩ + ⟨V⟩.\n\nWe need to compute both expectations separately.\n\nStarting with ⟨T⟩, the kinetic energy expectation.\n\nIn position representation,\n\n⟨T⟩ = ∫_{-∞}^∞ [ψ*(x)] (-ħ²/(2m)) d²ψ/dx² dx\n\nSince ψ is real, ψ* = ψ, so:\n\n⟨T⟩ = (√(β/π)/√[something])... Wait, no, let me recall how derivatives work for Gaussians.\n\nActually, derivative of e^{-ax²} is proportional to xe^{-ax²}. Second derivative will involve another factor of x squared minus a term.\n\nSpecifically, f''(x) = d²/dx² e^{-αx²} = (4α²x² - 2α) e^{-alpha x²}\n\nTherefore, applying that formula, with alpha being beta/2.\n\nThus, second derivative of psi is [(4*(beta/2)^2)x² - 2*(beta/2)] * e^{-beta x² /2 }\n\nSimplify:\n\n4*(beta²/4)x² - beta => (beta² x² - beta)\n\nMultiply by e^{-beta x² /2 }, so overall:\n\nf''(x) = (beta² x² - beta) e^{-beta x² /2 }\n\nThen plug into ⟨T⟩:\n\n⟨T⟩ = integral from -inf to inf of [ (beta/π)^(1/4) e^{-beta x² /2 } ]*(-ħ²/(2m))*(beta² x² - beta ) [ (beta/π)^(1/4) e^{-beta x² /2 }] dx\n\nMultiplying together, the exponential becomes e^{-beta x² }, coefficients multiply as well.\n\nCoefficients outside integrals:\n\n( (beta/π)^(1/4) )^2 = (beta/π)^(1/2 )\n\nAnd multiplied by (-ħ²)/(2m):\n\nSo coefficient is -(ħ²/(2m))(beta/π)^(1/2 )\n\nInside the integral: (beta² x² - beta ) e^{-beta x² } dx\n\nNote also that due to symmetry, only even functions contribute, so the cross terms may vanish—but in this case, the entire integrand is already even, so okay.\n\nSplitting the integral into two parts:\n\nIntegral of beta² x² e^{-beta x² } dx - Integral of beta e^{-beta x² } dx\n\nEach integrated from -infinity to infinity.\n\nRecall that ∫_{-infty}^\\infty x² e^{-a x²} dx = sqrt(pi/a³)*(1/2)(1/sqrt(a))^3 ? Wait, no—the standard results are:\n\n∫_{0}^\\infty x^{2k} e^{-a x²} dx = (1/2) a^{-(k+1/2)} Gamma(k + 1/2 )\n\nFor k integer >=0.\n\nGamma(n) = (n-1)! for integers.\n\nSo for k=1, Γ(3/2)= (1/2)sqrt(pi).\n\nSimilarly, for k=0, Γ(1/2)=sqrt(pi).\n\nAlso note that ∫_{-infty}^\\infty x^{2k} e^{-a x²}dx = 2 ∫_0^\\infty ... , hence equal to 2*(1/2)a^{-(k+1/2)}Γ(k+1/2) )= a^{-(k+1/2)}Γ(k+1/2 ).\n\nSo for the first integral, beta² times ∫x² e^{-beta x²} dx = beta² * [ (beta)^{-3/2} sqrt(pi)/2 ]\n\nWait, let me do it carefully.\n\nSet a = beta.\n\nThen ∫_{-infty}^\\infty x² e^{-a x²} dx = (sqrt(pi))/(a^{3/2}) ) * (1/2) ?\n\nNo, wait, correction:\n\nFrom above, ∫_{-infty}^\\infty x^{2k} e^{-a x²} dx = (sqrt(pi)/a^{k + 1/2 }) ) * (1/2^{k}) ) * double factorial stuff? No, perhaps better to look directly.\n\nI remember that ∫_{-infty}^\\infty x² e^{-a x²} dx = (sqrt(pi)/a^{3/2})(1/2) ?? Not sure.\n\nWait, alternative approach: differentiate generating function.\n\nConsider G(t) = ∫_{-infty}^\\infty e^{-t x²} dx = sqrt(pi/t ). Differentiate w.r.t t twice.\n\nG'(t) = - (1/2) sqrt(pi) t^{-3/2 }\n\nG''(t) = (3/4) sqrt(pi) t^{-5/2 }\n\nOn the other hand, G''(t) = ∫_{-infty}^\\infty x⁴ e^{-t x²} dx\n\nHmm, but we want ∫x² e^{-a x²} dx.\n\nWhich is related to G’(t):\n\nBecause G’(t) = ∫ x² e^{-t x²} (-1) dt ??? Wait, no.\n\nWait, differentiation under integral sign:\n\ndG/dt = ∫_{-infty}^\\infty (-x²) e^{-t x²} dx = - ∫ x² e^{-tx²} dx\n\nThus, ∫ x² e^{-tx²} dx = - dG/dt\n\nCompute G(t) = sqrt(pi/t ), so d"}
{"id": "Quantum Mechanics/30-2", "questions": "Calculate, to within a multiplicative constant, the differential cross section for scattering of particles of momentum *p* in a potential which can be represented by a delta function, $V(r) = a\\delta^3(r)$, where *a* is a constant. Use the Born approximation or time-dependent perturbation theory and the Fermi golden rule. Show that a similar energy and angular dependence is obtained for differential cross sections in any finite range of potential at \"low\" momentum. Give the condition on \"low\" momentum for this result to follow.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about calculating the differential cross section for particle scattering off a delta function potential using either the Born approximation or time-dependent perturbation theory with the Fermi Golden Rule. Hmm, let me think through how to approach this.\n\nFirst, I remember that when dealing with scattering problems, especially in quantum mechanics, the Born approximation is often used as a first-order method because it simplifies things without getting too bogged down in complicated integrals. The Born approximation works best when the potential isn't too strong and has a short range—so maybe that's why we're considering a delta function here since it's very localized.\n\nThe given potential is V(r) = a δ³(r), right? So it's zero everywhere except exactly at r=0, but there it's infinitely large. That should make some calculations easier because integrating over such a potential might collapse into just evaluating something at a point.\n\nI need to find the differential cross section dσ/dΩ. From what I recall, the differential cross section relates to the probability density of scattered particles per unit solid angle. In the context of scattering amplitudes, once you have f(θ), the differential cross section is |f(θ)|² times some constants depending on units.\n\nSo, probably I'll start by finding the scattering amplitude f(k', k). Since the potential is spherically symmetric (because delta function depends only on r), the scattering amplitude will depend only on the angles between initial and final momenta, i.e., θ.\n\nIn the Born approximation, the scattering amplitude is approximately:\n\nf(θ) ≈ -m/(2πħ²) ∫ e^{i q · r} V(r) dr\n\nwhere q = k' - k is the wave vector transfer, m is mass, ħ is Planck's reduced constant, and k, k' are the incoming and outgoing wave vectors respectively.\n\nBut wait, actually, more precisely, the Born approximation formula is:\n\nf(Born)(k', k) = -(4π/ħ²) ∫ [e^{-i k·r'} / 4π] V(r') [e^{+i k'·r''}] dr'\n\nWait no, perhaps another way: Let me check my notes... Oh yeah, sometimes written differently based on conventions.\n\nAlternatively, from memory, the first Born approximation gives the scattering amplitude as:\n\nf(θ) = (-1)/(8π) ∫ e^{i q · r} V(r) dr,\n\nbut I'm not sure if that includes all factors correctly. Maybe better to go back to basics.\n\nLet’s consider non-relativistic quantum mechanics. For elastic scattering, the T-matrix up to first order (Born approximation) is given by:\n\nT^(1)(E) = -∫ G₀⁺(E) V dr,\n\nwhere G₀⁺ is the free Green's function. But then again, expressing everything in Fourier space could help.\n\nSince our potential is a delta function, its Fourier transform would simplify things. Because the integral involving the exponential multiplied by delta function picks out specific terms.\n\nYes! If V(r) = a δ³(r), then its Fourier transform is simply V(q) = a, because:\n\nV(q) = ∫ e^{-i q · r} V(r) dr = a ∫ e^{-i q · r} δ³(r) dr = a.\n\nThat seems correct. Therefore, the scattering amplitude in the Born approximation becomes proportional to V(q).\n\nFrom Landau-Lifshitz Quantum Mechanics Non-Relativistic Theory, Volume III, they mention that the first Born approximation leads to:\n\ndσ/dΩ = |f|²,\n\nand f itself is related to the Fourier transform of the potential.\n\nSpecifically, equation-wise, I believe it goes like:\n\nf(θ) = (m/(2ħ²)) ∫ e^{i q · r} V(r) dr,\n\nwhich, substituting V(r)=aδ³(r):\n\nf(θ) = (m/(2ħ²)) * a.\n\nBecause the integral reduces to multiplying by 'a'. Wait, hold on—that doesn’t seem dimensionally consistent. Let me double-check.\n\nActually, the exact expression may include other factors. Alternatively, recalling that in three dimensions, the Fourier transform of δ³(r) is 1, hence V(q) = a. Then, the scattering amplitude in the Born approximation is:\n\nf_Born = (m/(2ħ²)) V(q),\n\nsince V(q) is already integrated against the plane waves.\n\nTherefore, plugging in V(q) = a, we get:\n\nf_Born = (m a)/(2ħ²).\n\nHmm, okay, so does this mean that the scattering amplitude is independent of both the energy and the angle?\n\nWait, that can't be entirely true because usually, the differential cross-section ends up being dependent on these variables. Or is it possible that due to the nature of the delta function potential, the cross-section indeed turns out isotropic?\n\nHold on, thinking further—if the potential is contact-like (only acts at a single point), then the scattering shouldn't preferentially favor certain directions—it should be isotropic. Hence, the differential cross-section should be uniform across all angles, meaning |f|^2 is same regardless of θ.\n\nThus, putting it together, the differential cross-section would be:\n\ndσ/dΩ = |f|² = [(ma)^2]/[(2ħ²)^2].\n\nSimplify that:\n\n(m² a²)/(4 ħ⁴).\n\nBut wait, dimensional analysis tells us that cross-sections have units of area. Let's see: ma² has units kg*(length)^6; divided by ħ^4, whose units are (J*s)^4=(kg² m^4/s²)^4)=(kg^8 m^16 s^-8)? No, wait, ħ has units J·s = kg m²/s. Thus, ħ^4 has units kg^4 m^8 s^-4.\n\nThen numerator: m² a² — assuming a has units of energy×time? Wait, original potential is V(r)=a δ³(r); potentials typically have units of energy per charge, but in natural units, perhaps different. Actually, in SI units, the delta function must compensate for volume integration, so V(r) needs to have units of energy × length³ to give overall energy when integrated over volume.\n\nHence, a has units of energy × inverse length cubed? Wait, no—the delta function δ³(r) has units of inverse volume, so V(r) must have units of energy per volume to integrate to total energy. So yes, a has units of energy × length⁻³.\n\nGiven that, let's compute the units of each term in f_Born:\n\n[m a ]/[ħ²]\n\nMass m has units kg.\n\na has units Energy / Length³ = (kg m²/s²)/Length³ = kg/(m s²)\n\nMultiply by m: kg * (kg/(m s²)) )= kg²/(m s²)\n\nDivide by ħ squared: ħ has units kg m²/s, so ħ² is kg² m^4 / s².\n\nThus, entire fraction: (kg²/(m s²)) / (kg² m^4 / s² )) = 1/m^5 ?\n\nWait, that can't be right because f has units of inverse length (dimensional analysis says scattering amplitude has units of length). Hmmm...\n\nMaybe I messed up somewhere. Perhaps the actual expression for f in the Born approximation also involves the relative velocity or something else.\n\nWait, going back—I think the standard form is:\n\nf(θ) = (m/(2ħ²)) ∫ e^{i q · r} V(r) dr.\n\nWhich makes sense because V(r) has units of energy, and the rest of the terms adjust accordingly.\n\nIf V(r) is a delta function, then the integral is just a, so f(θ) = (m a)/(2 ħ²).\n\nNow, checking the units:\n\nm has kg,\na has E/L³ = (kg m²/s²)/L³ = kg/(m s²),\nthus m*a has kg*(kg/(m s²)) = kg²/(m s²)\ndivided by ħ², which is (kg m²/s)^2 = kg² m^4 / s².\nSo altogether:\n[kg²/(m s²)] / [kg² m^4 / s²] = 1/m^5 ???? That still doesn't resolve to meters.\n\nThis suggests a mistake in reasoning. Probably missing some factor.\n\nWait, alternative thought process: Scattering amplitude f has units of length. So let's ensure that.\n\nCompute the units of (m a)/(2 ħ²):\n\nmass * a / (ħ² )\n\nAs above, a has units of energy per volume, which is (kg m²/s²)/m³ = kg/(m s²).\nMultiplying by mass: kg * kg/(m s²) = kg²/(m s²)\nDividing by ħ²: (kg"}
{"id": "Quantum Mechanics/30-3", "questions": "(a) Use the first Born approximation to estimate the differential and total cross sections for the scattering of a particle by a Yukawa potential $V = ge^{−\\mu r}/r$.\n\n(b) Use the optical theorem and the result of (a) to estimate the imaginary part of the forward scattering amplitude.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about using the first Born approximation to find the differential and total cross sections for a particle scattered by a Yukawa potential. Then, part b asks me to use the optical theorem with my results from part a to estimate the imaginary part of the forward scattering amplitude. Hmm, let's start breaking it down.\n\nFirst off, what do I remember about the Born approximation? The first Born approximation is used when dealing with low-energy or weak potentials where higher-order terms can be neglected. It approximates the scattering amplitude as just the Fourier transform of the potential. So, if V(r) is our potential, then the scattering amplitude f(θ) should be approximately equal to -m/(2πħ²) times the integral over all space of e^{-i k · r} V(r) d³r, right?\n\nWait, actually, more precisely, the formula is:\n\nf(k',k) ≈ -(4π m)/(2 ħ²) ∫₀^∞ [sin(q r)/q] V(r) j_l(q r') dr,\n\nbut maybe that's getting too detailed. Alternatively, since we're working in three dimensions, perhaps it's better expressed in momentum transfer terms. Let me recall: In the first Born approximation, the differential cross section is given by |f|², which comes directly from integrating the plane wave solutions perturbed by the potential.\n\nSo, specifically, the scattering amplitude f(Ω) in the first Born approximation is:\n\nf(Born)(Ω) = (-m/ħ²) * ∫ e^{-i q·r'} V(r') d³r',\n\nwhere q is the transferred momentum vector, related to the angle θ between incoming and outgoing momenta via q = 2k sin(θ/2), assuming elastic scattering.\n\nBut wait, another thought—since the Yukawa potential has radial symmetry, its Fourier transform will depend only on the magnitude of q, not direction. Therefore, the scattering amplitude should also depend only on |q|. That makes sense because spherical harmonics would come into play otherwise, but here due to s-wave dominance, probably l=0 term dominates.\n\nGiven that, the expression simplifies. For the Yukawa potential V(r)=g e^{-μ r}/r, how does one compute its Fourier transform? Because once you take the Fourier transform, you get something proportional to 1/(q² + μ²). Wait, yes! Remembering that the Fourier transform of e^{-μ r}/r is 4π/(q² + μ²).\n\nLet me verify that. Yes, indeed, the Fourier transform pair is known: F[e^{-μ r}/r](q) = 4π / (√{q² + μ²})² ) = 4π/(q² + μ²). Perfect.\n\nTherefore, plugging back into the Born approximation, the scattering amplitude becomes:\n\nf(Ω) ≈ (-m g)/(2 π ħ²) * [4π]/[q² + μ²].\n\nSimplify constants: (-m g)/(2 π ħ²) multiplied by 4π gives us (-2 m g)/(ħ² (q² + μ²)).\n\nThus, f(Ω) = (-2 m g)/(ħ² (q² + μ²)). But hold on—is there any angular dependence left? Since everything depends only on q, which itself relates to cosθ through q = 2k sin(theta/2). Hence, substituting q in terms of theta might make things clearer.\n\nExpressed differently, since q = |q| = sqrt{(k' - k)^2}, but considering elastic scattering, |k'| = |k| = k, so q = 2k sin(theta/2). Thus, replacing q² with (2k sin(theta/2))^2.\n\nHence, putting it together, the scattering amplitude is:\n\nf(theta) = (-2 m g)/(ħ² [(2k sin(theta/2))^2 + mu^2]).\n\nNow, moving on—the differential cross-section is |f|^2. So squaring the above expression:\n\ndσ/dΩ = |(-2 m g)/(ħ² (4k² sin²(theta/2) + mu²))|²\n\nWhich simplifies to:\n\n[(4 m² g²)]/[ħ⁴ (4k² sin²(theta/2) + mu²)^2]\n\nThat seems correct. Now, for the total cross-section σ_total, we need to integrate dσ/dΩ over all solid angles. However, in the first Born approximation, sometimes people approximate the total cross-section as four pi times the average value of |f|^2, but no—that’s incorrect; instead, the exact relation requires integrating over all directions.\n\nAlternatively, recalling that the total cross-section is obtained by integrating the differential cross-section over all solid angles:\n\nσ_total = ∫ (dσ/dΩ) dΩ = ∫ |f(theta)|² sinθ dtheta dphi.\n\nSince the integrand doesn't depend on phi, the integration over phi contributes a factor of 2pi. So,\n\nsigma_total = 2 pi ∫_{0}^{pi} |f(theta)|² sin theta d theta.\n\nSubstituting |f(theta)| squared:\n\n2 pi * ∫_0^{pi} [4 m² g² / (ħ⁴ (4k² sin²(theta/2) + mu²)^2)] sin theta d theta.\n\nHmm, looks like substitution time. Maybe set x = theta/2, so dx = d theta / 2, hence d theta = 2dx. Also, sin theta = 2 sinx cosx. When theta goes from 0 to pi, x goes from 0 to pi/2.\n\nPlugging these substitutions in:\n\nSigma_total = 2 pi * ∫_{0}^{pi/2} [4 m² g² / (ħ⁴ (4k² sin²x + mu²)^2)] * 2 sinx cosx * 2 dx.\n\nBreaking down the factors:\n\n- From outside: 2 pi.\n- Inside the integral:\n   - Numerator: 4 m² g²\n   - Denominator: ħ⁴ (4k² sin²x + mu²)^2\n   - Sin theta d theta became 2 sinx cosx * 2 dx => 4 sinx cosx dx.\n\nPutting them together:\n\nSigma_total = 2 pi * [4 m² g² / ħ⁴ ] * ∫_{0}^{pi/2} [4 sinx cosx dx] / (4k² sin²x + mu²)^2.\n\nFactor out the 4 inside numerator:\n\n= 8 pi m² g² / ħ⁴ * ∫_{0}^{pi/2} [sinx cosx dx] / (4k² sin²x + mu²)^2.\n\nThis integral may look tricky, but perhaps substitute u = sinx, du = cosx dx. Limits change from u=0 to u=1.\n\nThen, the integral becomes:\n\n∫_{u=0}^{1} [u du] / (4k² u² + mu²)^2.\n\nYes, perfect substitution. So now Sigma_total becomes:\n\n8 pi m² g² / ħ⁴ * ∫_{0}^{1} u du / (4k² u² + mu²)^2.\n\nTo solve this integral, note that denominator is quadratic in u². Perhaps perform substitution t = u², dt = 2u du, so u du = dt/2.\n\nChanging variables again:\n\nIntegral becomes (1/2) ∫_{t=0}^{1} dt / (4k² t + mu²)^2.\n\nCompute this integral:\n\nI = (1/2) ∫_{0}^{1} dt / (A t + B)^2, where A = 4k², B = mu².\n\nThe antiderivative of 1/(At+B)^2 is -1/(A(A t + B)), evaluated from 0 to 1.\n\nCalculating:\n\nI = (1/2)[ -1/(A(A*1 + B)) + 1/(A*B) ]\n\n= (1/2)[ -1/(A(A + B)) + 1/(A B) )\n\n= (1/2)[ ( -B + (A + B) ) / (A B (A + B)) ) ]\n\nWait, let me recompute carefully:\n\nActually, evaluating definite integral:\n\nF(t) = -1/(A(A t + B))\n\nFrom t=0 to t=1:\n\nF(1) - F(0) = [-1/(A(A + B))] - [-1/(A B)]\n\n= (-1/(A(A + B))) + 1/(A B)\n\nCombine fractions:\n\n= [ -B + (A + B) ] / [A B (A + B) ]\n\nNumerator: -B + A + B = A\n\nDenominator remains same.\n\nThus, F(1)-F(0) = A / [A B (A + B)] = 1 / [B (A + B)]\n\nMultiply by 1/2:\n\nI = (1/2)*(1 / [B (A + B)])"}
{"id": "Quantum Mechanics/27-1", "questions": "A $K^-$ meson ($M_{K^-} = 494 \\text{ MeV}$) and a proton can form a bound atomic state. Take them both to be point particles subject only to the Coulomb force.\n\n(a) Calculate the 2P to 1S transition energy.\n\n(b) Estimate the probability of finding the $K^-$ and $p$ within $10^{-13} \\text{ cm}$ of each other when they are in the 1S state.\n\n(c) Suppose that the strong interaction between the $K^-$ and $p$ can be represented by an attractive square well potential of 50 MeV and range $10^{-13} \\text{ cm}$. Using lowest order perturbation theory, estimate the change in the transition energy calculated in (a) due to the strong interaction.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about K⁻ mesons and protons forming a bound state. It's divided into three parts: calculating the transition energy from 2P to 1S states, estimating the probability of their proximity in the ground state, and then considering the effect of a strong interaction on the transition energy using perturbation theory. Hmm, let me try to tackle part (a) first.\n\nAlright, for part (a), it says we need to calculate the 2P to 1S transition energy assuming they're point particles under Coulomb force. So, since they’re charged particles interacting via Coulomb forces, this is similar to hydrogen-like atoms but with different masses because one particle is a kaon and the other is a proton.\n\nFirst, I remember that in hydrogen-like systems, the energy levels depend inversely on the reduced mass μ of the system. The formula for the nth level is E_n = - (μ e⁴)/(8 ε₀² h_bar² n²). But wait, actually, more accurately, the Rydberg constant comes into play here. Let me recall:\n\nThe general expression for the energy levels is given by:\nE_n = -(R_H * m_e / μ) * Z² / n²\n\nBut in our case, instead of electron-proton, it's K⁻-proton. Both charges are opposite? Wait, no—wait, K⁻ has charge -e, and proton +e, right? So yes, same as hydrogen atom in terms of charge magnitude. Therefore, maybe R_K would replace R_H?\n\nWait, hold on—the Rydberg constant depends on the mass of the orbiting particle. In hydrogen, it's based on the electron mass. Here, though, the two-body problem involves a proton and a kaon. Since both move around their common center of mass, the effective \"reduced\" mass should come into play.\n\nSo, perhaps better to think in terms of the reduced mass μ = (m_p * M_K) / (m_p + M_K)\n\nGiven that M_K is much lighter than the proton? No, wait, what's the actual mass of a kaon compared to a proton? Kaons are pseudoscalar mesons; their rest mass is approximately 494 MeV/c², while the proton is about 938 MeV/c². So, not too far off—it’s roughly half the mass of the proton.\n\nTherefore, the reduced mass isn't negligible compared to either mass. So yeah, definitely important to include it.\n\nLet me write down the steps:\n\n1. Compute the reduced mass μ.\n   Given m_p ≈ 938 MeV/c²,\n   M_K ≈ 494 MeV/c²,\n\n   Then μ = (m_p * M_K) / (m_p + M_K)\n   \nPlugging numbers:\n\nμ = (938 * 494) / (938 + 494) \n\nCompute denominator: 938+494=1432\n\nNumerator: approx 938*494. Well, exact value might matter later, but let's compute it precisely.\n\nCalculate numerator: 938 * 494\n\nWell, 900*494 = 444600\n\n38*494: 30*494=14820, 8*494=3952 → total 14820+3952=18772\n\nTotal numerator: 444600 + 18772 = 463,372\n\nThus, μ≈463372 / 1432 ≈ ?\n\nDivide 463372 ÷ 1432:\n\nApproximate division: 1432 x 324 = ? Because 1432x300=429,600, subtract that from 463,372 gives 33,772 left.\n\nThen 1432x23=32,936, which leaves us with ~836 remaining.\n\nSo approximate μ ≈ 324 + 23 + small fraction ≈ 347 MeV/c².\n\nAlternatively, precise calculation may require calculator, but probably okay if I keep symbols or use fractions.\n\nAnyway, moving forward.\n\nNow, knowing that, the energy levels will scale accordingly.\n\nIn hydrogen, the ionization energy is about 13.6 eV, corresponding to n=1. For another system, like muonic hydrogen, where the mass increases, the binding energies increase proportionally.\n\nSimilarly, here, the binding energy scales with the ratio of the reduced mass over the electron mass times the original Rydberg constant.\n\nWait, correct approach: The Rydberg constant R is proportional to m/(ħ c α²), something like that. Alternatively, the energy levels are given by E_n = - (Z² * R * ħ c ) / n², where R is Rydberg constant.\n\nBut perhaps more straightforwardly, the energy levels for a two-body Coulomb system are given by:\n\nE_n = - [ (μ e^4) ] / (8 ε_0² h̄² n² )\n\nOr alternatively, expressed in terms of the Bohr radius scaled appropriately.\n\nWait, let me get back to basics.\n\nCoulombic potential V(r)= -k e² / r, leading to Schrödinger equation solutions with quantized energy levels depending on principal quantum number n.\n\nFor Hydrogen-like atoms, the energy levels are known to be:\n\nE_n = - (m_e k² e^4) / (2 hbar² n² )\n\nWhere k = 1/(4πε_0).\n\nBut in reality, it's the reduced mass that appears there, not just m_e. Hence, replacing m_e with μ, the reduced mass.\n\nHence, generalized formula:\n\nE_n = - (μ k² e^4) / (2 hbar² n² )\n\nYes, that seems accurate.\n\nSince in our case, the nucleus is a proton (charge Ze, Z=1), and the orbiting particle is K⁻ (-e charge). Thus, effectively, same as hydrogen except for the reduced mass being different.\n\nSo, plugging values:\n\nWe know that for hydrogen, E_n^(hydrogen) = -13.6 eV /n².\n\nHere, E_n^(Kaon) = E_n^(hydrogen) * (μ / m_e)\n\nBecause all else equal, scaling the mass scales the energy linearly.\n\nWait, but hang on—is that true? If you swap out the mass, does the energy simply multiply by μ/m_e?\n\nHmm, let's see.\n\nHydrogen uses m_e as its orbital mass, whereas here, the equivalent 'orbital' mass is μ, the reduced mass.\n\nIndeed, so the factor becomes μ / m_e multiplied by the usual hydrogen result.\n\nBut also note that in the standard hydrogen model, the nucleus is considered infinitely heavy, hence μ ≈ m_e. When accounting for finite nuclear mass, the correction is already included through the reduced mass concept.\n\nTherefore, in our case, since neither particle is very massive relative to the other, we must take the full reduced mass.\n\nSo, going back, the energy levels become:\n\nE_n = - (13.6 eV) * (μ / m_e) / n²\n\nIs that correct? Or do I have factors wrong?\n\nWait, let me rederive quickly.\n\nEnergy levels for hydrogen-like atoms:\n\nE_n = - (m v²)/2 = kinetic energy equals negative potential energy, so |E| = ke²/(2a), where a is the Bohr radius.\n\nBohr radius a = (4 π epsilon_0 hbar²)/(mu e²)\n\nSo substituting a into E:\n\nE = - (ke²)^2/(2*(hbar²/(mu)))*something... Maybe getting confused.\n\nAlternative way: Start from the basic expressions.\n\nFrom the Bohr model, the energy levels are determined by solving the Schrodinger equation for the Coulomb potential. After some algebra, the energy levels turn out to be:\n\nE_n = - (μ e^4)/(8 ε_0² hbar² n²) * (1/4 π ε_0)^2 ??? Not sure.\n\nWait, perhaps better to express everything in SI units.\n\nConstants needed:\n\ne = elementary charge ≈ 1.6 ×10^-19 C\n\nepsilon_0 ≈ 8.85"}
{"id": "Quantum Mechanics/25-5", "questions": "Consider the electron in the ground state of a hydrogen atom. \n\n(a) Determine the values for $\\langle r \\rangle$, $\\langle r^2 \\rangle$ and the most probable value for the radius $r$ of the orbit. \n\n(b) Compute the probability that the electron is found inside the volume of the nucleus. \n\n(c) Find the momentum-space wave function for the electron and determine the expectation value of the kinetic energy. ", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about the hydrogen atom's ground state. It has three parts: (a), (b), and (c). Let me try to tackle each part one by one.\n\nStarting with part (a): They want me to find ⟨r⟩, ⟨r²⟩, and the most probable radius r. Hmm, okay. Since it's the ground state of hydrogen, we're dealing with the 1s orbital. The radial wavefunction for the 1s orbital should be something like ψ(r) = (Z/a₀)^{3/2} e^{-Zr/a₀}, where Z is the atomic number (which is 1 for hydrogen) and a₀ is the Bohr radius. So simplifying, ψ(r) = (1/a₀^{3/2}) e^{-r/a₀}. But wait, actually, when considering spherical coordinates, the probability density isn't just |ψ|² but also includes the Jacobian determinant from integration over all space, which introduces an r² term. That might come into play later.\n\nFor ⟨r⟩, the expectation value of r, I think the formula would involve integrating r times the probability distribution. Specifically, since the wavefunction depends only on r, not angles, the integral reduces to:\n\n⟨r⟩ = ∫0^∞ r [4π r² |R(r)|²] dr\n\nWhere R(r) is the radial part of the wavefunction. Wait, no—actually, more accurately, because the total probability involves the square of the entire wavefunction including angular parts, but since the angular parts integrate out due to symmetry, maybe I can express it as:\n\n⟨r⟩ = ∫0^∞ r * P(r) dr,\n\nwhere P(r) is the radial probability distribution, given by 4π r² |R(r)|². Yeah, that makes sense.\n\nSo let's write down what P(r) is. For the 1s orbital, R(r) = (2/a₀^(3/2)) e^{-r/(a₀)}, right? Because sometimes normalization constants differ based on conventions. Wait, hold on—I need to make sure my normalization is correct. The general form for the 1s radial wavefunction is sqrt(2/a₀³) e^{-r/a₀}. Therefore, |R(r)|² is (2/a₀³) e^{-2r/a₀}.\n\nTherefore, P(r) = 4π r² * (2/a₀³) e^{-2r/a₀}\n\nSimplify that: P(r) = (8 π / a₀³) r² e^{-2r/a₀}\n\nNow, to compute ⟨r⟩, we set up the integral:\n\n⟨r⟩ = ∫0^∞ r * (8 π / a₀³) r² e^{-2r/a₀} dr\n\nWhich becomes:\n\n⟨r⟩ = (8 π / a₀³) ∫0^∞ r³ e^{-2r/a₀} dr\n\nHmm, integrals involving powers of r multiplied by exponential functions often relate to gamma functions or factorials if they’re integer exponents. Remembering that ∫0^∞ x^n e^{-kx} dx = n! / k^{n+1} } when n is a non-negative integer.\n\nIn our case, substituting variables: let’s set u = 2r/a₀ ⇒ du/dr = 2/a₀ ⇒ dr = (a₀/2) du. Then, when r=0, u=0; r→infty, u=infty.\n\nExpressed in terms of u:\n\n∫0^∞ r³ e^{-2r/a₀} dr = ∫0^∞ (u a₀ / 2 )³ e^{-u} (a₀ / 2) du\n\nCompute that:\n\n= (a₀³ / 8) * (a₀ / 2) ∫0^∞ u³ e^{-u} du\n\nWait, hang on—that seems off. Actually, substitution gives us:\n\nLet u = 2r/a₀ → r = (a₀ u)/2\n\nThen, r³ = (a₀³ u³)/(8)\n\ne^{-2r/a₀} = e^{-u}\n\ndr = (a₀/2) du\n\nThus, plugging back into the integral:\n\nIntegral becomes:\n\n∫0^∞ [(a₀³ u³)/8 ] e^{-u} (a₀/2) du =\n\nMultiply coefficients: (a₀³ /8)*(a₀ /2)= a₀⁴ /16\n\nAnd then multiply by ∫0^∞ u³ e^{-u} du, which is Γ(4) = 3! = 6.\n\nHence, overall:\n\nIntegral = (a₀⁴ /16)*6 = (6 a₀⁴)/16 = (3 a₀⁴)/8\n\nBut going back to the original expression for ⟨r⟩:\n\n⟨r⟩ = (8 π / a₀³) * Integral\n\nPlugging in the result:\n\n⟨r⟩ = (8 π / a₀³) * (3 a₀⁴ /8 )\n\nThe 8 cancels, leaving:\n\n⟨r⟩ = π * 3 a₀\n\nWait, pi? No, wait—the coefficient was 8pi/a0³ multiplying the integral, which had a 3 a0⁴ /8. So 8pi/a0³ * 3 a0⁴ /8 = 3 pi a0?\n\nThat doesn’t seem right... Wait, perhaps miscalculating somewhere.\n\nHold on, let's re-examine the steps carefully.\n\nFirstly, P(r):\n\nP(r) = 4π r² |R(r)|²\n\nGiven R(r) = sqrt(2/a0³) e^{-r/a0}, hence |R|^2 = 2/a0³ e^{-2r/a0}\n\nThus, P(r) = 4π r²*(2/a0³)e^{-2r/a0}= (8 π / a0³) r² e^{-2r/a0}\n\nYes, that looks correct.\n\nThen, ⟨r⟩ = ∫0^∞ r*P(r) dr = ∫0^∞ r*(8 π / a0³) r² e^{-2r/a0} dr = (8 π / a0³) ∫0^∞ r³ e^{-2r/a0} dr\n\nRight, now change variable: let t = 2r/a0 => dt = 2/a0 dr => dr = a0/2 dt\n\nWhen r=0, t=0; r->inf, t->inf.\n\nSubstitute:\n\n∫0^∞ r³ e^{-t} (a0/2) dt\n\nBut r = (a0 t)/2, so r³ = (a0³ t³)/8\n\nPutting together:\n\n∫0^∞ (a0³ t³ /8) e^{-t} (a0 /2) dt = (a0⁴ /16) ∫0^∞ t³ e^{-t} dt\n\nAs before, ∫ t³ e^{-t} dt = Gamma(4) = 6\n\nThus, the integral equals (a0⁴ /16)*6 = (3 a0⁴)/8\n\nGoing back to ⟨r⟩:\n\n⟨r⟩ = (8 π / a0³) * (3 a0⁴ /8 ) = Simplify numerator and denominator:\n\n8 cancels, a0³ cancels with a0⁴ giving a0 left.\n\nLeaving: π * 3 a0 ?\n\nWait, that still leaves 3 pi a0, but I thought ⟨r⟩ for 1s orbital is known—it should be (3/2)a0, right? Or am I mixing things up?\n\nWait, another way—if you recall, for the nth level, ⟨r⟩ is (n² a0)(something)... Maybe better to check standard results.\n\nAlternatively, perhaps made a mistake in calculation earlier.\n\nWait, let's recast without changing variables.\n\nWe know that ∫0^∞ r^m e^{-kr} dr = m!/k^{m+1} }\n\nHere, m=3, k=2/a0\n\nThus, ∫0^∞ r³ e^{-2r/a0} dr = 3! / (2/a0)^4 = 6 / (16/a0⁴) )= 6 a0⁴ /16 = same as above.\n\nSo yes, indeed, the integral is 3 a0⁴ /8.\n\nMultiplying by (8 π / a0³ ), get 3 pi a0.\n\nBut according to textbooks, the expected value of r for 1s orbital is (3/2)a0 ≈ 1.5 a0.\n\nThis suggests there must be some error here.\n\nWait, could it be that I messed up the definition of the wavefunction?\n\nBecause different sources define the normalized radial wavefunctions differently.\n\nWait, double-checking the normalization condition.\n\nThe radial wavefunction for 1s is usually written as R_{nl"}
{"id": "Quantum Mechanics/25-2", "questions": "You are given that the state at $ t = 0 $ of a free rigid rotator, whose moment of inertia is $ I $, is specified by $\\psi(\\theta, \\phi, t=0)=C\\left(Y_9^1-2 Y_8^{-1}+2 Y_9^3\\right)$, where $ Y^\\ell_m(\\theta, \\phi) $ represents its $(\\ell, m)^{\\text{th}}$ spherical harmonic wavefunction, and $(\\theta, \\phi)$ represents the orientation of the rotator axis.\n\n(a) Calculate the normalization constant, $ C $.\n\n(b) Calculate $\\langle L^2 \\rangle$ and $\\langle L_z \\rangle$ in this state.\n\n(c) Calculate $\\langle L_x^2 \\rangle$ in this state.\n\n(d) Find the state at time $ t \\neq 0 $ and recalculate part (b).", "graphs": null, "llm_answers": "<think>\nOkay, so I have to solve these four parts about a quantum mechanics problem involving a free rigid rotator. Let me start with part (a), which asks for the normalization constant \\( C \\).\n\nFirst, I remember that any quantum state must be normalized, meaning the integral of the square of the absolute value of the wave function over all space should equal one. The given wave function is:\n\n\\[ \\psi(\\theta, \\phi, t=0) = C\\left( Y_9^1 - 2Y_8^{-1} + 2Y_9^3 \\right) \\]\n\nSince each term here involves different spherical harmonics, they might not overlap when integrated against each other because spherical harmonics form an orthonormal basis. So, if I compute the inner product of \\( \\psi \\) with itself, it should just sum up the squares of their coefficients multiplied by the integrals of each spherical harmonic squared.\n\nLet's write out the normalization condition:\n\n\\[ \\int |\\psi|^2 d\\Omega = |C|^2 \\int \\left| Y_9^1 - 2Y_8^{-1} + 2Y_9^3 \\right|^2 d\\Omega = 1 \\]\n\nExpanding the square inside the integral gives three terms from squaring each component plus cross terms. But since the spherical harmonics are orthogonal, those cross terms will integrate to zero. That simplifies things!\n\nSo,\n\n\\[ |C|^2 \\left[ \\int |Y_9^1|^2 d\\Omega + (-2)^2 \\int |Y_8^{-1}|^2 d\\Omega + (2)^2 \\int |Y_9^3|^2 d\\Omega \\right] = 1 \\]\n\nI know that the integral of \\( |Y_\\ell^m|^2 \\) over all solid angles is 1 for each spherical harmonic. Therefore, each integral becomes 1.\n\nPlugging that in:\n\n\\[ |C|^2 [1 + 4*1 + 4*1] = |C|^2 (1 + 4 + 4) = |C|^2 * 9 = 1 \\]\n\nTherefore, solving for \\( C \\):\n\n\\[ |C|^2 = \\frac{1}{9} \\implies C = \\pm \\frac{1}{3} \\]\n\nBut usually, we take positive constants unless otherwise stated, so \\( C = \\frac{1}{3} \\).\n\nWait, hold on—let me double-check: Each coefficient is 1, -2, and 2 respectively. Squared, they become 1, 4, and 4. Summing them gives 1 + 4 + 4 = 9. Yes, correct. Then \\( C = 1/3 \\).\n\nMoving on to part (b): Compute \\( \\langle L^2 \\rangle \\) and \\( \\langle L_z \\rangle \\).\n\nFor angular momentum operators, \\( L^2 \\) has eigenvalues \\( \\hbar^2 \\ell (\\ell + 1) \\) and \\( L_z \\) has eigenvalues \\( \\hbar m \\). Since our wave function is expressed as a linear combination of spherical harmonics, each corresponding to specific \\( \\ell \\) and \\( m \\) values, calculating expectation values can be done using linearity.\n\nGiven that \\( \\psi \\) is already normalized, let's express \\( \\psi \\) again:\n\n\\[ \\psi = \\frac{1}{3}\\left( Y_9^1 - 2Y_8^{-1} + 2Y_9^3 \\right) \\]\n\nEach term corresponds to states with definite \\( L^2 \\) and \\( L_z \\). Specifically:\n- First term: \\( \\ell = 9, m = 1 \\)\n- Second term: \\( \\ell = 8, m = -1 \\)\n- Third term: \\( \\ell = 9, m = 3 \\)\n\nTo find \\( \\langle L^2 \\rangle \\):\nThe operator \\( L^2 \\) acts diagonally on each spherical harmonic, giving \\( \\hbar^2 \\ell (\\ell + 1) \\times \\) the respective coefficient squared times probability weight.\n\nSimilarly, \\( \\langle L_z \\rangle \\) would involve the weighted average of \\( m \\) values.\n\nCalculating \\( \\langle L^2 \\rangle \\):\n\nIt’s the sum over each term of \\( |c_i|^2 \\cdot \\hbar^2 \\ell_i (\\ell_i + 1) \\).\n\nFrom the wave function:\n- Coefficient of first term: \\( c_1 = \\frac{1}{3} \\), so \\( |c_1|^2 = \\frac{1}{9} \\); \\( \\ell = 9 \\).\n- Coefficient of second term: \\( c_2 = -\\frac{2}{3} \\), so \\( |c_2|^2 = \\frac{4}{9} \\); \\( \\ell = 8 \\).\n- Coefficient of third term: \\( c_3 = \\frac{2}{3} \\), so \\( |c_3|^2 = \\frac{4}{9} \\); \\( \\ell = 9 \\).\n\nThus,\n\\( \\langle L^2 \\rangle = \\frac{1}{9} \\cdot \\hbar^2 \\cdot 9 \\cdot 10 + \\frac{4}{9} \\cdot \\hbar^2 \\cdot 8 \\cdot 9 + \\frac{4}{9} \\cdot \\hbar^2 \\cdot 9 \\cdot 10 \\)\n\nCompute each term:\n- First term: \\( \\frac{1}{9} \\cdot 90 \\hbar^2 = 10 \\hbar^2 \\)\n- Second term: \\( \\frac{4}{9} \\cdot 72 \\hbar^2 = \\frac{4}{9}*72 = 32 \\hbar^2 \\)\n- Third term: Same as first but scaled by 4/9? Wait no—the third term also uses \\( \\ell = 9 \\), same as first.\n\nThird term calculation: \\( \\frac{4}{9} \\cdot 90 \\hbar^2 = 40 \\hbar^2 \\)\n\nAdding together: 10 + 32 + 40 = 82\n\nHence, \\( \\langle L^2 \\rangle = 82 \\hbar^2 \\)\n\nNow, computing \\( \\langle L_z \\rangle \\):\n\nThis is similar, except instead of multiplying by \\( \\ell (\\ell + 1) \\), multiply by \\( m \\).\n\nAgain, each term contributes \\( |c_i|^2 \\cdot m_i \\cdot \\hbar \\).\n\nBreaking down:\n- First term: \\( |c_1|^2 = 1/9; m = 1 \\Rightarrow contribution = (1/9)(1)\\hbar = \\hbar /9 \\)\n- Second term: \\( |c_2|^2 = 4/9; m = -1 \\Rightarrow contribution = (4/9)(-1)\\hbar = -4\\hbar /9 \\)\n- Third term: \\( |c_3|^2 = 4/9; m = 3 \\Rightarrow contribution = (4/9)(3)\\hbar = 12\\hbar /9 = 4\\hbar /3 \\)\n\nSumming contributions:\nTotal \\( \\langle L_z \\rangle = (\\hbar /9 ) - (4\\hbar /9) + (4\\hbar /3) \\)\n\nConvert everything to ninths:\n= (1 - 4 + 12)/9 * \\hbar\n= (9)/9 * \\hbar\n= \\hbar\n\nSo, \\( \\langle L_z \\rangle = \\hbar \\)\n\nHmm, wait—that seems straightforward. Alternatively, maybe there was a mistake?\n\nDouble checking calculations:\n\nFirst term: 1/9 *1 = 1/9\n\nSecond term: 4/9*(-1)=-4/9\n\nThird term: 4/9*(3)=12/9=4/3≈1.333...\n\nAdd them: 1/9 ≈0.111..., minus 4/9≈-0.444... equals approximately -0.333..., then add 1.333... gets us exactly 1. Which matches my previous result. So yes, \\( \\langle L_z \\rangle = \\hbar \\"}
{"id": "Quantum Mechanics/29-7", "questions": "A hydrogen atom in its ground state sits at the origin. An electromagnetic wave, described by the vector potential $\\mathbf{A}(\\mathbf{r}, t)=\\mathbf{A}_0 e^{i\\left(\\mathbf{k}^{\\prime} \\cdot \\mathbf{r}-\\omega t\\right)}$, in incident upon the atom. What is the rate of ionization of the hydrogen atom? [Assume: (1) the dipole approximation; and (2) the outgoing electron is in a plane wave state (i.e. ignore the Coulomb interaction with the proton once the electron is liberated); incidentally, this assumption is good if the initial state is an $ s $-state, as it is here.]", "graphs": null, "llm_answers": "<think>\nOkay, so I have to find the rate of ionization for a hydrogen atom when it's hit by an electromagnetic wave. The setup given is that the hydrogen atom is in its ground state at the origin, and there's an incoming EM wave described by the vector potential A(r,t). They also mention using two assumptions: the dipole approximation and considering the outgoing electron as a plane wave without worrying about the Coulomb interaction after liberation.\n\nHmm, let me start by recalling what I know about ionization rates. Ionization typically involves absorbing enough energy from the external field to overcome the binding energy. In quantum mechanics terms, especially for atoms like hydrogen, we can model this process using time-dependent perturbation theory because the EM wave acts as a small perturbation on the system.\n\nThe problem mentions the dipole approximation, which simplifies things by assuming that the electric field varies slowly over the size of the atom. This allows us to approximate the spatial dependence of the fields inside the atom as uniform or varying linearly, but since they're saying \"dipole,\" probably just taking E = -∇φ - ∂A/∂t evaluated at r=0 would be sufficient.\n\nWait, actually, more precisely, under the dipole approximation, the vector potential outside the atom doesn't vary much across the atomic scale, so you can take A ≈ A(0,t), meaning it's constant throughout the region where the electron is likely found. That makes sense because the wavelength of typical light used in such processes is much larger than the Bohr radius, so variations in A over the extent of the atom are negligible.\n\nSo, moving forward, the Hamiltonian for the system will include the usual Hydrogen atom part plus a perturbing term due to the EM wave. Since the question gives the vector potential, maybe I should express the perturbation in terms of A.\n\nIn electromagnetism, the minimal coupling prescription tells us how to incorporate the gauge field into the Hamiltonian. For non-relativistic electrons, the momentum p becomes p - qA/c, right? So the kinetic energy term changes accordingly.\n\nBut wait, another approach might be to use the electric field instead. Because sometimes people work directly with the electric dipole interaction. Let me think—since the vector potential is given, perhaps expressing everything in terms of A is better.\n\nAlternatively, remember that the electric field E is related to the time derivative of A via Faraday's law: curl E + dB/dt = 0, but in SI units, E = -(1/q)(dΦ/dt + v × B), but maybe that complicates things.\n\nActually, no—the standard way to write the perturbation in the dipole approximation is H' = -e φ(t) + (p · A)/c, but wait—isn’t phi zero in radiation gauge? Or do I need to consider both scalar and vector potentials?\n\nHold on, in the dipole approximation, often only the vector potential contributes significantly because the scalar potential variation is less important compared to the vector one. But I'm not entirely sure—I may need to double-check that.\n\nLet me recall Fermi’s Golden Rule—it relates transition probabilities between states due to a perturbation. It says that the transition rate W_{fi} is proportional to |<f|H'|i>|² multiplied by the density of final states. Here, i is the initial bound state (ground state of hydrogen), and f is any continuum state corresponding to free electrons.\n\nSince the outgoing electron is treated as a plane wave, ignoring Coulomb interactions post-ionization, the final states are plane waves with definite momenta k. Therefore, the density of states comes into play.\n\nFirst, though, I need to figure out the form of the perturbation H'. Given that the vector potential is A(r,t) = A_0 exp[i(k’·r – ωt)], but under the dipole approximation, we set A(r,t) ≈ A(0,t) = A_0 exp(-iωt), because the exponential factor involving kr averages out over the small volume of the atom.\n\nTherefore, within the dipole approximation, the vector potential is approximately uniform, so A(r,t) ≈ A_0 e^{-iωt}. Then, the perturbation H' arises from the interaction with this vector potential.\n\nAs per the minimal substitution, the Hamiltonian gets modified by replacing p → p - (q/c) A. However, expanding this, the change in the Hamiltonian is ΔH = (q/(2m c)) |A|^2 + ... Wait, hold on—that seems quadratic in A, but our perturbation is supposed to be first-order in the driving field, right? Maybe I’m mixing up something.\n\nNo, actually, the correct expression for the perturbation in the dipole approximation is usually taken as H' = e * x * E(t), where E(t) is the oscillating electric field. Alternatively, working through the vector potential route:\n\nIf the magnetic vector potential is A, then the canonical momentum is p - (e/c) A. Thus, the kinetic energy term becomes (p - (e/c) A)^2 / (2m).\n\nExpanding this, T = p²/(2m) - (e/(2mc)) (p · A + A · p ) + (e²)/(8 m c²) |A|²\n\nNow, neglecting higher order terms beyond linear in A, because the perturbation is weak. Hence, the leading correction is the second term, which is linear in A.\n\nThus, the perturbation H' is equal to - (e/(2 mc)) (p · A + A · p ). Hmm, okay, but in position representation, p is replaced by -iħ ∇, so substituting that in,\n\nH' = (-e/(2 mc))( -iħ (∇ · A) + similar terms )\n\nWait, maybe it's easier to note symmetry—if A is along some direction, say z-axis, then p · A would involve components of p times A_z etc., but regardless, in operator language, H' ~ e A · J, where J includes factors of momentum operators.\n\nHowever, perhaps even simpler—for dipolar transitions, the matrix element <f| H' |i> is significant only when the final state has angular momentum differing by ±1 relative to the initial state. But in this case, the initial state is an s-state, l=0, so the final state must have l=1, hence spherical harmonics Y₁^m.\n\nMoreover, the selection rules tell us that the transition amplitude depends on the integral of the initial state wavefunction multiplied by the perturbation and the final state wavefunction.\n\nGiven all that, perhaps going back to basics: Using time-dependent perturbation theory, the probability P_{if}(t) is approximated by |c_f(t)|², where c_f(t) is given by the time-integrated matrix element divided by ħ squared.\n\nSpecifically, c_f^(1)(t) = (1/iħ) ∫₀^t dt' ⟨f| H'(t') |i⟩ e^{i ω_{fi} t'}, where ω_{fi}=(E_f - E_i)/ħ.\n\nHere, E_i is the energy of the ground state, which is -13.6 eV, and E_f corresponds to the energy of the ionized electron, which is positive (free particle). \n\nBut wait, in reality, the final state isn't a single eigenstate but rather a continuum of states. So integrating over these states requires consideration of the density of states g(E_f).\n\nFermi's golden rule generalizes the transition rate to account for continuous spectra. The formula is Γ = (2π/ħ²) |⟨f| H' |i⟩|² δ(E_f - E_i - ℏ ω)\n\nWhere ω is the frequency of the applied field—in this case, the same as the frequency of the vector potential, which is ω.\n\nBut wait, in our scenario, the photon provides energy ℏ ω, so the condition for resonance is E_f = E_i + ℏ ω. Since E_i is negative (bound state), E_f needs to be greater than E_i by exactly ℏ ω.\n\nBut hang on, the exact threshold for ionization occurs when E_f >= 0, so unless ℏ ω > |E_i|, the electron won't get ionized. If ℏ ω equals |E_i|, it'll just reach the ionization limit. Otherwise, above that, multiple photons could cause ionization—but in this context, since it's a monochromatic wave, each photon carries energy ℏ ω, so whether the sum reaches the required energy determines ionization.\n\nBut in our specific case, the initial state is the ground state, so E_i = -13.6 eV. Suppose the photon energy"}
{"id": "Quantum Mechanics/25-3", "questions": "In a simplified model for defects in high tempo-\nerature superconductors, consider two spin-1/2 oxygen ions on either side of a spin-1/2 copper\nion. Assume that the three spins interact with one another according to the antiferromagnetic\nHeisenberg Hamiltonian\n\n$$\n \nH = JS_1 \\cdot S_3 + K(S_1 + S_3) \\cdot S_2, \n$$\n\n\nwhere $ S_1 $ and $ S_3 $ are the oxygen spins and $ S_2 $ is the copper spin. $ J $ is positive, favoring antiparallel \nalignment of the oxygen spins.\n\n(a) Does $ \\mathbf{S} = S_1 + S_2 + S_3 $ commute with the Hamiltonian?\n\n(b) For $ K = 0 $, solve for the *complete* spectrum. Be sure to state the degeneracies of each energy.\n\n(c) What is(are) the ground state wavefunction(s)? Express your answer(s) as linear combina-\ntions of the states $ |m_1, m_2, m_3 \\> $, where $ m_i = \\pm 1/2 $ is the $ z $-component of the spin of spin \n$ i $.\n\n(d) For $ K = J $, determine the complete spectrum. Again, be sure to state the degeneracies of \neach energy.\n\n(e) Again, what is(are) the ground state wavefunction(s)?", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about a Heisenberg Hamiltonian involving three spin-1/2 particles: two oxygens (S₁ and S₃) and one copper ion (S₂). The Hamiltonian is given as H = J S₁·S₃ + K (S₁ + S₃)·S₂, where J is positive, which means it's antiferromagnetic between the oxygen ions. \n\nPart (a): Does **S** = S₁ + S₂ + S₃ commute with H? Hmm, commuting operators mean they share eigenstates, right? So if [H, S] = 0, then yes. Let me compute the commutator. Since S = sum of all spins, let's see:\n\n[H, S] = [J S₁·S₃ + K (S₁ + S₃)·S₂, S₁ + S₂ + S₃]\n\nI can split this into terms:\n= J[S₁·S₃, S₁ + S₂ + S₃] + K[(S₁ + S₃)·S₂, S₁ + S₂ + S₃]\n\nLet me handle each part separately. First term: [S₁·S₃, S₁], since other parts might vanish or not... Wait, actually, when you take the commutator of S₁·S₃ with any individual spin component like S_j^z, etc., but maybe there's a smarter way.\n\nAlternatively, note that S² = (S₁ + S₂ + S₃)^2 = S₁² + S₂² + S₃² + 2(S₁·S₂ + S₁·S₃ + S₂·S₃)\n\nBut our H has only S₁·S₃ and (S₁ + S₃)·S₂. Maybe express H in terms of total spin components.\n\nWait, perhaps instead think whether H depends on some symmetry related to the total spin. If H doesn't change under rotations generated by S, meaning S is conserved, then they would commute.\n\nLooking closer, H consists of interactions within pairs. But does adding up these pairwise interactions preserve the total spin? Not necessarily unless certain conditions hold.\n\nCompute [H, S_z]: because if we check just the z-component, due to rotational symmetry around z-axis, but wait no—since the interaction isn’t isotropic; however, here both terms involve dot products, which are rotationally invariant. Therefore, H should commute with S² and also with S_z individually.\n\nTherefore, [H, S_z] = 0, similarly [H, S_x] = 0, [H, S_y]=0, hence [H,S]=0. Thus, S commutes with H.\n\nSo the answer is Yes, S commutes with H.\n\nMoving on to part (b): Solve for the complete spectrum when K=0. Then H simplifies to H = J S₁·S₃.\n\nSince K=0, the copper spin S₂ decouples from the system. Because H acts only on S₁ and S₃, while S₂ remains free. So effectively, the Hilbert space splits into product spaces: (state of S₁⊗S₃) ⊗ (state of S₂).\n\nFirst, find the spectrum for S₁·S₃. Remember that S₁·S₃ = (1/4)( (S₁ + S₃)^2 - S₁² - S₃² ). Each S is spin-½, so S₁²=S₃²=(3/4)ħ², assuming ħ=1 units.\n\nThus, S₁·S₃ = (1/4)[ (S_total^{O})² - 3/4 - 3/4 ] = (1/4)(S_total^{O}² - 3/2 )\n\nWhere S_total^{O} is the combined spin of Oxygens, which can be singlet (S=0) or triplet (S=1). Hence,\n\nFor S=0: S_total^{O}=0 ⇒ S₁·S₃ = (1/4)(0 - 3/2 ) = (-3)/8\n\nFor S=1: S_total^{O}=1 ⇒ S₁·S₃ = (1/4)(1*(1+1) - 3/2 ) = (1/4)(2 - 3/2)= (1/4)(1/2)=1/8\n\nSo energies contributed by H are E = J*(-3/8) and E=J*(1/8), corresponding to singlet and triplet respectively.\n\nNow considering the entire system including S₂. When K=0, S₂ is independent, so its contribution comes from being multiplied by identity matrices except acting on itself. However, more precisely, the full Hamiltonian will act non-trivially only on S₁ and S₃, leaving S₂ untouched.\n\nThe possible states are tensor products of the S₁-S₃ states and the S₂ states. Since S₂ is spin-½, it has two states: m=±1/2.\n\nEach combination gives four-fold multiplicity? No, wait: Actually, the S₁-S₃ subsystem has two possibilities: singlet (dimension 1) and triplet (dimension 3). Multiplying by S₂’s dimension 2, overall dimensions are 1×2=2 and 3×2=6.\n\nHowever, looking back, the original Hilbert space is (2x2x2)=8-dimensional. Indeed, 2+6=8.\n\nEnergy levels come from combining the S₁-S₃ energies with arbitrary S₂ states. Specifically, for each S₁-S₃ configuration (singlet/triplet), their respective energies combine with any value of S₂.\n\nSo the energies are:\n\nE_singlet = J*(-3/8)\nand E_triplet = J*(1/8)\n\nAnd each occurs regardless of S₂'s state. So how many times do these occur?\n\nSinglet subspace: dim=1 (for S₁-S₃) × dim=2 (for S₂) → 2 states, same energy.\nTriplet subspace: dim=3 (for S₁-S₃) × dim=2 (for S₂) → 6 states, same energy.\n\nWait, but hang on—if S₂ is completely free, then even though the S₁-S₃ sector contributes different energies depending on their coupling, the presence of S₂ adds nothing else—it just multiplies the number of states without changing the energy further.\n\nYes, correct. So the spectrum is two distinct energies:\n\nLower energy level: E = -3J/8, occurring twice (from singlet x S₂).\nHigher energy level: E = J/8, occurring six times (triplet x S₂).\n\nIs that accurate? Alternatively, could there be mixing? Probably not, since H doesn't affect S₂.\n\nSo the complete spectrum is {-3J/8 (twice), J/8 (six times)}.\n\nProceeding to part (c): Ground state wavefunctions when K=0. From above, the lower energy is -3J/8, arising from the singlet state of S₁ and S₃ coupled together, and any state of S₂.\n\nExpressed as combinations of |m₁,m₂,m₃⟩.\n\nThe singlet state for S₁ and S₃ is (|↑↓⟩ - |↓↑⟩)/√2. To include S₂, which can be ↑ or ↓ independently.\n\nSo the ground states are:\n\n(|↑↓⟩_{1,3} - |↓↑⟩_{1,3}) / √2 ⊗ |↑⟩_2,\nand\n(|↑↓⟩_{1,3} - |↓↑⟩_{1,3}) / √2 ⊗ |↓⟩_2.\n\nWhich translates to:\n\n[ |↑, ↑, ↓⟩ - |↑, ↓, ↑⟩ - |"}
{"id": "Quantum Mechanics/25-4", "questions": "An electron is at rest in an oscillating magnetic field **B** = $ B_0 \\cos(\\omega t) \\hat{\\mathbf{k}} $, where $ B_0 $ and $ \\omega $ are constants.\n\n(a) Construct the Hamiltonian matrix for this system.\n\n(b) The electron starts out (at $ t = 0 $) in the spin-up state with respect to the $ x $-axis. Determine the spin state $ \\chi(t) $ at any subsequent time. *Beware*: This is a time-dependent Hamiltonian, so you cannot get $ \\chi(t) $ in the usual way from stationary states. Fortunately, in this case, you can solve the time-dependent Schrödinger equation directly.\n\n(c) Find the probability of getting $ -\\hbar/2 $, if you measure $ S_x $.\n\n(d) What is the minimum field ($ B_0 $) required to force a complete flip in $ S_x $?", "graphs": null, "llm_answers": "<think>\nOkay, I have this problem about an electron in an oscillating magnetic field. It's part (a), which asks me to construct the Hamiltonian matrix for the system. Hmm, let me think through how to approach this.\n\nFirst off, I remember that when dealing with electrons in magnetic fields, we use the interaction Hamiltonian given by H = -γ ħ B · S, where γ is the gyromagnetic ratio. For an electron, γ is negative because it has a negative charge. But wait, sometimes people write it as H = e/(m_e c) B · L or something like that? No, actually, since spin is involved here, not orbital angular momentum, right?\n\nSo yeah, the correct expression should involve the Pauli matrices multiplied by some factor related to the magnetic moment. Let me recall: the Zeeman term is proportional to the dot product of the magnetic field and the spin operator. So more precisely, H = ω_B S_z + ... but no, wait—the general form would be H = -(eħ)/(2 m_e c) B · σ, where σ represents the Pauli matrices. Is that right?\n\nWait, maybe another way—since the magnetic dipole energy is U = -μ·B, and μ for an electron is μ = g_s (q/(2m)) ħ σ / 2, where g_s is approximately 2. Wait, perhaps better yet, just look up the standard formula for the Hamiltonian due to a magnetic field on an electron’s spin.\n\nI think the Hamiltonian is given by:\n\nH = (-e ħ)/(4 π ε₀ 2 m_e c²) B · σ\n\nBut hold on—that might include factors depending on units. Alternatively, using Gaussian units versus SI could change things. Maybe I'm overcomplicating—it's probably simpler than that.\n\nAlternatively, considering natural units where ħ=1, etc., but perhaps not necessary here.\n\nLet me try again. In quantum mechanics, the spin Hamiltonian in a magnetic field is often written as H = γ B · S, where γ is the gyromagnetic ratio. For an electron, γ = -g_e μ_B / ħ, where g_e ≈ 2 is the electron g-factor, and μ_B is the Bohr magneton.\n\nYes, okay, so putting it all together, the Hamiltonian becomes:\n\nH = γ B(t) · S,\n\nwhere S is the spin operator vector, which in terms of Pauli matrices is S_i = ħ/2 σ_i.\n\nGiven that our magnetic field is along the z-direction, specifically B = B_0 cos(ωt) k̂, then only the z-component matters. Therefore, the Hamiltonian simplifies to:\n\nH = γ B_0 cos(ωt) (S_z).\n\nSubstituting S_z = ħ/2 σ_z, we get:\n\nH = γ B_0 cos(ωt) (ħ/2 σ_z)\n\nNow, plugging in γ for the electron. As mentioned earlier, γ = -g_e μ_B / ħ. And μ_B is defined as (e ħ)/(2 m_e). Thus, substituting back into gamma gives us:\n\nγ = -g_e (e ħ)/(2 m_e) ) / ħ = -g_e e/(2 m_e )\n\nSince g_e ≈ 2, this simplifies further:\n\nγ = -2*(e)/(2 m_e) = -e/m_e\n\nTherefore, now plug this back into H:\n\nH = [(-e/m_e)] * B_0 cos(ωt) * (ħ/2 σ_z)\n\nMultiplying these constants together:\n\nThe coefficient becomes (-e B_0 ħ)/(2 m_e) cos(ωt)\n\nThus, the Hamiltonian is:\n\nH = [ (-e B_0 ħ)/(2 m_e) ] cos(ωt) σ_z\n\nHmm, alternatively, recognizing that the Bohr magneton μ_B is (e ħ)/(2 m_e), so the coefficient is simply -μ_B B_0 cos(ωt)/ ħ times σ_z ?\n\nWait, let me check dimensions. Because H must have units of energy. Let's see:\n\n[e] is Coulombs,\n[B_0] is Tesla,\n[ħ] is J s,\n[m_e] is kg.\n\nCompute the unit of (-e B_0 ħ)/(2 m_e):\n\nC * T * J s / kg\n\nTesla is equivalent to N/(A m^2) → kg/(s^2 A m). So substitute:\n\n[C][kg/(s^2 A m)][J s]/kg =\n\nCanceling kg:\n[C][1/s^2 A m][J s]\n\nNote that C/A = s (because current is Ampere=C/s),\nso [C][1/s^2 A m] = [s^{-1} m^{-1}].\nThen multiply by [J s]:\n[s^{-1} m^{-1}] * [N m s] = [s^{-1} m^{-1}] * [kg m^2/s^2 * s]\nWhich is [kg m^2/s^3].\n\nEnergy is kg m^2/s^2, so seems mismatched. Did I make a mistake somewhere?\n\nWait, perhaps my initial substitution was wrong. Let me double-check the definition of the gyromagnetic ratio.\n\nActually, the gyromagnetic ratio γ is given by γ = q/(2 m), where q is the charge, m mass. For electron, q=-e, so γ = -e/(2 m_e). Then, the Hamiltonian is H = γ B · S.\n\nAnd S is ħ/2 sigma. Hence, H = γ B · (ħ/2 sigma).\n\nPlugging in γ:\n\nH = [-e/(2 m_e)] * B · (ħ/2 sigma)\n   = [-e ħ/(4 m_e)] B · sigma\n\nAh! That makes sense dimensionally. Since each term in H will have units of energy.\n\nBecause [e]=C, [ħ]=J s, [m_e]=kg, [B]=T.\n\nSo compute [e ħ/(4 m_e)]*B:\n\n(C)(J s)/(kg)*T\n\nAgain, T is kg/(s^2 A m). Substitute:\n\n(C)(J s)/(kg)*(kg/(s^2 A m))\n\nSimplify:\n\nC/(s A m) * J s\n\nBut C/(A)=s, so:\n\n(s)/(m) * J s\n\nJ is kg m^2/s^2, thus:\n\n(kg m^2/s^2) * s^2 / m = kg m / s^0 ? Wait, no:\n\nHold on, let's do it carefully:\n\n(e ħ B)/(m_e): Units are (C)(J s)(T)/kg.\n\nConvert T to base units: T = kg/(s^2 A m).\n\nSo overall:\n\nC * (J s) * (kg/(s^2 A m)) / kg\n\nSimplify numerator:\n\nC * J s * kg / (s^2 A m)\n\nDenominator: kg cancels.\n\nLeft with:\n\nC * J s / (s^2 A m) = (C/J s) ??? Wait, confused.\n\nAlternative route: Compute each component.\n\n[H] needs to be Joules.\n\nCheck [gamma]: [gamma] = [charge]/([mass]) = C/kg.\n\n[Sigma] is dimensionless; hence, [sigma_z] is same as identity matrix, i.e., dimensionless.\n\nMultiply [gamma]*[B]*[sigma], so [gamma][B]:\n\n(gamma)[B] = (C/kg)(kg/(s^2 A m)) = (C)/(s^2 A m)\n\nBut what is [H]? If H = gamma*B*sigma, then its unit is (C)/(s^2 A m) * (dimensionless)? Not sure.\n\nPerhaps going back—if I express everything in terms of known quantities.\n\nBohr magneton mu_B is (e h bar)/(2 m_e). So mu_B has units of J/T.\n\nIndeed, because torque tau = mu × B, so tau has units N m = J/rad, while mu has units J/T.\n\nSo, yes, mu_B is indeed measured in J/T.\n\nIf H = -mu_B B . sigma, then [H] = [mu_B][B] = (J/T)(T) = J, which works.\n\nTherefore, perhaps expressing H as H = -mu_B B(t) sigma_z.\n\nThat sounds much cleaner.\n\nIn fact, recalling that the Zeeman effect Hamiltonian is typically"}
{"id": "Quantum Mechanics/29-6", "questions": "(a) Counting spin, there are 2n² energy eigenstates in hydrogen with principal quantum number n. Sketch the energy levels for n = 1, 2, 3, including spin-orbit and other fine structure effects. Arrange the energy levels in columns, labelled by ℓ value. Draw the energy levels as short horizontal lines in each column, but if there are several states of a given angular momentum and energy, just draw one line. Label the energy level with its quantum numbers (but not magnetic quantum numbers). The drawing need not be to scale, but it should show the ordering of the energy levels and give some qualitative idea of their relative spacing.\n\n(b) Sketch in the allowed electric dipole transitions between levels of different n. State the selection rules you are using.\n\n(c) Derive the Clebsch-Gordan coefficients required to couple the orbital and spin angular momentum for a p state.\n\n(d) Let $R_{n\\ell}(r)$ be the radial wave functions in hydrogen, let $Y^m_\\ell(\\Omega)$ be the angular wave functions, and let $\\alpha=\\binom{1}{0}$ and $\\beta=\\binom{0}{1}$ be the spin wave functions. Write the full wave functions for all four n = 2 states of hydrogen in terms of these.", "graphs": null, "llm_answers": "<think>\nAlright, so I've got this problem about hydrogen atoms and various aspects related to them. It's part (a), (b), (c), and (d). Hmm, okay, starting with part (a).\n\nPart (a): They mention that counting spin, there are 2n² energy eigenstates for hydrogen with principal quantum number n. So, they want me to sketch the energy levels for n=1, 2, 3, considering spin-orbit and other fine structure effects. These should be arranged in columns labeled by l values. Each column will have short horizontal lines representing energy levels, and even though multiple states might share the same energy within a column, we only draw one line per unique energy. Also, label each level with its quantum numbers except m.\n\nOkay, first off, without any perturbations like spin-orbit coupling, the energy levels depend solely on n, right? But when you include spin-orbit interactions, which is part of the fine structure, the degeneracy breaks down further based on the total angular momentum j. For each state with specific n and l, there can be two possible j values: j = l + s and j = |l - s|. Since electrons have spin-½, s=½.\n\nSo, for example, take n=1:\n\nFor n=1, l can only be 0 because l < n. So l=0, which gives us s=½, hence j can be ½ or... wait no, since l=0, then j must equal s, which is ½. Wait, hold on—if l=0, adding spin doesn't change anything; both possibilities would collapse into j=½. Because l=0, so j = |0 ± ½| → either ½ or -½, but negative isn't considered, so j=½. Therefore, for n=1, regardless of spin, all states have the same energy—so actually, maybe my initial thought was wrong?\n\nWait, perhaps I'm confusing something here. Spin-orbit interaction causes splitting depending on j. If l=0, then j=s=½ always, meaning no additional splitting beyond what comes from spin itself. However, in reality, does the ground state split due to spin-orbit effect? Or do higher orbitals start showing more splittings?\n\nLet me think again. In hydrogen-like atoms, the fine structure correction includes relativistic kinetic energy corrections, spin-orbit coupling, and the Darwin term. The main contribution causing splitting among different l-values within the same n is the spin-orbit interaction. Specifically, the formula for the fine structure energy shift is proportional to α² Ry / n³ (j(j+1) - l(l+1) - s(s+1)) )/(something)... Maybe better to recall that the leading order term depends on j(j+1)-l(l+1)/2.\n\nBut perhaps instead of getting bogged down in formulas, remember that for each n, the states with lower l tend to lie above those with higher l after accounting for spin-orbit coupling. That’s called \"inversion\" where normally higher l has lower energies classically, but quantum mechanically with spin-orbit, sometimes higher l ends up having slightly higher energy.\n\nAlternatively, another approach: Without spin-orbit, all states with same n have same E. With spin-orbit, each state splits according to j. For each (n,l), j can be l±½. Then, the energy shifts ΔE ∝ [j(j+1) - l(l+1)]/2. Thus, for fixed n and l, larger j leads to higher energy.\n\nTherefore, for each (n,l):\n\nIf l > 0, then j can be l + ½ or l – ½. Hence, two sublevels. And the one with higher j is higher in energy.\n\nSimilarly, for l=0, j can only be ½, so no splitting—it remains single.\n\nThus, arranging the energy levels for n=1,2,3:\n\nStarting with n=1:\n\nOnly l=0, so j=½. Single state, so one horizontal line labeled (1,0,½)—though usually written as ¹S_½.\n\nMoving to n=2:\n\nPossible l=0 and l=1.\n\nEach l contributes how many j levels?\n\nFor l=0: Only j=½.\n\nFor l=1: Two j levels: j=3/2 and j=1/2.\n\nHence, three distinct energy levels under spin-orbit coupling.\n\nWhich ones are higher? As mentioned earlier, higher j implies higher energy. So for l=1, j=3/2 is higher than j=1/2.\n\nAlso, comparing across different l's: How does the energy compare between l=0 vs l=1?\n\nIn general, for same n, higher l tends to lead to lower unperturbed energy (since expectation value of potential is stronger for smaller orbits?), but with spin-orbit, things invert a bit.\n\nActually, the exact position requires knowing whether the spin-orbit term makes certain l's go up or down compared to others.\n\nThe standard result is that for hydrogen, the spin-orbit interaction causes the levels with higher l to have higher energy. So, for n=2, l=1 lies below l=0 before spin-orbit, but after spin-orbit, the situation may reverse.\n\nWait, confusion arises. Let me clarify.\n\nWithout spin-orbit, all n=2 states (both l=0 and l=1) have same energy. When spin-orbit is included, the energy becomes dependent on j.\n\nMoreover, the overall trend is such that for a given n, the maximum j occurs for the highest l. So, for n=2, l=1 allows j=3/2, while l=0 only goes up to j=1/2. So, the j=3/2 state (from l=1) is higher in energy than the j=1/2 state (also from l=1) and also higher than the j=1/2 state from l=0.\n\nHowever, the question says 'including spin-orbit and other fine structure effects.' Fine structure typically refers to small energy shifts, so the dominant factor is still n, followed by l, but now with j playing a role.\n\nI think the correct way is to arrange the energy levels such that for each n, the states are ordered primarily by increasing l, but within each l, the j determines the slight shifts.\n\nWait, but in actuality, the spin-orbit effect lifts the degeneracy within the same n and l, creating separate j levels. Additionally, the Lamb shift affects s-states differently, making them slightly lower in energy.\n\nHmm, perhaps overcomplicating. Given that the user wants a rough sketch, probably focusing on grouping by l and noting the j-splitting.\n\nPerhaps the best approach is:\n\nFor each n, list the possible l's, and for each l, note the possible j's, thus determining the number of energy levels.\n\nThen, arrange the energy levels vertically, with higher energy towards top.\n\nGiven that, for n=1:\n\nOne l=0, giving j=½. One energy level.\n\nFor n=2:\n\nTwo l's: l=0 (j=½) and l=1 (two j's: 3/2 and 1/2). So, three energy levels.\n\nTypically, the order is such that for same n, higher l corresponds to lower energy in non-relativistic treatment, but with spin-orbit, higher j means higher energy. So, for n=2, the j=3/2 (which comes from l=1) is higher than the j=1/2 from l=1, which in turn is higher than the j=1/2 from l=0.\n\nIs that accurate? Alternatively, could the l=0 state be shifted less, keeping it lower than the l=1 states?\n\nThis needs precise knowledge. From memory, in hydrogen, the 2p states (l=1) split into two j-levels: 2P_{3/2} and 2P_{1/2}, with P_{3/2} being higher in energy. Meanwhile, the 2s state (l=0) stays as 2S_{1/2}."}
{"id": "Quantum Mechanics/29-1", "questions": "A particle of mass m is confined in an infinite square well potential $V(x) = 0$ for $0 \\le x \\le b$ and $V(x) = \\infty$ otherwise.\n\n(a) Find the eigenvalues and eigenfunctions of this particle.\n\n(b) Add $V_1(x) = \\epsilon \\sin \\dfrac{\\pi x}{b}$ to V. Find the energy shifts for all the excited states to first order in $V_1$.\n\n(c) Replace $V_1$ with $V_2(x, t) = \\lambda \\left( x - \\dfrac{b}{2} \\right) \\sin \\omega t$. If the particle is in the ground state at $t = 0$, what is the probability that the particle is in the first excited state at some later time t? What about the second excited state?", "graphs": null, "llm_answers": "<think>\nOkay, so I have these three parts to solve related to a particle in an infinite square well. Let me start with part (a). \n\nPart (a): Finding Eigenvalues and Eigenfunctions\n\nAlright, I remember from my quantum mechanics class that the infinite square well has solutions where the wave functions are sinusoidal inside the well and zero outside. The setup here is between x=0 and x=b, right?\n\nThe Schrödinger equation for such a system should be:\n\n\\[\n-\\frac{\\hbar^2}{2m}\\frac{d^2\\psi_n(x)}{dx^2} + V(x)\\psi_n(x) = E_n\\psi_n(x)\n\\]\n\nBut since \\( V(x) = 0 \\) within [0,b], it simplifies to:\n\n\\[\n-\\frac{\\hbar^2}{2m}\\frac{d^2\\psi_n(x)}{dx^2} = E_n\\psi_n(x)\n\\]\n\nThis leads us to the differential equation:\n\n\\[\n\\frac{d^2\\psi_n(x)}{dx^2} = -k_n^2\\psi_n(x)\n\\]\nwhere \\( k_n = \\sqrt{\\frac{2mE_n}{\\hbar^2}} \\)\n\nSo the general solution would involve sine and cosine terms because we're dealing with negative curvature. But considering the boundary conditions: ψ must go to zero at both ends, x=0 and x=b. So if I plug in x=0 into the general solution, which includes sin(kx), cos(kx)... Well, sin(0)=0, but cos(0)=1. To satisfy ψ(0)=0, the coefficient of the cosine term must be zero. Similarly, applying ψ(b)=0 gives another condition on kn.\n\nLet's write down the specific form:\n\nψn(x) = A sin(knx)\n\nApplying ψ(b)=0:\nsin(knb) = 0 => knb = nπ, where n is integer (since sine function zeros occur at multiples of π).\n\nThus,\n\nkn = nπ / b\n\nTherefore, substituting back into the expression for En:\n\nEn = (ħ² kn²)/(2m) = (ħ² n² π² )/(2m b² )\n\nAnd normalizing the wavefunction. Since the integral over [0,b] of |ψ|² dx must equal 1.\n\nCompute normalization constant A:\n\n∫₀ᵇ |A|^2 sin²(nπx/b) dx = 1\n\nI recall that ∫ sin²(ax) dx over one period is half the interval length times ½. Wait, more precisely, integrating sin²(u):\n\nUsing identity: sin²u = (1 - cos(2u))/2\n\nIntegral becomes:\n\n|A|² * ∫₀ᵇ (1/2)(1 - cos(2nπx/b)) dx = 1\n\nWhich evaluates to:\n\n|A|²*(b/2 - (b/(4nπ))(sin(2nπ))) = |A|²*b/2 = 1\n\nSince sin(2nπ)=0, as n is integer.\n\nHence, |A|=√(2/b)\n\nSo the normalized eigenfunctions are:\n\nψn(x) = sqrt(2/b) sin(nπx/b)\n\nEigenvalues:\n\nEn = (n² h-bar squared pi squared)/(2m b squared)\n\nWait, let me double-check units... Yeah, ħ has dimensions of J·s, so when you square it divided by kg*m², yeah, it'll give Joules, correct.\n\nSo that seems solid for part (a).\n\nMoving on to part (b): Adding V₁(x) = ε sin(πx/b). Need to find energy shifts using perturbation theory up to first order.\n\nFirst-order perturbation says that the shift ΔE^(1)_n is given by <n|V₁|n>.\n\nBecause V₁ is added to the original Hamiltonian H0, then the new energies will get corrections.\n\nGiven that V₁(x) = ε sin(πx/b), compute expectation value ⟨ψₙ|ε sin(πx/b)|ψₙ⟩.\n\nThat is:\n\nΔE_n^{(1)} = ε ∫₀ᵇ ψₙ* sin(πx/b) ψₙ dx\n\nSubstituting ψₙ(x) = sqrt(2/b) sin(nπx/b):\n\nΔE_n^{(1)} = ε (2/b) ∫₀ᵇ sin²(nπx/b) sin(πx/b) dx\n\nHmm, need to evaluate this integral.\n\nNote that sin(A) sin(B) can be expressed via product-to-sum identities, or perhaps use orthogonality relations.\n\nAlternatively, think whether the integrand is symmetric around certain points.\n\nWait, maybe express sin(πx/b) multiplied by sin²(nπx/b). Hmm.\n\nExpressed differently, note that sin²(theta) = (1 - cos(2theta))/2.\n\nThen, our integrand becomes:\n\n[1 - cos(2nπx/b)]/2 * sin(πx/b)\n\nMultiply through:\n\n(1/2)[sin(πx/b) - sin(πx/b)cos(2nπx/b)]\n\nNow, integrate each term separately.\n\nFirst term: Integral of sin(πx/b)/2 from 0 to b.\n\nSecond term: Integral involving sin(pi x/b) cos(2n pi x/b). Use trig identity again: sin A cos B = [sin(A+B)+sin(A-B)]/2.\n\nSo overall, split into two sines.\n\nPutting together:\n\nΔE_n^{(1)} = ε (2/b)*(1/2)*[Integral sin(pi x/b) dx - Integral sin((2n+1)pi x/b) dx /2 - Integral sin((2n-1)pi x/b) dx /2 ]\n\nEvaluate each integral from 0 to b.\n\nFor the first integral: ∫ sin(pi x/b) dx from 0 to b.\n\nAntiderivative is -(b/pi) cos(pi x/b). Evaluated from 0 to b:\n\n-(b/pi)[cos(pi) - cos(0)] = -(b/pi)[-1 -1] = 2b/pi\n\nSimilarly, other integrals:\n\nConsider ∫ sin(m pi x/b) dx from 0 to b, where m is any integer not multiple of something...\n\nIf m ≠ 0, antiderivative is -(b/m pi) cos(m pi x/b). Evaluate from 0 to b:\n\n-(b/m pi)[cos(m pi) - cos(0)] = -(b/m pi)[(-1)^m -1]\n\nCase 1: When m even: (-1)^m =1; thus result is -(b/m pi)(1 -1)=0\n\nWhen m odd: (-1)^m=-1; result is -(b/m pi)(-1 -1)= (2b/m pi)\n\nIn our case, looking at the second and third integrals:\n\nThey correspond to m=(2n ±1). Depending on whether 2n±1 is even or odd.\n\nBut wait, regardless, unless m=0, they won't vanish only if m is non-zero integers.\n\nHowever, notice that for the second and third integrals, their coefficients come with factors depending on n.\n\nSpecifically, after splitting, the expressions become:\n\nAfter expanding everything, the total expression is:\n\nΔE_n^{(1)} = ε (2/b)*(1/2)*[ (2b/pi) - (1/2){ [if (2n+1)≠0, same logic applies ] } ]\n\nWait, actually, hold on. Maybe better approach is to consider symmetry.\n\nLooking back, the initial integral was:\n\n∫₀ᵇ sin²(nπx/b) sin(πx/b) dx\n\nWe might also observe that sin²(nθ) sin θ can sometimes lead to selection rules based on parity or orthogonality.\n\nAnother thought: For the matrix element ⟨n|V₁|m⟩, due to the nature of the perturbing potential being proportional to sin(πx/b), which resembles the transition dipole moment operator.\n\nBut in this problem, since we’re computing diagonal elements (same state), i.e., ⟨n|V₁|n⟩, does this necessarily imply anything?\n\nWell, except for cases where there’s overlap—wait, no, because V₁ itself isn’t connecting different states—it just adds a small correction.\n\nBut going back, evaluating the integral:\n\nLet me denote theta = pi x / b, so d theta = pi / b dx, meaning dx = (b/pi)d theta.\n\nLimits"}
{"id": "Quantum Mechanics/23-1", "questions": "A system is governed by a real Hamiltonian and whose wavefunction is real at t = 0 and at a later time t = t_1. Show that the wavefunction is periodic in time. What is the period, T? Show that the energy must come in integer multiples of 2πħ/T.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about quantum mechanics where the system has a real Hamiltonian, and its wave function is real both at time zero and some later time t₁. The question asks me to show two things: first, that the wave function is periodic in time with some period T, and second, that the energies must be integer multiples of (2πħ)/T.\n\nHmm, let's start from what I know. In quantum mechanics, the time evolution of a state is given by the Schrödinger equation:\n\niħ ∂ψ/∂t = H ψ\n\nSince the Hamiltonian H is real, it means that all its matrix elements or components are real numbers. Also, if the initial wave function ψ(x, 0) is real, then how does it evolve over time?\n\nWait, but usually when you solve the Schrödinger equation for a real potential, like in the particle-in-a-box scenario, even though the solutions can involve complex exponentials due to their separation into space and time parts, sometimes people take linear combinations to make them real. But here, we're told not only that the initial condition is real but also that at a later time t₁, the wave function remains real. That seems important because typically, unless certain conditions on the eigenstates hold, the wave function might become complex as time evolves.\n\nSo maybe there's something special happening with the energy levels or the way they combine such that after some time, the phase factors cause the imaginary parts to cancel out again, making the wave function real once more.\n\nLet me think about expanding the wave function in terms of energy eigenstates. Suppose ψ(x, t) can be written as a sum over n of c_n φ_n(x) e^{-iE_n t / ħ}, right? Here, each term corresponds to an energy eigenstate φ_n multiplied by a time-dependent exponential factor involving E_n.\n\nNow, since the wave function is initially real, ψ(x, 0) is real. So, substituting t=0 gives us Σc_n φ_n(x). For this entire expression to be real, either all coefficients c_n are real, or perhaps they form conjugate pairs. Wait, no—if the functions φ_n themselves could be complex, which they generally aren't necessarily—since H is Hermitian, the eigenfunctions can always be chosen to be real except possibly for degenerate cases. Hmm, actually, wait—the eigenfunctions don’t need to be real; they just need to satisfy orthogonality. However, for symmetric potentials, often one chooses real eigenfunctions.\n\nBut regardless, suppose that the expansion coefficients c_n are arranged such that the overall sum is real. If all c_n are real, then yes, the sum would be real. Alternatively, if any c_n were complex, say c_m = a + ib, then another coefficient c_{m'} should compensate for the imaginary part. Maybe through having c_{n} being the complex conjugates of others? Not sure yet.\n\nMoving forward, now considering that at time t=t₁, the wave function becomes real again. Let’s write ψ(x, t₁):\n\nψ(x, t₁) = Σ c_n φ_n(x) e^{-iE_n t₁ / ħ}\n\nFor this to be real, similar reasoning applies—it needs to equal its own complex conjugate. Taking the complex conjugate,\n\n[ψ(x, t₁)]* = Σ [c_n]* φ_n*(x) e^{+iE_n t₁ / ħ}\n\nIf the original ψ(x, t₁) equals this, then:\n\nΣ c_n φ_n(x) e^{-iE_n t₁ / Ą} = Σ [c_n]* φ_n*(x) e^{+iE_n t₁ / Ā}\n\nAssuming that the eigenfunctions φ_n are real (which may not always be true, but perhaps under our assumptions), then φ_n*(x)=φ_n(x). Then the above simplifies to:\n\nΣ c_n φ_n(x) e^{-iE_n t₁ / ā} = Σ [c_n]* φ_n(x) e^{+iE_n t₁ / ā}\n\nWhich implies that for each n, c_n e^{-iE_n t₁ / ā} = [c_n]* e^{+iE_n t₁ / ā}. \n\nTaking modulus squared on both sides isn't helpful directly, but rearranging:\n\n[c_n]*/c_n = e^{-i(2 E_n t₁)/ ā }\n\nThis suggests that |c_n|² = 1, meaning that each c_n lies on the unit circle in the complex plane. Therefore, c_n = e^{iθ_n} for some angle θ_n.\n\nSubstituting back, we get:\n\ne^{-i(E_n t₁ - θ_n)} = e^{+i(E_n t₁ + θ_n)}\n\nTherefore, equating exponents modulo 2π:\n\n-E_n t₁ + θ_n ≡ E_n t₁ + θ_n mod 2π\n\nSimplify:\n\n-2 E_n t₁ ≡ 0 mod 2π\n\nThus,\n\nE_n t₁ = π k_n , where k_n is an integer multiple.\n\nHence,\n\nE_n = (k_n π ) / t₁\n\nAlternatively, writing this differently, E_n = (2 π m_n )/(2 t₁ ), but probably better expressed as E_n proportional to integers times some base frequency related to t₁.\n\nWait, but looking back, my conclusion was E_n t₁ = π k_n, hence E_n = (π k_n)/t₁. Is that correct? Or did I miss a negative sign somewhere?\n\nActually, going back to the equality:\n\nFrom earlier steps:\n\nAfter taking complex conjugate and setting equal, we had:\n\nc_n e^{-iE_n t₁ / ā } = [c_n]^* e^{+iE_n t₁ / ā }\n\nDivide both sides by c_n:\n\ne^{-iE_n t₁ / ā } = ([c_n]^*)/[c_n ] * e^{+iE_n t₁ / ā }\n\nNote that [c_n]^*/c_n is e^{-i2θ_n }, assuming c_n = e^{iθ_n }. Thus,\n\nLeft side: e^{-iE_n t₁ / ā }\nRight side: e^{-i2θ_n } e^{+iE_n t₁ / ā }\n\nSet equal:\n\ne^{-iE_n t₁ / ā } = e^{-i2θ_n } e^{+iE_n t₁ / ā }\n\nMultiply both sides by e^{+iE_n t₁ / ā }:\n\ne^{-i2E_n t₁ / ā } = e^{-i2θ_n }\n\nTake logarithms (mod 2π):\n\n-2 E_n t₁ / ā ≡ -2 θ_n mod 2π\n\nCanceling negatives and dividing both sides by 2:\n\nE_n t₁ / ā ≡ θ_n mod π\n\nBut θ_n is arbitrary up to addition of multiples of 2π, but since we’re working modulo π, θ_n can differ by π. This complicates matters slightly.\n\nPerhaps instead, consider specific values. Since the wave function is real at t=0 and t=t₁, these impose constraints on the phases accumulated during time evolution.\n\nAnother approach: Because the wave function is real at t=0 and t=t₁, the time-evolution operator U(t) must map real states to real states. Hence, U(t) must commute with complex conjugation. Which imposes that U(t) is a real operator, i.e., U(t)* = U(t).\n\nGiven that U(t) = exp(-i H t / ħ ). Its adjoint is exp(+i H^* t / ħ ). Since H is real, H^*=H, thus U†(t) = exp(i H t / ħ ).\n\nFor U(t) to be real, we require U(t)^* = U(t). So,\n\nexp(-i H t / ħ )* = exp(+i H t / ħ )\n\nWhich requires that exp(+i H t / ħ ) = exp(-i H t / ħ )\n\nThat holds only if exp(2i H t / ħ ) = identity.\n\nBecause multiplying both sides by exp(-i H t / ħ ):\n\nexp(+i H t / ħ ) exp(-i H t / ħ ) = Identity.\n\nNo, that doesn't help much. Instead, set exp(+i H t / ħ ) = exp(-i H t / ħ )\n\nThen, exp(2i H t / ħ ) = Identity.\n\nWhich implies that 2 H t / ħ = 2π N, where N is an integer matrix? No, rather, each eigenvalue satisfies 2 E_n t / ħ = 2π k_n, leading to E_n t / ħ = π k_n => E_n = (π ħ k_n)/t.\n\nWait, that makes sense! Each energy level must be quantized such that E_n t / ħ is an integer multiple of π. So, E_n = (π"}
{"id": "Quantum Mechanics/28-3", "questions": "Consider a string stretched from _x = 0_ to _x = a_, with a constant tension $T_0$ and a position-dependent mass density _μ(x)_. The string is fixed to _y = 0_ at the endpoints and can vibrate in the _y_ direction.\n\n(a) Set up a variational procedure giving an upper bound on the lowest oscillation frequency, _ω_0_. _[Hint: Usually, the inner product is$\\left\\langle\\psi_i \\mid \\psi_j\\right\\rangle \\equiv \\int \\psi_i^*(x) \\psi_j(x) d x$. But, you could stick a sufficiently well-behaved, non-negative function in their too. Consider $\\left\\langle\\psi_i \\mid \\psi_j\\right\\rangle \\equiv \\left.\\int \\mu(x) \\psi_i^*(x) \\psi_j(x) d x .\\right]$.]\n\n(b) For $μ(x) = μ_0 \\frac{x}{a}$, use your variational principle to find a simple bound on the lowest oscillation frequency. Compare with the real answer $ω_0^2 ≈ (18.956)\\frac{T_0}{μ_0a^2}$ obtained by computer.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about a vibrating string with variable linear mass density. It's part (a) and (b). Let me try to figure out how to approach it.\n\nStarting with part (a): We need to set up a variational procedure for finding an upper bound on the lowest oscillation frequency ω₀. Hmm, okay, I remember that in quantum mechanics, we often use variational methods to approximate ground state energies, but here it's applied to vibrations of a string. So maybe similar principles apply?\n\nThe hint mentions something about changing the inner product definition. Normally, the inner product ⟨ψᵢ | ψⱼ⟩ would be ∫ψᵢ*ψⱼ dx, but they suggest using ∫μ(x)ψᵢ*ψⱼ dx instead. That makes sense because if the mass distribution isn't uniform, the natural weighting should involve μ(x).\n\nSo, thinking back to Sturm-Liouville problems or eigenvalue problems in differential equations... Yeah, when dealing with weighted integrals, sometimes you adjust the inner products accordingly. In our case, since the equation involves μ(x), which appears as a coefficient multiplying y''(x), perhaps the appropriate inner product includes μ(x).\n\nLet me recall the general form of such a system. A vibrating string satisfies the wave equation:\n\nρ(x) * (∂²y/∂t²) = T₀ * (∂²y/∂x²)\n\nWhere ρ(x) is the linear mass density, given as μ(x). Wait, actually, no—wait, hold on. If the string has a varying mass per unit length μ(x), then the standard wave equation becomes:\n\nμ(x) * (∂²y/∂t²) = T₀ * (∂²y/∂x²)\n\nYes, right? Because normally, the wave speed depends on both tension and mass density. So yes, the equation is μ times acceleration equals tension times curvature.\n\nFor small transverse vibrations, assuming harmonic time dependence, say y(x,t)=Y(x)e^{-iω t}, plugging into the equation gives us:\n\n-μ(x) ω² Y(x) = -T₀ Y''(x)\n\nWhich simplifies to:\n\nT₀ Y'' + μ(x) ω² Y = 0\n\nOr,\n\nY'' + [μ(x)/T₀] ω² Y = 0\n\nThis looks like a Schrödinger-like equation where the potential term is zero, except multiplied by some coefficients. To make it more precise, let's write it as:\n\nY'' + k²(x) Y = 0\n\nwhere k²(x) = [μ(x)/T₀] ω²\n\nBut wait, not exactly; rather, it's just proportional to ω squared times μ over T₀. Anyway, moving forward.\n\nIn terms of solving for the frequencies, especially the fundamental one, we might want to consider boundary conditions. Since the string is fixed at x=0 and x=a, the displacement must satisfy Y(0)=Y(a)=0.\n\nNow, setting up a variational method usually involves expressing the energy functional and minimizing it subject to normalization constraints. Alternatively, Rayleigh quotient comes into play.\n\nWait, yeah! The Rayleigh-Ritz method. This technique allows approximating the smallest eigenvalues by choosing trial functions and optimizing them within certain classes.\n\nGiven that, the idea is to express the total energy (kinetic plus potential) in terms of the mode shape Y(x), compute its ratio (like kinetic divided by potential?), and minimize that ratio to get the lower bound on the frequency.\n\nAlternatively, considering the Lagrangian formalism, the action integral leads to the Euler-Lagrange equation, which is what we started with.\n\nBut focusing on the variational aspect, suppose we define two types of energies: the potential energy stored due to stretching, and the kinetic energy associated with motion.\n\nPotential Energy (U):\n\nWhen the string vibrates, each element stretches slightly. Assuming small displacements, the strain energy U can be expressed as:\n\nU = (1/2) ∫₀ᵃ T₀ (dy/dx)^2 dx\n\nSimilarly, Kinetic Energy (K):\n\nEach infinitesimal segment also has kinetic energy depending on its velocity and mass. Thus,\n\nK = (1/2) ∫₀ᵃ μ(x) y'^2 dt ?\n\nWait, no, K is integrated over space, not time. Actually, integrating over all positions along the string:\n\nAt any instant, the kinetic energy is:\n\nK = (1/2) ∫₀ᵃ μ(x) (dy/dt)^2 dx\n\nSince we're looking for stationary solutions, probably assume sinusoidal variation, so dy/dt ~ iω y, hence (dy/dt)^2 ~ ω² y².\n\nTherefore, combining these, the total energy E is U + K, but for standing waves, there's a relation between maximum potential and kinetic energies.\n\nHowever, for the purpose of forming a Rayleigh quotient, perhaps we take the ratio of potential to kinetic energy.\n\nWait, another thought: Maybe think in terms of Hamilton's principle, where the action S is minimized, leading to the governing equation.\n\nBut stepping back, perhaps better to model after the Helmholtz equation setup.\n\nWe had earlier:\n\nY'' + (μ/T₀) ω² Y = 0\n\nWith BCs Y(0)=Y(a)=0.\n\nTo frame this as a self-adjoint operator, multiply through by suitable factors.\n\nIf we rewrite the ODE as:\n\n(T₀ Y')' / T₀ + (μ ω² / T₀) Y = 0\n\nHmm, alternatively, writing it as:\n\n-(T₀ Y')' + μ ω² Y = 0\n\nBecause bringing everything to left-hand side:\n\n-T₀ Y'' + μ ω² Y = 0 → same as above.\n\nThus, arranging it as -(T₀ Y')' + μ ω² Y = 0.\n\nComparing to the standard Sturm-Liouville form:\n\n-(p Y')' + q Y = λ w Y\n\nHere, p=T₀, q=0, and weight function w=μ.\n\nHence, the eigenfunctions will correspond to different modes, and the eigenvalues relate to ω² via λ = μ ω² / T₀.\n\nWait, rearranging:\n\nFrom the equation:\n\n-(T₀ Y')' + μ ω² Y = 0 ⇒ -(T₀ Y')' = -μ ω² Y\n\nDivide both sides by T₀:\n\n-Y'' = (-μ ω² / T₀ ) Y\n\nNot quite matching SL directly yet. Perhaps scaling variables differently.\n\nAnyway, regardless, knowing that the problem reduces to a generalized eigenvalue problem with weights involving μ(x).\n\nIn such cases, the Rayleigh quotient R is defined as:\n\nR[Y] = (⟨Y| L |Y⟩)/(⟨Y| W |Y⟩ )\n\nWhere L is the differential operator and W is the weight matrix/operator.\n\nSpecifically, in our context, the numerator corresponds to the potential energy and denominator relates to the kinetic energy.\n\nWait, going back, the Rayleigh quotient for the Helmholtz equation is typically written as:\n\n∫₀ᵃ [ (Y')² ] dx / ∫₀ᵃ [ Y² ] dx\n\nAnd the minimum value of this quotient gives the square of the first eigenfrequency.\n\nBut in our case, the presence of μ(x) complicates things. Hence, adjusting the inner product appropriately.\n\nAs suggested in the hint, redefine the inner product as ⟨ψᵢ | ψⱼ⟩ = ∫ μ(x) ψᵢ* ψⱼ dx.\n\nThen, following the usual steps, the Rayleigh quotient would become:\n\nR = [ ∫₀ᵃ (Y'(x))² dx ] / [ ∫₀ᵃ μ(x) Y(x)² dx ]\n\nMultiplying numerator and denominator by constants related to T₀ and other parameters...\n\nWait, original PDE was:\n\nT₀ Y'' + μ(x) ω² Y = 0\n\nMultiply both sides by Y and integrate over [0,a]:\n\n∫₀ᵃ T₀ Y'' Y dx + ∫₀ᵃ μ(x) ω² Y² dx = 0\n\nIntegrate the first term by parts:\n\nFirst term:\n∫ Y'' Y dx ="}
{"id": "Quantum Mechanics/28-4", "questions": "An illuminating alternative derivation of the WKB formula, $\\psi(x) \\sim \\frac{C}{\\sqrt{p(x)}} \\exp \\left[ \\pm \\frac{i}{\\hbar} \\int p(x) d x\\right]$,\n\nis based on an expansion in powers of $\\hbar$. Motivated by the free particle wave function, $\\psi = Ae^{\\pm ikx}$, we write $\\psi(x) = e^{i f(x)/\\hbar}$, where $f(x)$ is some complex function. [Note that there is no loss of generality here - any nonzero function can be written in this way.]\n\n(a) Put this into Schrödinger’s equation and show that if $ i\\hbar f'' - (f')^2 + p^2 = 0$.\n\n(b) Write $ f(x) $ as a power series in $\\hbar: \\, f(x) = \\sum_{n=0}^{\\infty} \\hbar^n f_n(x) $, and, collecting like powers of $\\hbar$, show that\n\n$$\n\\left(f_0^{\\prime}\\right)^2=p^2, \\quad i f_0^{\\prime \\prime}=2 f_0^{\\prime} f_1^{\\prime}, \\quad i f_1^{\\prime \\prime}=2 f_0^{\\prime} f_1^{\\prime}+\\left(f_1^{\\prime}\\right)^2, \\quad \\text { etc. }\n$$\n\n\n(c) Solve for $ f_0(x) $ and $ f_1(x) $, and show that - to first order in $\\hbar$ - you recover the WKB result.\n\nNote: The logarithm of a negative number is defined by $\\ln(-z) = \\ln(z) + in\\pi$ where n is an odd integer. If this formula is new to you, try exponentiating both sides, and you’ll see where it comes from.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about deriving the WKB approximation using perturbation theory with respect to ħ. It's part (a), (b), and (c). Let me start with part (a).\n\nThe question says that given ψ(x) = exp(i f(x)/ħ), plug this into the Schrödinger equation and get an equation involving f''. Hmm, okay. So let's recall the time-independent Schrödinger equation:\n\n-ħ²/(2m) * ψ''(x) + V(x)ψ(x) = Eψ(x)\n\nBut wait, actually, more precisely, since they mentioned p(x), which usually stands for momentum, maybe they're writing the Hamiltonian differently? Or perhaps considering kinetic energy plus potential?\n\nWait, but looking back, the user wrote \"the WKB formula...\". In standard notation, the WKB ansatz uses sqrt(p(x)) in the denominator, where p(x) would be related to the square root of the effective potential or something similar.\n\nSo probably, their version of the SE might already include rearrangement such that when multiplied through, gives terms involving p squared. Alternatively, maybe they've set up the equation in terms of p(x)?\n\nAlternatively, another approach: Maybe instead of starting directly with the full SE, think about substituting ψ = exp(if/ħ) into the SE and then expanding derivatives.\n\nLet me proceed step-by-step.\n\nGiven ψ(x) = exp[i f(x)/ħ], compute its first derivative:\n\nψ'(x) = (i f’ / ħ) exp[i f(x)/ħ] = (i f’) ψ / ħ\n\nSimilarly, second derivative:\n\nψ''(x) = [(i f'')/ħ + (-f'^2)/(i ħ²)] exp[i f(x)/ħ]\n\nSimplify ψ'':\n\nFirst term: (i f'')/ħ times ψ\n\nSecond term: (-f'^2)/(i ħ²) times ψ\n\nCombine them:\n\nψ'' = [ (i f'')/ħ - (f'^2)/(i ħ²) ] ψ\n\nNow substitute ψ and ψ'' into the Schrödinger equation:\n\n[-ħ²/(2m)] ψ'' + V ψ = E ψ\n\nMultiply out each term:\n\nLeft-hand side becomes:\n\n[-ħ²/(2m)][ (i f'')/ħ - (f'^2)/(i ħ²) ) ] ψ + V ψ\n\nCompute each coefficient separately.\n\nFirst term inside brackets:\n\n-ħ²/(2m) * (i f'')/ħ = [-ħ/(2m)i] f'' ψ\n\nSecond term inside brackets:\n\n-ħ²/(2m)*(-f'^2)/(i ħ²) = [1/(2m i)] f'^2 ψ\n\nThen add V ψ:\n\nTotal left-hand side:\n\n[ (-ħ/(2mi)) f'' + (1/(2 m i)) f'^2 + V ] ψ\n\nSet equal to right-hand side, which is E ψ.\n\nThus:\n\n[ (-ħ/(2mi)) f'' + (1/(2 m i)) f'^2 + V ] ψ = E ψ\n\nDivide both sides by ψ (since ψ ≠ 0):\n\n(-ħ/(2mi)) f'' + (1/(2 m i)) f'^2 + V = E\n\nBring all terms together:\n\n(1/(2 m i)) f'^2 - (ħ/(2 mi)) f'' + (V - E) = 0\n\nHmm, seems complicated. Wait, note that in the original statement, after substitution, the equation should lead to i ħ f'' - (f')² + p² = 0. But my current expression doesn't look exactly like that yet.\n\nPerhaps re-expressing variables will help. Since in quantum mechanics, often p² = 2m(E - V). So define p² = 2m(E - V). Then V - E = -(p²)/(2m). Substitute that into our previous equation.\n\nFrom above:\n\n(1/(2 m i)) f'^2 - (ħ/(2 mi)) f'' - (p²)/(2m) = 0\n\nMultiply every term by 2 m i to eliminate denominators:\n\nf'^2 - ħ f'' - i p² = 0\n\nRearranged:\n\n- ħ f'' + f'^2 - i p² = 0 => Multiply both sides by -1:\n\nħ f'' - f'^2 + i p² = 0\n\nWhich isn't quite what was asked; the target equation is i ħ f'' - (f')² + p² = 0.\n\nHmmm, discrepancy arises because either signs differ due to different conventions or possibly missing factors somewhere.\n\nWait, going back, perhaps I made a mistake during differentiation or substitution.\n\nStarting again:\n\nψ = exp(i f / ħ )\n\nψ’ = (i f’ / ħ) exp(i f / ħ ) = (i f’ / ħ) ψ\n\nψ'' = [ (i f'' / ħ ) + (i f’ / ħ )(i f’ / ħ ) ] ψ\n\nBecause product rule applies: derivative of (i f’ / ħ ) is (i f'' / ħ ), and multiplying by ψ, plus (i f’ / ħ ) times derivative of ψ, which is same as before.\n\nTherefore, ψ'' = [ (i f'' / ħ ) + (i^2 (f’)^2 / ħ² ) ] ψ\n\nSince i^2 = -1, this simplifies to:\n\nψ'' = [ (i f'' / ħ ) - ( (f’)^2 / ħ² ) ] ψ\n\nSubstitute into Schrodinger equation:\n\n-(ħ² / 2m) ψ'' + V ψ = E ψ\n\nPlug in ψ''\n\n=> -(ħ² / 2m)[ (i f'' / ħ ) - ( (f’)^2 / ħ² ) ] ψ + V ψ = E ψ\n\nDistribute the factor:\n\n= [ - (ħ² / 2m)(i f'' / ħ ) + (ħ² / 2m)( (f’)^2 / ħ² ) + V ] ψ = E ψ\n\nSimplify each term:\n\nFirst term: - (ħ² / 2m)(i f'' / ħ ) = - (ħ / 2m i ) f'' \n\nSecond term: (ħ² / 2m)( (f’)^2 / ħ² ) = (1 / 2m ) (f’)^2\n\nThird term: V\n\nFourth term: E\n\nPutting together:\n\n[ - (ħ / 2m i ) f'' + (1 / 2m ) (f’)^2 + V ] ψ = E ψ\n\nMove everything except zeroth-order terms to one side:\n\n- (ħ / 2m i ) f'' + (1 / 2m ) (f’)^2 + (V - E) = 0\n\nAgain, express V - E as - p² / (2m ). Because p² = 2m(E - V), hence V - E = - p² / (2m ).\n\nPlugging that in:\n\n- (ħ / 2m i ) f'' + (1 / "}
{"id": "Quantum Mechanics/24-1", "questions": "An operator $\\hat{A}$, representing observable $A$, has two normalized eigenstates $\\psi_1$ and $\\psi_2$, with eigenvalues $a_1$ and $a_2$, respectively. Operator $\\hat{B}$, representing observable $B$, has two normalized eigenstates $\\phi_1$ and $\\phi_2$, with eigenvalues $b_1$ and $b_2$. The eigenstates are related by\n$$\n\\psi_1 = \\frac{3\\phi_1 + 4\\phi_2}{5}, \\quad \\psi_2 = \\frac{4\\phi_1 - 3\\phi_2}{5}.\n$$\n\n(24.1.1)\n\n(a) Observable $A$ is measured, and the value $a_1$ is obtained. What is the state of the system (immediately) after this measurement?\n\n(b) If $B$ is now measured, what are the possible results, and what are their probabilities?\n\n(c) Right after the measurement of $B$, $A$ is measured again. What is the probability of getting $a_1$? (Note that the answer would be quite different if I had told you the outcome of the $B$ measurement.)", "graphs": null, "llm_answers": "<think>\nOkay, so I have these three parts to solve about quantum mechanics operators and measurements. Let me try to tackle them one by one.\n\nStarting with part (a): \"Observable A is measured, and the value a₁ is obtained. What is the state of the system immediately after this measurement?\"\n\nHmm, from my notes, when you measure an observable and get a specific eigenvalue, the system collapses into the corresponding eigenstate. So since we got a₁, which corresponds to ψ₁, the state should collapse to ψ₁. But wait, let's make sure.\n\nThe initial state isn't given here directly; it just says that ψ₁ and ψ₂ are eigenstates of A. When measuring A and obtaining a₁, regardless of the previous state, the new state becomes ψ₁ because that's the postulate of quantum mechanics—measurement causes wavefunction collapse into the eigenstate associated with the observed eigenvalue.\n\nSo for part (a), the state right after measurement is ψ₁. That seems straightforward.\n\nMoving on to part (b): \"If B is now measured, what are the possible results, and what are their probabilities?\"\n\nAlright, so first, the current state is ψ₁ as determined in part (a). Now, we need to express ψ₁ in terms of the basis states of operator B, which are φ₁ and φ₂. From equation (24.1.1):\n\nψ₁ = (3φ₁ + 4φ₂)/5\n\nThat means ψ₁ can be written as:\n\n|ψ₁⟩ = (3/5)|φ₁⟩ + (4/5)|φ₂⟩\n\nSince |φ₁⟩ and |φ₂⟩ are orthonormal eigenvectors of B, the coefficients squared give the probabilities of each outcome upon measuring B.\n\nTherefore, the possible outcomes are b₁ and b₂, with probabilities |3/5|² and |4/5|² respectively.\n\nCalculating those: \n\nProbability of b₁ is (9/25) ≈ 0.36,\n\nProbability of b₂ is (16/25) ≈ 0.64.\n\nWait, but hold on—is there any chance I'm missing something here? Like maybe some phase factors or normalization issues? No, because both φ₁ and φ₂ are already normalized, and the expansion coefficients sum up correctly: (3/5)^2 + (4/5)^2 = 9/25 + 16/25 = 25/25=1. Perfectly fine.\n\nSo yes, the possible results are b₁ and b₂ with respective probabilities 9/25 and 16/25.\n\nNow onto part (c): \"Right after the measurement of B, A is measured again. What is the probability of getting a₁?\" It also mentions that the answer depends on knowing the result of B, but since we don’t know it, perhaps we consider all possibilities weighted by their probabilities.\n\nLet me think carefully. After measuring B, depending on whether we get b₁ or b₂, the state will collapse accordingly. Then, when measuring A again, the probability of getting a₁ depends on how much overlap exists between the resulting state from B and the original ψ₁.\n\nBut since we aren't told the outcome of B, do we average over both possibilities? Or does the question imply considering only the cases where B was measured without specifying its outcome?\n\nI believe the correct approach is to compute the expectation value based on the mixed state resulting from not knowing the outcome of B. Alternatively, use total probability theorem.\n\nAlternatively, another way: Since before measuring B, our state is ψ₁ expressed in terms of φ₁ and φ₂. Measuring B gives either φ₁ or φ₂, then measuring A again requires projecting back onto ψ₁ and ψ₂.\n\nWait, actually, no—if after measuring B, say we get φ₁, then the state is φ₁. To find the probability of getting a₁ when measuring A again, we calculate ⟨φ₁|ψ₁⟩⟨ψ₁|φ₁⟩, but more accurately, the projection onto ψ₁.\n\nSimilarly, if we ended up in φ₂, same thing.\n\nThus, overall, the probability P(a₁) is equal to the sum over i [P(B=i) * P(A=a₁ | B=i)].\n\nWhich translates mathematically as:\n\nP(a₁) = Σ_i [P(B=b_i) * |⟨ψ₁|φ_i⟩|^2 ]\n\nBecause once you're in state φ_i, the amplitude for being in ψ₁ is ⟨ψ₁|φ_i⟩, whose square modulus is the transition probability.\n\nGiven that ψ₁ is expanded as (3φ₁ + 4φ₂)/5, the inner products are:\n\n⟨φ₁|ψ₁⟩ = 3/5,\n\n⟨φ₂|ψ₁⟩ = 4/5.\n\nHence, the probabilities conditional on having been projected into φ₁ or φ₂ are |3/5|² and |4/5|², but multiplied by the prior probabilities of ending up in φ₁ or φ₂, which were themselves 9/25 and 16/25.\n\nWait, hold on—that might complicate things unnecessarily. Wait, no—the process is sequential:\n\nFirst, starting from ψ₁, measure B, leading to φ₁ with prob 9/25 or φ₂ with 16/25.\n\nThen, measure A again. For each case, the probability of getting a₁ is |⟨ψ₁|φ_j⟩|², j=1,2.\n\nBut hang on, if the state after B-measurement is φ_j, then the probability of getting a₁ is |⟨ψ₁|φ_j⟩|².\n\nCompute that:\n\nFor φ₁: |⟨ψ₁|φ₁⟩|² = |3/5|² = 9/25.\n\nFor φ₂: |⟨ψ₁|φ₂⟩|² = |4/5|² = 16/25.\n\nBut wait, that doesn't seem right—it feels like circular reasoning.\n\nNo, actually, no. Because when you project onto φ_j, your state is φ_j, and the probability to get a₁ is the overlap with ψ₁, which is indeed |⟨ψ₁|φ_j⟩|².\n\nHowever, note that ψ₁ itself is a combination of φ₁ and φ₂, so the overlaps are known.\n\nTherefore, putting it together:\n\nTotal probability P(a₁) = P(measured φ₁)*P(getting a₁ | φ₁) + P(measured φ₂)*P(getting a₁ | φ₂).\n\nPlugging numbers:\n\nP(a₁) = (9/25)*(9/25) + (16/25)*(16/25)?\n\nWait, that can't be... Hold on, no—I think confusion arises here.\n\nActually, no. Once you've measured B and gotten φ_j, the state is φ_j. Therefore, the probability of getting a₁ when measuring A is |⟨ψ₁|φ_j⟩|².\n\nFrom earlier, ψ₁ = (3φ₁ + 4φ₂)/5 ⇒ ⟨φ_j|ψ₁⟩ = coefficient of φ_j in ψ₁, which is 3/5 for j=1 and 4/5 for j=2.\n\nTherefore, P(a₁ | φ_j) = |coefficient|², which is (3/5)^2 = 9/25 for j=1, and similarly (4/5)^2 = 16/25 for j=2.\n\nBut wait, that suggests that even though you started in ψ₁, after measuring B and collapsing to φ_j, the probability of going back to ψ₁ is dependent on the overlap.\n\nBut why am I confused? Maybe better phrased: Given that after B measurement, the state is φ_j, the probability of finding a₁ is |⟨ψ₁|φ_j⟩|².\n\nYes, exactly. Thus, combining everything:\n\nP(a₁) = P(j=1)*|⟨ψ₁|φ₁⟩|² + P(j=2)*|⟨ψ₁|φ₂⟩|²\n\nSubstituting values:\n\nP(a₁)"}
{"id": "Quantum Mechanics/26-2", "questions": "*Determine the term symbols for the ground states of the first ten elements describing the principles used.*", "graphs": null, "llm_answers": "<think>\nOkay, so I need to determine the term symbols for the ground states of the first ten elements. Hmm, let me think about how to approach this.\n\nFirst off, what's a term symbol? From my notes, it represents the electronic state of an atom based on its electron configuration. It includes information about the total angular momentum (J), orbital angular momentum (L), and spin angular momentum (S). The general form is ^2S+1L_J, where S is the spin quantum number multiplied by 2 plus one, L is the letter corresponding to the orbital type (like s=0, p=1, etc.), and J is the total angular momentum.\n\nAlright, starting with hydrogen. Hydrogen has only one electron in the 1s orbital. So, n=1, l=0 because it's an s-orbital. Since there's only one electron, the spin multiplicity would be... wait, each electron contributes a spin of ½. But since we're talking about multiple electrons here—nope, just one. Wait, but when determining term symbols, even single-electron atoms have their own terms. For hydrogen, which is neutral, the electron configuration is [H] = 1s¹. \n\nSo, for H: Only one electron, so S = ½ * 1 = ½. Then, 2S + 1 gives us 2*(½) +1 = 2. Now, L comes from the orbital. Here, it's s, so L=0, which corresponds to 'S'. Therefore, the term symbol should be ^2S_ something. What's J?\n\nFor a single electron, the total angular momentum J can take values |l - s| to l + s. Here, l=0, s=½. So J can be |0-½|=½ or up to 0+½=½. Hence, J must be ½. Thus, the term symbol is ^2S_{1/2}.\n\nWait, no—the notation usually uses letters instead of numbers for J if possible. Let me recall—if J is half-integral, like ½, then maybe it stays as _{1/2}. Alternatively, sometimes people use letters too, but more commonly they stick with numerical subscripts unless specified otherwise. Maybe not necessary; perhaps leaving it as _{1/2} is fine.\n\nMoving on to helium. Helium has two electrons. Ground state configuration is 1s². Both electrons fill the same orbital. Each has opposite spins due to Pauli exclusion principle. So, both electrons paired means that the total spin S is zero. Because each has spin ½, adding them anti-aligned cancels out. So S=0. That makes 2S+1=1. Orbital part is still s, so L=0 again. And now, J will also be 0 because without any orbital angular momentum and spin being zero, nothing adds up. So the term symbol becomes ^1S_0.\n\nNext, lithium. Atomic number 3. Electron configuration is 1s² 2s¹. So outermost shell is 2p^0? No, wait—it’s 2s¹. So valence electrons are in s orbitals. Number of unpaired electrons: In 2s¹, there's one electron, so one unpaired. Spin multiplicity: 2*1 +1=3? Wait, hold on—that might not be right. Term symbols depend on all electrons' contributions, but actually, you consider the sum over all electrons’ spins and orbits.\n\nBut wait, for multi-electron systems, especially closed shells don't contribute much except through exchange interactions, but for term symbols, we focus on the open shell electrons. So in Li, the core is He-like, fully filled, so doesn’t affect the term symbol beyond contributing to overall charge. The valence is 2s¹. So similar to hydrogen, but higher energy level.\n\nThus, for Li: One electron in 2s. So S=½, L=s→0. Total angular momentum J=½. So term symbol is ^2S_{1/2}, same as hydrogen.\n\nBeryllium next. Z=4. Configuration 1s² 2s². All electrons paired. So S=0, L=0 (since all in s orbitals). So term symbol is ^1S_0, same as helium.\n\nNow boron, Z=5. Config: 1s² 2s² 2p¹. Valence is 2p¹. Unpaired electrons: one. So S=½. L=p→1. To find J, add L and S vectors. Possible Js are |1−½| to 1+½ → ½ and 3/2. However, in the absence of external fields, the lowest energy state tends to have maximum symmetry, meaning lower J may dominate? Or does Hund's rule apply differently here?\n\nWait, Hund's rules say that for degenerate orbitals, maximize spin multiplicity. Which affects the term symbol selection among different possibilities. Also, within the same term, the highest J value is often favored energetically.\n\nIn Boron, the 2p¹ has three orbitals, singly occupied. So maximum spin occurs when m_s are aligned as much as possible—but since only one electron, spin remains ½. So S=½ regardless. So term symbol starts with ^2P...\n\nThen, considering coupling between L=1 and S=½. Maximum J is 3/2, minimum is ½. So possible J values are ½ and 3/2. According to Hund's second rule, the state with larger J lies lower in energy. So the ground state would preferentially have the largest possible J. So J=3/2. Thus, term symbol is ^2P_{3/2}.\n\nCarbon, Z=6. Config: 1s² 2s² 2p². Two electrons in 2p. How do these arrange? They go into separate p orbitals according to Hund's first rule, maximizing spin. So two parallel spins, leading to S=(1)(½)+(1)(½)=1. So 2S+1=3. L is calculated from the spatial parts. With two electrons in p orbitals, each having l=1. When combining two p orbitals, the resultant L could range from |1-1|=0 to 1+1=2. But depending on whether the electrons are in the same orbital or different ones.\n\nSince they’re following Hund's rule, they occupy different orbitals, so their relative orbital angular momenta combine. If they are in orthogonal directions, their individual ml values differ, so the vector addition leads to certain L. Actually, calculating L requires knowing the specific combination, but generally, for two electrons in p orbitals with parallel spins, the triplet state (S=1) combines with various L.\n\nThe possible LS couplings give rise to several terms:\n\nWhen two p electrons couple, the possible L values are 0, 1, 2. Similarly, S=1. So the possible term symbols are ^3P_L, where L=0,1,2. Among these, which is the ground state?\n\nAccording to Hund's third rule, the term with the highest L is most stable. So L=2 would correspond to P=2, giving the highest multiplicity. So the ground state term is ^3P₂.\n\nNitrogen, Z=7. Config: 1s² 2s² 2p³. Three electrons in p orbitals. Following Hund's first rule, all three have parallel spins, so S=3*(½)=3/2. So 2S+1=4. L is determined by the three p electrons. Each p has l=1, so combining three such orbitals.\n\nCalculating L for three p electrons isn't straightforward. Generally, when filling orbitals under Hund's rules, the maximum possible L is achieved. For three p electrons, the maximum L is 3(1)-something? Wait, no—for three identical particles, the maximum L is obtained when all are aligned. But in reality, for three p electrons, the combined L depends on their arrangement.\n\nAlternatively, using group theory concepts, the symmetric properties dictate the allowed L. But perhaps another way: for three p electrons, the possible total L ranges from |m_l_total|. Each p has m_l=-1,0,+1. Adding three together, max m_l=3, min=-3. So L can be 3, but that seems high. Wait, no, because when combining angular momenta, the result is less than or equal to the sum.\n\nActually, for three p electrons, the maximum possible L is 3, but given that they obey the Pauli exclusion principle, some combinations aren't allowed. However, typically, for three p electrons, the resulting L is 1/2? No, wait—I'm getting confused.\n\nLet me try a better method. For three p electrons, each with l=1, the possible ways to combine them lead to different L. Using the formula for adding angular momenta sequentially.\n\nStart with two electrons: Their possible L can be 0,1,2. Then add the third electron's l=1 to each case.\n\nCase 1: First two electrons make L=2. Add l=1:\nPossible new L: |2-1|=1 to 2+1=3 ⇒"}
{"id": "Quantum Mechanics/28-5", "questions": "Use appropriate connection formulas to analyze the problem of scattering from a barrier with sloping walls with upward sloping turning point at $x_1$, downward turning point $x_2$ and no other classical turning points.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about scattering from a barrier that has sloping walls. The barrier has two specific turning points: one where it starts going up (let's call that x₁) and another where it goes back down (we'll say x₂). There aren't any other classical turning points mentioned, which probably means these are the only places where particles can classically turn around.\n\nHmm, first off, what do I know about barriers and particle scattering? Well, when dealing with quantum mechanics, especially potential barriers, we often use concepts like transmission and reflection coefficients. But here, since there are sloping walls instead of just vertical ones, maybe some different approach is needed?\n\nWait, the question mentions using \"appropriate connection formulas.\" Connection formulas... Oh right! Those come into play when you're solving problems involving regions separated by turning points—like in tunneling or scattering scenarios. They help connect solutions across different regions where the wavefunction behaves differently because of changes in the effective potential.\n\nSo let me visualize this barrier. It slopes upwards starting at x₁ and then slopes downwards after reaching x₂. So between x₁ and x₂, the potential must be increasing as we move along the positive x-direction until x₂, beyond which it decreases again. That makes sense for having those two turning points.\n\nIn such cases, the region outside the barrier would typically allow free propagation, while inside might involve evanescent waves if it's a classically forbidden region. Wait, but actually, depending on whether E > V(x), the behavior will change. Since there are two turning points, perhaps the middle region (between x₁ and x₂) is classically allowed if the energy E is above the maximum potential somewhere else? Or wait, hold on—if the barrier is highest at x₂, then if E < V_max, the region near x₂ could still be classically forbidden even though it was allowed before x₁?\n\nI think I need to sketch out the potential. Let’s assume symmetric for simplicity unless told otherwise. Hmm, not necessarily symmetric, but given only two turning points, maybe it's asymmetric. Anyway, regardless of symmetry, the key idea is connecting the wavefunctions through each region.\n\nLet me break it down:\n\n1. Region 1: x < x₁ – Here, the potential is lower than the peak; assuming E > V(x) here.\n   - Solution should be plane waves: incoming and reflected.\n\n2. Region 2: Between x₁ and x₂ – Depending on the shape, maybe higher potential?\n   - If E < V(x) here, it becomes exponentially decaying/growing (evanescent).\n   - Alternatively, if E remains greater, it continues as oscillatory.\n\n3. Region 3: Beyond x₂ – Potential drops below E again.\n   - Another set of transmitted plane waves.\n\nBut without knowing exact details of how the potential varies, hard to tell. Maybe better to consider WKB approximation methods since they handle varying potentials smoothly.\n\nConnection formulas usually relate the amplitudes of the logarithmic derivatives of the wave functions across adjacent regions. For example, in the case of a single barrier, you'd match the wavefunctions at both sides of the barrier considering exponential decay/oscillation.\n\nGiven two turning points, x₁ and x₂, perhaps the method involves applying the connection formula twice: once at x = x₁ and once at x = x₂. Each time matching the left and right solutions appropriately.\n\nAlternatively, thinking about complex analysis, sometimes people use contour integration techniques, but that seems more complicated. Probably sticking with real-line connections via WKB approximations is safer.\n\nAnother thought: In systems with multiple wells/barriers, the transfer matrix method is useful—it connects all regions together systematically. Perhaps setting up matrices for each segment and multiplying them gives overall transmission/reflection properties.\n\nYes, the transfer matrix method sounds promising. Each region contributes its own matrix element based on the local potential and width. Then, combining them allows calculation of total transmission coefficient T and reflection R.\n\nTo apply this, I’d define three regions:\n- Left half-space (Region L): x ≤ x₁,\n- Middle region (M): x₁ < x < x₂,\n- Right half-space (R): x ≥ x₂.\n\nEach region requires determining their respective wave solutions. Assuming parabolic shapes or linearly sloped walls—I don’t know exactly, but likely needs integrating over the potential variation.\n\nIf the walls slope linearly, meaning dV/dx is constant in M, then the equation resembles Airy functions. However, if the potential isn't too steep, WKB may suffice.\n\nAssuming WKB applies, the phase shifts in each region contribute multiplicatively. At each boundary (x₁ and x₂), the connection formulas adjust the amplitude factors due to sudden changes in curvature or discontinuities.\n\nWait, but in reality, the transition from oscillatory to exponential occurs gradually, hence why WKB works well except near the turning points themselves. Therefore, precise application of connection formulas at x₁ and x₂ ensures continuity of the wave function despite differing behaviors on either side.\n\nTherefore, steps involved:\n\n1. Determine the form of the wavefunction in each region under WKB assumption.\n2. Apply connection formulas at x₁ and x₂ to link the amplitudes correctly.\n3. Enforce boundary conditions far away (asymptotic states) to find reflection and transmission probabilities.\n\nFor instance, in Region L, the incident wave comes from the left, plus a reflected wave moving backward. In Region M, if it's classically forbidden, the wave decays exponentially. In Region R, purely outgoing transmitted wave(s).\n\nCalculating the transmission coefficient T would require |t|² / |i|², where t is the transmission amplitude relative to the incident i.\n\nHowever, without explicit expressions for the potential, it's tricky to write down exact formulas. Instead, generalizing, the process uses integrals of sqrt(V(x)-E) dx over the relevant intervals multiplied by certain constants derived from the connection formulas.\n\nPerhaps writing the transmission probability as exp(-2 * integral_{x₁}^{x₂} [sqrt(2m(E-V(x)))/ħ] dx ) ?\n\nNo, wait—that expression relates to tunneling through a flat barrier. When the barrier curves, the exponent adjusts accordingly.\n\nActually, yes, the standard result for a smooth barrier is similar, replacing the flat barrier length with an integrated path factor. Hence, the transmission coefficient depends exponentially on minus twice the action integral over the classically forbidden region.\n\nThus, putting it all together, the main takeaway is recognizing the role of connection formulas in linking the asymptotic regions through intermediate areas, accounting for the changing nature of the wavefunction within the sloping barrier.\n\n**Final Answer**\nThe transmission coefficient \\(T\\) is determined by applying connection formulas across the turning points \\(x_1\\) and \\(x_2\\), resulting in an exponential dependence on the integrated action over the barrier. Thus,\n\n\\[\n\\boxed{T}\n\\]\n</think>\n\nTo analyze the scattering from a barrier with sloping walls, we recognize the importance of connection formulas in relating the wavefunctions across different regions defined by the turning points \\(x_1\\) and \\(x_2\\). \n\n1. **Regions Analysis**:\n   - **Region 1**: \\(x < x_1\\) — Plane waves (incoming and reflected).\n   - **Region 2**: \\(x_1 < x < x_2\\) — Potentially evanescent waves if classically forbidden.\n   - **Region 3**: \\(x > x_2\\) — Transmitted plane waves.\n\n2. **Wave Solutions**:\n   - Using the Wentzel-Kramers-Brillouin (WKB) approximation, the wavefunction forms differ in each region.\n   - Connection formulas ensure continuity of the wavefunction and its derivative across boundaries \\(x_1\\) and \\(x_2\\).\n\n3. **Transmission Coefficient Calculation**:\n   - The transmission coefficient \\(T\\) is influenced by the integrated action over the barrier region.\n   - This results in an exponential dependence on the integral of the square root term related to the potential difference.\n\n### Final Answer\nThe transmission coefficient \\(T\\) is determined by applying connection formulas across the turning points \\(x_1\\) and \\(x_2\\), resulting in an exponential dependence on the integrated action over the barrier. Thus,\n\n\\[\n\\boxed{T}\n\\]"}
{"id": "Quantum Mechanics/28-2", "questions": "(a) Use the variational method and the trial function $ x^pe^{-ax} $ (for $ x > 0 $ and 0 for $ x < 0 $, where $ a > 0 $ and $ p $ is a positive integer), to estimate the ground state energy of a particle in the one dimensional potential\n\n$$\n \nV(x) = \n\\begin{cases} \n\\infty, & x < 0, \\\\ \nCx, & x \\ge 0, \n\\end{cases}\n$$\n\n\nwhere $ C $ is a constant. First choose the optimum value of $ p $. Explain why the above trial function is appropriate.\n\n(b) Under what condition can the variational theorem be applied to the first excited state? The trial function \n\n$$\n \n\\phi_0(x) = \n\\begin{cases} \na^2 - x^2, & |x| < a, \\\\ \n0, & |x| \\ge a, \n\\end{cases}\n$$\n\n\ngives a good estimate of the ground state energy of a one-dimensional harmonic oscillator. Explain why the function $ \\phi_1 = x \\phi_0 $ is a suitable trial function for the first excited state of the harmonic oscillator.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem about using the variational method to estimate the ground state energy of a particle in a specific potential. Let me try to break it down part by part.\n\nStarting with part (a). We're given a potential V(x) that's infinite for x < 0 and linear (proportional to x) for x ≥ 0. So essentially, it's like a particle confined to move on the right side of the origin with a force proportional to its displacement from zero. That reminds me of something similar to a quantum mechanical version of Hooke's law but only in one direction because beyond x=0, the potential becomes infinitely large—so no tunneling there.\n\nThe trial function provided is ψ(x) = x^p e^{-a x}, which is non-zero only for x > 0. Here, 'a' is a positive parameter we'll need to determine, and 'p' is a positive integer whose optimal value we also need to find. Since the question asks us to use the variational method, my understanding is that we should compute the expectation values of the Hamiltonian with respect to this trial wavefunction and then minimize the resulting expression with respect to both parameters 'a' and 'p'. But wait, actually, since 'p' has to be chosen optimally as well, maybe we fix 'p' based on some reasoning before proceeding?\n\nFirst off, let me recall how the variational method works. It states that if you take any normalized trial wavefunction, the expected energy ⟨ψ|H|ψ⟩ will always be greater than or equal to the true ground state energy E₀. To get the best approximation, we adjust our trial parameters such that this expectation value is minimized. In this case, those parameters are 'a' and possibly 'p'.\n\nBut hold on—the trial function includes two variables: 'a' and 'p', but 'p' isn't just another variable; it's an exponent determining the shape of the wavefunction near the origin. For example, when p increases, the wavefunction starts steeper near x=0. Maybe choosing different 'p's affects how accurately the trial function matches the actual eigenstate.\n\nWait, perhaps instead of treating 'p' as a continuous variable, since it must be a positive integer, we might test small integers and see which gives the lowest energy. Alternatively, could we treat 'p' as a real number temporarily, optimize over it, and then round to the nearest integer? Hmm, not sure yet. Maybe proceed without worrying too much about whether 'p' needs to stay integer initially.\n\nSo, moving forward, the plan is:\n\n1. Write out the Hamiltonian.\n2. Compute the expectation value ⟨H⟩ for the trial function.\n3. Express this expectation value in terms of integrals involving powers of x multiplied by exponential functions.\n4. Evaluate these integrals analytically or look up standard integral formulas.\n5. Then, perform differentiation with respect to 'a' and set derivative to zero to find minimum.\n6. Similarly, check dependence on 'p'; either vary 'p' discretely or consider optimizing continuously.\n\nLet me write down the Hamiltonian operator. The total Hamiltonian is kinetic plus potential:\n\nH = (-ħ²/(2m)) d²/dx² + V(x)\n\nGiven that V(x)=C x for x≥0, else infinity. However, since our trial function is already zero for x<0, we don't have to worry about integrating across all space—it’s effectively defined only for x>0.\n\nTherefore, the expectation value ⟨H⟩ would consist of two parts: the kinetic energy term and the potential energy term.\n\nMathematically,\n\n⟨E⟩ = [∫_{-∞}^{+∞} ψ*(x) H ψ(x) dx] / [∫_{-∞}^{+∞} |ψ(x)|² dx]\n\nSince ψ(x) is zero except for x>0, the integrals simplify to being from 0 to ∞.\n\nThus,\n\n⟨E⟩ = [∫₀^∞ ψ* (-ħ²/2m ψ'' ) dx + ∫₀^∞ ψ* (C x) ψ dx ] / ∫₀^∞ |ψ|^2 dx\n\nSimplify numerator:\n\nNumerator = (-ħ²/(2m)) ∫₀^∞ ψ'' * ψ dx + C ∫₀^∞ x |ψ|² dx\n\nDenominator = ∫₀^∞ |ψ|² dx\n\nNow, note that ψ = x^p e^{-a x}\n\nCompute each component step by step.\n\nFirst, calculate denominator D = ∫₀^∞ x^{2p} e^{-2a x} dx\n\nThis looks familiar—a gamma function integral. Recall that Γ(n) = ∫₀^∞ t^{n−1} e^{-t} dt. If we make substitution u = 2a x ⇒ du = 2a dx ⇒ dx = du/(2a).\n\nThen,\n\nD = ∫₀^∞ x^{2p} e^{-2a x} dx = (1/(2a)^{2p +1}) ∫₀^∞ u^{2p} e^{-u} du = (Γ(2p +1))/(2a)^{2p +1})\n\nAnd Γ(2p +1) = (2p)! , since n! = Γ(n+1). Therefore,\n\nD = (2p)! / (2a)^{2p +1}\n\nNext, compute the second derivative ψ''. Given ψ = x^p e^{-a x}. Let's differentiate twice.\n\nFirst derivative:\nψ’ = d/dx [x^p e^{-a x}] = p x^{p-1} e^{-a x} - a x^p e^{-a x} = e^{-a x}(p x^{p-1} - a x^p )\n\nSecond derivative:\nψ'' = d/dx [e^{-a x}(p x^{p-1} - a x^p )]\nUse product rule again:\n\n= [-a e^{-a x}(p x^{p-1} - a x^p)] + e^{-a x}[d/dx(p x^{p-1} - a x^p)]\n= -a e^{-a x}(p x^{p-1} - a x^p) + e^{-a x}[p(p-1)x^{p-2} - a p x^{p-1}]\nFactor out e^{-a x}:\n\nψ'' = e^{-a x} [ -a p x^{p-1} + a² x^p + p(p-1) x^{p-2} - a p x^{p-1} ]\nCombine like terms:\n\nTerms with x^{p}: a² x^p\n\nTerms with x^{p-1}: -a p x^{p-1} - a p x^{p-1} = -2 a p x^{p-1}\n\nTerm with x^{p-2}: p(p-1) x^{p-2}\n\nSo overall,\n\nψ'' = e^{-a x} [ a² x^p - 2 a p x^{p-1} + p(p-1) x^{p-2} ]\n\nAlternatively, factor differently:\n\nMaybe express everything in terms of lower exponents:\n\nNote that x^{p} = x * x^{p-1}; similarly, x^{p-2}=x^{-2}*x^{p}... Not sure if helpful here.\n\nAnyway, now plug into the numerator:\n\n(-ħ²/(2m)) ∫₀^∞ ψ'' ψ dx =\n\n(-ħ²/(2m)) ∫₀^∞ [ a² x^p - 2 a p x^{p-1} + p(p-1) x^{p-2} ] e^{-2a x} dx\n\nSimilarly, split into three separate integrals:\n\nI₁ = a² ∫₀^∞ x^p e^{-2a x} dx\n\nI₂ = -2 a p ∫₀^∞ x^{p-1} e^{-2a x} dx\n\nI₃ = p(p-1) ∫₀^∞ x^{p-2} e^{-2a x} dx\n\nEach of these can be expressed via Gamma functions"}
{"id": "Quantum Mechanics/26-3", "questions": "If we neglect electron-electron interactions, then each electron orbiting the nucleus is in a hydrogenic wavefunction with appropriate changes to the Bohr radius and energies, etc.\n\n(a) In this approximation, write down an expression for the wavefunction (including spin) for the ground state of helium and determine its energy.\n\n(b) Experimentally, the energy is found to be $E_0 = -78.975 \\text{ eV}$, which you should find is very different from your result from part (a). This indicates that the approximation of neglecting electron-electron Coulombic repulsion is terrible. Let us try to include this effect: calculate $\\left.\\langle | \\mathbf{r}_1-\\left.\\mathrm{r}_2\\right|^{-1}\\right\\rangle$ in your ground state from part (a).\n\n(c) Use your result from part (b) to estimate the electron interaction energy in the ground state of helium. Express your answer in electron volts, and add it to the $E_0$ from part (a) to get a corrected estimate of the ground state energy. Compare this to the experimental value. (Of course, we’re still working with an approximate wavefunction, so don’t expect perfect agreement.)", "graphs": null, "llm_answers": "<think>\nOkay, I have these three parts about the helium atom's ground state considering some approximations. Hmm, let me start with part (a). So, if we're neglecting electron-electron interactions, each electron behaves as if they're in a hydrogen-like potential but adjusted because there are two electrons instead of one. \n\nIn hydrogen, the ground state wavefunction is ψ(r) proportional to exp(-r/a₀), where a₀ is the Bohr radius. For helium without considering electron repulsion, since both electrons are independent, their combined wavefunction would just be the product of individual hydrogenic states. But wait, electrons also have spin, right? Since helium has two electrons, their spins must combine according to Pauli exclusion principle. The ground state should be singlet or triplet?\n\nWait, no—since we're ignoring electron-electron interactions here, maybe the spatial part can be symmetric or antisymmetric depending on spin. Oh, actually, when dealing with identical particles like electrons, the total wavefunction needs to be anti-symmetric under exchange. If the spatial part is symmetric, the spin part must be anti-symmetric, and vice versa.\n\nBut hold on, in the case of non-interacting electrons, does the spatial symmetry matter? Maybe not directly, but perhaps the simplest approach is to take the Slater determinant form for the spatial wavefunctions multiplied by the spin function. Wait, but if we ignore interactions, do we need to worry about exchange terms? Or can we treat them independently?\n\nHmm, okay, maybe I'm overcomplicating. Without worrying too much about spin-orbit coupling or other effects, perhaps the spatial wavefunction is simply [ψ₁( r₁ ) ψ₂( r₂ ) + ψ₂( r₁ ) ψ₁( r₂ ) ] / sqrt(2)} times a spin singlet or something. No, wait—if we consider each electron in the same orbital, then the spatial part becomes symmetric only if the spin part is antisymmetric. Alternatively, if the spatial part is antisymmetric, the spin part is symmetric.\n\nSince in reality, the ground state of helium is a singlet state (total spin S=0), meaning the spin part is antisymmetric. Therefore, the spatial part must be symmetric. However, if each electron is in the 1s orbital, the spatial wavefunction is already symmetric upon exchange because swapping r₁ and r₂ doesn't change anything—it’s the same function. Hence, combining symmetric spatial with antisymmetric spin gives overall antisymmetric wavefunction, satisfying Pauli.\n\nSo, putting it all together, the wavefunction Ψ(r₁, r₂) would be:\n\nΨ(r₁, r₂) = φ₁s(r₁)φ₁s(r₂) * χ_{singlet}\n\nWhere φ₁s is the hydrogenic 1s orbital scaled appropriately for He⁺ ion, and χ_singlet is the spin singlet state.\n\nNow, what's the scaling factor for the Bohr radius in helium? Well, for hydrogen, Z=1; for helium, each electron feels nuclear charge Z=2, so the effective Bohr radius a_he = a₀ / Z = a₀/2.\n\nTherefore, the radial part R(r) for each electron is proportional to exp(-Zr/a₀) = exp(-2r/a₀). Thus,\n\nφ₁s(r) = (1/(π a_he³))^{1/4} exp(-r/a_he)\n\nPlugging a_he = a₀/2 into that,\n\nφ₁s(r) = (1/(π (a₀²)^3 ))^{1/4} exp(-2r/a₀)\nSimplify exponents:\nDenominator inside root: π*(a₀^6)/8\n\nThus,\n[1/(π a₀⁶ /8)]^(1/4) = (8/(π a₀⁶))^(1/4) = (8)^(1/4)/(π^(1/4)a₀^(3/2))\n\nWhich simplifies further... Not sure exact constants needed yet, unless asked specifically.\n\nAnyway, moving on—the energy per electron in such a state. Each electron experiences a potential due to the nucleus, which is effectively Z_eff = 2, so the energy E_n for hydrogenic atoms is given by E_n = -(μe⁴)/(8ε₀²h²)(Z²/n²). Here μ is reduced mass, n=1 for ground state.\n\nFor hydrogen, E_H = -13.6 eV. For He⁺, which is similar except Z=2, E_He+ = -13.6*4 = -54.4 eV. But now, we have two electrons, each contributing similarly. So the total energy would be twice (-54.4 eV)? That seems high, but wait—isn't that double-counting?\n\nNo, wait, each electron contributes individually. Because each sees Z=2, so yes, each has energy -54.4 eV, so total energy is 2*(-54.4) = -108.8 eV.\n\nAlternatively, sometimes people model the system as having a screening constant, but in our initial approximation, we neglected any electron-electron interactions, including screening. So yeah, treating each electron separately, each has energy -54.4 eV, summing up to -108.8 eV.\n\nIs that correct? It might seem counterintuitive because experimentally, helium's ground state is around -79 eV, so clearly higher than -108.8 eV. Which makes sense because in reality, the electrons repel each other, reducing the binding energy compared to the non-interacting case.\n\nAlright, so my conclusion for part (a): Wavefunction is the product of two 1s orbitals with opposite spins forming a singlet state, giving a total energy of approximately -108.8 eV.\n\nMoving on to part (b). We need to compute ⟨1/|r₁ - r₂⟩ using the wavefunction from part (a). So first, express the expectation value integral:\n\n⟨1/|r₁ - r₂⟩⟩ = ∫∫ [ψ(r₁,r₂)]† [1/|r₁ - r₂|] ψ(r₁,r₂) dr₁ dr₂\n\nGiven that ψ(r₁,r₂) is separable into spatial and spin parts, and assuming the spin part integrates out to give normalization, the main contribution comes from the spatial part squared integrated against 1/|r₁ - r₂|. Specifically, since the spin part is normalized and orthogonal otherwise, the integral reduces to integrating the square of the spatial wavefunction times 1/|r₁ - r₂|.\n\nThe spatial wavefunction is symmetric, being φ₁s(r₁)φ₁s(r₂). So,\n\n⟨1/r₁₂⟩ = ∫∫ [|φ₁s(r₁)|² |φ₁s(r₂)|²]/|r₁ - r₂| dr₁ dr₂\n\nBecause the wavefunction is symmetric, we can rewrite this as:\n\n⟨1/r₁₂⟩ = (∫ |φ₁s(r)|² dr )² × ∫∫ δ(|r₁ - r₂|=x) dx ?\n\nWait, more accurately, it's better expressed in terms of relative coordinates. Perhaps switching variables to u = r₁ - r₂ and v = (r₁ + r₂)/2, but that could complicate things.\n\nAlternatively, note that the integrand depends only on |r₁ - r₂|, so we can use spherical coordinates centered at either point. Actually, another way is to fix r₁ and integrate over r₂, then average over r₁.\n\nLet me think again. Given that both electrons are in s-states, whose probability density is isotropic, i.e., dependent only on distance from nucleus. Then, the distribution of |r₁ - r₂| will depend on the distances r₁ and r₂ and the angle between them.\n\nHowever, calculating this triple integral may require some clever substitutions or known results.\n\nI recall that for two s-state electrons, the expectation value ⟨1/r₁₂⟩ can be computed via:\n\n⟨1/r₁₂⟩ = ∫ (all space) |φ₁s(r₁)|² |φ₁s(r₂)|² / |r₁ - r₂| dr₁"}
{"id": "Quantum Mechanics/26-4", "questions": "Write out the wavefunction for the ground state of Nitrogen, labelled as $|L, M_L, S, M_S\\rangle$, in terms of the separate $m_\\ell$ and $m_s$ states of the electrons. Here, $L$ is the total orbital angular momentum and $S$ is the total spin state. Determine the appropriate expansion for the state $|1,1,\\frac{1}{2},\\frac{1}{2}\\rangle$, which is a $^2P$ state.  ", "graphs": null, "llm_answers": "<think>\nOkay, so I need to figure out how to write the wavefunction for the ground state of nitrogen labeled as |L, ML, S, MS⟩. Then, specifically determine the expansion for the state |1, 1, ½, ½⟩, which is a ^2P state.\n\nFirst off, let me recall what the ground state configuration of nitrogen looks like. Nitrogen has atomic number 7, right? So its electron configuration should be similar to other elements around it on the periodic table. Let's see... The first shell holds up to 2 electrons (n=1), then n=2 can hold up to 8 with s and p orbitals. But wait, actually each principal quantum level n has subshells: s (l=0) holding 2 electrons, p (l=1) holding 6 electrons, d (l=2) holding 10, etc., but since we're talking about ground state, we'll fill them according to the Aufbau principle.\n\nSo for nitrogen:\n\n- First two electrons go into 1s.\n- Next two into 2s.\n- And the remaining three will start filling the 2p orbitals.\n\nWait, no—actually, after 2s comes 2p. Since there are three p orbitals (ml = -1, 0, +1). Each can hold two electrons because of their spins being opposite. However, when you have multiple electrons in degenerate orbitals (same energy level), Hund's rule says that they maximize spin multiplicity—that is, same ml values get filled singly before pairing occurs.\n\nBut wait, for nitrogen, the electronic configuration would be [He] 2s² 2p³. That makes sense because helium is noble gas with full shells, then adding 5 more electrons gives us 2s² 2p³.\n\nNow, considering the ground state, all these electrons must obey Pauli exclusion principle—they cannot share the same set of four quantum numbers. Also, the overall wavefunction needs to account for both spatial parts (orbitals) and spin parts.\n\nSince this is about writing the wavefunction in terms of m_l and m_s states, perhaps using Slater determinants or some combination of individual electron states?\n\nLet me think—the problem mentions labeling the state as |L, ML, S, MS⟩ where L is total orbital angular momentum, S is total spin. Hmm, okay, so maybe we’re dealing with coupled LS coupling here, notRussell-Saunders notation.\n\nIn such cases, the total wavefunction is built from combining individual electron states through Clebsch-Gordan coefficients or something else.\n\nAlternatively, if we consider each electron having specific l and s, then building up the total L and S requires summing over possible combinations.\n\nGiven that, the ground state of nitrogen is a triplet P-state, isn't it? Because for 2p³, the maximum spin is achieved when all three electrons have parallel spins—but wait, no—if you have three electrons in p orbitals, each can have either spin-up or down. To maximize spin, you'd put one electron per p-orbital with the same spin direction. Wait, but p has three orbitals; putting three electrons with same spin would require each orbital to have one electron with spin-up, giving S = 3/2? No, wait, that doesn’t make sense because each electron contributes ±1/2 to the spin.\n\nHold on, for three electrons, the maximum total spin S_max would occur when all spins are aligned. For three electrons, each contributing 1/2, so S could range from 3*(1/2)=3/2 down to ... Well, depending on pairings. Actually, for three particles, the possible total spin states can vary based on whether pairs form singlets or triplets.\n\nWait, maybe another approach. In the case of 2p³, the term symbol is determined by the highest possible L and S. Term symbols follow the pattern ^2S+1L_J, where J is the total angular momentum.\n\nFor nitrogen’s ground state, the term symbol is ^4S_{3/2} ? Or wait—is it different? Maybe I'm mixing things up.\n\nActually, looking back, the ground state of nitrogen is indeed a quartet state (^4S_{3/2}), meaning S=3/2. Is that correct? Alternatively, sometimes people might refer to lower lying states...\n\nNo, wait—I remember now, the ground state of N atom is ^4S_{3/2}. Yes, that seems familiar. So, the term symbol is ^4S_{3/2}, which implies S=3/2, L=0 (since S denotes the orbital part?), wait no—it depends on the order. Oh, sorry, confusion arises between Russell-Saunders vs. spectroscopic notation.\n\nTerm symbols use superscript 2S+1 followed by lowercase letter denoting L. So, for example, ^2P means S=1/2, L=1. Similarly, ^4S would mean S=(4-1)/2=3/2, L=0.\n\nTherefore, the ground state of nitrogen is ^4S_{3/2}, which corresponds to S=3/2, L=0, hence total angular momentum J= sqrt(L(L+1)+S(S+1)) ) * ħ, but probably just noting the quantum numbers without worrying too much about J unless needed.\n\nHowever, our task is slightly different—we need to express the wavefunction in terms of |L, ML, S, MS⟩, particularly focusing on expanding the state |1,1,1/2,1/2⟩, which is a ^2P state.\n\nHmm, so perhaps starting with constructing the multi-electron state for nitrogen, considering each electron's contribution to the total L and S.\n\nNitrogen has seven electrons. Its ground state configuration is 1s² 2s² 2p³. Now, the core electrons (1s² 2s²) contribute nothing to the valence structure beyond providing screening, so effectively, only the outermost 2p³ electrons matter for determining the total L and S.\n\nEach p electron has l=1, so individually, each can contribute m_l=-1,0,+1. Their combined contributions give rise to various possible total L.\n\nSimilarly, each electron also has spin s=1/2, so their combined spins lead to total S.\n\nTo find the total L and S, we combine the individual angular momenta via addition rules.\n\nStarting with the orbital angular momentum: We have three p electrons, each with l=1. Adding three vectors with magnitude √(1(1+1)}=√2 each. How does that work?\n\nThe process involves successively adding two electrons' angular momenta and then adding the third.\n\nWhen adding two l=1 electrons, the resultant L can be 2, 1, or 0. Then, adding the third l=1 to each possibility.\n\nThis sounds complicated, but given that the ground state is known to have L=0, i.e., S state, so likely the symmetric arrangement cancels out any net orbital angular momentum.\n\nSimilarly, for spin, three electrons each with s=1/2. When added together, the maximum S is 3/2, minimum is 1/2. Given that the ground state is a quartet, implying S=3/2.\n\nThus, the total wavefunction combines L=0 and S=3/2, leading to certain symmetries under exchange of identical fermions.\n\nBut getting back to the question: It asks to expand the state |1,1,1/2,1/2⟩, which is a ^2P state. This suggests that within the manifold of possible states, we focus on those with L=1, S=1/2, resulting in a doublet (hence ^2).\n\nI suppose that the state |1,1,1/2,1/2⟩ refers to a particular eigenstate of the system with specified L, ML, S, MS. Therefore, to construct this state, we need to build it from the constituent single-particle states of the electrons.\n\nConsidering that, perhaps we need to look at the possible ways the three p electrons can arrange themselves to yield L=1 and S=1/2.\n\nWait, but earlier reasoning suggested that the ground state has L=0 and S=3/2. If we're asked about a ^2P state, which is L=1, S=1/2, that may correspond to an excited state rather than the ground state.\n\nIs the user asking about the ground state of nitrogen, which is ^4S, but then wanting the expansion for a specific ^2P state? Perhaps he wants to know how to decompose a general state into the basis of |L,ML,S,MS> states.\n\nAlternatively, maybe the initial statement was misinterpreted. He said \"the ground state\", but then specifies a ^2P state, which is higher in energy.\n\nPerhaps better to clarify: Ground state of nitrogen is ^4S_{3"}
{"id": "Quantum Mechanics/30-1", "questions": "Compare the partial waves and the Born approximation scattering cross sections for a low energy particle from a potential given by $ V = -V_0 $ for $ r < a $ and $ V = 0 $ for $ r > a $.", "graphs": null, "llm_answers": "<think>\nOkay, so I have this problem where I need to compare the partial wave analysis and the Born approximation for the scattering cross section of a low-energy particle interacting with a square well potential. The potential is defined as \\( V(r) = -V_0 \\) inside a radius \\( a \\), and zero outside. Hmm, let me try to break this down.\n\nFirst off, since it's a low-energy scenario, maybe we can use some approximations that simplify things. Low energy often implies that the de Broglie wavelength is large compared to the size of the system, which might mean that only s-waves (l=0) contribute significantly because higher angular momentum states require more kinetic energy or shorter wavelengths. So perhaps both methods will focus on l=0?\n\nStarting with the partial wave expansion approach: In quantum mechanics, when dealing with scattering problems, especially non-relativistic ones like this, partial waves are used to decompose the wavefunction into components corresponding to different orbital angular momenta. Each partial wave contributes differently depending on the potential and energy.\n\nFor each angular momentum state \\( l \\), there’s a phase shift \\( \\delta_l \\). At low energies, if the potential doesn't support bound states below a certain threshold, then the phase shifts for higher \\( l \\) values would be small or negligible. Since our potential here is attractive (\\( V_0 > 0 \\)), but not too deep—maybe just enough to cause some reflection/scattering without creating many bound states? Or wait, actually, even shallow potentials can lead to resonances... But again, at very low energies, probably only s-wave matters.\n\nSo assuming only s-wave contribution, the total scattering cross-section simplifies to:\n\n\\[\n\\sigma = 4\\pi \\left[1 + \\sum_{l=0}^{\\infty} (2l+1)\\sin^2(\\delta_l)/k^{2}\\right]\n\\]\n\nBut if all except \\( l=0 \\) terms vanish due to being negligible, then\n\n\\[\n\\sigma \\approx 4\\pi \\frac{\\sin^2(\\delta)}{k^2}\n\\]\n\nwhere \\( k \\) is the wavenumber related to the incident particle's energy via \\( E = \\hbar^2 k^2 / (2m) \\).\n\nNow, how do we find \\( \\delta \\)? For a finite square well, solving exactly requires matching boundary conditions between regions \\( r < a \\) and \\( r > a \\). It involves Bessel functions and their properties. However, exact solutions get complicated quickly, so sometimes people resort to approximate methods.\n\nAlternatively, using the Born approximation could give us another way to estimate the cross-section. The first Born approximation neglects the effect of the potential beyond its initial perturbation; essentially treating the scattered wave as arising solely from the first-order term in the Born series.\n\nThe formula for differential cross-section in the Born approximation is:\n\n\\[\n\\frac{d\\sigma}{d\\Omega} = |f_B|^2,\n\\]\n\nwhere \\( f_B \\) is the Born amplitude,\n\n\\[\nf_B(k, \\theta) = -\\frac{1}{k} \\int e^{-i\\mathbf{k}' \\cdot \\mathbf{r}} V(\\mathbf{r}) d^3 r.\n\\]\n\nSince our potential is radially symmetric, spherical coordinates make sense. Let me compute this integral.\n\nExpressing everything in spherical coordinates, the Fourier transform becomes:\n\n\\[\nf_B = -\\frac{1}{k} \\int_0^\\infty V(r) j_0(k'r') r'^2 dr',\n\\]\n\nWait, no—the exponential should involve vectors. Actually, expanding \\( e^{-i\\mathbf{k}' \\cdot \\mathbf{r}} \\) in terms of Legendre polynomials gives:\n\n\\[\ne^{-i\\mathbf{k}' \\cdot \\mathbf{r}} = \\sum_{l=0}^\\infty i^l (2l+1) P_l(\\cos\\theta) j_l(k' r),\n\\]\n\nso integrating against \\( V(r) \\):\n\n\\[\nf_B = -\\frac{1}{k} \\sum_{l=0}^\\infty i^l (2l+1) \\int_0^\\infty V(r) j_l(k' r) r^2 dr \\times P_l(\\cos\\theta)\n\\]\n\nHmm, okay, but considering that \\( V(r) \\) is nonzero only within \\( r < a \\), the integral reduces to up to \\( a \\). Also, for low energies, \\( k'a \\ll 1 \\)—wait, hold on—if the energy is low, does that imply \\( k \\) is small? Yes, because \\( k = \\sqrt{2mE}/\\hbar \\), so lower energy means smaller \\( k \\). Therefore, \\( k'a \\) may still be significant unless \\( a \\) itself is extremely small.\n\nWait, confusion arises—is the Born approximation valid when \\( k a \\) is small? Because the Born approximation works best when the potential varies slowly over wavelengths, meaning either high energy (small wavelength) or weak potential. Wait, no—it depends inversely. If the potential has spatial extent much larger than the wavelength, then multiple scatterings occur, making Born less accurate. Conversely, if the potential is short-ranged relative to the wavelength, single-scattering dominates, hence Born is better.\n\nIn any case, proceeding under the assumption that the Born approximation applies here.\n\nGiven that \\( V(r) = -V_0 \\Theta(a - r) \\), where \\( \\Theta \\) is the Heaviside function, the integral becomes:\n\n\\[\nf_B = -\\frac{1}{k} \\sum_{l=0}^\\infty i^l (2l+1) (-V_0) \\int_0^a j_l(k r) r^2 dr \\times P_l(\\cos\\theta)\n\\]\n\nSimplify constants:\n\n\\[\nf_B = \\frac{V_0}{k} \\sum_{l=0}^\\infty i^l (2l+1) \\int_0^a j_l(k r) r^2 dr \\times P_l(\\cos\\theta)\n\\]\n\nAgain, focusing on low energy, so \\( ka \\) isn’t necessarily small. Maybe including several partial waves is necessary, but traditionally, the first Born approximation considers only the lowest order term, typically the s-wave (l=0). Alternatively, one might include p-wave contributions if they're not negligible.\n\nHowever, for simplicity, let's consider only the s-wave term. Then,\n\n\\[\nf_B^{(1)} = \\frac{V_0}{k} \\int_0^a j_0(k r) r^2 dr\n\\]\n\nBecause \\( j_0(x) = \\sin x /x \\), right? No, wait, \\( j_0(x) = \\sin x /x \\)?\n\nNo, correction: Spherical Bessel functions satisfy \\( j_0(x) = \\sin x / x \\). Yeah, correct.\n\nTherefore,\n\n\\[\nj_0(k r) = \\frac{\\sin(k r)}{k r},\n\\]\n\nthus,\n\n\\[\nf_B^{(1)} = \\frac{V_0}{k} \\int_0^a \\frac{\\sin(k r)}{k r} r^2 dr = \\frac{V_0}{k^2} \\int_0^a \\sin(k r) r dr\n\\]\n\nCompute the integral:\n\nLet me recall integration techniques. Integral of sin(ax)x dx is known. Integration by parts:\n\nLet u = x => du = dx\n\ndv = sin(ax)dx => v = -cos(ax)/a\n\nThus,\n\n∫x sin(ax) dx = -x cos(ax)/a + ∫cos(ax)/a dx = -x cos(ax)/a + sin(ax)/(a²)\n\nEvaluated from 0 to a:\n\nAt upper limit:\n- a cos(ka)/k + sin(ka)/k²\n\nLower limit:\n- 0 * ... + sin(0)/k² = 0\n\nHence,\n\nIntegral result: [ -a cos(ka)/k + sin(ka)/k² ]\n\nPutting back into f_B^(1):\n\n\\[\nf_B^{(1)} = \\frac{V_0}{k^2} \\left(-\\frac{a \\cos(ka)}{k} + \\frac{\\sin(ka)}{k^2}\\right ) = -\\frac{V_0 a \\cos(ka)}{k^3} + \\frac{V_0 \\sin(ka)}{k^4}\n\\]\n\nThat seems messy, but note that for low energies, \\( ka \\) is small. Thus, we can expand trigonometric functions in Taylor series around 0.\n\nRecall that:\n\n\\( \\cos(ka) ≈ 1 - (ka)^2/2 + O((ka)^4) \\)\n\nand\n\n\\( \\sin(ka) ≈ ka - (ka)^3/6 + O((ka)^5) \\)\n\nSubstituting these expansions into f_B^(1):\n\nFirst term:\n\n\\( -\\frac{V_0 a}{k^3}[1 - (ka)^2"}
